<!---
Copyright 2020 The HuggingFace Team. Todos os direitos reservados.

Licenciado sob a Licen√ßa Apache, Vers√£o 2.0 (a "Licen√ßa");
voc√™ n√£o pode usar este arquivo exceto em conformidade com a Licen√ßa.
Voc√™ pode obter uma c√≥pia da Licen√ßa em

    http://www.apache.org/licenses/LICENSE-2.0

A menos que exigido pela legisla√ß√£o aplic√°vel ou acordado por escrito, o software
distribu√≠do sob a Licen√ßa √© distribu√≠do "COMO EST√Å",
SEM GARANTIAS OU CONDI√á√ïES DE QUALQUER TIPO, expressas ou impl√≠citas.
Veja a Licen√ßa para a linguagem espec√≠fica que rege permiss√µes e
limita√ß√µes sob a Licen√ßa.
-->

<p align="center">
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-dark.svg">
    <source media="(prefers-color-scheme: light)" srcset="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg">
    <img alt="Hugging Face Transformers Library" src="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg" width="352" height="59" style="max-width: 100%;">
  </picture>
  <br/>
  <br/>
</p>

<p align="center">
    <a href="https://huggingface.com/models"><img alt="Checkpoints no Hub" src="https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen"></a>
    <a href="https://circleci.com/gh/huggingface/transformers"><img alt="Build" src="https://img.shields.io/circleci/build/github/huggingface/transformers/main"></a>
    <a href="https://github.com/huggingface/transformers/blob/main/LICENSE"><img alt="GitHub" src="https://img.shields.io/github/license/huggingface/transformers.svg?color=blue"></a>
    <a href="https://huggingface.co/docs/transformers/index"><img alt="Documenta√ß√£o" src="https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online"></a>
    <a href="https://github.com/huggingface/transformers/releases"><img alt="GitHub release" src="https://img.shields.io/github/release/huggingface/transformers.svg"></a>
    <a href="https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md"><img alt="Contributor Covenant" src="https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg"></a>
    <a href="https://zenodo.org/badge/latestdoi/155220641"><img src="https://zenodo.org/badge/155220641.svg" alt="DOI"></a>
</p>

<h4 align="center">
    <p>
        <b>English</b> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hans.md">ÁÆÄ‰Ωì‰∏≠Êñá</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hant.md">ÁπÅÈ´î‰∏≠Êñá</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ko.md">ÌïúÍµ≠Ïñ¥</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_es.md">Espa√±ol</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ja.md">Êó•Êú¨Ë™û</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_hd.md">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ru.md">–†—É—Å—Å–∫–∏–π</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_pt-br.md">–†ortugu√™s</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_te.md">‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_fr.md">Fran√ßais</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_de.md">Deutsch</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_vi.md">Ti·∫øng Vi·ªát</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ar.md">ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ur.md">ÿßÿ±ÿØŸà</a> |
    </p>
</h4>

<h3 align="center">
    <p>Modelos pr√©-treinados de √∫ltima gera√ß√£o para infer√™ncia e treinamento</p>
</h3>

<h3 align="center">
    <a href="https://hf.co/course"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/course_banner.png"></a>
</h3>

Transformers √© uma biblioteca de modelos pr√©-treinados de texto, vis√£o computacional, √°udio, v√≠deo e multimodais para infer√™ncia e treinamento. Use o Transformers para ajustar modelos aos seus dados, construir aplica√ß√µes de infer√™ncia e para casos de uso de IA generativa em m√∫ltiplas modalidades.

Existem mais de 500 mil [checkpoints de modelos](https://huggingface.co/models?library=transformers&sort=trending) do Transformers no [Hugging Face Hub](https://huggingface.com/models) que voc√™ pode utilizar.

Explore o [Hub](https://huggingface.com/) hoje para encontrar um modelo e usar o Transformers para come√ßar imediatamente.

## Instala√ß√£o

O Transformers funciona com Python 3.9+, [PyTorch](https://pytorch.org/get-started/locally/) 2.1+, [TensorFlow](https://www.tensorflow.org/install/pip) 2.6+ e [Flax](https://flax.readthedocs.io/en/latest/) 0.4.1+.

Crie e ative um ambiente virtual com [venv](https://docs.python.org/3/library/venv.html) ou [uv](https://docs.astral.sh/uv/), um r√°pido gerenciador de pacotes e projetos Python baseado em Rust.

```py
# venv
python -m venv .my-env
source .my-env/bin/activate
# uv
uv venv .my-env
source .my-env/bin/activate
```

Instale o Transformers no seu ambiente virtual.

```py
# pip
pip install "transformers[torch]"

# uv
uv pip install "transformers[torch]"
```

Instale o Transformers a partir do c√≥digo-fonte caso queira as √∫ltimas atualiza√ß√µes da biblioteca ou esteja interessado em contribuir. Entretanto, a vers√£o *mais recente* pode n√£o ser est√°vel. Sinta-se √† vontade para abrir um [issue](https://github.com/huggingface/transformers/issues) caso encontre algum erro.

```shell
git clone https://github.com/huggingface/transformers.git
cd transformers

# pip
pip install .[torch]

# uv
uv pip install .[torch]
```

## In√≠cio R√°pido

Comece a usar o Transformers imediatamente com a API [Pipeline](https://huggingface.co/docs/transformers/pipeline_tutorial). O `Pipeline` √© uma classe de infer√™ncia de alto n√≠vel que suporta tarefas de texto, √°udio, vis√£o e multimodais. Ele faz o pr√©-processamento da entrada e retorna a sa√≠da apropriada.

Instancie um pipeline e especifique o modelo a ser usado para gera√ß√£o de texto. O modelo √© baixado e cacheado para que voc√™ possa reutiliz√°-lo facilmente. Por fim, forne√ßa um texto para instruir o modelo.

```py
from transformers import pipeline

pipeline = pipeline(task="text-generation", model="Qwen/Qwen2.5-1.5B")
pipeline("the secret to baking a really good cake is ")
[{'generated_text': 'the secret to baking a really good cake is 1) to use the right ingredients and 2) to follow the recipe exactly. the recipe for the cake is as follows: 1 cup of sugar, 1 cup of flour, 1 cup of milk, 1 cup of butter, 1 cup of eggs, 1 cup of chocolate chips. if you want to make 2 cakes, how much sugar do you need? To make 2 cakes, you will need 2 cups of sugar.'}]
```

Para conversar com um modelo, o padr√£o de uso √© o mesmo. A √∫nica diferen√ßa √© que voc√™ precisa construir um hist√≥rico de conversa (a entrada para o `Pipeline`) entre voc√™ e o sistema.

> [!DICA]
> Voc√™ tamb√©m pode conversar com um modelo diretamente pela linha de comando.
> ```shell
> transformers chat Qwen/Qwen2.5-0.5B-Instruct
> ```

```py
import torch
from transformers import pipeline

chat = [
    {"role": "system", "content": "Voc√™ √© um rob√¥ irreverente e sarc√°stico, como imaginado por Hollywood em 1986."},
    {"role": "user", "content": "Ei, voc√™ pode me indicar coisas divertidas para fazer em Nova York?"}
]

pipeline = pipeline(task="text-generation", model="meta-llama/Meta-Llama-3-8B-Instruct", torch_dtype=torch.bfloat16, device_map="auto")
response = pipeline(chat, max_new_tokens=512)
print(response[0]["generated_text"][-1]["content"])
```

Expanda os exemplos abaixo para ver como o `Pipeline` funciona para diferentes modalidades e tarefas.

<details>
<summary>Reconhecimento autom√°tico de fala</summary>

```py
from transformers import pipeline

pipeline = pipeline(task="automatic-speech-recognition", model="openai/whisper-large-v3")
pipeline("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}
```

</details>

<details>
<summary>Classifica√ß√£o de imagens</summary>

<h3 align="center">
    <a><img src="https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png"></a>
</h3>

```py
from transformers import pipeline

pipeline = pipeline(task="image-classification", model="facebook/dinov2-small-imagenet1k-1-layer")
pipeline("https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png")
[{'label': 'macaw', 'score': 0.997848391532898},
 {'label': 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',
  'score': 0.0016551691805943847},
 {'label': 'lorikeet', 'score': 0.00018523589824326336},
 {'label': 'African grey, African gray, Psittacus erithacus',
  'score': 7.85409429227002e-05},
 {'label': 'quail', 'score': 5.502637941390276e-05}]
```

</details>

<details>
<summary>Resposta visual a perguntas</summary>


<h3 align="center">
    <a><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg"></a>
</h3>

```py
from transformers import pipeline

pipeline = pipeline(task="visual-question-answering", model="Salesforce/blip-vqa-base")
pipeline(
    image="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg",
    question="O que h√° na imagem?",
)
[{'answer': 'statue of liberty'}]
```

</details>

## Por que devo usar o Transformers?

1. Modelos de √∫ltima gera√ß√£o f√°ceis de usar:
    - Alto desempenho em tarefas de entendimento e gera√ß√£o de linguagem natural, vis√£o computacional, √°udio, v√≠deo e multimodais.
    - Baixa barreira de entrada para pesquisadores, engenheiros e desenvolvedores.
    - Poucas abstra√ß√µes voltadas para o usu√°rio, com apenas tr√™s classes para aprender.
    - Uma API unificada para usar todos os nossos modelos pr√©-treinados.

1. Reduza custos computacionais e pegada de carbono:
    - Compartilhe modelos treinados ao inv√©s de treinar do zero.
    - Reduza o tempo computacional e custos de produ√ß√£o.
    - Dezenas de arquiteturas de modelos com mais de 1 milh√£o de checkpoints pr√©-treinados em todas as modalidades.

1. Escolha o framework certo para cada etapa do ciclo de vida do modelo:
    - Treine modelos de √∫ltima gera√ß√£o em 3 linhas de c√≥digo.
    - Mova um √∫nico modelo entre os frameworks PyTorch/JAX/TF2.0 quando quiser.
    - Escolha o framework certo para treinamento, avalia√ß√£o e produ√ß√£o.

1. Personalize facilmente um modelo ou um exemplo para suas necessidades:
    - Fornecemos exemplos para cada arquitetura para reproduzir os resultados publicados pelos autores originais.
    - Os internos dos modelos s√£o expostos de forma t√£o consistente quanto poss√≠vel.
    - Os arquivos dos modelos podem ser usados independentemente da biblioteca para experimentos r√°pidos.

<a target="_blank" href="https://huggingface.co/enterprise">
    <img alt="Hugging Face Enterprise Hub" src="https://github.com/user-attachments/assets/247fb16d-d251-4583-96c4-d3d76dda4925">
</a><br>

## Por que n√£o devo usar o Transformers?

- Esta biblioteca n√£o √© uma caixa de ferramentas modular de blocos de constru√ß√£o para redes neurais. O c√≥digo nos arquivos de modelos n√£o foi refatorado com abstra√ß√µes adicionais de prop√≥sito, para que pesquisadores possam iterar rapidamente em cada modelo sem precisar lidar com abstra√ß√µes/arquivos extras.
- A API de treinamento √© otimizada para funcionar com modelos PyTorch fornecidos pelo Transformers. Para la√ßos gen√©ricos de machine learning, voc√™ deve usar outra biblioteca como a [Accelerate](https://huggingface.co/docs/accelerate).
- Os [scripts de exemplo]((https://github.com/huggingface/transformers/tree/main/examples)) s√£o apenas *exemplos*. Eles podem n√£o funcionar imediatamente para o seu caso de uso espec√≠fico e voc√™ ter√° que adaptar o c√≥digo.

## 100 projetos usando Transformers

Transformers √© mais do que uma caixa de ferramentas para usar modelos pr√©-treinados, √© uma comunidade de projetos constru√≠da ao redor dele e do
Hugging Face Hub. Queremos que o Transformers permita que desenvolvedores, pesquisadores, estudantes, professores, engenheiros e qualquer
outra pessoa construa os projetos dos seus sonhos.

Para comemorar as 100.000 estrelas do Transformers, queremos dar destaque √†
comunidade com a p√°gina [awesome-transformers](./awesome-transformers.md) que lista 100
projetos incr√≠veis constru√≠dos com Transformers.

Se voc√™ possui ou usa um projeto que acredita que deveria fazer parte da lista, por favor abra um PR para adicion√°-lo!

## Exemplos de modelos

Voc√™ pode testar a maioria dos nossos modelos diretamente nas suas [p√°ginas de modelos no Hub](https://huggingface.co/models).

Expanda cada modalidade abaixo para ver alguns exemplos de modelos para v√°rios casos de uso.

<details>
<summary>√Åudio</summary>

- Classifica√ß√£o de √°udio com [Whisper](https://huggingface.co/openai/whisper-large-v3-turbo)
- Reconhecimento autom√°tico de fala com [Moonshine](https://huggingface.co/UsefulSensors/moonshine)
- Detec√ß√£o de palavras-chave com [Wav2Vec2](https://huggingface.co/superb/wav2vec2-base-superb-ks)
- Gera√ß√£o de fala para fala com [Moshi](https://huggingface.co/kyutai/moshiko-pytorch-bf16)
- Texto para √°udio com [MusicGen](https://huggingface.co/facebook/musicgen-large)
- Texto para fala com [Bark](https://huggingface.co/suno/bark)

</details>

<details>
<summary>Vis√£o computacional</summary>

- Gera√ß√£o autom√°tica de m√°scaras com [SAM](https://huggingface.co/facebook/sam-vit-base)
- Estimativa de profundidade com [DepthPro](https://huggingface.co/apple/DepthPro-hf)
- Classifica√ß√£o de imagens com [DINO v2](https://huggingface.co/facebook/dinov2-base)
- Detec√ß√£o de pontos chave com [SuperGlue](https://huggingface.co/magic-leap-community/superglue_outdoor)
- Correspond√™ncia de pontos chave com [SuperGlue](https://huggingface.co/magic-leap-community/superglue)
- Detec√ß√£o de objetos com [RT-DETRv2](https://huggingface.co/PekingU/rtdetr_v2_r50vd)
- Estimativa de pose com [VitPose](https://huggingface.co/usyd-community/vitpose-base-simple)
- Segmenta√ß√£o universal com [OneFormer](https://huggingface.co/shi-labs/oneformer_ade20k_swin_large)
- Classifica√ß√£o de v√≠deo com [VideoMAE](https://huggingface.co/MCG-NJU/videomae-large)

</details>

<details>
<summary>Multimodal</summary>

- √Åudio ou texto para texto com [Qwen2-Audio](https://huggingface.co/Qwen/Qwen2-Audio-7B)
- Perguntas e respostas em documentos com [LayoutLMv3](https://huggingface.co/microsoft/layoutlmv3-base)
- Imagem ou texto para texto com [Qwen-VL](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct)
- Legenda de imagens com [BLIP-2](https://huggingface.co/Salesforce/blip2-opt-2.7b)
- Compreens√£o de documentos baseada em OCR com [GOT-OCR2](https://huggingface.co/stepfun-ai/GOT-OCR-2.0-hf)
- Perguntas e respostas em tabelas com [TAPAS](https://huggingface.co/google/tapas-base)
- Compreens√£o e gera√ß√£o multimodal unificada com [Emu3](https://huggingface.co/BAAI/Emu3-Gen)
- Vis√£o para texto com [Llava-OneVision](https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf)
- Perguntas visuais e respostas com [Llava](https://huggingface.co/llava-hf/llava-1.5-7b-hf)
- Segmenta√ß√£o de express√µes referenciais visuais com [Kosmos-2](https://huggingface.co/microsoft/kosmos-2-patch14-224)

</details>

<details>
<summary>PLN (Processamento de Linguagem Natural)</summary>

- Preenchimento de palavra mascarada com [ModernBERT](https://huggingface.co/answerdotai/ModernBERT-base)
- Reconhecimento de entidade nomeada com [Gemma](https://huggingface.co/google/gemma-2-2b)
- Perguntas e respostas com [Mixtral](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)
- Resumiriza√ß√£o com [BART](https://huggingface.co/facebook/bart-large-cnn)
- Tradu√ß√£o com [T5](https://huggingface.co/google-t5/t5-base)
- Gera√ß√£o de texto com [Llama](https://huggingface.co/meta-llama/Llama-3.2-1B)
- Classifica√ß√£o de texto com [Qwen](https://huggingface.co/Qwen/Qwen2.5-0.5B)

</details>

## Cita√ß√£o

Agora temos um [artigo](https://www.aclweb.org/anthology/2020.emnlp-demos.6/) que voc√™ pode citar para a biblioteca ü§ó Transformers:
```bibtex
@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R√©mi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}
```

---

[Powered By OpenAiTx](https://github.com/OpenAiTx/OpenAiTx)

---