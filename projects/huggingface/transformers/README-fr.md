<!---
Copyright 2020 The HuggingFace Team. Tous droits r√©serv√©s.

Sous licence Apache License, Version 2.0 (la "Licence");
vous ne pouvez pas utiliser ce fichier sauf en conformit√© avec la Licence.
Vous pouvez obtenir une copie de la Licence √† l'adresse suivante :

    http://www.apache.org/licenses/LICENSE-2.0

Sauf si la loi l'exige ou sauf accord √©crit, le logiciel distribu√© sous la Licence est distribu√© "EN L'√âTAT",
SANS GARANTIE OU CONDITION D'AUCUNE SORTE, expresse ou implicite.
Voir la Licence pour conna√Ætre la langue sp√©cifique r√©gissant les permissions et limitations sous la Licence.
-->

<p align="center">
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-dark.svg">
    <source media="(prefers-color-scheme: light)" srcset="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg">
    <img alt="Biblioth√®que Hugging Face Transformers" src="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg" width="352" height="59" style="max-width: 100%;">
  </picture>
  <br/>
  <br/>
</p>

<p align="center">
    <a href="https://huggingface.com/models"><img alt="Points de contr√¥le sur le Hub" src="https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen"></a>
    <a href="https://circleci.com/gh/huggingface/transformers"><img alt="Build" src="https://img.shields.io/circleci/build/github/huggingface/transformers/main"></a>
    <a href="https://github.com/huggingface/transformers/blob/main/LICENSE"><img alt="GitHub" src="https://img.shields.io/github/license/huggingface/transformers.svg?color=blue"></a>
    <a href="https://huggingface.co/docs/transformers/index"><img alt="Documentation" src="https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online"></a>
    <a href="https://github.com/huggingface/transformers/releases"><img alt="GitHub release" src="https://img.shields.io/github/release/huggingface/transformers.svg"></a>
    <a href="https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md"><img alt="Contributor Covenant" src="https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg"></a>
    <a href="https://zenodo.org/badge/latestdoi/155220641"><img src="https://zenodo.org/badge/155220641.svg" alt="DOI"></a>
</p>

<h4 align="center">
    <p>
        <b>English</b> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hans.md">ÁÆÄ‰Ωì‰∏≠Êñá</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hant.md">ÁπÅÈ´î‰∏≠Êñá</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ko.md">ÌïúÍµ≠Ïñ¥</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_es.md">Espa√±ol</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ja.md">Êó•Êú¨Ë™û</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_hd.md">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ru.md">–†—É—Å—Å–∫–∏–π</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_pt-br.md">–†ortugu√™s</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_te.md">‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_fr.md">Fran√ßais</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_de.md">Deutsch</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_vi.md">Ti·∫øng Vi·ªát</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ar.md">ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ur.md">ÿßÿ±ÿØŸà</a> |
    </p>
</h4>

<h3 align="center">
    <p>Mod√®les pr√©entra√Æn√©s √† la pointe de la technologie pour l‚Äôinf√©rence et l‚Äôentra√Ænement</p>
</h3>

<h3 align="center">
    <a href="https://hf.co/course"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/course_banner.png"></a>
</h3>

Transformers est une biblioth√®que de mod√®les pr√©entra√Æn√©s pour le texte, la vision par ordinateur, l‚Äôaudio, la vid√©o et le multimodal, d√©di√©e √† l‚Äôinf√©rence et √† l‚Äôentra√Ænement. Utilisez Transformers pour ajuster les mod√®les sur vos donn√©es, construire des applications d‚Äôinf√©rence et pour des cas d‚Äôusage d‚ÄôIA g√©n√©rative sur de multiples modalit√©s.

Il existe plus de 500 000 [points de contr√¥le de mod√®les](https://huggingface.co/models?library=transformers&sort=trending) Transformers sur le [Hub Hugging Face](https://huggingface.com/models) que vous pouvez utiliser.

Explorez d√®s aujourd‚Äôhui le [Hub](https://huggingface.com/) pour trouver un mod√®le et utiliser Transformers pour d√©marrer imm√©diatement.

## Installation

Transformers fonctionne avec Python 3.9+, [PyTorch](https://pytorch.org/get-started/locally/) 2.1+, [TensorFlow](https://www.tensorflow.org/install/pip) 2.6+ et [Flax](https://flax.readthedocs.io/en/latest/) 0.4.1+.

Cr√©ez et activez un environnement virtuel avec [venv](https://docs.python.org/3/library/venv.html) ou [uv](https://docs.astral.sh/uv/), un gestionnaire de paquets et de projets Python rapide bas√© sur Rust.

```py
# venv
python -m venv .my-env
source .my-env/bin/activate
# uv
uv venv .my-env
source .my-env/bin/activate
```

Installez Transformers dans votre environnement virtuel.

```py
# pip
pip install "transformers[torch]"

# uv
uv pip install "transformers[torch]"
```

Installez Transformers √† partir de la source si vous souhaitez b√©n√©ficier des derni√®res modifications de la biblioth√®que ou contribuer au projet. Cependant, la version *la plus r√©cente* peut ne pas √™tre stable. N‚Äôh√©sitez pas √† ouvrir une [issue](https://github.com/huggingface/transformers/issues) si vous rencontrez une erreur.

```shell
git clone https://github.com/huggingface/transformers.git
cd transformers

# pip
pip install .[torch]

# uv
uv pip install .[torch]
```

## D√©marrage rapide

Commencez avec Transformers d√®s maintenant avec l‚ÄôAPI [Pipeline](https://huggingface.co/docs/transformers/pipeline_tutorial). Le `Pipeline` est une classe d‚Äôinf√©rence de haut niveau prenant en charge les t√¢ches de texte, d‚Äôaudio, de vision et multimodales. Il g√®re le pr√©traitement de l‚Äôentr√©e et retourne la sortie appropri√©e.

Instanciez un pipeline et sp√©cifiez le mod√®le √† utiliser pour la g√©n√©ration de texte. Le mod√®le est t√©l√©charg√© et mis en cache, ce qui vous permet de le r√©utiliser facilement. Enfin, fournissez un texte pour amorcer le mod√®le.

```py
from transformers import pipeline

pipeline = pipeline(task="text-generation", model="Qwen/Qwen2.5-1.5B")
pipeline("the secret to baking a really good cake is ")
[{'generated_text': 'the secret to baking a really good cake is 1) to use the right ingredients and 2) to follow the recipe exactly. the recipe for the cake is as follows: 1 cup of sugar, 1 cup of flour, 1 cup of milk, 1 cup of butter, 1 cup of eggs, 1 cup of chocolate chips. if you want to make 2 cakes, how much sugar do you need? To make 2 cakes, you will need 2 cups of sugar.'}]
```

Pour discuter avec un mod√®le, le sch√©ma d‚Äôutilisation est le m√™me. La seule diff√©rence est que vous devez construire un historique de conversation (l‚Äôentr√©e de `Pipeline`) entre vous et le syst√®me.

> [!TIP]
> Vous pouvez √©galement discuter avec un mod√®le directement depuis la ligne de commande.
> ```shell
> transformers chat Qwen/Qwen2.5-0.5B-Instruct
> ```

```py
import torch
from transformers import pipeline

chat = [
    {"role": "system", "content": "Vous √™tes un robot impertinent et plein d'esprit, tel qu‚Äôimagin√© par Hollywood vers 1986."},
    {"role": "user", "content": "Salut, peux-tu me dire des choses amusantes √† faire √† New York ?"}
]

pipeline = pipeline(task="text-generation", model="meta-llama/Meta-Llama-3-8B-Instruct", torch_dtype=torch.bfloat16, device_map="auto")
response = pipeline(chat, max_new_tokens=512)
print(response[0]["generated_text"][-1]["content"])
```

D√©veloppez les exemples ci-dessous pour voir comment `Pipeline` fonctionne pour diff√©rentes modalit√©s et t√¢ches.

<details>
<summary>Reconnaissance automatique de la parole</summary>

```py
from transformers import pipeline

pipeline = pipeline(task="automatic-speech-recognition", model="openai/whisper-large-v3")
pipeline("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}
```

</details>

<details>
<summary>Classification d'images</summary>

<h3 align="center">
    <a><img src="https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png"></a>
</h3>

```py
from transformers import pipeline

pipeline = pipeline(task="image-classification", model="facebook/dinov2-small-imagenet1k-1-layer")
pipeline("https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png")
[{'label': 'macaw', 'score': 0.997848391532898},
 {'label': 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',
  'score': 0.0016551691805943847},
 {'label': 'lorikeet', 'score': 0.00018523589824326336},
 {'label': 'African grey, African gray, Psittacus erithacus',
  'score': 7.85409429227002e-05},
 {'label': 'quail', 'score': 5.502637941390276e-05}]
```

</details>

<details>
<summary>Questions-r√©ponses visuelles</summary>


<h3 align="center">
    <a><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg"></a>
</h3>

```py
from transformers import pipeline

pipeline = pipeline(task="visual-question-answering", model="Salesforce/blip-vqa-base")
pipeline(
    image="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg",
    question="What is in the image?",
)
[{'answer': 'statue of liberty'}]
```

</details>

## Pourquoi utiliser Transformers ?

1. Des mod√®les √† la pointe de la technologie faciles √† utiliser :
    - Hautes performances en compr√©hension et g√©n√©ration du langage naturel, vision par ordinateur, audio, vid√©o et t√¢ches multimodales.
    - Faible barri√®re √† l‚Äôentr√©e pour les chercheurs, ing√©nieurs et d√©veloppeurs.
    - Peu d‚Äôabstractions c√¥t√© utilisateur, seulement trois classes √† apprendre.
    - Une API unifi√©e pour utiliser tous nos mod√®les pr√©entra√Æn√©s.

1. R√©duction des co√ªts de calcul et de l‚Äôempreinte carbone :
    - Partagez des mod√®les entra√Æn√©s au lieu d‚Äôentra√Æner depuis z√©ro.
    - R√©duisez le temps de calcul et les co√ªts de production.
    - Des dizaines d‚Äôarchitectures de mod√®les avec plus de 1M de points de contr√¥le pr√©entra√Æn√©s toutes modalit√©s confondues.

1. Choisissez le bon framework √† chaque √©tape du cycle de vie d‚Äôun mod√®le :
    - Entra√Ænez des mod√®les √† la pointe de la technologie en 3 lignes de code.
    - D√©placez un mod√®le unique entre les frameworks PyTorch/JAX/TF2.0 √† volont√©.
    - Choisissez le bon framework pour l‚Äôentra√Ænement, l‚Äô√©valuation et la production.

1. Personnalisez facilement un mod√®le ou un exemple selon vos besoins :
    - Nous fournissons des exemples pour chaque architecture afin de reproduire les r√©sultats publi√©s par leurs auteurs d‚Äôorigine.
    - Les internals des mod√®les sont expos√©s aussi uniform√©ment que possible.
    - Les fichiers de mod√®les peuvent √™tre utilis√©s ind√©pendamment de la biblioth√®que pour des exp√©rimentations rapides.

<a target="_blank" href="https://huggingface.co/enterprise">
    <img alt="Hugging Face Enterprise Hub" src="https://github.com/user-attachments/assets/247fb16d-d251-4583-96c4-d3d76dda4925">
</a><br>

## Pourquoi ne pas utiliser Transformers ?

- Cette biblioth√®que n‚Äôest pas une bo√Æte √† outils modulaire de briques de construction pour les r√©seaux neuronaux. Le code des fichiers de mod√®les n‚Äôest pas refactoris√© avec des abstractions suppl√©mentaires, afin que les chercheurs puissent it√©rer rapidement sur chaque mod√®le sans avoir √† naviguer dans des abstractions/fichiers suppl√©mentaires.
- L‚ÄôAPI d‚Äôentra√Ænement est optimis√©e pour fonctionner avec les mod√®les PyTorch fournis par Transformers. Pour des boucles de machine learning g√©n√©riques, vous devriez utiliser une autre biblioth√®que comme [Accelerate](https://huggingface.co/docs/accelerate).
- Les [scripts d‚Äôexemple]((https://github.com/huggingface/transformers/tree/main/examples)) sont seulement *des exemples*. Ils peuvent ne pas fonctionner imm√©diatement pour votre cas d‚Äôusage sp√©cifique et vous devrez adapter le code pour qu‚Äôil fonctionne.

## 100 projets utilisant Transformers

Transformers est plus qu‚Äôun ensemble d‚Äôoutils pour utiliser des mod√®les pr√©entra√Æn√©s, c‚Äôest une communaut√© de projets construits autour de lui et du Hub Hugging Face. Nous voulons que Transformers permette aux d√©veloppeurs, chercheurs, √©tudiants, professeurs, ing√©nieurs et toute autre personne de r√©aliser leurs projets de r√™ve.

Pour c√©l√©brer les 100 000 √©toiles de Transformers, nous avons voulu mettre en avant la communaut√© avec la page [awesome-transformers](./awesome-transformers.md) qui recense 100 projets incroyables construits avec Transformers.

Si vous poss√©dez ou utilisez un projet qui, selon vous, devrait faire partie de la liste, n‚Äôh√©sitez pas √† ouvrir une PR pour l‚Äôajouter !

## Exemples de mod√®les

Vous pouvez tester la plupart de nos mod√®les directement sur leurs [pages mod√®les du Hub](https://huggingface.co/models).

D√©veloppez chaque modalit√© ci-dessous pour voir quelques exemples de mod√®les pour diff√©rents cas d‚Äôutilisation.

<details>
<summary>Audio</summary>

- Classification audio avec [Whisper](https://huggingface.co/openai/whisper-large-v3-turbo)
- Reconnaissance automatique de la parole avec [Moonshine](https://huggingface.co/UsefulSensors/moonshine)
- D√©tection de mots-cl√©s avec [Wav2Vec2](https://huggingface.co/superb/wav2vec2-base-superb-ks)
- G√©n√©ration parole √† parole avec [Moshi](https://huggingface.co/kyutai/moshiko-pytorch-bf16)
- Texte vers audio avec [MusicGen](https://huggingface.co/facebook/musicgen-large)
- Texte vers parole avec [Bark](https://huggingface.co/suno/bark)

</details>

<details>
<summary>Vision par ordinateur</summary>

- G√©n√©ration automatique de masque avec [SAM](https://huggingface.co/facebook/sam-vit-base)
- Estimation de profondeur avec [DepthPro](https://huggingface.co/apple/DepthPro-hf)
- Classification d‚Äôimages avec [DINO v2](https://huggingface.co/facebook/dinov2-base)
- D√©tection de points cl√©s avec [SuperGlue](https://huggingface.co/magic-leap-community/superglue_outdoor)
- Appariement de points cl√©s avec [SuperGlue](https://huggingface.co/magic-leap-community/superglue)
- D√©tection d‚Äôobjets avec [RT-DETRv2](https://huggingface.co/PekingU/rtdetr_v2_r50vd)
- Estimation de pose avec [VitPose](https://huggingface.co/usyd-community/vitpose-base-simple)
- Segmentation universelle avec [OneFormer](https://huggingface.co/shi-labs/oneformer_ade20k_swin_large)
- Classification vid√©o avec [VideoMAE](https://huggingface.co/MCG-NJU/videomae-large)

</details>

<details>
<summary>Multimodal</summary>

- Audio ou texte vers texte avec [Qwen2-Audio](https://huggingface.co/Qwen/Qwen2-Audio-7B)
- Questions-r√©ponses sur documents avec [LayoutLMv3](https://huggingface.co/microsoft/layoutlmv3-base)
- Image ou texte vers texte avec [Qwen-VL](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct)
- G√©n√©ration de l√©gendes d‚Äôimages avec [BLIP-2](https://huggingface.co/Salesforce/blip2-opt-2.7b)
- Compr√©hension de documents bas√©e OCR avec [GOT-OCR2](https://huggingface.co/stepfun-ai/GOT-OCR-2.0-hf)
- Questions-r√©ponses sur tableaux avec [TAPAS](https://huggingface.co/google/tapas-base)
- Compr√©hension et g√©n√©ration multimodale unifi√©e avec [Emu3](https://huggingface.co/BAAI/Emu3-Gen)
- Vision vers texte avec [Llava-OneVision](https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf)
- Questions-r√©ponses visuelles avec [Llava](https://huggingface.co/llava-hf/llava-1.5-7b-hf)
- Segmentation d‚Äôexpression r√©f√©rentielle visuelle avec [Kosmos-2](https://huggingface.co/microsoft/kosmos-2-patch14-224)

</details>

<details>
<summary>TALN</summary>

- Compl√©tion de mots masqu√©s avec [ModernBERT](https://huggingface.co/answerdotai/ModernBERT-base)
- Reconnaissance d‚Äôentit√©s nomm√©es avec [Gemma](https://huggingface.co/google/gemma-2-2b)
- Questions-r√©ponses avec [Mixtral](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)
- R√©sum√© avec [BART](https://huggingface.co/facebook/bart-large-cnn)
- Traduction avec [T5](https://huggingface.co/google-t5/t5-base)
- G√©n√©ration de texte avec [Llama](https://huggingface.co/meta-llama/Llama-3.2-1B)
- Classification de texte avec [Qwen](https://huggingface.co/Qwen/Qwen2.5-0.5B)

</details>

## Citation

Nous avons d√©sormais un [article](https://www.aclweb.org/anthology/2020.emnlp-demos.6/) que vous pouvez citer pour la biblioth√®que ü§ó Transformers :
```bibtex
@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R√©mi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}
```


---

[Powered By OpenAiTx](https://github.com/OpenAiTx/OpenAiTx)

---