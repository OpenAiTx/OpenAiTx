<!---
Copyright 2020 The HuggingFace Team. Todos los derechos reservados.

Licenciado bajo la Licencia Apache, Versi√≥n 2.0 (la "Licencia");
no puede usar este archivo excepto en cumplimiento con la Licencia.
Puede obtener una copia de la Licencia en

    http://www.apache.org/licenses/LICENSE-2.0

A menos que la ley aplicable lo requiera o se acuerde por escrito, el software
distribuido bajo la Licencia se distribuye "TAL CUAL",
SIN GARANT√çAS NI CONDICIONES DE NING√öN TIPO, ya sean expresas o impl√≠citas.
Consulte la Licencia para conocer el lenguaje espec√≠fico que rige los permisos y
limitaciones bajo la Licencia.
-->

<p align="center">
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-dark.svg">
    <source media="(prefers-color-scheme: light)" srcset="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg">
    <img alt="Hugging Face Transformers Library" src="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg" width="352" height="59" style="max-width: 100%;">
  </picture>
  <br/>
  <br/>
</p>

<p align="center">
    <a href="https://huggingface.com/models"><img alt="Checkpoints en Hub" src="https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen"></a>
    <a href="https://circleci.com/gh/huggingface/transformers"><img alt="Build" src="https://img.shields.io/circleci/build/github/huggingface/transformers/main"></a>
    <a href="https://github.com/huggingface/transformers/blob/main/LICENSE"><img alt="GitHub" src="https://img.shields.io/github/license/huggingface/transformers.svg?color=blue"></a>
    <a href="https://huggingface.co/docs/transformers/index"><img alt="Documentaci√≥n" src="https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online"></a>
    <a href="https://github.com/huggingface/transformers/releases"><img alt="GitHub release" src="https://img.shields.io/github/release/huggingface/transformers.svg"></a>
    <a href="https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md"><img alt="Contributor Covenant" src="https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg"></a>
    <a href="https://zenodo.org/badge/latestdoi/155220641"><img src="https://zenodo.org/badge/155220641.svg" alt="DOI"></a>
</p>

<h4 align="center">
    <p>
        <b>English</b> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hans.md">ÁÆÄ‰Ωì‰∏≠Êñá</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hant.md">ÁπÅÈ´î‰∏≠Êñá</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ko.md">ÌïúÍµ≠Ïñ¥</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_es.md">Espa√±ol</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ja.md">Êó•Êú¨Ë™û</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_hd.md">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ru.md">–†—É—Å—Å–∫–∏–π</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_pt-br.md">–†ortugu√™s</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_te.md">‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_fr.md">Fran√ßais</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_de.md">Deutsch</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_vi.md">Ti·∫øng Vi·ªát</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ar.md">ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ur.md">ÿßÿ±ÿØŸà</a> |
    </p>
</h4>

<h3 align="center">
    <p>Modelos preentrenados de √∫ltima generaci√≥n para inferencia y entrenamiento</p>
</h3>

<h3 align="center">
    <a href="https://hf.co/course"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/course_banner.png"></a>
</h3>

Transformers es una biblioteca de modelos preentrenados para texto, visi√≥n por computadora, audio, video y modelos multimodales para inferencia y entrenamiento. Utiliza Transformers para ajustar modelos con tus datos, crear aplicaciones de inferencia y para casos de uso de IA generativa en m√∫ltiples modalidades.

Hay m√°s de 500K+ [checkpoints de modelos](https://huggingface.co/models?library=transformers&sort=trending) de Transformers en el [Hugging Face Hub](https://huggingface.com/models) que puedes utilizar.

Explora el [Hub](https://huggingface.com/) hoy mismo para encontrar un modelo y utiliza Transformers para ayudarte a comenzar de inmediato.

## Instalaci√≥n

Transformers funciona con Python 3.9+ [PyTorch](https://pytorch.org/get-started/locally/) 2.1+, [TensorFlow](https://www.tensorflow.org/install/pip) 2.6+, y [Flax](https://flax.readthedocs.io/en/latest/) 0.4.1+.

Crea y activa un entorno virtual con [venv](https://docs.python.org/3/library/venv.html) o [uv](https://docs.astral.sh/uv/), un gestor de paquetes y proyectos de Python r√°pido basado en Rust.

```py
# venv
python -m venv .my-env
source .my-env/bin/activate
# uv
uv venv .my-env
source .my-env/bin/activate
```

Instala Transformers en tu entorno virtual.

```py
# pip
pip install "transformers[torch]"

# uv
uv pip install "transformers[torch]"
```

Instala Transformers desde el c√≥digo fuente si deseas los √∫ltimos cambios en la biblioteca o si est√°s interesado en contribuir. Sin embargo, la versi√≥n *m√°s reciente* puede no ser estable. Si encuentras alg√∫n error, no dudes en abrir un [issue](https://github.com/huggingface/transformers/issues).

```shell
git clone https://github.com/huggingface/transformers.git
cd transformers

# pip
pip install .[torch]

# uv
uv pip install .[torch]
```

## Inicio r√°pido

Comienza a usar Transformers de inmediato con la API [Pipeline](https://huggingface.co/docs/transformers/pipeline_tutorial). `Pipeline` es una clase de inferencia de alto nivel que soporta tareas de texto, audio, visi√≥n y multimodales. Se encarga del preprocesamiento de la entrada y devuelve la salida apropiada.

Instancia un pipeline y especifica el modelo a usar para generaci√≥n de texto. El modelo se descarga y almacena en cach√© para que puedas reutilizarlo f√°cilmente. Finalmente, pasa un texto para solicitar al modelo.

```py
from transformers import pipeline

pipeline = pipeline(task="text-generation", model="Qwen/Qwen2.5-1.5B")
pipeline("el secreto para hornear un pastel realmente bueno es ")
[{'generated_text': 'el secreto para hornear un pastel realmente bueno es 1) usar los ingredientes correctos y 2) seguir la receta exactamente. La receta para el pastel es la siguiente: 1 taza de az√∫car, 1 taza de harina, 1 taza de leche, 1 taza de mantequilla, 1 taza de huevos, 1 taza de chispas de chocolate. Si deseas hacer 2 pasteles, ¬øcu√°nta az√∫car necesitas? Para hacer 2 pasteles, necesitar√°s 2 tazas de az√∫car.'}]
```

Para chatear con un modelo, el patr√≥n de uso es el mismo. La √∫nica diferencia es que necesitas construir un historial de chat (la entrada para `Pipeline`) entre t√∫ y el sistema.

> [!TIP]
> Tambi√©n puedes chatear con un modelo directamente desde la l√≠nea de comandos.
> ```shell
> transformers chat Qwen/Qwen2.5-0.5B-Instruct
> ```

```py
import torch
from transformers import pipeline

chat = [
    {"role": "system", "content": "Eres un robot descarado y sarc√°stico como imaginado por Hollywood en 1986."},
    {"role": "user", "content": "Oye, ¬øpuedes decirme cosas divertidas para hacer en Nueva York?"}
]

pipeline = pipeline(task="text-generation", model="meta-llama/Meta-Llama-3-8B-Instruct", torch_dtype=torch.bfloat16, device_map="auto")
response = pipeline(chat, max_new_tokens=512)
print(response[0]["generated_text"][-1]["content"])
```

Expande los ejemplos a continuaci√≥n para ver c√≥mo funciona `Pipeline` para diferentes modalidades y tareas.

<details>
<summary>Reconocimiento autom√°tico de voz</summary>

```py
from transformers import pipeline

pipeline = pipeline(task="automatic-speech-recognition", model="openai/whisper-large-v3")
pipeline("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}
```

</details>

<details>
<summary>Clasificaci√≥n de im√°genes</summary>

<h3 align="center">
    <a><img src="https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png"></a>
</h3>

```py
from transformers import pipeline

pipeline = pipeline(task="image-classification", model="facebook/dinov2-small-imagenet1k-1-layer")
pipeline("https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png")
[{'label': 'macaw', 'score': 0.997848391532898},
 {'label': 'cacat√∫a de cresta azufrada, Kakatoe galerita, Cacatua galerita',
  'score': 0.0016551691805943847},
 {'label': 'lorito', 'score': 0.00018523589824326336},
 {'label': 'loro gris africano, Psittacus erithacus',
  'score': 7.85409429227002e-05},
 {'label': 'codorniz', 'score': 5.502637941390276e-05}]
```

</details>

<details>
<summary>Respuesta visual a preguntas</summary>


<h3 align="center">
    <a><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg"></a>
</h3>

```py
from transformers import pipeline

pipeline = pipeline(task="visual-question-answering", model="Salesforce/blip-vqa-base")
pipeline(
    image="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg",
    question="¬øQu√© hay en la imagen?",
)
[{'answer': 'estatua de la libertad'}]
```

</details>

## ¬øPor qu√© deber√≠a usar Transformers?

1. Modelos de √∫ltima generaci√≥n f√°ciles de usar:
    - Alto rendimiento en comprensi√≥n y generaci√≥n de lenguaje natural, visi√≥n por computadora, audio, video y tareas multimodales.
    - Baja barrera de entrada para investigadores, ingenieros y desarrolladores.
    - Pocas abstracciones orientadas al usuario con solo tres clases por aprender.
    - Una API unificada para usar todos nuestros modelos preentrenados.

1. Menor costo de c√≥mputo, menor huella de carbono:
    - Comparte modelos entrenados en lugar de entrenar desde cero.
    - Reduce el tiempo de c√≥mputo y los costos de producci√≥n.
    - Docenas de arquitecturas de modelos con m√°s de 1M+ checkpoints preentrenados en todas las modalidades.

1. Elige el framework adecuado para cada etapa del ciclo de vida del modelo:
    - Entrena modelos de √∫ltima generaci√≥n en 3 l√≠neas de c√≥digo.
    - Mueve un solo modelo entre frameworks PyTorch/JAX/TF2.0 a voluntad.
    - Elige el framework adecuado para entrenamiento, evaluaci√≥n y producci√≥n.

1. Personaliza f√°cilmente un modelo o un ejemplo seg√∫n tus necesidades:
    - Proporcionamos ejemplos para cada arquitectura para reproducir los resultados publicados por sus autores originales.
    - Los internos del modelo est√°n expuestos de la manera m√°s consistente posible.
    - Los archivos del modelo pueden usarse independientemente de la biblioteca para experimentos r√°pidos.

<a target="_blank" href="https://huggingface.co/enterprise">
    <img alt="Hugging Face Enterprise Hub" src="https://github.com/user-attachments/assets/247fb16d-d251-4583-96c4-d3d76dda4925">
</a><br>

## ¬øPor qu√© no deber√≠a usar Transformers?

- Esta biblioteca no es una caja de herramientas modular de bloques de construcci√≥n para redes neuronales. El c√≥digo en los archivos de modelo no est√° refactorizado con abstracciones adicionales a prop√≥sito, para que los investigadores puedan iterar r√°pidamente en cada uno de los modelos sin sumergirse en archivos/abstracciones adicionales.
- La API de entrenamiento est√° optimizada para trabajar con modelos PyTorch proporcionados por Transformers. Para bucles gen√©ricos de machine learning, deber√≠as usar otra biblioteca como [Accelerate](https://huggingface.co/docs/accelerate).
- Los [scripts de ejemplo]((https://github.com/huggingface/transformers/tree/main/examples)) son solo *ejemplos*. Puede que no funcionen directamente en tu caso de uso espec√≠fico y necesitar√°s adaptar el c√≥digo para que funcione.

## 100 proyectos usando Transformers

Transformers es m√°s que una caja de herramientas para usar modelos preentrenados, es una comunidad de proyectos construidos alrededor de √©l y del Hugging Face Hub. Queremos que Transformers permita a desarrolladores, investigadores, estudiantes, profesores, ingenieros y a cualquier otra persona construir los proyectos de sus sue√±os.

Para celebrar los 100,000 estrellas de Transformers, quisimos resaltar a la comunidad con la p√°gina [awesome-transformers](./awesome-transformers.md) que lista 100 proyectos incre√≠bles construidos con Transformers.

Si tienes o usas un proyecto que crees que deber√≠a ser parte de la lista, ¬°por favor abre un PR para agregarlo!

## Modelos de ejemplo

Puedes probar la mayor√≠a de nuestros modelos directamente en sus [p√°ginas de modelos en el Hub](https://huggingface.co/models).

Expande cada modalidad a continuaci√≥n para ver algunos modelos de ejemplo para varios casos de uso.

<details>
<summary>Audio</summary>

- Clasificaci√≥n de audio con [Whisper](https://huggingface.co/openai/whisper-large-v3-turbo)
- Reconocimiento autom√°tico de voz con [Moonshine](https://huggingface.co/UsefulSensors/moonshine)
- Detecci√≥n de palabras clave con [Wav2Vec2](https://huggingface.co/superb/wav2vec2-base-superb-ks)
- Generaci√≥n de voz a voz con [Moshi](https://huggingface.co/kyutai/moshiko-pytorch-bf16)
- Texto a audio con [MusicGen](https://huggingface.co/facebook/musicgen-large)
- Texto a voz con [Bark](https://huggingface.co/suno/bark)

</details>

<details>
<summary>Visi√≥n por computadora</summary>

- Generaci√≥n autom√°tica de m√°scaras con [SAM](https://huggingface.co/facebook/sam-vit-base)
- Estimaci√≥n de profundidad con [DepthPro](https://huggingface.co/apple/DepthPro-hf)
- Clasificaci√≥n de im√°genes con [DINO v2](https://huggingface.co/facebook/dinov2-base)
- Detecci√≥n de puntos clave con [SuperGlue](https://huggingface.co/magic-leap-community/superglue_outdoor)
- Emparejamiento de puntos clave con [SuperGlue](https://huggingface.co/magic-leap-community/superglue)
- Detecci√≥n de objetos con [RT-DETRv2](https://huggingface.co/PekingU/rtdetr_v2_r50vd)
- Estimaci√≥n de pose con [VitPose](https://huggingface.co/usyd-community/vitpose-base-simple)
- Segmentaci√≥n universal con [OneFormer](https://huggingface.co/shi-labs/oneformer_ade20k_swin_large)
- Clasificaci√≥n de video con [VideoMAE](https://huggingface.co/MCG-NJU/videomae-large)

</details>

<details>
<summary>Multimodal</summary>

- Audio o texto a texto con [Qwen2-Audio](https://huggingface.co/Qwen/Qwen2-Audio-7B)
- Respuesta a preguntas en documentos con [LayoutLMv3](https://huggingface.co/microsoft/layoutlmv3-base)
- Imagen o texto a texto con [Qwen-VL](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct)
- Descripci√≥n de im√°genes con [BLIP-2](https://huggingface.co/Salesforce/blip2-opt-2.7b)
- Comprensi√≥n de documentos basada en OCR con [GOT-OCR2](https://huggingface.co/stepfun-ai/GOT-OCR-2.0-hf)
- Respuesta a preguntas en tablas con [TAPAS](https://huggingface.co/google/tapas-base)
- Comprensi√≥n y generaci√≥n multimodal unificada con [Emu3](https://huggingface.co/BAAI/Emu3-Gen)
- Visi√≥n a texto con [Llava-OneVision](https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf)
- Respuesta visual a preguntas con [Llava](https://huggingface.co/llava-hf/llava-1.5-7b-hf)
- Segmentaci√≥n de expresiones referidas visualmente con [Kosmos-2](https://huggingface.co/microsoft/kosmos-2-patch14-224)

</details>

<details>
<summary>PLN (Procesamiento de Lenguaje Natural)</summary>

- Completar palabras enmascaradas con [ModernBERT](https://huggingface.co/answerdotai/ModernBERT-base)
- Reconocimiento de entidades nombradas con [Gemma](https://huggingface.co/google/gemma-2-2b)
- Respuesta a preguntas con [Mixtral](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)
- Resumen con [BART](https://huggingface.co/facebook/bart-large-cnn)
- Traducci√≥n con [T5](https://huggingface.co/google-t5/t5-base)
- Generaci√≥n de texto con [Llama](https://huggingface.co/meta-llama/Llama-3.2-1B)
- Clasificaci√≥n de texto con [Qwen](https://huggingface.co/Qwen/Qwen2.5-0.5B)

</details>

## Citaci√≥n

Ahora tenemos un [art√≠culo](https://www.aclweb.org/anthology/2020.emnlp-demos.6/) que puedes citar para la biblioteca ü§ó Transformers:
```bibtex
@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R√©mi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}
```

---

[Powered By OpenAiTx](https://github.com/OpenAiTx/OpenAiTx)

---