<!---
Copyright 2020 Zesp√≥≈Ç HuggingFace. Wszelkie prawa zastrze≈ºone.

Licencjonowane na podstawie licencji Apache, wersja 2.0 (‚ÄûLicencja‚Äù);
nie mo≈ºesz u≈ºywaƒá tego pliku poza zakresem Licencji.
Kopiƒô Licencji mo≈ºesz uzyskaƒá pod adresem

    http://www.apache.org/licenses/LICENSE-2.0

O ile prawo nie stanowi inaczej lub nie zosta≈Ço to pisemnie uzgodnione, oprogramowanie
dystrybuowane na podstawie Licencji jest dostarczane na zasadzie ‚ÄûTAK JAK JEST‚Äù,
BEZ ≈ªADNYCH GWARANCJI ANI WARUNK√ìW, wyra≈∫nych ani domniemanych.
Zobacz Licencjƒô, aby uzyskaƒá szczeg√≥≈Çowe informacje dotyczƒÖce uprawnie≈Ñ i ogranicze≈Ñ wynikajƒÖcych z Licencji.
-->

<p align="center">
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-dark.svg">
    <source media="(prefers-color-scheme: light)" srcset="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg">
    <img alt="Biblioteka Hugging Face Transformers" src="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg" width="352" height="59" style="max-width: 100%;">
  </picture>
  <br/>
  <br/>
</p>

<p align="center">
    <a href="https://huggingface.com/models"><img alt="Checkpointy na Hubie" src="https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen"></a>
    <a href="https://circleci.com/gh/huggingface/transformers"><img alt="Build" src="https://img.shields.io/circleci/build/github/huggingface/transformers/main"></a>
    <a href="https://github.com/huggingface/transformers/blob/main/LICENSE"><img alt="GitHub" src="https://img.shields.io/github/license/huggingface/transformers.svg?color=blue"></a>
    <a href="https://huggingface.co/docs/transformers/index"><img alt="Dokumentacja" src="https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online"></a>
    <a href="https://github.com/huggingface/transformers/releases"><img alt="GitHub release" src="https://img.shields.io/github/release/huggingface/transformers.svg"></a>
    <a href="https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md"><img alt="Contributor Covenant" src="https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg"></a>
    <a href="https://zenodo.org/badge/latestdoi/155220641"><img src="https://zenodo.org/badge/155220641.svg" alt="DOI"></a>
</p>

<h4 align="center">
    <p>
        <b>English</b> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hans.md">ÁÆÄ‰Ωì‰∏≠Êñá</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hant.md">ÁπÅÈ´î‰∏≠Êñá</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ko.md">ÌïúÍµ≠Ïñ¥</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_es.md">Espa√±ol</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ja.md">Êó•Êú¨Ë™û</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_hd.md">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ru.md">–†—É—Å—Å–∫–∏–π</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_pt-br.md">–†ortugu√™s</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_te.md">‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_fr.md">Fran√ßais</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_de.md">Deutsch</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_vi.md">Ti·∫øng Vi·ªát</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ar.md">ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ur.md">ÿßÿ±ÿØŸà</a> |
    </p>
</h4>

<h3 align="center">
    <p>Najnowocze≈õniejsze modele wstƒôpnie wytrenowane do wnioskowania i uczenia</p>
</h3>

<h3 align="center">
    <a href="https://hf.co/course"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/course_banner.png"></a>
</h3>

Transformers to biblioteka wstƒôpnie wytrenowanych modeli tekstowych, komputerowego widzenia, audio, wideo i multimodalnych do wnioskowania i uczenia. U≈ºyj Transformers do dostrajania modeli na w≈Çasnych danych, budowania aplikacji inferencyjnych oraz do zastosowa≈Ñ generatywnej AI w r√≥≈ºnych modalno≈õciach.

Na [Hugging Face Hub](https://huggingface.com/models) dostƒôpnych jest ponad 500 tys. [checkpoint√≥w modeli](https://huggingface.co/models?library=transformers&sort=trending), kt√≥rych mo≈ºesz u≈ºywaƒá.

Odwied≈∫ [Hub](https://huggingface.com/) ju≈º dzi≈õ, aby znale≈∫ƒá model i od razu zaczƒÖƒá korzystaƒá z Transformers.

## Instalacja

Transformers wsp√≥≈Çpracuje z Pythonem 3.9+, [PyTorch](https://pytorch.org/get-started/locally/) 2.1+, [TensorFlow](https://www.tensorflow.org/install/pip) 2.6+ oraz [Flax](https://flax.readthedocs.io/en/latest/) 0.4.1+.

Utw√≥rz i aktywuj ≈õrodowisko wirtualne za pomocƒÖ [venv](https://docs.python.org/3/library/venv.html) lub [uv](https://docs.astral.sh/uv/), szybkiego mened≈ºera pakiet√≥w i projekt√≥w Pythona opartego na Rust.

```py
# venv
python -m venv .my-env
source .my-env/bin/activate
# uv
uv venv .my-env
source .my-env/bin/activate
```

Zainstaluj Transformers w swoim ≈õrodowisku wirtualnym.

```py
# pip
pip install "transformers[torch]"

# uv
uv pip install "transformers[torch]"
```

Zainstaluj Transformers ze ≈∫r√≥d≈Ça, je≈õli chcesz mieƒá najnowsze zmiany w bibliotece lub chcesz wsp√≥≈Çtworzyƒá. Jednak *najnowsza* wersja mo≈ºe byƒá niestabilna. Je≈õli napotkasz b≈ÇƒÖd, ≈õmia≈Ço otw√≥rz [issue](https://github.com/huggingface/transformers/issues).

```shell
git clone https://github.com/huggingface/transformers.git
cd transformers

# pip
pip install .[torch]

# uv
uv pip install .[torch]
```

## Szybki start

Rozpocznij pracƒô z Transformers od razu dziƒôki API [Pipeline](https://huggingface.co/docs/transformers/pipeline_tutorial). `Pipeline` to wysokopoziomowa klasa do wnioskowania wspierajƒÖca zadania tekstowe, audio, wizualne i multimodalne. Obs≈Çuguje wstƒôpne przetwarzanie danych wej≈õciowych i zwraca odpowiednie wyj≈õcie.

Utw√≥rz pipeline i okre≈õl model do generowania tekstu. Model zostanie pobrany i zapisany w cache, wiƒôc mo≈ºesz ≈Çatwo u≈ºyƒá go ponownie. Na ko≈Ñcu podaj tekst, aby zainicjowaƒá model.

```py
from transformers import pipeline

pipeline = pipeline(task="text-generation", model="Qwen/Qwen2.5-1.5B")
pipeline("the secret to baking a really good cake is ")
[{'generated_text': 'the secret to baking a really good cake is 1) to use the right ingredients and 2) to follow the recipe exactly. the recipe for the cake is as follows: 1 cup of sugar, 1 cup of flour, 1 cup of milk, 1 cup of butter, 1 cup of eggs, 1 cup of chocolate chips. if you want to make 2 cakes, how much sugar do you need? To make 2 cakes, you will need 2 cups of sugar.'}]
```

Aby rozmawiaƒá z modelem, wzorzec u≈ºycia jest taki sam. Jedyna r√≥≈ºnica polega na konieczno≈õci utworzenia historii czatu (wej≈õcia do `Pipeline`) pomiƒôdzy tobƒÖ a systemem.

> [!TIP]
> Mo≈ºesz r√≥wnie≈º rozmawiaƒá z modelem bezpo≈õrednio z linii polece≈Ñ.
> ```shell
> transformers chat Qwen/Qwen2.5-0.5B-Instruct
> ```

```py
import torch
from transformers import pipeline

chat = [
    {"role": "system", "content": "Jeste≈õ zadziornym, dowcipnym robotem wyobra≈ºonym przez Hollywood oko≈Ço 1986 roku."},
    {"role": "user", "content": "Hej, mo≈ºesz poleciƒá jakie≈õ fajne rzeczy do zrobienia w Nowym Jorku?"}
]

pipeline = pipeline(task="text-generation", model="meta-llama/Meta-Llama-3-8B-Instruct", torch_dtype=torch.bfloat16, device_map="auto")
response = pipeline(chat, max_new_tokens=512)
print(response[0]["generated_text"][-1]["content"])
```

Rozwi≈Ñ poni≈ºsze przyk≈Çady, aby zobaczyƒá jak `Pipeline` dzia≈Ça dla r√≥≈ºnych modalno≈õci i zada≈Ñ.

<details>
<summary>Automatyczne rozpoznawanie mowy</summary>

```py
from transformers import pipeline

pipeline = pipeline(task="automatic-speech-recognition", model="openai/whisper-large-v3")
pipeline("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}
```

</details>

<details>
<summary>Klasyfikacja obraz√≥w</summary>

<h3 align="center">
    <a><img src="https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png"></a>
</h3>

```py
from transformers import pipeline

pipeline = pipeline(task="image-classification", model="facebook/dinov2-small-imagenet1k-1-layer")
pipeline("https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png")
[{'label': 'macaw', 'score': 0.997848391532898},
 {'label': 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',
  'score': 0.0016551691805943847},
 {'label': 'lorikeet', 'score': 0.00018523589824326336},
 {'label': 'African grey, African gray, Psittacus erithacus',
  'score': 7.85409429227002e-05},
 {'label': 'quail', 'score': 5.502637941390276e-05}]
```

</details>

<details>
<summary>Wizualne odpowiadanie na pytania</summary>


<h3 align="center">
    <a><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg"></a>
</h3>

```py
from transformers import pipeline

pipeline = pipeline(task="visual-question-answering", model="Salesforce/blip-vqa-base")
pipeline(
    image="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg",
    question="Co znajduje siƒô na obrazie?",
)
[{'answer': 'statue of liberty'}]
```

</details>

## Dlaczego warto u≈ºywaƒá Transformers?

1. ≈Åatwe w u≈ºyciu najnowocze≈õniejsze modele:
    - Wysoka wydajno≈õƒá w zadaniach rozumienia i generowania jƒôzyka naturalnego, komputerowego widzenia, audio, wideo i multimodalnych.
    - Niski pr√≥g wej≈õcia dla naukowc√≥w, in≈ºynier√≥w i programist√≥w.
    - Niewiele abstrakcji skierowanych do u≈ºytkownika ‚Äì wystarczƒÖ trzy klasy do nauczenia siƒô.
    - Ujednolicone API do korzystania ze wszystkich naszych modeli wstƒôpnie wytrenowanych.

1. Ni≈ºsze koszty obliczeniowe, mniejszy ≈õlad wƒôglowy:
    - Udostƒôpniaj wytrenowane modele zamiast trenowaƒá od zera.
    - Skr√≥ƒá czas oblicze≈Ñ i koszty produkcji.
    - DziesiƒÖtki architektur modeli z ponad milionem checkpoint√≥w wstƒôpnie wytrenowanych we wszystkich modalno≈õciach.

1. Wybierz odpowiedni framework na ka≈ºdym etapie ≈ºycia modelu:
    - Trenuj najnowocze≈õniejsze modele w 3 linijkach kodu.
    - Przeno≈õ pojedynczy model miƒôdzy frameworkami PyTorch/JAX/TF2.0 wedle potrzeb.
    - Wybierz odpowiedni framework do trenowania, ewaluacji i wdro≈ºenia.

1. ≈Åatwo dostosuj model lub przyk≈Çad do swoich potrzeb:
    - Dostarczamy przyk≈Çady dla ka≈ºdej architektury, aby odtworzyƒá wyniki opublikowane przez jej autor√≥w.
    - Wnƒôtrze modeli jest eksponowane tak sp√≥jnie, jak to mo≈ºliwe.
    - Pliki modeli mo≈ºna u≈ºywaƒá niezale≈ºnie od biblioteki do szybkich eksperyment√≥w.

<a target="_blank" href="https://huggingface.co/enterprise">
    <img alt="Hugging Face Enterprise Hub" src="https://github.com/user-attachments/assets/247fb16d-d251-4583-96c4-d3d76dda4925">
</a><br>

## Dlaczego nie powiniene≈õ u≈ºywaƒá Transformers?

- Ta biblioteka nie jest modularnym zestawem klock√≥w do budowy sieci neuronowych. Kod w plikach modeli nie jest celowo refaktoryzowany o dodatkowe abstrakcje, aby badacze mogli szybko iterowaƒá na ka≈ºdym modelu bez konieczno≈õci zag≈Çƒôbiania siƒô w kolejne warstwy/plik√≥w.
- API trenowania jest zoptymalizowane pod kƒÖtem modeli PyTorch dostarczanych przez Transformers. Do og√≥lnych pƒôtli uczenia maszynowego powiniene≈õ u≈ºyƒá innej biblioteki, np. [Accelerate](https://huggingface.co/docs/accelerate).
- [Skrypty przyk≈Çad√≥w]((https://github.com/huggingface/transformers/tree/main/examples)) sƒÖ tylko *przyk≈Çadami*. MogƒÖ nie dzia≈Çaƒá od razu w twoim przypadku i bƒôdziesz musia≈Ç dostosowaƒá kod.

## 100 projekt√≥w wykorzystujƒÖcych Transformers

Transformers to wiƒôcej ni≈º narzƒôdzie do korzystania z wstƒôpnie wytrenowanych modeli ‚Äì to spo≈Çeczno≈õƒá projekt√≥w zbudowanych wok√≥≈Ç niego i Hugging Face Hub. Chcemy, aby Transformers umo≈ºliwia≈Ç deweloperom, badaczom, studentom, profesorom, in≈ºynierom i ka≈ºdemu innemu realizowanie w≈Çasnych wymarzonych projekt√≥w.

Aby uczciƒá przekroczenie 100 000 gwiazdek przez Transformers, postanowili≈õmy wyr√≥≈ºniƒá spo≈Çeczno≈õƒá poprzez stronƒô [awesome-transformers](./awesome-transformers.md), na kt√≥rej znajdziesz 100 niesamowitych projekt√≥w zbudowanych z Transformers.

Je≈õli posiadasz lub u≈ºywasz projektu, kt√≥ry twoim zdaniem powinien znale≈∫ƒá siƒô na tej li≈õcie, otw√≥rz PR, aby go dodaƒá!

## Przyk≈Çadowe modele

Wiƒôkszo≈õƒá naszych modeli mo≈ºesz przetestowaƒá bezpo≈õrednio na ich [stronach na Hubie](https://huggingface.co/models).

Rozwi≈Ñ ka≈ºdƒÖ modalno≈õƒá poni≈ºej, aby zobaczyƒá kilka przyk≈Çadowych modeli do r√≥≈ºnych zastosowa≈Ñ.

<details>
<summary>Audio</summary>

- Klasyfikacja d≈∫wiƒôku z [Whisper](https://huggingface.co/openai/whisper-large-v3-turbo)
- Automatyczne rozpoznawanie mowy z [Moonshine](https://huggingface.co/UsefulSensors/moonshine)
- Wykrywanie s≈Ç√≥w kluczowych z [Wav2Vec2](https://huggingface.co/superb/wav2vec2-base-superb-ks)
- Generowanie mowy z mowy z [Moshi](https://huggingface.co/kyutai/moshiko-pytorch-bf16)
- Zamiana tekstu na audio z [MusicGen](https://huggingface.co/facebook/musicgen-large)
- Zamiana tekstu na mowƒô z [Bark](https://huggingface.co/suno/bark)

</details>

<details>
<summary>Komputerowe widzenie</summary>

- Automatyczne generowanie masek z [SAM](https://huggingface.co/facebook/sam-vit-base)
- Szacowanie g≈Çƒôboko≈õci z [DepthPro](https://huggingface.co/apple/DepthPro-hf)
- Klasyfikacja obraz√≥w z [DINO v2](https://huggingface.co/facebook/dinov2-base)
- Wykrywanie punkt√≥w kluczowych z [SuperGlue](https://huggingface.co/magic-leap-community/superglue_outdoor)
- Dopasowywanie punkt√≥w kluczowych z [SuperGlue](https://huggingface.co/magic-leap-community/superglue)
- Wykrywanie obiekt√≥w z [RT-DETRv2](https://huggingface.co/PekingU/rtdetr_v2_r50vd)
- Estymacja pozy z [VitPose](https://huggingface.co/usyd-community/vitpose-base-simple)
- Uniwersalna segmentacja z [OneFormer](https://huggingface.co/shi-labs/oneformer_ade20k_swin_large)
- Klasyfikacja wideo z [VideoMAE](https://huggingface.co/MCG-NJU/videomae-large)

</details>

<details>
<summary>Multimodalno≈õƒá</summary>

- Audio lub tekst na tekst z [Qwen2-Audio](https://huggingface.co/Qwen/Qwen2-Audio-7B)
- Odpowiadanie na pytania o dokumenty z [LayoutLMv3](https://huggingface.co/microsoft/layoutlmv3-base)
- Obraz lub tekst na tekst z [Qwen-VL](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct)
- Opisywanie obraz√≥w [BLIP-2](https://huggingface.co/Salesforce/blip2-opt-2.7b)
- Rozumienie dokument√≥w OCR z [GOT-OCR2](https://huggingface.co/stepfun-ai/GOT-OCR-2.0-hf)
- Odpowiadanie na pytania o tabelach z [TAPAS](https://huggingface.co/google/tapas-base)
- Zunifikowane rozumienie i generowanie multimodalne z [Emu3](https://huggingface.co/BAAI/Emu3-Gen)
- Wizja na tekst z [Llava-OneVision](https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf)
- Wizualne odpowiadanie na pytania z [Llava](https://huggingface.co/llava-hf/llava-1.5-7b-hf)
- Wizualna segmentacja wyra≈ºe≈Ñ referencyjnych z [Kosmos-2](https://huggingface.co/microsoft/kosmos-2-patch14-224)

</details>

<details>
<summary>NLP</summary>

- Uzupe≈Çnianie maskowanych s≈Ç√≥w z [ModernBERT](https://huggingface.co/answerdotai/ModernBERT-base)
- Rozpoznawanie nazwanych encji z [Gemma](https://huggingface.co/google/gemma-2-2b)
- Odpowiadanie na pytania z [Mixtral](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)
- Streszczanie z [BART](https://huggingface.co/facebook/bart-large-cnn)
- T≈Çumaczenie z [T5](https://huggingface.co/google-t5/t5-base)
- Generowanie tekstu z [Llama](https://huggingface.co/meta-llama/Llama-3.2-1B)
- Klasyfikacja tekstu z [Qwen](https://huggingface.co/Qwen/Qwen2.5-0.5B)

</details>

## Cytowanie

Obecnie mamy [artyku≈Ç](https://www.aclweb.org/anthology/2020.emnlp-demos.6/), kt√≥ry mo≈ºesz zacytowaƒá dla biblioteki ü§ó Transformers:
```bibtex
@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R√©mi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}
```

---

[Powered By OpenAiTx](https://github.com/OpenAiTx/OpenAiTx)

---