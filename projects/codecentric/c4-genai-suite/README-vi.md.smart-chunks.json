[
  {
    "Id": 1,
    "Content": "# c4 GenAI Suite\n\nAn AI chatbot application with Model Context Provider (MCP) integration, powered by Langchain and compatibility for all major Large Language Models (LLMs) and Embedding Models.\n\nAdministrators can create assistants with different capabilities by adding extensions, such as RAG (Retrieval-Augmented Generation) services or MCP servers. The application is built using a modern tech stack, including React, NestJS, and Python FastAPI for the REI-S service.\n\nUsers can interact with assistants through a user-friendly interface. Depending on the assistant's configuration, users may be able to ask questions, upload their own files, or use other features. The assistants interact with various LLM providers to provide responses based on the configured extensions. Contextual information provided by the configured extensions allows the assistants to answer domain-specific questions and provide relevant information.\n\nThe application is designed to be modular and extensible, allowing users to create assistants with different capabilities by adding extensions.\n\n![short demo video of basic usage](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/preview.webp)\n\n## Features\n\n### Large Language Models (LLM) and Multimodal Models\n\nThe c4 GenAI Suite supports already many models directly. And if your preferred model is not supported already, it should be easy to write an extension to support it.\n\n* OpenAI compatible models\n* Azure OpenAI models\n* Bedrock models\n* Google GenAI models\n* Ollama compatible models\n",
    "ContentSha": "VePwjxi+F5ttFJfuckWjhr5ujnZTyNWk2c6pu+FMFPM=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "# c4 GenAI Suite\n\nMột ứng dụng chatbot AI với tích hợp Nhà cung cấp Ngữ cảnh Mô hình (Model Context Provider - MCP), được hỗ trợ bởi Langchain và tương thích với tất cả các Mô hình Ngôn ngữ Lớn (LLM) và Mô hình Nhúng chính.\n\nQuản trị viên có thể tạo các trợ lý với các khả năng khác nhau bằng cách thêm các phần mở rộng, chẳng hạn như dịch vụ RAG (Generation Tăng cường Truy hồi) hoặc máy chủ MCP. Ứng dụng được xây dựng bằng một ngăn xếp công nghệ hiện đại, bao gồm React, NestJS và Python FastAPI cho dịch vụ REI-S.\n\nNgười dùng có thể tương tác với các trợ lý thông qua giao diện thân thiện với người dùng. Tùy thuộc vào cấu hình của trợ lý, người dùng có thể đặt câu hỏi, tải lên tệp của riêng mình hoặc sử dụng các tính năng khác. Các trợ lý sẽ tương tác với các nhà cung cấp LLM khác nhau để cung cấp phản hồi dựa trên các phần mở rộng đã cấu hình. Thông tin ngữ cảnh do các phần mở rộng cấu hình cung cấp cho phép các trợ lý trả lời các câu hỏi chuyên ngành và cung cấp thông tin liên quan.\n\nỨng dụng được thiết kế theo hướng mô-đun và có thể mở rộng, cho phép người dùng tạo các trợ lý với các khả năng khác nhau bằng cách thêm các phần mở rộng.\n\n![video demo ngắn về cách sử dụng cơ bản](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/preview.webp)\n\n## Tính năng\n\n### Mô hình Ngôn ngữ Lớn (LLM) và Mô hình Đa phương tiện\n\nc4 GenAI Suite đã hỗ trợ trực tiếp nhiều mô hình. Nếu mô hình ưa thích của bạn chưa được hỗ trợ, bạn cũng có thể dễ dàng viết phần mở rộng để tích hợp nó.\n\n* Mô hình tương thích OpenAI\n* Mô hình Azure OpenAI\n* Mô hình Bedrock\n* Mô hình Google GenAI\n* Mô hình tương thích Ollama",
    "Status": "ok"
  },
  {
    "Id": 2,
    "Content": "### Retrieval Augmented Generation (RAG)\n\nThe c4 GenAI Suite includes REI-S, a server to prepare files for consumption by the LLM.\n\n* REI-S, a custom integrated RAG server\n  * Vector stores\n    * pgvector\n    * Azure AI Search\n  * Embedding models\n    * OpenAI compatible embeddings\n    * Azure OpenAI embeddings\n    * Ollama compatible embeddings\n  * File formats:\n    * pdf, docx, pptx, xlsx, ...\n    * audio file voice transcription (via Whisper)\n\n### Extensions\n\nThe c4 GenAI Suite is designed for extensibility. Writing extensions is easy, as is using an already existing MCP server.\n\n* Model Context Protocol (MCP) servers\n* Custom systemprompt\n* Bing Search\n* Calculator\n\n",
    "ContentSha": "wvT4e6071qGfJDWClvklgUJiJRM71gUVIUQKaaDgVPo=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### Retrieval Augmented Generation (RAG)\n\nBộ công cụ c4 GenAI Suite bao gồm REI-S, một máy chủ để chuẩn bị các tệp cho việc sử dụng bởi LLM.\n\n* REI-S, một máy chủ RAG tích hợp tùy chỉnh\n  * Kho vector\n    * pgvector\n    * Azure AI Search\n  * Mô hình embedding\n    * Embedding tương thích OpenAI\n    * Embedding Azure OpenAI\n    * Embedding tương thích Ollama\n  * Định dạng tệp:\n    * pdf, docx, pptx, xlsx, ...\n    * Chuyển đổi giọng nói từ tệp âm thanh (qua Whisper)\n\n### Phần mở rộng\n\nBộ công cụ c4 GenAI Suite được thiết kế để dễ dàng mở rộng. Viết phần mở rộng rất dễ, cũng như sử dụng một máy chủ MCP đã có sẵn.\n\n* Máy chủ Model Context Protocol (MCP)\n* Hệ thống prompt tùy chỉnh\n* Tìm kiếm Bing\n* Máy tính (Calculator)\n",
    "Status": "ok"
  },
  {
    "Id": 3,
    "Content": "## Getting Started\n\n### Using Docker-Compose\n\n- Run `docker compose up` in the project root.\n- Open the [application](http://localhost:3333) in a browser. The default login credentials are user `admin@example.com` and password `secret`.\n\n![video showing assistant configuration](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/assistants.webp)\n\n### Using Helm & Kubernetes\n\nFor deployment in Kubernetes environments, please refer to the [README of our Helm Chart](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/./helm-chart/README.md).\n\n### Setting up Assistants and Extensions\n\nThe c4 GenAI Suite revolves around *assistants*.\nEach assistant consists of a set of extensions, which determine the LLM model and which tools it can use.\n\n- In the admin area (click the username on the bottom left), go to the [assistants section](http://localhost:3333/admin/assistants).\n- Add an assistant with the green `+` button next to the section title. Choose a name and a description.\n- Select the created assistant and click the green `+ Add Extension`.\n- Select the model and fill in the credentials.\n- Use the `Test` Button to check that it works and `save`.\n\nNow you can return to the [chat page](http://localhost:3333/chat) (click on `c4 GenAI Suite` in the top left) and start a new conversation with your new assistant.\n\n> [!TIP]\n> Our `docker-compose` includes a local Ollama, which runs on the CPU. You can use this for quick testing. But it will be slow and you probably want to use another model. If you want to use it, just create the following model extension in your Assistant.\n> * Extension: `Dev: Ollama`\n> * Endpoint: `http://ollama:11434`\n> * Model: `llama3.2`\n",
    "ContentSha": "bLa8wjqGffqM6ThWqBWA15QpuTagUJbPEAqi4FPI4X4=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## Bắt đầu\n\n### Sử dụng Docker-Compose\n\n- Chạy `docker compose up` trong thư mục gốc của dự án.\n- Mở [ứng dụng](http://localhost:3333) trên trình duyệt. Thông tin đăng nhập mặc định là người dùng `admin@example.com` và mật khẩu `secret`.\n\n![video hướng dẫn cấu hình trợ lý](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/assistants.webp)\n\n### Sử dụng Helm & Kubernetes\n\nĐể triển khai trong môi trường Kubernetes, vui lòng tham khảo [README của Helm Chart chúng tôi](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/./helm-chart/README.md).\n\n### Thiết lập Trợ lý và Tiện ích mở rộng\n\nBộ công cụ c4 GenAI Suite xoay quanh các *trợ lý*.\nMỗi trợ lý bao gồm một tập hợp các tiện ích mở rộng, xác định mô hình LLM và các công cụ mà nó có thể sử dụng.\n\n- Trong khu vực quản trị (nhấp vào tên người dùng ở góc dưới bên trái), vào phần [trợ lý](http://localhost:3333/admin/assistants).\n- Thêm một trợ lý bằng nút `+` màu xanh lá bên cạnh tiêu đề phần. Chọn tên và mô tả.\n- Chọn trợ lý vừa tạo và nhấp vào `+ Thêm tiện ích mở rộng` màu xanh lá.\n- Chọn mô hình và điền thông tin xác thực.\n- Sử dụng nút `Kiểm tra` để kiểm tra hoạt động và `lưu`.\n\nBây giờ bạn có thể quay lại [trang trò chuyện](http://localhost:3333/chat) (nhấp vào `c4 GenAI Suite` ở góc trên bên trái) và bắt đầu cuộc trò chuyện mới với trợ lý của mình.\n\n> [!TIP]\n> `docker-compose` của chúng tôi bao gồm một Ollama cục bộ, chạy trên CPU. Bạn có thể dùng nó để kiểm thử nhanh. Tuy nhiên nó sẽ chậm và có lẽ bạn sẽ muốn sử dụng một mô hình khác. Nếu bạn muốn sử dụng Ollama, chỉ cần tạo tiện ích mở rộng mô hình sau trong Trợ lý của bạn.\n> * Tiện ích mở rộng: `Dev: Ollama`\n> * Endpoint: `http://ollama:11434`\n> * Mô hình: `llama3.2`\n",
    "Status": "ok"
  },
  {
    "Id": 4,
    "Content": "### Model Context Protocol (MCP) [optional]\n\nUse any MCP server offering an `sse` interface with the `MCP Tools` Extension (or use our `mcp-tool-as-server` as a proxy in front of an `stdio` MCP server).\nEach MCP server can be configured in detail as an extension.\n\n### Retrieval Augmented Generation (RAG) / File Search [optional]\n\nUse our RAG server `REI-S` to search user provided files. Just configure a `Search Files` extension for the assistant.\nThis process is described in detail in [the `services/reis` subdirectory](services/reis/#example-configuration-in-c4).\n\n## Contributing & Development\n\n* See [CONTRIBUTING.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/CONTRIBUTING.md) for guidelines on how to contribute.\n* For developer onboarding, check [DEVELOPERS.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/DEVELOPERS.md).\n\n## Main Building Blocks\n\nThe application consists of a **Frontend** , a **Backend**  and a **REI-S**  service.\n\n```",
    "ContentSha": "X5WCsc3MMbiH/lsJf83tTEO6Cfl/16Nw2tPJwgCKYpc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### Giao thức Bối cảnh Mô hình (MCP) [tùy chọn]\n\nSử dụng bất kỳ máy chủ MCP nào cung cấp giao diện `sse` với Tiện ích mở rộng `MCP Tools` (hoặc sử dụng `mcp-tool-as-server` của chúng tôi làm proxy phía trước một máy chủ MCP `stdio`).\nMỗi máy chủ MCP có thể được cấu hình chi tiết như một tiện ích mở rộng.\n\n### Tạo sinh tăng cường truy xuất (RAG) / Tìm kiếm Tệp [tùy chọn]\n\nSử dụng máy chủ RAG `REI-S` của chúng tôi để tìm kiếm các tệp do người dùng cung cấp. Chỉ cần cấu hình một tiện ích mở rộng `Search Files` cho trợ lý.\nQuy trình này được mô tả chi tiết trong [thư mục con `services/reis`](services/reis/#example-configuration-in-c4).\n\n## Đóng góp & Phát triển\n\n* Xem [CONTRIBUTING.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/CONTRIBUTING.md) để biết hướng dẫn đóng góp.\n* Đối với nhà phát triển mới, hãy kiểm tra [DEVELOPERS.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/DEVELOPERS.md).\n\n## Các Khối Xây Dựng Chính\n\nỨng dụng bao gồm **Frontend**, **Backend** và dịch vụ **REI-S**.\n\n```",
    "Status": "ok"
  },
  {
    "Id": 5,
    "Content": "┌──────────┐\n│   User   │\n└─────┬────┘\n      │ access\n      ▼\n┌──────────┐\n│ Frontend │\n└─────┬────┘\n      │ access\n      ▼\n┌──────────┐     ┌─────────────────┐\n│ Backend  │────►│      LLM        │\n└─────┬────┘     └─────────────────┘\n      │ access\n      ▼\n┌──────────┐     ┌─────────────────┐\n│  REI-S   │────►│ Embedding Model │\n│          │     └─────────────────┘\n│          │\n│          │     ┌─────────────────┐\n│          │────►│  Vector Store   │\n└──────────┘     └─────────────────┘\n```",
    "ContentSha": "IO09RXKALRyYzMSXIVzGJTmyROh56hbqbTTI4NQfxtQ=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "┌──────────┐\n│   Người dùng   │\n└─────┬────┘\n      │ truy cập\n      ▼\n┌──────────┐\n│ Giao diện người dùng │\n└─────┬────┘\n      │ truy cập\n      ▼\n┌──────────┐     ┌─────────────────┐\n│ Backend  │────►│      LLM        │\n└─────┬────┘     └─────────────────┘\n      │ truy cập\n      ▼\n┌──────────┐     ┌─────────────────┐\n│  REI-S   │────►│ Mô hình Embedding │\n│          │     └─────────────────┘\n│          │\n│          │     ┌─────────────────┐\n│          │────►│  Kho vector     │\n└──────────┘     └─────────────────┘\n```",
    "Status": "ok"
  },
  {
    "Id": 6,
    "Content": "\n\n### Frontend\n\nThe frontend is built with React and TypeScript, providing a user-friendly interface for interacting with the backend and REI-S service. It includes features for managing assistants, extensions, and chat functionalities.\n\n> Sources: `/frontend`\n\n### Backend\n\nThe backend is developed using NestJS and TypeScript, serving as the main API layer for the application. It handles requests from the frontend and interacts with llm providers to facilitate chat functionalities. The backend also manages assistants and their extensions, allowing users to configure and use various AI models for their chats.\n\nAdditionally, the backend manages user authentication, and communicates with the REI-S service for file indexing and retrieval.\n\nFor data persistence, the backend uses a **PostgreSQL** database.\n\n> Sources: `/backend`\n\n### REI-S\n\nThe REI-S (**R**etrieval **E**xtraction **I**ngestion **S**erver) is a Python-based server that provides basic RAG (Retrieval-Augmented Generation) capabilities. It allows for file content extraction, indexing and querying, enabling the application to handle large datasets efficiently. The REI-S service is designed to work seamlessly with the backend, providing necessary data for chat functionalities and file searches.\n\nThe REI-S supports Azure AI Search and pgvector for vector storage, allowing for flexible and scalable data retrieval options. The service can be configured using environment variables to specify the type of vector store and connection details.\n\n> Sources: `/services/reis`\n",
    "ContentSha": "Sceh4YX3ZYIBDEtdTMWIBUucYWH+hEUJMben1cxfrAc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### Frontend\n\nFrontend được xây dựng bằng React và TypeScript, cung cấp giao diện thân thiện với người dùng để tương tác với backend và dịch vụ REI-S. Nó bao gồm các tính năng quản lý trợ lý, tiện ích mở rộng và chức năng trò chuyện.\n\n> Nguồn: `/frontend`\n\n### Backend\n\nBackend được phát triển bằng NestJS và TypeScript, đóng vai trò là lớp API chính cho ứng dụng. Nó xử lý các yêu cầu từ frontend và tương tác với các nhà cung cấp llm để hỗ trợ chức năng trò chuyện. Backend cũng quản lý các trợ lý và tiện ích mở rộng của chúng, cho phép người dùng cấu hình và sử dụng nhiều mô hình AI khác nhau cho các cuộc trò chuyện của mình.\n\nNgoài ra, backend còn quản lý xác thực người dùng, và giao tiếp với dịch vụ REI-S để lập chỉ mục và truy xuất tệp tin.\n\nĐể lưu trữ dữ liệu, backend sử dụng cơ sở dữ liệu **PostgreSQL**.\n\n> Nguồn: `/backend`\n\n### REI-S\n\nREI-S (**R**etrieval **E**xtraction **I**ngestion **S**erver) là một máy chủ dựa trên Python cung cấp các khả năng cơ bản của RAG (Retrieval-Augmented Generation). Nó cho phép trích xuất nội dung tệp, lập chỉ mục và truy vấn, giúp ứng dụng xử lý các tập dữ liệu lớn một cách hiệu quả. Dịch vụ REI-S được thiết kế để hoạt động liền mạch với backend, cung cấp dữ liệu cần thiết cho chức năng trò chuyện và tìm kiếm tệp tin.\n\nREI-S hỗ trợ Azure AI Search và pgvector cho lưu trữ vector, cho phép các tùy chọn truy xuất dữ liệu linh hoạt và khả năng mở rộng cao. Dịch vụ có thể được cấu hình bằng các biến môi trường để chỉ định loại kho lưu trữ vector và thông tin kết nối.\n\n> Nguồn: `/services/reis`",
    "Status": "ok"
  }
]