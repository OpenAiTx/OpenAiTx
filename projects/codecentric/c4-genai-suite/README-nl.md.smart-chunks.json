[
  {
    "Id": 1,
    "Content": "# c4 GenAI Suite\n\nAn AI chatbot application with Model Context Provider (MCP) integration, powered by Langchain and compatibility for all major Large Language Models (LLMs) and Embedding Models.\n\nAdministrators can create assistants with different capabilities by adding extensions, such as RAG (Retrieval-Augmented Generation) services or MCP servers. The application is built using a modern tech stack, including React, NestJS, and Python FastAPI for the REI-S service.\n\nUsers can interact with assistants through a user-friendly interface. Depending on the assistant's configuration, users may be able to ask questions, upload their own files, or use other features. The assistants interact with various LLM providers to provide responses based on the configured extensions. Contextual information provided by the configured extensions allows the assistants to answer domain-specific questions and provide relevant information.\n\nThe application is designed to be modular and extensible, allowing users to create assistants with different capabilities by adding extensions.\n\n![short demo video of basic usage](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/preview.webp)\n\n## Features\n\n### Large Language Models (LLM) and Multimodal Models\n\nThe c4 GenAI Suite supports already many models directly. And if your preferred model is not supported already, it should be easy to write an extension to support it.\n\n* OpenAI compatible models\n* Azure OpenAI models\n* Bedrock models\n* Google GenAI models\n* Ollama compatible models\n",
    "ContentSha": "VePwjxi+F5ttFJfuckWjhr5ujnZTyNWk2c6pu+FMFPM=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "# c4 GenAI Suite\n\nEen AI-chatbotapplicatie met Model Context Provider (MCP) integratie, aangedreven door Langchain en compatibiliteit met alle grote Large Language Models (LLM's) en Embedding Modellen.\n\nBeheerders kunnen assistenten creëren met verschillende mogelijkheden door extensies toe te voegen, zoals RAG (Retrieval-Augmented Generation) services of MCP-servers. De applicatie is gebouwd met behulp van een moderne tech stack, waaronder React, NestJS en Python FastAPI voor de REI-S service.\n\nGebruikers kunnen met assistenten communiceren via een gebruiksvriendelijke interface. Afhankelijk van de configuratie van de assistent kunnen gebruikers vragen stellen, hun eigen bestanden uploaden of andere functies gebruiken. De assistenten communiceren met verschillende LLM-aanbieders om antwoorden te geven op basis van de geconfigureerde extensies. Contextuele informatie, geleverd door de geconfigureerde extensies, stelt de assistenten in staat om domeinspecifieke vragen te beantwoorden en relevante informatie te geven.\n\nDe applicatie is ontworpen om modulair en uitbreidbaar te zijn, zodat gebruikers assistenten kunnen creëren met verschillende mogelijkheden door extensies toe te voegen.\n\n![korte demovideo van basisgebruik](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/preview.webp)\n\n## Functies\n\n### Large Language Models (LLM) en Multimodale Modellen\n\nDe c4 GenAI Suite ondersteunt al veel modellen direct. En als uw voorkeursmodel nog niet wordt ondersteund, zou het eenvoudig moeten zijn om een extensie te schrijven om deze te ondersteunen.\n\n* OpenAI-compatibele modellen\n* Azure OpenAI-modellen\n* Bedrock-modellen\n* Google GenAI-modellen\n* Ollama-compatibele modellen",
    "Status": "ok"
  },
  {
    "Id": 2,
    "Content": "### Retrieval Augmented Generation (RAG)\n\nThe c4 GenAI Suite includes REI-S, a server to prepare files for consumption by the LLM.\n\n* REI-S, a custom integrated RAG server\n  * Vector stores\n    * pgvector\n    * Azure AI Search\n  * Embedding models\n    * OpenAI compatible embeddings\n    * Azure OpenAI embeddings\n    * Ollama compatible embeddings\n  * File formats:\n    * pdf, docx, pptx, xlsx, ...\n    * audio file voice transcription (via Whisper)\n\n### Extensions\n\nThe c4 GenAI Suite is designed for extensibility. Writing extensions is easy, as is using an already existing MCP server.\n\n* Model Context Protocol (MCP) servers\n* Custom systemprompt\n* Bing Search\n* Calculator\n\n",
    "ContentSha": "wvT4e6071qGfJDWClvklgUJiJRM71gUVIUQKaaDgVPo=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### Retrieval Augmented Generation (RAG)\n\nDe c4 GenAI Suite bevat REI-S, een server om bestanden voor te bereiden voor gebruik door het LLM.\n\n* REI-S, een aangepast geïntegreerde RAG-server\n  * Vector stores\n    * pgvector\n    * Azure AI Search\n  * Embedding modellen\n    * OpenAI-compatibele embeddings\n    * Azure OpenAI embeddings\n    * Ollama-compatibele embeddings\n  * Bestandsformaten:\n    * pdf, docx, pptx, xlsx, ...\n    * audio-bestand spraaktranscriptie (via Whisper)\n\n### Extensies\n\nDe c4 GenAI Suite is ontworpen voor uitbreidbaarheid. Het schrijven van extensies is eenvoudig, evenals het gebruik van een reeds bestaande MCP-server.\n\n* Model Context Protocol (MCP) servers\n* Aangepaste systemprompt\n* Bing Search\n* Rekenmachine",
    "Status": "ok"
  },
  {
    "Id": 3,
    "Content": "## Getting Started\n\n### Using Docker-Compose\n\n- Run `docker compose up` in the project root.\n- Open the [application](http://localhost:3333) in a browser. The default login credentials are user `admin@example.com` and password `secret`.\n\n![video showing assistant configuration](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/assistants.webp)\n\n### Using Helm & Kubernetes\n\nFor deployment in Kubernetes environments, please refer to the [README of our Helm Chart](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/./helm-chart/README.md).\n\n### Setting up Assistants and Extensions\n\nThe c4 GenAI Suite revolves around *assistants*.\nEach assistant consists of a set of extensions, which determine the LLM model and which tools it can use.\n\n- In the admin area (click the username on the bottom left), go to the [assistants section](http://localhost:3333/admin/assistants).\n- Add an assistant with the green `+` button next to the section title. Choose a name and a description.\n- Select the created assistant and click the green `+ Add Extension`.\n- Select the model and fill in the credentials.\n- Use the `Test` Button to check that it works and `save`.\n\nNow you can return to the [chat page](http://localhost:3333/chat) (click on `c4 GenAI Suite` in the top left) and start a new conversation with your new assistant.\n\n> [!TIP]\n> Our `docker-compose` includes a local Ollama, which runs on the CPU. You can use this for quick testing. But it will be slow and you probably want to use another model. If you want to use it, just create the following model extension in your Assistant.\n> * Extension: `Dev: Ollama`\n> * Endpoint: `http://ollama:11434`\n> * Model: `llama3.2`\n",
    "ContentSha": "bLa8wjqGffqM6ThWqBWA15QpuTagUJbPEAqi4FPI4X4=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## Aan de slag\n\n### Gebruik van Docker-Compose\n\n- Voer `docker compose up` uit in de hoofdmap van het project.\n- Open de [applicatie](http://localhost:3333) in een browser. De standaard inloggegevens zijn gebruiker `admin@example.com` en wachtwoord `secret`.\n\n![video die assistentconfiguratie toont](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/assistants.webp)\n\n### Gebruik van Helm & Kubernetes\n\nVoor implementatie in Kubernetes-omgevingen, raadpleeg de [README van onze Helm Chart](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/./helm-chart/README.md).\n\n### Assistenten en Extensies instellen\n\nDe c4 GenAI Suite draait om *assistenten*.\nElke assistent bestaat uit een set extensies, die bepalen welk LLM-model en welke tools hij kan gebruiken.\n\n- Ga in het beheerdersgedeelte (klik op de gebruikersnaam linksonder) naar de [assistenten sectie](http://localhost:3333/admin/assistants).\n- Voeg een assistent toe met de groene `+` knop naast de sectietitel. Kies een naam en een beschrijving.\n- Selecteer de aangemaakte assistent en klik op de groene `+ Extensie toevoegen`.\n- Selecteer het model en vul de inloggegevens in.\n- Gebruik de knop `Test` om te controleren of alles werkt en `opslaan`.\n\nNu kun je terugkeren naar de [chatpagina](http://localhost:3333/chat) (klik op `c4 GenAI Suite` linksboven) en een nieuw gesprek starten met je nieuwe assistent.\n\n> [!TIP]\n> Onze `docker-compose` bevat een lokale Ollama, die op de CPU draait. Je kunt deze gebruiken voor snel testen. Maar het zal traag zijn en je wilt waarschijnlijk een ander model gebruiken. Als je het wilt gebruiken, maak dan gewoon de volgende modelextensie aan in je Assistent.\n> * Extensie: `Dev: Ollama`\n> * Endpoint: `http://ollama:11434`\n> * Model: `llama3.2`",
    "Status": "ok"
  },
  {
    "Id": 4,
    "Content": "### Model Context Protocol (MCP) [optional]\n\nUse any MCP server offering an `sse` interface with the `MCP Tools` Extension (or use our `mcp-tool-as-server` as a proxy in front of an `stdio` MCP server).\nEach MCP server can be configured in detail as an extension.\n\n### Retrieval Augmented Generation (RAG) / File Search [optional]\n\nUse our RAG server `REI-S` to search user provided files. Just configure a `Search Files` extension for the assistant.\nThis process is described in detail in [the `services/reis` subdirectory](services/reis/#example-configuration-in-c4).\n\n## Contributing & Development\n\n* See [CONTRIBUTING.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/CONTRIBUTING.md) for guidelines on how to contribute.\n* For developer onboarding, check [DEVELOPERS.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/DEVELOPERS.md).\n\n## Main Building Blocks\n\nThe application consists of a **Frontend** , a **Backend**  and a **REI-S**  service.\n\n```",
    "ContentSha": "X5WCsc3MMbiH/lsJf83tTEO6Cfl/16Nw2tPJwgCKYpc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### Model Context Protocol (MCP) [optioneel]\n\nGebruik elke MCP-server die een `sse`-interface aanbiedt met de `MCP Tools`-extensie (of gebruik onze `mcp-tool-as-server` als een proxy voor een `stdio` MCP-server).\nElke MCP-server kan in detail worden geconfigureerd als een extensie.\n\n### Retrieval Augmented Generation (RAG) / Bestanden Zoeken [optioneel]\n\nGebruik onze RAG-server `REI-S` om door door de gebruiker aangeleverde bestanden te zoeken. Configureer hiervoor eenvoudig een `Bestanden Zoeken`-extensie voor de assistent.\nDit proces wordt in detail beschreven in [de submap `services/reis`](services/reis/#example-configuration-in-c4).\n\n## Bijdragen & Ontwikkeling\n\n* Zie [CONTRIBUTING.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/CONTRIBUTING.md) voor richtlijnen over hoe je kunt bijdragen.\n* Voor onboarding van ontwikkelaars, bekijk [DEVELOPERS.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/DEVELOPERS.md).\n\n## Belangrijkste Bouwstenen\n\nDe applicatie bestaat uit een **Frontend**, een **Backend** en een **REI-S**-service.\n\n```",
    "Status": "ok"
  },
  {
    "Id": 5,
    "Content": "┌──────────┐\n│   User   │\n└─────┬────┘\n      │ access\n      ▼\n┌──────────┐\n│ Frontend │\n└─────┬────┘\n      │ access\n      ▼\n┌──────────┐     ┌─────────────────┐\n│ Backend  │────►│      LLM        │\n└─────┬────┘     └─────────────────┘\n      │ access\n      ▼\n┌──────────┐     ┌─────────────────┐\n│  REI-S   │────►│ Embedding Model │\n│          │     └─────────────────┘\n│          │\n│          │     ┌─────────────────┐\n│          │────►│  Vector Store   │\n└──────────┘     └─────────────────┘\n```",
    "ContentSha": "IO09RXKALRyYzMSXIVzGJTmyROh56hbqbTTI4NQfxtQ=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "┌──────────┐\n│   Gebruiker  │\n└─────┬────┘\n      │ toegang\n      ▼\n┌──────────┐\n│ Frontend │\n└─────┬────┘\n      │ toegang\n      ▼\n┌──────────┐     ┌─────────────────┐\n│ Backend  │────►│      LLM        │\n└─────┬────┘     └─────────────────┘\n      │ toegang\n      ▼\n┌──────────┐     ┌─────────────────┐\n│  REI-S   │────►│ Embedding Model │\n│          │     └─────────────────┘\n│          │\n│          │     ┌─────────────────┐\n│          │────►│  Vector Store   │\n└──────────┘     └─────────────────┘\n```",
    "Status": "ok"
  },
  {
    "Id": 6,
    "Content": "\n\n### Frontend\n\nThe frontend is built with React and TypeScript, providing a user-friendly interface for interacting with the backend and REI-S service. It includes features for managing assistants, extensions, and chat functionalities.\n\n> Sources: `/frontend`\n\n### Backend\n\nThe backend is developed using NestJS and TypeScript, serving as the main API layer for the application. It handles requests from the frontend and interacts with llm providers to facilitate chat functionalities. The backend also manages assistants and their extensions, allowing users to configure and use various AI models for their chats.\n\nAdditionally, the backend manages user authentication, and communicates with the REI-S service for file indexing and retrieval.\n\nFor data persistence, the backend uses a **PostgreSQL** database.\n\n> Sources: `/backend`\n\n### REI-S\n\nThe REI-S (**R**etrieval **E**xtraction **I**ngestion **S**erver) is a Python-based server that provides basic RAG (Retrieval-Augmented Generation) capabilities. It allows for file content extraction, indexing and querying, enabling the application to handle large datasets efficiently. The REI-S service is designed to work seamlessly with the backend, providing necessary data for chat functionalities and file searches.\n\nThe REI-S supports Azure AI Search and pgvector for vector storage, allowing for flexible and scalable data retrieval options. The service can be configured using environment variables to specify the type of vector store and connection details.\n\n> Sources: `/services/reis`\n",
    "ContentSha": "Sceh4YX3ZYIBDEtdTMWIBUucYWH+hEUJMben1cxfrAc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### Frontend\n\nDe frontend is gebouwd met React en TypeScript en biedt een gebruiksvriendelijke interface voor interactie met de backend en de REI-S service. Het bevat functionaliteiten voor het beheren van assistenten, extensies en chatfuncties.\n\n> Bronnen: `/frontend`\n\n### Backend\n\nDe backend is ontwikkeld met NestJS en TypeScript en dient als de belangrijkste API-laag van de applicatie. Het verwerkt verzoeken van de frontend en communiceert met llm-providers om chatfuncties mogelijk te maken. De backend beheert ook assistenten en hun extensies, waardoor gebruikers verschillende AI-modellen kunnen configureren en gebruiken voor hun chats.\n\nDaarnaast beheert de backend gebruikersauthenticatie en communiceert met de REI-S service voor het indexeren en ophalen van bestanden.\n\nVoor gegevensopslag gebruikt de backend een **PostgreSQL**-database.\n\n> Bronnen: `/backend`\n\n### REI-S\n\nDe REI-S (**R**etrieval **E**xtraction **I**ngestion **S**erver) is een Python-gebaseerde server die basis RAG (Retrieval-Augmented Generation) mogelijkheden biedt. Het maakt het mogelijk om bestandsinhoud te extraheren, indexeren en op te vragen, waardoor de applicatie grote datasets efficiënt kan verwerken. De REI-S service is ontworpen om naadloos samen te werken met de backend en voorziet in benodigde data voor chatfuncties en bestandszoekopdrachten.\n\nDe REI-S ondersteunt Azure AI Search en pgvector voor vectoropslag, waardoor flexibele en schaalbare dataterugvindopties mogelijk zijn. De service kan worden geconfigureerd met omgevingsvariabelen om het type vectorstore en verbindingsgegevens te specificeren.\n\n> Bronnen: `/services/reis`\n",
    "Status": "ok"
  }
]