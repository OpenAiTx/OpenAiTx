[
  {
    "Id": 1,
    "Content": "# c4 GenAI Suite\n\nAn AI chatbot application with Model Context Provider (MCP) integration, powered by Langchain and compatibility for all major Large Language Models (LLMs) and Embedding Models.\n\nAdministrators can create assistants with different capabilities by adding extensions, such as RAG (Retrieval-Augmented Generation) services or MCP servers. The application is built using a modern tech stack, including React, NestJS, and Python FastAPI for the REI-S service.\n\nUsers can interact with assistants through a user-friendly interface. Depending on the assistant's configuration, users may be able to ask questions, upload their own files, or use other features. The assistants interact with various LLM providers to provide responses based on the configured extensions. Contextual information provided by the configured extensions allows the assistants to answer domain-specific questions and provide relevant information.\n\nThe application is designed to be modular and extensible, allowing users to create assistants with different capabilities by adding extensions.\n\n![short demo video of basic usage](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/preview.webp)\n\n## Features\n\n### Large Language Models (LLM) and Multimodal Models\n\nThe c4 GenAI Suite supports already many models directly. And if your preferred model is not supported already, it should be easy to write an extension to support it.\n\n* OpenAI compatible models\n* Azure OpenAI models\n* Bedrock models\n* Google GenAI models\n* Ollama compatible models\n",
    "ContentSha": "VePwjxi+F5ttFJfuckWjhr5ujnZTyNWk2c6pu+FMFPM=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "# c4 GenAI Suite\n\nUne application chatbot IA avec intégration du Fournisseur de Contexte de Modèle (MCP), propulsée par Langchain et compatible avec tous les principaux grands modèles de langage (LLM) et modèles d'embedding.\n\nLes administrateurs peuvent créer des assistants dotés de différentes capacités en ajoutant des extensions, telles que des services RAG (génération augmentée par récupération) ou des serveurs MCP. L'application est construite avec une pile technologique moderne, incluant React, NestJS et Python FastAPI pour le service REI-S.\n\nLes utilisateurs peuvent interagir avec les assistants via une interface conviviale. Selon la configuration de l'assistant, les utilisateurs peuvent poser des questions, télécharger leurs propres fichiers, ou utiliser d'autres fonctionnalités. Les assistants interagissent avec différents fournisseurs de LLM pour fournir des réponses basées sur les extensions configurées. Les informations contextuelles fournies par les extensions configurées permettent aux assistants de répondre à des questions spécifiques à un domaine et de fournir des informations pertinentes.\n\nL'application est conçue pour être modulaire et extensible, permettant aux utilisateurs de créer des assistants dotés de différentes capacités en ajoutant des extensions.\n\n![courte vidéo de démonstration de l'utilisation de base](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/preview.webp)\n\n## Fonctionnalités\n\n### Grands Modèles de Langage (LLM) et Modèles Multimodaux\n\nLa c4 GenAI Suite prend déjà en charge de nombreux modèles directement. Et si votre modèle préféré n'est pas encore pris en charge, il devrait être facile d'écrire une extension pour l'ajouter.\n\n* Modèles compatibles OpenAI\n* Modèles Azure OpenAI\n* Modèles Bedrock\n* Modèles Google GenAI\n* Modèles compatibles Ollama",
    "Status": "ok"
  },
  {
    "Id": 2,
    "Content": "### Retrieval Augmented Generation (RAG)\n\nThe c4 GenAI Suite includes REI-S, a server to prepare files for consumption by the LLM.\n\n* REI-S, a custom integrated RAG server\n  * Vector stores\n    * pgvector\n    * Azure AI Search\n  * Embedding models\n    * OpenAI compatible embeddings\n    * Azure OpenAI embeddings\n    * Ollama compatible embeddings\n  * File formats:\n    * pdf, docx, pptx, xlsx, ...\n    * audio file voice transcription (via Whisper)\n\n### Extensions\n\nThe c4 GenAI Suite is designed for extensibility. Writing extensions is easy, as is using an already existing MCP server.\n\n* Model Context Protocol (MCP) servers\n* Custom systemprompt\n* Bing Search\n* Calculator\n\n",
    "ContentSha": "wvT4e6071qGfJDWClvklgUJiJRM71gUVIUQKaaDgVPo=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### Génération Augmentée par la Recherche (RAG)\n\nLa suite c4 GenAI comprend REI-S, un serveur pour préparer les fichiers à la consommation par le LLM.\n\n* REI-S, un serveur RAG intégré et personnalisé\n  * Stockages vectoriels\n    * pgvector\n    * Azure AI Search\n  * Modèles d’embedding\n    * Embeddings compatibles OpenAI\n    * Embeddings Azure OpenAI\n    * Embeddings compatibles Ollama\n  * Formats de fichiers :\n    * pdf, docx, pptx, xlsx, ...\n    * Transcription vocale de fichiers audio (via Whisper)\n\n### Extensions\n\nLa suite c4 GenAI est conçue pour être extensible. Écrire des extensions est facile, tout comme utiliser un serveur MCP déjà existant.\n\n* Serveurs Model Context Protocol (MCP)\n* Systemprompt personnalisé\n* Recherche Bing\n* Calculatrice",
    "Status": "ok"
  },
  {
    "Id": 3,
    "Content": "## Getting Started\n\n### Using Docker-Compose\n\n- Run `docker compose up` in the project root.\n- Open the [application](http://localhost:3333) in a browser. The default login credentials are user `admin@example.com` and password `secret`.\n\n![video showing assistant configuration](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/assistants.webp)\n\n### Using Helm & Kubernetes\n\nFor deployment in Kubernetes environments, please refer to the [README of our Helm Chart](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/./helm-chart/README.md).\n\n### Setting up Assistants and Extensions\n\nThe c4 GenAI Suite revolves around *assistants*.\nEach assistant consists of a set of extensions, which determine the LLM model and which tools it can use.\n\n- In the admin area (click the username on the bottom left), go to the [assistants section](http://localhost:3333/admin/assistants).\n- Add an assistant with the green `+` button next to the section title. Choose a name and a description.\n- Select the created assistant and click the green `+ Add Extension`.\n- Select the model and fill in the credentials.\n- Use the `Test` Button to check that it works and `save`.\n\nNow you can return to the [chat page](http://localhost:3333/chat) (click on `c4 GenAI Suite` in the top left) and start a new conversation with your new assistant.\n\n> [!TIP]\n> Our `docker-compose` includes a local Ollama, which runs on the CPU. You can use this for quick testing. But it will be slow and you probably want to use another model. If you want to use it, just create the following model extension in your Assistant.\n> * Extension: `Dev: Ollama`\n> * Endpoint: `http://ollama:11434`\n> * Model: `llama3.2`\n",
    "ContentSha": "bLa8wjqGffqM6ThWqBWA15QpuTagUJbPEAqi4FPI4X4=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## Démarrage\n\n### Utilisation de Docker-Compose\n\n- Exécutez `docker compose up` à la racine du projet.\n- Ouvrez l'[application](http://localhost:3333) dans un navigateur. Les identifiants de connexion par défaut sont utilisateur `admin@example.com` et mot de passe `secret`.\n\n![vidéo montrant la configuration de l’assistant](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/assistants.webp)\n\n### Utilisation de Helm & Kubernetes\n\nPour le déploiement dans des environnements Kubernetes, veuillez consulter le [README de notre Helm Chart](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/./helm-chart/README.md).\n\n### Configuration des assistants et des extensions\n\nLa suite c4 GenAI s'articule autour des *assistants*.\nChaque assistant se compose d'un ensemble d'extensions, qui déterminent le modèle LLM et les outils qu'il peut utiliser.\n\n- Dans la zone d'administration (cliquez sur le nom d'utilisateur en bas à gauche), allez dans la [section assistants](http://localhost:3333/admin/assistants).\n- Ajoutez un assistant avec le bouton vert `+` à côté du titre de la section. Choisissez un nom et une description.\n- Sélectionnez l’assistant créé et cliquez sur le bouton vert `+ Ajouter une extension`.\n- Sélectionnez le modèle et renseignez les identifiants.\n- Utilisez le bouton `Tester` pour vérifier que cela fonctionne et `enregistrez`.\n\nVous pouvez maintenant retourner sur la [page de chat](http://localhost:3333/chat) (cliquez sur `c4 GenAI Suite` en haut à gauche) et démarrer une nouvelle conversation avec votre nouvel assistant.\n\n> [!ASTUCE]\n> Notre `docker-compose` inclut un Ollama local, qui fonctionne sur le CPU. Vous pouvez l'utiliser pour des tests rapides. Mais il sera lent et vous voudrez probablement utiliser un autre modèle. Si vous souhaitez l'utiliser, créez simplement l'extension de modèle suivante dans votre assistant.\n> * Extension : `Dev: Ollama`\n> * Endpoint : `http://ollama:11434`\n> * Modèle : `llama3.2`\n",
    "Status": "ok"
  },
  {
    "Id": 4,
    "Content": "### Model Context Protocol (MCP) [optional]\n\nUse any MCP server offering an `sse` interface with the `MCP Tools` Extension (or use our `mcp-tool-as-server` as a proxy in front of an `stdio` MCP server).\nEach MCP server can be configured in detail as an extension.\n\n### Retrieval Augmented Generation (RAG) / File Search [optional]\n\nUse our RAG server `REI-S` to search user provided files. Just configure a `Search Files` extension for the assistant.\nThis process is described in detail in [the `services/reis` subdirectory](services/reis/#example-configuration-in-c4).\n\n## Contributing & Development\n\n* See [CONTRIBUTING.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/CONTRIBUTING.md) for guidelines on how to contribute.\n* For developer onboarding, check [DEVELOPERS.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/DEVELOPERS.md).\n\n## Main Building Blocks\n\nThe application consists of a **Frontend** , a **Backend**  and a **REI-S**  service.\n\n```",
    "ContentSha": "X5WCsc3MMbiH/lsJf83tTEO6Cfl/16Nw2tPJwgCKYpc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### Protocole de Contexte de Modèle (MCP) [optionnel]\n\nUtilisez n'importe quel serveur MCP offrant une interface `sse` avec l'extension `MCP Tools` (ou utilisez notre `mcp-tool-as-server` comme proxy devant un serveur MCP `stdio`).\nChaque serveur MCP peut être configuré en détail en tant qu'extension.\n\n### Génération Augmentée par la Recherche (RAG) / Recherche de Fichiers [optionnel]\n\nUtilisez notre serveur RAG `REI-S` pour rechercher dans les fichiers fournis par l'utilisateur. Il suffit de configurer une extension `Search Files` pour l’assistant.\nCe processus est décrit en détail dans [le sous-répertoire `services/reis`](services/reis/#example-configuration-in-c4).\n\n## Contribution & Développement\n\n* Voir [CONTRIBUTING.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/CONTRIBUTING.md) pour les directives de contribution.\n* Pour l'intégration des développeurs, consultez [DEVELOPERS.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/DEVELOPERS.md).\n\n## Principaux Blocs de Construction\n\nL’application se compose d’un **Frontend**, d’un **Backend** et d’un service **REI-S**.\n\n```",
    "Status": "ok"
  },
  {
    "Id": 5,
    "Content": "┌──────────┐\n│   User   │\n└─────┬────┘\n      │ access\n      ▼\n┌──────────┐\n│ Frontend │\n└─────┬────┘\n      │ access\n      ▼\n┌──────────┐     ┌─────────────────┐\n│ Backend  │────►│      LLM        │\n└─────┬────┘     └─────────────────┘\n      │ access\n      ▼\n┌──────────┐     ┌─────────────────┐\n│  REI-S   │────►│ Embedding Model │\n│          │     └─────────────────┘\n│          │\n│          │     ┌─────────────────┐\n│          │────►│  Vector Store   │\n└──────────┘     └─────────────────┘\n```",
    "ContentSha": "IO09RXKALRyYzMSXIVzGJTmyROh56hbqbTTI4NQfxtQ=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "┌──────────┐\n│ Utilisateur │\n└─────┬────┘\n      │ accès\n      ▼\n┌──────────┐\n│ Frontend │\n└─────┬────┘\n      │ accès\n      ▼\n┌──────────┐     ┌─────────────────┐\n│ Backend  │────►│      LLM        │\n└─────┬────┘     └─────────────────┘\n      │ accès\n      ▼\n┌──────────┐     ┌─────────────────┐\n│  REI-S   │────►│ Modèle d’Embedding │\n│          │     └─────────────────┘\n│          │\n│          │     ┌─────────────────┐\n│          │────►│  Vector Store   │\n└──────────┘     └─────────────────┘\n```",
    "Status": "ok"
  },
  {
    "Id": 6,
    "Content": "\n\n### Frontend\n\nThe frontend is built with React and TypeScript, providing a user-friendly interface for interacting with the backend and REI-S service. It includes features for managing assistants, extensions, and chat functionalities.\n\n> Sources: `/frontend`\n\n### Backend\n\nThe backend is developed using NestJS and TypeScript, serving as the main API layer for the application. It handles requests from the frontend and interacts with llm providers to facilitate chat functionalities. The backend also manages assistants and their extensions, allowing users to configure and use various AI models for their chats.\n\nAdditionally, the backend manages user authentication, and communicates with the REI-S service for file indexing and retrieval.\n\nFor data persistence, the backend uses a **PostgreSQL** database.\n\n> Sources: `/backend`\n\n### REI-S\n\nThe REI-S (**R**etrieval **E**xtraction **I**ngestion **S**erver) is a Python-based server that provides basic RAG (Retrieval-Augmented Generation) capabilities. It allows for file content extraction, indexing and querying, enabling the application to handle large datasets efficiently. The REI-S service is designed to work seamlessly with the backend, providing necessary data for chat functionalities and file searches.\n\nThe REI-S supports Azure AI Search and pgvector for vector storage, allowing for flexible and scalable data retrieval options. The service can be configured using environment variables to specify the type of vector store and connection details.\n\n> Sources: `/services/reis`\n",
    "ContentSha": "Sceh4YX3ZYIBDEtdTMWIBUucYWH+hEUJMben1cxfrAc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### Frontend\n\nLe frontend est construit avec React et TypeScript, offrant une interface conviviale pour interagir avec le backend et le service REI-S. Il inclut des fonctionnalités pour gérer les assistants, les extensions et les fonctions de chat.\n\n> Sources : `/frontend`\n\n### Backend\n\nLe backend est développé avec NestJS et TypeScript, servant de couche principale d’API pour l’application. Il gère les requêtes du frontend et interagit avec les fournisseurs llm pour faciliter les fonctionnalités de chat. Le backend gère également les assistants et leurs extensions, permettant aux utilisateurs de configurer et d’utiliser divers modèles d’IA pour leurs discussions.\n\nDe plus, le backend gère l’authentification des utilisateurs et communique avec le service REI-S pour l’indexation et la récupération de fichiers.\n\nPour la persistance des données, le backend utilise une base de données **PostgreSQL**.\n\n> Sources : `/backend`\n\n### REI-S\n\nLe REI-S (**R**etrieval **E**xtraction **I**ngestion **S**erver) est un serveur basé sur Python qui fournit des capacités de base RAG (Retrieval-Augmented Generation). Il permet l’extraction de contenu de fichiers, l’indexation et l’interrogation, permettant à l’application de gérer efficacement de grands ensembles de données. Le service REI-S est conçu pour fonctionner de manière transparente avec le backend, fournissant les données nécessaires pour les fonctionnalités de chat et les recherches de fichiers.\n\nLe REI-S prend en charge Azure AI Search et pgvector pour le stockage vectoriel, offrant des options de récupération de données flexibles et évolutives. Le service peut être configuré à l’aide de variables d’environnement pour spécifier le type de stockage vectoriel et les détails de connexion.\n\n> Sources : `/services/reis`",
    "Status": "ok"
  }
]