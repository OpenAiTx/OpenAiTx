[
  {
    "Id": 1,
    "Content": "# c4 GenAI Suite\n\nAn AI chatbot application with Model Context Provider (MCP) integration, powered by Langchain and compatibility for all major Large Language Models (LLMs) and Embedding Models.\n\nAdministrators can create assistants with different capabilities by adding extensions, such as RAG (Retrieval-Augmented Generation) services or MCP servers. The application is built using a modern tech stack, including React, NestJS, and Python FastAPI for the REI-S service.\n\nUsers can interact with assistants through a user-friendly interface. Depending on the assistant's configuration, users may be able to ask questions, upload their own files, or use other features. The assistants interact with various LLM providers to provide responses based on the configured extensions. Contextual information provided by the configured extensions allows the assistants to answer domain-specific questions and provide relevant information.\n\nThe application is designed to be modular and extensible, allowing users to create assistants with different capabilities by adding extensions.\n\n![short demo video of basic usage](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/preview.webp)\n\n## Features\n\n### Large Language Models (LLM) and Multimodal Models\n\nThe c4 GenAI Suite supports already many models directly. And if your preferred model is not supported already, it should be easy to write an extension to support it.\n\n* OpenAI compatible models\n* Azure OpenAI models\n* Bedrock models\n* Google GenAI models\n* Ollama compatible models\n",
    "ContentSha": "VePwjxi+F5ttFJfuckWjhr5ujnZTyNWk2c6pu+FMFPM=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "# c4 GenAI Suite\n\nUn'applicazione chatbot AI con integrazione Model Context Provider (MCP), alimentata da Langchain e compatibile con tutti i principali Large Language Models (LLM) e modelli di Embedding.\n\nGli amministratori possono creare assistenti con diverse capacità aggiungendo estensioni, come servizi RAG (Retrieval-Augmented Generation) o server MCP. L'applicazione è costruita utilizzando uno stack tecnologico moderno, tra cui React, NestJS e Python FastAPI per il servizio REI-S.\n\nGli utenti possono interagire con gli assistenti tramite un'interfaccia intuitiva. A seconda della configurazione dell'assistente, gli utenti possono porre domande, caricare i propri file o utilizzare altre funzionalità. Gli assistenti interagiscono con vari provider LLM per fornire risposte basate sulle estensioni configurate. Le informazioni contestuali fornite dalle estensioni configurate consentono agli assistenti di rispondere a domande specifiche del dominio e fornire informazioni rilevanti.\n\nL'applicazione è progettata per essere modulare ed estensibile, consentendo agli utenti di creare assistenti con diverse capacità aggiungendo estensioni.\n\n![short demo video of basic usage](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/preview.webp)\n\n## Funzionalità\n\n### Large Language Models (LLM) e modelli multimodali\n\nLa c4 GenAI Suite supporta già molti modelli direttamente. E se il modello preferito non è ancora supportato, dovrebbe essere semplice scrivere un'estensione per aggiungere il supporto.\n\n* Modelli compatibili con OpenAI\n* Modelli Azure OpenAI\n* Modelli Bedrock\n* Modelli Google GenAI\n* Modelli compatibili con Ollama",
    "Status": "ok"
  },
  {
    "Id": 2,
    "Content": "### Retrieval Augmented Generation (RAG)\n\nThe c4 GenAI Suite includes REI-S, a server to prepare files for consumption by the LLM.\n\n* REI-S, a custom integrated RAG server\n  * Vector stores\n    * pgvector\n    * Azure AI Search\n  * Embedding models\n    * OpenAI compatible embeddings\n    * Azure OpenAI embeddings\n    * Ollama compatible embeddings\n  * File formats:\n    * pdf, docx, pptx, xlsx, ...\n    * audio file voice transcription (via Whisper)\n\n### Extensions\n\nThe c4 GenAI Suite is designed for extensibility. Writing extensions is easy, as is using an already existing MCP server.\n\n* Model Context Protocol (MCP) servers\n* Custom systemprompt\n* Bing Search\n* Calculator\n\n",
    "ContentSha": "wvT4e6071qGfJDWClvklgUJiJRM71gUVIUQKaaDgVPo=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### Retrieval Augmented Generation (RAG)\n\nLa c4 GenAI Suite include REI-S, un server per preparare i file per il consumo da parte dell’LLM.\n\n* REI-S, un server RAG integrato personalizzato\n  * Vector stores\n    * pgvector\n    * Azure AI Search\n  * Modelli di embedding\n    * Embedding compatibili con OpenAI\n    * Embedding Azure OpenAI\n    * Embedding compatibili con Ollama\n  * Formati di file:\n    * pdf, docx, pptx, xlsx, ...\n    * trascrizione vocale di file audio (tramite Whisper)\n\n### Estensioni\n\nLa c4 GenAI Suite è progettata per l’estensibilità. Scrivere estensioni è semplice, così come utilizzare un server MCP già esistente.\n\n* Server Model Context Protocol (MCP)\n* Systemprompt personalizzato\n* Bing Search\n* Calcolatrice",
    "Status": "ok"
  },
  {
    "Id": 3,
    "Content": "## Getting Started\n\n### Using Docker-Compose\n\n- Run `docker compose up` in the project root.\n- Open the [application](http://localhost:3333) in a browser. The default login credentials are user `admin@example.com` and password `secret`.\n\n![video showing assistant configuration](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/assistants.webp)\n\n### Using Helm & Kubernetes\n\nFor deployment in Kubernetes environments, please refer to the [README of our Helm Chart](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/./helm-chart/README.md).\n\n### Setting up Assistants and Extensions\n\nThe c4 GenAI Suite revolves around *assistants*.\nEach assistant consists of a set of extensions, which determine the LLM model and which tools it can use.\n\n- In the admin area (click the username on the bottom left), go to the [assistants section](http://localhost:3333/admin/assistants).\n- Add an assistant with the green `+` button next to the section title. Choose a name and a description.\n- Select the created assistant and click the green `+ Add Extension`.\n- Select the model and fill in the credentials.\n- Use the `Test` Button to check that it works and `save`.\n\nNow you can return to the [chat page](http://localhost:3333/chat) (click on `c4 GenAI Suite` in the top left) and start a new conversation with your new assistant.\n\n> [!TIP]\n> Our `docker-compose` includes a local Ollama, which runs on the CPU. You can use this for quick testing. But it will be slow and you probably want to use another model. If you want to use it, just create the following model extension in your Assistant.\n> * Extension: `Dev: Ollama`\n> * Endpoint: `http://ollama:11434`\n> * Model: `llama3.2`\n",
    "ContentSha": "bLa8wjqGffqM6ThWqBWA15QpuTagUJbPEAqi4FPI4X4=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## Per Iniziare\n\n### Utilizzo di Docker-Compose\n\n- Esegui `docker compose up` nella directory principale del progetto.\n- Apri l'[applicazione](http://localhost:3333) in un browser. Le credenziali di accesso predefinite sono utente `admin@example.com` e password `secret`.\n\n![video che mostra la configurazione dell'assistente](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/assistants.webp)\n\n### Utilizzo di Helm & Kubernetes\n\nPer il deployment in ambienti Kubernetes, fare riferimento al [README del nostro Helm Chart](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/./helm-chart/README.md).\n\n### Configurazione di Assistenti ed Estensioni\n\nLa c4 GenAI Suite ruota intorno agli *assistenti*.\nOgni assistente è composto da un set di estensioni, che determinano il modello LLM e quali strumenti può utilizzare.\n\n- Nell'area amministrativa (clicca sul nome utente in basso a sinistra), vai alla [sezione assistenti](http://localhost:3333/admin/assistants).\n- Aggiungi un assistente con il pulsante verde `+` accanto al titolo della sezione. Scegli un nome e una descrizione.\n- Seleziona l'assistente creato e clicca sul verde `+ Aggiungi Estensione`.\n- Seleziona il modello e inserisci le credenziali.\n- Utilizza il pulsante `Test` per verificare che funzioni e `salva`.\n\nOra puoi tornare alla [pagina chat](http://localhost:3333/chat) (clicca su `c4 GenAI Suite` in alto a sinistra) e iniziare una nuova conversazione con il tuo nuovo assistente.\n\n> [!TIP]\n> Il nostro `docker-compose` include un Ollama locale, che gira sulla CPU. Puoi usarlo per test veloci. Tuttavia sarà lento e probabilmente vorrai usare un altro modello. Se vuoi usarlo, crea semplicemente la seguente estensione modello nel tuo Assistente.\n> * Estensione: `Dev: Ollama`\n> * Endpoint: `http://ollama:11434`\n> * Modello: `llama3.2`\n",
    "Status": "ok"
  },
  {
    "Id": 4,
    "Content": "### Model Context Protocol (MCP) [optional]\n\nUse any MCP server offering an `sse` interface with the `MCP Tools` Extension (or use our `mcp-tool-as-server` as a proxy in front of an `stdio` MCP server).\nEach MCP server can be configured in detail as an extension.\n\n### Retrieval Augmented Generation (RAG) / File Search [optional]\n\nUse our RAG server `REI-S` to search user provided files. Just configure a `Search Files` extension for the assistant.\nThis process is described in detail in [the `services/reis` subdirectory](services/reis/#example-configuration-in-c4).\n\n## Contributing & Development\n\n* See [CONTRIBUTING.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/CONTRIBUTING.md) for guidelines on how to contribute.\n* For developer onboarding, check [DEVELOPERS.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/DEVELOPERS.md).\n\n## Main Building Blocks\n\nThe application consists of a **Frontend** , a **Backend**  and a **REI-S**  service.\n\n```",
    "ContentSha": "X5WCsc3MMbiH/lsJf83tTEO6Cfl/16Nw2tPJwgCKYpc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### Protocollo Model Context (MCP) [opzionale]\n\nUtilizza qualsiasi server MCP che offra un'interfaccia `sse` con l'Estensione `MCP Tools` (oppure utilizza il nostro `mcp-tool-as-server` come proxy davanti a un server MCP `stdio`).\nOgni server MCP può essere configurato in dettaglio come estensione.\n\n### Retrieval Augmented Generation (RAG) / Ricerca File [opzionale]\n\nUtilizza il nostro server RAG `REI-S` per cercare nei file forniti dall’utente. È sufficiente configurare un’estensione `Search Files` per l’assistente.\nQuesto processo è descritto in dettaglio nella [sottodirectory `services/reis`](services/reis/#example-configuration-in-c4).\n\n## Contribuire & Sviluppo\n\n* Consulta [CONTRIBUTING.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/CONTRIBUTING.md) per le linee guida su come contribuire.\n* Per l’onboarding degli sviluppatori, consulta [DEVELOPERS.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/DEVELOPERS.md).\n\n## Principali Componenti\n\nL’applicazione è composta da un **Frontend**, un **Backend** e un servizio **REI-S**.\n\n```",
    "Status": "ok"
  },
  {
    "Id": 5,
    "Content": "┌──────────┐\n│   User   │\n└─────┬────┘\n      │ access\n      ▼\n┌──────────┐\n│ Frontend │\n└─────┬────┘\n      │ access\n      ▼\n┌──────────┐     ┌─────────────────┐\n│ Backend  │────►│      LLM        │\n└─────┬────┘     └─────────────────┘\n      │ access\n      ▼\n┌──────────┐     ┌─────────────────┐\n│  REI-S   │────►│ Embedding Model │\n│          │     └─────────────────┘\n│          │\n│          │     ┌─────────────────┐\n│          │────►│  Vector Store   │\n└──────────┘     └─────────────────┘\n```",
    "ContentSha": "IO09RXKALRyYzMSXIVzGJTmyROh56hbqbTTI4NQfxtQ=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "┌──────────┐\n│   Utente │\n└─────┬────┘\n      │ accesso\n      ▼\n┌──────────┐\n│ Frontend │\n└─────┬────┘\n      │ accesso\n      ▼\n┌──────────┐     ┌──────────────────┐\n│ Backend  │────►│      LLM         │\n└─────┬────┘     └──────────────────┘\n      │ accesso\n      ▼\n┌──────────┐     ┌──────────────────┐\n│  REI-S   │────►│ Modello di       │\n│          │     │ Embedding        │\n│          │     └──────────────────┘\n│          │\n│          │     ┌──────────────────┐\n│          │────►│ Vector Store     │\n└──────────┘     └──────────────────┘\n```",
    "Status": "ok"
  },
  {
    "Id": 6,
    "Content": "\n\n### Frontend\n\nThe frontend is built with React and TypeScript, providing a user-friendly interface for interacting with the backend and REI-S service. It includes features for managing assistants, extensions, and chat functionalities.\n\n> Sources: `/frontend`\n\n### Backend\n\nThe backend is developed using NestJS and TypeScript, serving as the main API layer for the application. It handles requests from the frontend and interacts with llm providers to facilitate chat functionalities. The backend also manages assistants and their extensions, allowing users to configure and use various AI models for their chats.\n\nAdditionally, the backend manages user authentication, and communicates with the REI-S service for file indexing and retrieval.\n\nFor data persistence, the backend uses a **PostgreSQL** database.\n\n> Sources: `/backend`\n\n### REI-S\n\nThe REI-S (**R**etrieval **E**xtraction **I**ngestion **S**erver) is a Python-based server that provides basic RAG (Retrieval-Augmented Generation) capabilities. It allows for file content extraction, indexing and querying, enabling the application to handle large datasets efficiently. The REI-S service is designed to work seamlessly with the backend, providing necessary data for chat functionalities and file searches.\n\nThe REI-S supports Azure AI Search and pgvector for vector storage, allowing for flexible and scalable data retrieval options. The service can be configured using environment variables to specify the type of vector store and connection details.\n\n> Sources: `/services/reis`\n",
    "ContentSha": "Sceh4YX3ZYIBDEtdTMWIBUucYWH+hEUJMben1cxfrAc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### Frontend\n\nIl frontend è sviluppato con React e TypeScript, offrendo un'interfaccia utente intuitiva per interagire con il backend e il servizio REI-S. Include funzionalità per la gestione degli assistenti, delle estensioni e delle funzionalità di chat.\n\n> Fonti: `/frontend`\n\n### Backend\n\nIl backend è sviluppato utilizzando NestJS e TypeScript, fungendo da principale livello API per l'applicazione. Gestisce le richieste provenienti dal frontend e interagisce con i provider llm per facilitare le funzionalità di chat. Il backend gestisce inoltre assistenti e le loro estensioni, permettendo agli utenti di configurare e utilizzare vari modelli AI per le loro chat.\n\nInoltre, il backend gestisce l'autenticazione degli utenti e comunica con il servizio REI-S per l'indicizzazione e il recupero dei file.\n\nPer la persistenza dei dati, il backend utilizza un database **PostgreSQL**.\n\n> Fonti: `/backend`\n\n### REI-S\n\nIl REI-S (**R**etrieval **E**xtraction **I**ngestion **S**erver) è un server basato su Python che fornisce funzionalità di base RAG (Retrieval-Augmented Generation). Consente l’estrazione, l’indicizzazione e l’interrogazione dei contenuti dei file, permettendo all’applicazione di gestire grandi dataset in modo efficiente. Il servizio REI-S è progettato per funzionare in modo integrato con il backend, fornendo i dati necessari per le funzionalità di chat e la ricerca dei file.\n\nIl REI-S supporta Azure AI Search e pgvector per l’archiviazione vettoriale, offrendo opzioni di recupero dati flessibili e scalabili. Il servizio può essere configurato tramite variabili d'ambiente per specificare il tipo di archivio vettoriale e i dettagli di connessione.\n\n> Fonti: `/services/reis`",
    "Status": "ok"
  }
]