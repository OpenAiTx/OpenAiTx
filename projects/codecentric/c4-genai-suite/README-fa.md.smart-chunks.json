[
  {
    "Id": 1,
    "Content": "# c4 GenAI Suite\n\nAn AI chatbot application with Model Context Provider (MCP) integration, powered by Langchain and compatibility for all major Large Language Models (LLMs) and Embedding Models.\n\nAdministrators can create assistants with different capabilities by adding extensions, such as RAG (Retrieval-Augmented Generation) services or MCP servers. The application is built using a modern tech stack, including React, NestJS, and Python FastAPI for the REI-S service.\n\nUsers can interact with assistants through a user-friendly interface. Depending on the assistant's configuration, users may be able to ask questions, upload their own files, or use other features. The assistants interact with various LLM providers to provide responses based on the configured extensions. Contextual information provided by the configured extensions allows the assistants to answer domain-specific questions and provide relevant information.\n\nThe application is designed to be modular and extensible, allowing users to create assistants with different capabilities by adding extensions.\n\n![short demo video of basic usage](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/preview.webp)\n\n## Features\n\n### Large Language Models (LLM) and Multimodal Models\n\nThe c4 GenAI Suite supports already many models directly. And if your preferred model is not supported already, it should be easy to write an extension to support it.\n\n* OpenAI compatible models\n* Azure OpenAI models\n* Bedrock models\n* Google GenAI models\n* Ollama compatible models\n",
    "ContentSha": "VePwjxi+F5ttFJfuckWjhr5ujnZTyNWk2c6pu+FMFPM=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "# c4 GenAI Suite\n\nیک برنامه چت‌بات هوش مصنوعی با ادغام فراهم‌کننده زمینه مدل (MCP)، مبتنی بر Langchain و سازگار با تمامی مدل‌های بزرگ زبانی (LLM) و مدل‌های تعبیه‌سازی.\n\nمدیران می‌توانند با افزودن افزونه‌هایی مانند خدمات RAG (تولید تقویت‌شده با بازیابی) یا سرورهای MCP، دستیارانی با قابلیت‌های مختلف ایجاد کنند. این برنامه با استفاده از فناوری‌های مدرن، از جمله React، NestJS و Python FastAPI برای سرویس REI-S ساخته شده است.\n\nکاربران می‌توانند از طریق یک رابط کاربری آسان با دستیاران تعامل داشته باشند. بسته به پیکربندی دستیار، کاربران ممکن است بتوانند سوال بپرسند، فایل‌های خود را بارگذاری کنند یا از سایر ویژگی‌ها استفاده کنند. دستیاران برای ارائه پاسخ بر اساس افزونه‌های پیکربندی‌شده، با ارائه‌دهندگان مختلف LLM تعامل دارند. اطلاعات زمینه‌ای که توسط افزونه‌های پیکربندی‌شده فراهم می‌شود، به دستیاران اجازه می‌دهد تا به سوالات خاص دامنه پاسخ دهند و اطلاعات مرتبط ارائه دهند.\n\nاین برنامه به صورت ماژولار و قابل توسعه طراحی شده است و به کاربران اجازه می‌دهد با افزودن افزونه‌ها، دستیارانی با قابلیت‌های مختلف ایجاد کنند.\n\n![ویدیو کوتاه پیش‌نمایش از استفاده پایه](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/preview.webp)\n\n## ویژگی‌ها\n\n### مدل‌های بزرگ زبانی (LLM) و مدل‌های چندوجهی\n\nمجموعه c4 GenAI در حال حاضر از بسیاری از مدل‌ها به صورت مستقیم پشتیبانی می‌کند. و اگر مدل مورد علاقه شما در حال حاضر پشتیبانی نمی‌شود، باید به راحتی بتوانید افزونه‌ای برای پشتیبانی از آن بنویسید.\n\n* مدل‌های سازگار با OpenAI\n* مدل‌های Azure OpenAI\n* مدل‌های Bedrock\n* مدل‌های Google GenAI\n* مدل‌های سازگار با Ollama",
    "Status": "ok"
  },
  {
    "Id": 2,
    "Content": "### Retrieval Augmented Generation (RAG)\n\nThe c4 GenAI Suite includes REI-S, a server to prepare files for consumption by the LLM.\n\n* REI-S, a custom integrated RAG server\n  * Vector stores\n    * pgvector\n    * Azure AI Search\n  * Embedding models\n    * OpenAI compatible embeddings\n    * Azure OpenAI embeddings\n    * Ollama compatible embeddings\n  * File formats:\n    * pdf, docx, pptx, xlsx, ...\n    * audio file voice transcription (via Whisper)\n\n### Extensions\n\nThe c4 GenAI Suite is designed for extensibility. Writing extensions is easy, as is using an already existing MCP server.\n\n* Model Context Protocol (MCP) servers\n* Custom systemprompt\n* Bing Search\n* Calculator\n\n",
    "ContentSha": "wvT4e6071qGfJDWClvklgUJiJRM71gUVIUQKaaDgVPo=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### تولید افزوده بر اساس بازیابی (RAG)\n\nپکیج c4 GenAI شامل REI-S است، یک سرور برای آماده‌سازی فایل‌ها جهت استفاده توسط LLM.\n\n* REI-S، یک سرور RAG سفارشی و یکپارچه\n  * پایگاه‌های برداری\n    * pgvector\n    * Azure AI Search\n  * مدل‌های تعبیه‌سازی\n    * تعبیه‌سازی سازگار با OpenAI\n    * تعبیه‌سازی Azure OpenAI\n    * تعبیه‌سازی سازگار با Ollama\n  * قالب‌های فایل:\n    * pdf، docx، pptx، xlsx، ...\n    * رونویسی صدای فایل‌های صوتی (از طریق Whisper)\n\n### افزونه‌ها\n\nپکیج c4 GenAI برای توسعه‌پذیری طراحی شده است. نوشتن افزونه‌ها آسان است، همچنین استفاده از یک سرور MCP از پیش موجود نیز به سادگی انجام می‌شود.\n\n* سرورهای پروتکل زمینه مدل (MCP)\n* systemprompt سفارشی\n* جستجوی Bing\n* ماشین حساب",
    "Status": "ok"
  },
  {
    "Id": 3,
    "Content": "## Getting Started\n\n### Using Docker-Compose\n\n- Run `docker compose up` in the project root.\n- Open the [application](http://localhost:3333) in a browser. The default login credentials are user `admin@example.com` and password `secret`.\n\n![video showing assistant configuration](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/assistants.webp)\n\n### Using Helm & Kubernetes\n\nFor deployment in Kubernetes environments, please refer to the [README of our Helm Chart](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/./helm-chart/README.md).\n\n### Setting up Assistants and Extensions\n\nThe c4 GenAI Suite revolves around *assistants*.\nEach assistant consists of a set of extensions, which determine the LLM model and which tools it can use.\n\n- In the admin area (click the username on the bottom left), go to the [assistants section](http://localhost:3333/admin/assistants).\n- Add an assistant with the green `+` button next to the section title. Choose a name and a description.\n- Select the created assistant and click the green `+ Add Extension`.\n- Select the model and fill in the credentials.\n- Use the `Test` Button to check that it works and `save`.\n\nNow you can return to the [chat page](http://localhost:3333/chat) (click on `c4 GenAI Suite` in the top left) and start a new conversation with your new assistant.\n\n> [!TIP]\n> Our `docker-compose` includes a local Ollama, which runs on the CPU. You can use this for quick testing. But it will be slow and you probably want to use another model. If you want to use it, just create the following model extension in your Assistant.\n> * Extension: `Dev: Ollama`\n> * Endpoint: `http://ollama:11434`\n> * Model: `llama3.2`\n",
    "ContentSha": "bLa8wjqGffqM6ThWqBWA15QpuTagUJbPEAqi4FPI4X4=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## شروع به کار\n\n### استفاده از Docker-Compose\n\n- در ریشه پروژه دستور `docker compose up` را اجرا کنید.\n- [برنامه](http://localhost:3333) را در یک مرورگر باز کنید. اطلاعات ورود پیش‌فرض، کاربر `admin@example.com` و رمز عبور `secret` است.\n\n![ویدیو نمایش تنظیمات دستیار](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/assistants.webp)\n\n### استفاده از Helm و Kubernetes\n\nبرای استقرار در محیط‌های Kubernetes، لطفاً به [فایل README چارت Helm ما](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/./helm-chart/README.md) مراجعه کنید.\n\n### راه‌اندازی دستیارها و افزونه‌ها\n\nمجموعه c4 GenAI حول محور *دستیارها* می‌چرخد.\nهر دستیار شامل مجموعه‌ای از افزونه‌ها است که مدل LLM و ابزارهایی که می‌تواند استفاده کند را تعیین می‌کند.\n\n- در بخش مدیریت (روی نام کاربری در پایین سمت چپ کلیک کنید)، به [بخش دستیارها](http://localhost:3333/admin/assistants) بروید.\n- با دکمه سبز رنگ `+` کنار عنوان بخش، یک دستیار اضافه کنید. یک نام و توضیح انتخاب کنید.\n- دستیار ایجاد شده را انتخاب کرده و روی دکمه سبز `+ افزودن افزونه` کلیک کنید.\n- مدل را انتخاب کرده و اطلاعات دسترسی را وارد کنید.\n- از دکمه `تست` برای بررسی عملکرد استفاده کنید و سپس `ذخیره` را بزنید.\n\nاکنون می‌توانید به [صفحه چت](http://localhost:3333/chat) (روی `c4 GenAI Suite` در بالا سمت چپ کلیک کنید) بازگردید و یک گفتگوی جدید با دستیار خود آغاز کنید.\n\n> [!نکته]\n> `docker-compose` ما شامل یک Ollama محلی است که روی CPU اجرا می‌شود. می‌توانید از آن برای تست سریع استفاده کنید، اما کند خواهد بود و احتمالاً می‌خواهید از مدل دیگری استفاده کنید. اگر می‌خواهید از آن استفاده کنید، کافی است افزونه مدل زیر را در دستیار خود ایجاد کنید.\n> * افزونه: `Dev: Ollama`\n> * نقطه پایانی: `http://ollama:11434`\n> * مدل: `llama3.2`",
    "Status": "ok"
  },
  {
    "Id": 4,
    "Content": "### Model Context Protocol (MCP) [optional]\n\nUse any MCP server offering an `sse` interface with the `MCP Tools` Extension (or use our `mcp-tool-as-server` as a proxy in front of an `stdio` MCP server).\nEach MCP server can be configured in detail as an extension.\n\n### Retrieval Augmented Generation (RAG) / File Search [optional]\n\nUse our RAG server `REI-S` to search user provided files. Just configure a `Search Files` extension for the assistant.\nThis process is described in detail in [the `services/reis` subdirectory](services/reis/#example-configuration-in-c4).\n\n## Contributing & Development\n\n* See [CONTRIBUTING.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/CONTRIBUTING.md) for guidelines on how to contribute.\n* For developer onboarding, check [DEVELOPERS.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/DEVELOPERS.md).\n\n## Main Building Blocks\n\nThe application consists of a **Frontend** , a **Backend**  and a **REI-S**  service.\n\n```",
    "ContentSha": "X5WCsc3MMbiH/lsJf83tTEO6Cfl/16Nw2tPJwgCKYpc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### پروتکل زمینه مدل (MCP) [اختیاری]\n\nاز هر سرور MCP که یک رابط `sse` با افزونه `MCP Tools` ارائه می‌دهد استفاده کنید (یا از `mcp-tool-as-server` ما به عنوان پراکسی در جلوی یک سرور MCP مبتنی بر `stdio` بهره بگیرید).\nهر سرور MCP می‌تواند به صورت جزئی به عنوان یک افزونه پیکربندی شود.\n\n### تولید افزوده با بازیابی (RAG) / جستجوی فایل [اختیاری]\n\nاز سرور RAG ما با نام `REI-S` برای جستجوی فایل‌های ارائه‌شده توسط کاربر استفاده کنید. تنها کافی است یک افزونه `Search Files` برای دستیار پیکربندی نمایید.\nاین فرآیند به طور کامل در [زیرشاخه `services/reis`](services/reis/#example-configuration-in-c4) شرح داده شده است.\n\n## مشارکت و توسعه\n\n* برای راهنمایی‌های مربوط به مشارکت، به [CONTRIBUTING.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/CONTRIBUTING.md) مراجعه کنید.\n* برای آشنایی توسعه‌دهندگان، [DEVELOPERS.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/DEVELOPERS.md) را بررسی کنید.\n\n## اجزای اصلی\n\nاین برنامه از یک **فرانت‌اند**، یک **بک‌اند** و سرویس **REI-S** تشکیل شده است.\n\n```",
    "Status": "ok"
  },
  {
    "Id": 5,
    "Content": "┌──────────┐\n│   User   │\n└─────┬────┘\n      │ access\n      ▼\n┌──────────┐\n│ Frontend │\n└─────┬────┘\n      │ access\n      ▼\n┌──────────┐     ┌─────────────────┐\n│ Backend  │────►│      LLM        │\n└─────┬────┘     └─────────────────┘\n      │ access\n      ▼\n┌──────────┐     ┌─────────────────┐\n│  REI-S   │────►│ Embedding Model │\n│          │     └─────────────────┘\n│          │\n│          │     ┌─────────────────┐\n│          │────►│  Vector Store   │\n└──────────┘     └─────────────────┘\n```",
    "ContentSha": "IO09RXKALRyYzMSXIVzGJTmyROh56hbqbTTI4NQfxtQ=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "┌──────────┐\n│   کاربر   │\n└─────┬────┘\n      │ دسترسی\n      ▼\n┌──────────┐\n│   فرانت‌اند │\n└─────┬────┘\n      │ دسترسی\n      ▼\n┌──────────┐     ┌─────────────────┐\n│  بک‌اند   │────►│      LLM        │\n└─────┬────┘     └─────────────────┘\n      │ دسترسی\n      ▼\n┌──────────┐     ┌─────────────────┐\n│  REI-S   │────►│ مدل تعبیه‌سازی   │\n│          │     └─────────────────┘\n│          │\n│          │     ┌─────────────────┐\n│          │────►│  ذخیره‌سازی برداری │\n└──────────┘     └─────────────────┘\n```",
    "Status": "ok"
  },
  {
    "Id": 6,
    "Content": "\n\n### Frontend\n\nThe frontend is built with React and TypeScript, providing a user-friendly interface for interacting with the backend and REI-S service. It includes features for managing assistants, extensions, and chat functionalities.\n\n> Sources: `/frontend`\n\n### Backend\n\nThe backend is developed using NestJS and TypeScript, serving as the main API layer for the application. It handles requests from the frontend and interacts with llm providers to facilitate chat functionalities. The backend also manages assistants and their extensions, allowing users to configure and use various AI models for their chats.\n\nAdditionally, the backend manages user authentication, and communicates with the REI-S service for file indexing and retrieval.\n\nFor data persistence, the backend uses a **PostgreSQL** database.\n\n> Sources: `/backend`\n\n### REI-S\n\nThe REI-S (**R**etrieval **E**xtraction **I**ngestion **S**erver) is a Python-based server that provides basic RAG (Retrieval-Augmented Generation) capabilities. It allows for file content extraction, indexing and querying, enabling the application to handle large datasets efficiently. The REI-S service is designed to work seamlessly with the backend, providing necessary data for chat functionalities and file searches.\n\nThe REI-S supports Azure AI Search and pgvector for vector storage, allowing for flexible and scalable data retrieval options. The service can be configured using environment variables to specify the type of vector store and connection details.\n\n> Sources: `/services/reis`\n",
    "ContentSha": "Sceh4YX3ZYIBDEtdTMWIBUucYWH+hEUJMben1cxfrAc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### فرانت‌اند\n\nفرانت‌اند با استفاده از React و TypeScript ساخته شده است و یک رابط کاربری کاربرپسند برای تعامل با بک‌اند و سرویس REI-S فراهم می‌کند. این بخش شامل قابلیت‌هایی برای مدیریت دستیارها، افزونه‌ها و امکانات چت می‌باشد.\n\n> منابع: `/frontend`\n\n### بک‌اند\n\nبک‌اند با استفاده از NestJS و TypeScript توسعه یافته است و به عنوان لایه اصلی API برای برنامه عمل می‌کند. این بخش درخواست‌های فرانت‌اند را مدیریت کرده و با ارائه‌دهندگان llm برای تسهیل قابلیت‌های چت تعامل دارد. بک‌اند همچنین مدیریت دستیارها و افزونه‌های آن‌ها را بر عهده دارد و به کاربران اجازه می‌دهد مدل‌های مختلف هوش مصنوعی را برای چت‌های خود پیکربندی و استفاده کنند.\n\nعلاوه بر این، بک‌اند مدیریت احراز هویت کاربران را انجام داده و با سرویس REI-S برای ایندکس‌گذاری و بازیابی فایل‌ها ارتباط برقرار می‌کند.\n\nبرای ماندگاری داده‌ها، بک‌اند از پایگاه داده **PostgreSQL** استفاده می‌کند.\n\n> منابع: `/backend`\n\n### REI-S\n\nREI-S (**R**etrieval **E**xtraction **I**ngestion **S**erver) یک سرور مبتنی بر پایتون است که قابلیت‌های پایه RAG (تولید افزوده‌شده با بازیابی) را ارائه می‌دهد. این سرویس امکان استخراج محتوا از فایل‌ها، ایندکس‌گذاری و جستجو را فراهم می‌کند و به برنامه اجازه می‌دهد تا مجموعه داده‌های بزرگ را به طور کارآمد مدیریت نماید. سرویس REI-S به گونه‌ای طراحی شده است که به طور یکپارچه با بک‌اند کار کند و داده‌های لازم برای قابلیت‌های چت و جستجوی فایل را فراهم آورد.\n\nREI-S از Azure AI Search و pgvector برای ذخیره‌سازی برداری پشتیبانی می‌کند و گزینه‌های منعطف و مقیاس‌پذیری برای بازیابی داده‌ها ارائه می‌دهد. این سرویس را می‌توان با استفاده از متغیرهای محیطی برای تعیین نوع فروشگاه برداری و جزئیات اتصال پیکربندی کرد.\n\n> منابع: `/services/reis`",
    "Status": "ok"
  }
]