[
  {
    "Id": 1,
    "Content": "# c4 GenAI Suite\n\nAn AI chatbot application with Model Context Provider (MCP) integration, powered by Langchain and compatibility for all major Large Language Models (LLMs) and Embedding Models.\n\nAdministrators can create assistants with different capabilities by adding extensions, such as RAG (Retrieval-Augmented Generation) services or MCP servers. The application is built using a modern tech stack, including React, NestJS, and Python FastAPI for the REI-S service.\n\nUsers can interact with assistants through a user-friendly interface. Depending on the assistant's configuration, users may be able to ask questions, upload their own files, or use other features. The assistants interact with various LLM providers to provide responses based on the configured extensions. Contextual information provided by the configured extensions allows the assistants to answer domain-specific questions and provide relevant information.\n\nThe application is designed to be modular and extensible, allowing users to create assistants with different capabilities by adding extensions.\n\n![short demo video of basic usage](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/preview.webp)\n\n## Features\n\n### Large Language Models (LLM) and Multimodal Models\n\nThe c4 GenAI Suite supports already many models directly. And if your preferred model is not supported already, it should be easy to write an extension to support it.\n\n* OpenAI compatible models\n* Azure OpenAI models\n* Bedrock models\n* Google GenAI models\n* Ollama compatible models\n",
    "ContentSha": "VePwjxi+F5ttFJfuckWjhr5ujnZTyNWk2c6pu+FMFPM=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "# c4 GenAI Suite\n\nLangchainによって強化され、すべての主要な大規模言語モデル（LLM）および埋め込みモデルに対応した、Model Context Provider（MCP）統合のAIチャットボットアプリケーションです。\n\n管理者は、RAG（検索拡張生成）サービスやMCPサーバーなどの拡張機能を追加することで、異なる機能を持つアシスタントを作成できます。このアプリケーションは、React、NestJS、REI-Sサービス用のPython FastAPIなど、モダンな技術スタックで構築されています。\n\nユーザーは、使いやすいインターフェースを通じてアシスタントと対話できます。アシスタントの設定に応じて、ユーザーは質問をしたり、自分のファイルをアップロードしたり、その他の機能を利用したりすることができます。アシスタントは、さまざまなLLMプロバイダーと連携し、設定された拡張機能に基づいて応答を提供します。設定された拡張機能が提供するコンテキスト情報により、アシスタントはドメイン固有の質問に答えたり、関連情報を提供したりできます。\n\nこのアプリケーションはモジュール式かつ拡張性を持つように設計されており、ユーザーは拡張機能を追加することで、さまざまな機能を持つアシスタントを作成できます。\n\n![short demo video of basic usage](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/preview.webp)\n\n## 特徴\n\n### 大規模言語モデル（LLM）およびマルチモーダルモデル\n\nc4 GenAI Suiteは、多くのモデルをすでに直接サポートしています。また、お好みのモデルがまだサポートされていない場合でも、それをサポートする拡張機能を簡単に作成できるはずです。\n\n* OpenAI互換モデル\n* Azure OpenAIモデル\n* Bedrockモデル\n* Google GenAIモデル\n* Ollama互換モデル",
    "Status": "ok"
  },
  {
    "Id": 2,
    "Content": "### Retrieval Augmented Generation (RAG)\n\nThe c4 GenAI Suite includes REI-S, a server to prepare files for consumption by the LLM.\n\n* REI-S, a custom integrated RAG server\n  * Vector stores\n    * pgvector\n    * Azure AI Search\n  * Embedding models\n    * OpenAI compatible embeddings\n    * Azure OpenAI embeddings\n    * Ollama compatible embeddings\n  * File formats:\n    * pdf, docx, pptx, xlsx, ...\n    * audio file voice transcription (via Whisper)\n\n### Extensions\n\nThe c4 GenAI Suite is designed for extensibility. Writing extensions is easy, as is using an already existing MCP server.\n\n* Model Context Protocol (MCP) servers\n* Custom systemprompt\n* Bing Search\n* Calculator\n\n",
    "ContentSha": "wvT4e6071qGfJDWClvklgUJiJRM71gUVIUQKaaDgVPo=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### 検索拡張生成（RAG）\n\nc4 GenAI Suite には、LLM で利用するファイルを準備するためのサーバー REI-S が含まれています。\n\n* REI-S：カスタム統合型RAGサーバー\n  * ベクトルストア\n    * pgvector\n    * Azure AI Search\n  * 埋め込みモデル\n    * OpenAI 互換埋め込み\n    * Azure OpenAI 埋め込み\n    * Ollama 互換埋め込み\n  * ファイルフォーマット：\n    * pdf、docx、pptx、xlsx、…\n    * 音声ファイル音声書き起こし（Whisper 経由）\n\n### 拡張機能\n\nc4 GenAI Suite は拡張性を考慮して設計されています。拡張機能の作成は簡単で、既存の MCP サーバーの利用も容易です。\n\n* モデルコンテキストプロトコル（MCP）サーバー\n* カスタム systemprompt\n* Bing 検索\n* 電卓",
    "Status": "ok"
  },
  {
    "Id": 3,
    "Content": "## Getting Started\n\n### Using Docker-Compose\n\n- Run `docker compose up` in the project root.\n- Open the [application](http://localhost:3333) in a browser. The default login credentials are user `admin@example.com` and password `secret`.\n\n![video showing assistant configuration](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/assistants.webp)\n\n### Using Helm & Kubernetes\n\nFor deployment in Kubernetes environments, please refer to the [README of our Helm Chart](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/./helm-chart/README.md).\n\n### Setting up Assistants and Extensions\n\nThe c4 GenAI Suite revolves around *assistants*.\nEach assistant consists of a set of extensions, which determine the LLM model and which tools it can use.\n\n- In the admin area (click the username on the bottom left), go to the [assistants section](http://localhost:3333/admin/assistants).\n- Add an assistant with the green `+` button next to the section title. Choose a name and a description.\n- Select the created assistant and click the green `+ Add Extension`.\n- Select the model and fill in the credentials.\n- Use the `Test` Button to check that it works and `save`.\n\nNow you can return to the [chat page](http://localhost:3333/chat) (click on `c4 GenAI Suite` in the top left) and start a new conversation with your new assistant.\n\n> [!TIP]\n> Our `docker-compose` includes a local Ollama, which runs on the CPU. You can use this for quick testing. But it will be slow and you probably want to use another model. If you want to use it, just create the following model extension in your Assistant.\n> * Extension: `Dev: Ollama`\n> * Endpoint: `http://ollama:11434`\n> * Model: `llama3.2`\n",
    "ContentSha": "bLa8wjqGffqM6ThWqBWA15QpuTagUJbPEAqi4FPI4X4=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## はじめに\n\n### Docker-Composeの使用\n\n- プロジェクトのルートディレクトリで`docker compose up`を実行します。\n- ブラウザで[アプリケーション](http://localhost:3333)を開きます。デフォルトのログイン認証情報は、ユーザー名が`admin@example.com`、パスワードが`secret`です。\n\n![アシスタント設定の動画](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/assistants.webp)\n\n### HelmとKubernetesの使用\n\nKubernetes環境でのデプロイについては、[HelmチャートのREADME](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/./helm-chart/README.md)をご参照ください。\n\n### アシスタントと拡張機能の設定\n\nc4 GenAI Suiteは*アシスタント*を中心に構成されています。\n各アシスタントは一連の拡張機能で構成されており、どのLLMモデルを使用し、どのツールが利用可能かを決定します。\n\n- 管理エリア（左下のユーザー名をクリック）で、[アシスタントセクション](http://localhost:3333/admin/assistants)に移動します。\n- セクションタイトルの横にある緑色の`+`ボタンでアシスタントを追加します。名前と説明を入力してください。\n- 作成したアシスタントを選択し、緑色の`+ 拡張機能を追加`をクリックします。\n- モデルを選択し、認証情報を入力します。\n- `テスト`ボタンで動作確認し、`保存`してください。\n\nこれで[チャットページ](http://localhost:3333/chat)（左上の`c4 GenAI Suite`をクリック）に戻り、新しいアシスタントとの会話を開始できます。\n\n> [!TIP]\n> `docker-compose`にはローカルOllamaが含まれており、CPU上で動作します。これを使って簡単にテストできますが、動作は遅くなるため、他のモデルの利用をおすすめします。使用したい場合は、以下のモデル拡張機能をアシスタントに作成してください。\n> * 拡張機能: `Dev: Ollama`\n> * エンドポイント: `http://ollama:11434`\n> * モデル: `llama3.2`\n",
    "Status": "ok"
  },
  {
    "Id": 4,
    "Content": "### Model Context Protocol (MCP) [optional]\n\nUse any MCP server offering an `sse` interface with the `MCP Tools` Extension (or use our `mcp-tool-as-server` as a proxy in front of an `stdio` MCP server).\nEach MCP server can be configured in detail as an extension.\n\n### Retrieval Augmented Generation (RAG) / File Search [optional]\n\nUse our RAG server `REI-S` to search user provided files. Just configure a `Search Files` extension for the assistant.\nThis process is described in detail in [the `services/reis` subdirectory](services/reis/#example-configuration-in-c4).\n\n## Contributing & Development\n\n* See [CONTRIBUTING.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/CONTRIBUTING.md) for guidelines on how to contribute.\n* For developer onboarding, check [DEVELOPERS.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/DEVELOPERS.md).\n\n## Main Building Blocks\n\nThe application consists of a **Frontend** , a **Backend**  and a **REI-S**  service.\n\n```",
    "ContentSha": "X5WCsc3MMbiH/lsJf83tTEO6Cfl/16Nw2tPJwgCKYpc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "",
    "Status": "error"
  },
  {
    "Id": 5,
    "Content": "┌──────────┐\n│   User   │\n└─────┬────┘\n      │ access\n      ▼\n┌──────────┐\n│ Frontend │\n└─────┬────┘\n      │ access\n      ▼\n┌──────────┐     ┌─────────────────┐\n│ Backend  │────►│      LLM        │\n└─────┬────┘     └─────────────────┘\n      │ access\n      ▼\n┌──────────┐     ┌─────────────────┐\n│  REI-S   │────►│ Embedding Model │\n│          │     └─────────────────┘\n│          │\n│          │     ┌─────────────────┐\n│          │────►│  Vector Store   │\n└──────────┘     └─────────────────┘\n```",
    "ContentSha": "IO09RXKALRyYzMSXIVzGJTmyROh56hbqbTTI4NQfxtQ=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "┌──────────┐\n│   ユーザー │\n└─────┬────┘\n      │ アクセス\n      ▼\n┌──────────┐\n│ フロントエンド │\n└─────┬────┘\n      │ アクセス\n      ▼\n┌──────────┐     ┌─────────────────┐\n│ バックエンド │────►│      LLM        │\n└─────┬────┘     └─────────────────┘\n      │ アクセス\n      ▼\n┌──────────┐     ┌─────────────────┐\n│  REI-S   │────►│ 埋め込みモデル    │\n│          │     └─────────────────┘\n│          │\n│          │     ┌─────────────────┐\n│          │────►│ ベクターストア   │\n└──────────┘     └─────────────────┘\n```",
    "Status": "ok"
  },
  {
    "Id": 6,
    "Content": "\n\n### Frontend\n\nThe frontend is built with React and TypeScript, providing a user-friendly interface for interacting with the backend and REI-S service. It includes features for managing assistants, extensions, and chat functionalities.\n\n> Sources: `/frontend`\n\n### Backend\n\nThe backend is developed using NestJS and TypeScript, serving as the main API layer for the application. It handles requests from the frontend and interacts with llm providers to facilitate chat functionalities. The backend also manages assistants and their extensions, allowing users to configure and use various AI models for their chats.\n\nAdditionally, the backend manages user authentication, and communicates with the REI-S service for file indexing and retrieval.\n\nFor data persistence, the backend uses a **PostgreSQL** database.\n\n> Sources: `/backend`\n\n### REI-S\n\nThe REI-S (**R**etrieval **E**xtraction **I**ngestion **S**erver) is a Python-based server that provides basic RAG (Retrieval-Augmented Generation) capabilities. It allows for file content extraction, indexing and querying, enabling the application to handle large datasets efficiently. The REI-S service is designed to work seamlessly with the backend, providing necessary data for chat functionalities and file searches.\n\nThe REI-S supports Azure AI Search and pgvector for vector storage, allowing for flexible and scalable data retrieval options. The service can be configured using environment variables to specify the type of vector store and connection details.\n\n> Sources: `/services/reis`\n",
    "ContentSha": "Sceh4YX3ZYIBDEtdTMWIBUucYWH+hEUJMben1cxfrAc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### フロントエンド\n\nフロントエンドはReactとTypeScriptで構築されており、バックエンドおよびREI-Sサービスと連携するためのユーザーフレンドリーなインターフェースを提供します。アシスタント、拡張機能、チャット機能の管理機能が含まれています。\n\n> ソース: `/frontend`\n\n### バックエンド\n\nバックエンドはNestJSとTypeScriptを使用して開発されており、アプリケーションの主要なAPIレイヤーとして機能します。フロントエンドからのリクエストを処理し、llmプロバイダーと連携してチャット機能を提供します。バックエンドはまた、アシスタントおよびその拡張機能の管理も行い、ユーザーがさまざまなAIモデルをチャットで設定・利用できるようにします。\n\nさらに、バックエンドはユーザー認証を管理し、ファイルのインデックス作成および取得のためにREI-Sサービスと通信します。\n\nデータ永続化のために、バックエンドは**PostgreSQL**データベースを使用しています。\n\n> ソース: `/backend`\n\n### REI-S\n\nREI-S（**R**etrieval **E**xtraction **I**ngestion **S**erver）は、Pythonベースのサーバーで、基本的なRAG（Retrieval-Augmented Generation）機能を提供します。ファイル内容の抽出、インデックス作成、クエリを可能にし、アプリケーションが大規模なデータセットを効率的に扱えるようにします。REI-Sサービスはバックエンドとシームレスに連携するよう設計されており、チャット機能やファイル検索に必要なデータを提供します。\n\nREI-Sは、ベクトルストレージとしてAzure AI Searchおよびpgvectorをサポートしており、柔軟かつスケーラブルなデータ検索オプションを実現します。サービスは環境変数によってベクトルストアの種類や接続情報を設定できます。\n\n> ソース: `/services/reis`\n",
    "Status": "ok"
  }
]