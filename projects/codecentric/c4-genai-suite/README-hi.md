# c4 GenAI Suite

एक एआई चैटबॉट एप्लिकेशन जिसमें मॉडल कॉन्टेक्स्ट प्रोवाइडर (MCP) एकीकरण है, जो लैंगचेन द्वारा संचालित है और सभी प्रमुख लार्ज लैंग्वेज मॉडल्स (LLMs) और एम्बेडिंग मॉडल्स के लिए संगत है।

प्रशासक विभिन्न क्षमताओं वाले असिस्टेंट्स बना सकते हैं, जैसे RAG (रिट्रीवल-ऑग्मेंटेड जेनरेशन) सर्विसेज या MCP सर्वर जोड़कर एक्सटेंशन के माध्यम से। यह एप्लिकेशन एक आधुनिक टेक्नोलॉजी स्टैक का उपयोग करके बनाई गई है, जिसमें REI-S सेवा के लिए React, NestJS, और Python FastAPI शामिल हैं।

उपयोगकर्ता एक यूज़र-फ्रेंडली इंटरफेस के माध्यम से असिस्टेंट्स के साथ संवाद कर सकते हैं। असिस्टेंट के कॉन्फ़िगरेशन के आधार पर, उपयोगकर्ता प्रश्न पूछ सकते हैं, अपनी खुद की फाइलें अपलोड कर सकते हैं, या अन्य सुविधाओं का उपयोग कर सकते हैं। असिस्टेंट्स विभिन्न LLM प्रदाताओं के साथ संवाद करके, कॉन्फ़िगर किए गए एक्सटेंशन के आधार पर उत्तर प्रदान करते हैं। कॉन्फ़िगर किए गए एक्सटेंशनों द्वारा प्रदान की गई संदर्भ जानकारी असिस्टेंट्स को डोमेन-विशिष्ट प्रश्नों के उत्तर देने और प्रासंगिक जानकारी प्रदान करने में सक्षम बनाती है।

एप्लिकेशन को मॉड्यूलर और एक्स्टेंसिबल रूप से डिज़ाइन किया गया है, जिससे उपयोगकर्ता एक्सटेंशन जोड़कर विभिन्न क्षमताओं वाले असिस्टेंट्स बना सकते हैं।

![मूल उपयोग का संक्षिप्त डेमो वीडियो](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/preview.webp)

## फीचर्स

### लार्ज लैंग्वेज मॉडल्स (LLM) और मल्टीमोडल मॉडल्स

c4 GenAI Suite पहले से ही कई मॉडल्स का सीधा समर्थन करता है। और यदि आपका पसंदीदा मॉडल पहले से समर्थित नहीं है, तो उसके लिए एक्सटेंशन लिखना भी आसान है।

* OpenAI संगत मॉडल्स
* Azure OpenAI मॉडल्स
* Bedrock मॉडल्स
* Google GenAI मॉडल्स
* Ollama संगत मॉडल्स
### पुनः प्राप्ति संवर्धित जनरेशन (RAG)

c4 GenAI सुइट में REI-S शामिल है, जो LLM द्वारा फ़ाइलों की खपत के लिए उन्हें तैयार करने वाला एक सर्वर है।

* REI-S, एक कस्टम इंटीग्रेटेड RAG सर्वर
  * वेक्टर स्टोर्स
    * pgvector
    * Azure AI Search
  * एम्बेडिंग मॉडल्स
    * OpenAI संगत एम्बेडिंग्स
    * Azure OpenAI एम्बेडिंग्स
    * Ollama संगत एम्बेडिंग्स
  * फ़ाइल फ़ॉर्मेट्स:
    * pdf, docx, pptx, xlsx, ...
    * ऑडियो फ़ाइल वॉइस ट्रांसक्रिप्शन (Whisper के माध्यम से)

### एक्सटेंशन

c4 GenAI सुइट को विस्तारशीलता के लिए डिज़ाइन किया गया है। एक्सटेंशन लिखना आसान है, जैसा कि पहले से मौजूद MCP सर्वर का उपयोग करना भी।

* मॉडल कॉन्टेक्स्ट प्रोटोकॉल (MCP) सर्वर
* कस्टम सिस्टमप्रॉम्प्ट
* बिंग सर्च
* कैलकुलेटर
## शुरुआत करें

### Docker-Compose का उपयोग करना

- प्रोजेक्ट रूट में `docker compose up` चलाएँ।
- ब्राउज़र में [एप्लिकेशन](http://localhost:3333) खोलें। डिफ़ॉल्ट लॉगिन क्रेडेंशियल्स हैं यूज़र `admin@example.com` और पासवर्ड `secret`।

![video showing assistant configuration](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/assistants.webp)

### Helm और Kubernetes का उपयोग करना

Kubernetes वातावरण में डिप्लॉयमेंट के लिए, कृपया हमारे Helm Chart के [README](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/./helm-chart/README.md) को देखें।

### असिस्टेंट्स और एक्सटेंशन सेटअप करना

c4 GenAI Suite *असिस्टेंट्स* के इर्द-गिर्द घूमती है।
प्रत्येक असिस्टेंट में एक्सटेंशन्स का एक सेट होता है, जो LLM मॉडल और उपलब्ध टूल्स को निर्धारित करता है।

- एडमिन एरिया में (नीचे बाईं ओर यूज़रनेम पर क्लिक करें), [असिस्टेंट्स सेक्शन](http://localhost:3333/admin/assistants) पर जाएँ।
- सेक्शन टाइटल के पास हरे रंग के `+` बटन से एक नया असिस्टेंट जोड़ें। एक नाम और विवरण चुनें।
- बनाए गए असिस्टेंट को चुनें और हरे `+ Add Extension` पर क्लिक करें।
- मॉडल चुनें और क्रेडेंशियल्स भरें।
- यह जांचने के लिए कि सब कुछ सही है, `Test` बटन का उपयोग करें और फिर `save` करें।

अब आप [चैट पेज](http://localhost:3333/chat) (ऊपर बाईं ओर `c4 GenAI Suite` पर क्लिक करें) पर वापस जा सकते हैं और अपने नए असिस्टेंट के साथ एक नई बातचीत शुरू कर सकते हैं।

> [!TIP]
> हमारे `docker-compose` में एक लोकल Ollama शामिल है, जो CPU पर चलता है। आप इसका उपयोग त्वरित परीक्षण के लिए कर सकते हैं। लेकिन यह धीमा होगा और संभवतः आप कोई दूसरा मॉडल इस्तेमाल करना चाहेंगे। यदि आप इसका उपयोग करना चाहते हैं, तो अपने असिस्टेंट में निम्नलिखित मॉडल एक्सटेंशन बनाएं।
> * एक्सटेंशन: `Dev: Ollama`
> * एंडपॉइंट: `http://ollama:11434`
> * मॉडल: `llama3.2`

### मॉडल कंटेक्स्ट प्रोटोकॉल (MCP) [वैकल्पिक]

किसी भी MCP सर्वर का उपयोग करें जो `sse` इंटरफेस के साथ `MCP Tools` एक्सटेंशन प्रदान करता है (या हमारे `mcp-tool-as-server` का उपयोग करें जो `stdio` MCP सर्वर के सामने एक प्रॉक्सी के रूप में कार्य करता है)।
प्रत्येक MCP सर्वर को विस्तार से एक एक्सटेंशन के रूप में कॉन्फ़िगर किया जा सकता है।

### रिट्रीवल ऑगमेंटेड जनरेशन (RAG) / फ़ाइल खोज [वैकल्पिक]

उपयोगकर्ता द्वारा प्रदान की गई फ़ाइलों को खोजने के लिए हमारे RAG सर्वर `REI-S` का उपयोग करें। बस सहायक के लिए एक `Search Files` एक्सटेंशन कॉन्फ़िगर करें।
इस प्रक्रिया का विस्तृत विवरण [ `services/reis` उपनिर्देशिका ](services/reis/#example-configuration-in-c4) में दिया गया है।

## योगदान और विकास

* योगदान करने के लिए दिशानिर्देशों के लिए देखें [CONTRIBUTING.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/CONTRIBUTING.md)।
* डेवलपर ऑनबोर्डिंग के लिए देखें [DEVELOPERS.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/DEVELOPERS.md)।

## मुख्य निर्माण खंड

एप्लिकेशन में एक **फ्रंटएंड**, एक **बैकएंड** और एक **REI-S** सेवा होती है।

```
┌──────────┐
│   उपयोगकर्ता   │
└─────┬────┘
      │ पहुँच
      ▼
┌──────────┐
│ फ्रंटएंड │
└─────┬────┘
      │ पहुँच
      ▼
┌──────────┐     ┌─────────────────┐
│ बैकएंड   │────►│      LLM        │
└─────┬────┘     └─────────────────┘
      │ पहुँच
      ▼
┌──────────┐     ┌─────────────────┐
│  REI-S   │────►│ एम्बेडिंग मॉडल  │
│          │     └─────────────────┘
│          │
│          │     ┌─────────────────┐
│          │────►│  वेक्टर स्टोर    │
└──────────┘     └─────────────────┘
```
### फ्रंटएंड

फ्रंटएंड React और TypeScript के साथ बनाया गया है, जो बैकएंड और REI-S सेवा के साथ इंटरैक्ट करने के लिए एक यूज़र-फ्रेंडली इंटरफेस प्रदान करता है। इसमें असिस्टेंट्स, एक्सटेंशन्स और चैट फंक्शनैलिटी को प्रबंधित करने के लिए फीचर्स शामिल हैं।

> स्रोत: `/frontend`

### बैकएंड

बैकएंड NestJS और TypeScript का उपयोग करके विकसित किया गया है, जो एप्लिकेशन के लिए मुख्य API लेयर के रूप में कार्य करता है। यह फ्रंटएंड से अनुरोधों को संभालता है और चैट फंक्शनैलिटी को सक्षम करने के लिए llm प्रदाताओं के साथ इंटरैक्ट करता है। बैकएंड असिस्टेंट्स और उनकी एक्सटेंशन्स का भी प्रबंधन करता है, जिससे उपयोगकर्ता अपनी चैट्स के लिए विभिन्न AI मॉडल्स को कॉन्फ़िगर और उपयोग कर सकते हैं।

इसके अतिरिक्त, बैकएंड यूज़र ऑथेंटिकेशन का प्रबंधन करता है, और फ़ाइल इंडेक्सिंग और रिट्रीवल के लिए REI-S सेवा के साथ संचार करता है।

डेटा को सुरक्षित रखने के लिए, बैकएंड **PostgreSQL** डेटाबेस का उपयोग करता है।

> स्रोत: `/backend`

### REI-S

REI-S (**R**etrieval **E**xtraction **I**ngestion **S**erver) एक Python-आधारित सर्वर है जो बेसिक RAG (Retrieval-Augmented Generation) क्षमताएँ प्रदान करता है। यह फ़ाइल सामग्री निष्कर्षण, इंडेक्सिंग और क्वेरी करने की सुविधा देता है, जिससे एप्लिकेशन बड़े डेटा सेट्स को कुशलतापूर्वक संभाल सकता है। REI-S सेवा को बैकएंड के साथ सहज रूप से काम करने के लिए डिज़ाइन किया गया है, जो चैट फंक्शनैलिटी और फ़ाइल खोज के लिए आवश्यक डेटा प्रदान करता है।

REI-S Azure AI Search और pgvector को वेक्टर स्टोरेज के लिए सपोर्ट करता है, जिससे लचीले और स्केलेबल डेटा रिट्रीवल विकल्प उपलब्ध होते हैं। सेवा को वातावरण वेरिएबल्स का उपयोग करके कॉन्फ़िगर किया जा सकता है, जिसमें वेक्टर स्टोर का प्रकार और कनेक्शन डिटेल्स निर्दिष्ट की जाती हैं।

> स्रोत: `/services/reis`


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-09

---