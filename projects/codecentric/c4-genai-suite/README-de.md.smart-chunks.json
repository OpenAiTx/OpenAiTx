[
  {
    "Id": 1,
    "Content": "# c4 GenAI Suite\n\nAn AI chatbot application with Model Context Provider (MCP) integration, powered by Langchain and compatibility for all major Large Language Models (LLMs) and Embedding Models.\n\nAdministrators can create assistants with different capabilities by adding extensions, such as RAG (Retrieval-Augmented Generation) services or MCP servers. The application is built using a modern tech stack, including React, NestJS, and Python FastAPI for the REI-S service.\n\nUsers can interact with assistants through a user-friendly interface. Depending on the assistant's configuration, users may be able to ask questions, upload their own files, or use other features. The assistants interact with various LLM providers to provide responses based on the configured extensions. Contextual information provided by the configured extensions allows the assistants to answer domain-specific questions and provide relevant information.\n\nThe application is designed to be modular and extensible, allowing users to create assistants with different capabilities by adding extensions.\n\n![short demo video of basic usage](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/preview.webp)\n\n## Features\n\n### Large Language Models (LLM) and Multimodal Models\n\nThe c4 GenAI Suite supports already many models directly. And if your preferred model is not supported already, it should be easy to write an extension to support it.\n\n* OpenAI compatible models\n* Azure OpenAI models\n* Bedrock models\n* Google GenAI models\n* Ollama compatible models\n",
    "ContentSha": "VePwjxi+F5ttFJfuckWjhr5ujnZTyNWk2c6pu+FMFPM=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "# c4 GenAI Suite\n\nEine KI-Chatbot-Anwendung mit Model Context Provider (MCP)-Integration, basierend auf Langchain und kompatibel mit allen gängigen Large Language Models (LLMs) und Embedding Models.\n\nAdministratoren können Assistenten mit unterschiedlichen Fähigkeiten erstellen, indem sie Erweiterungen wie RAG- (Retrieval-Augmented Generation) Dienste oder MCP-Server hinzufügen. Die Anwendung ist mit einem modernen Tech-Stack entwickelt, darunter React, NestJS und Python FastAPI für den REI-S Dienst.\n\nBenutzer können über eine benutzerfreundliche Oberfläche mit den Assistenten interagieren. Je nach Konfiguration des Assistenten können Benutzer Fragen stellen, eigene Dateien hochladen oder andere Funktionen nutzen. Die Assistenten interagieren mit verschiedenen LLM-Anbietern, um Antworten basierend auf den konfigurierten Erweiterungen zu liefern. Kontextbezogene Informationen, die durch die konfigurierten Erweiterungen bereitgestellt werden, ermöglichen es den Assistenten, domänenspezifische Fragen zu beantworten und relevante Informationen zu liefern.\n\nDie Anwendung ist modular und erweiterbar konzipiert, sodass Benutzer Assistenten mit unterschiedlichen Fähigkeiten durch das Hinzufügen von Erweiterungen erstellen können.\n\n![kurzes Demovideo zur grundlegenden Nutzung](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/preview.webp)\n\n## Funktionen\n\n### Large Language Models (LLM) und Multimodale Modelle\n\nDie c4 GenAI Suite unterstützt bereits viele Modelle direkt. Und falls Ihr bevorzugtes Modell noch nicht unterstützt wird, sollte es einfach sein, eine Erweiterung zur Unterstützung zu schreiben.\n\n* OpenAI-kompatible Modelle\n* Azure OpenAI Modelle\n* Bedrock Modelle\n* Google GenAI Modelle\n* Ollama-kompatible Modelle",
    "Status": "ok"
  },
  {
    "Id": 2,
    "Content": "### Retrieval Augmented Generation (RAG)\n\nThe c4 GenAI Suite includes REI-S, a server to prepare files for consumption by the LLM.\n\n* REI-S, a custom integrated RAG server\n  * Vector stores\n    * pgvector\n    * Azure AI Search\n  * Embedding models\n    * OpenAI compatible embeddings\n    * Azure OpenAI embeddings\n    * Ollama compatible embeddings\n  * File formats:\n    * pdf, docx, pptx, xlsx, ...\n    * audio file voice transcription (via Whisper)\n\n### Extensions\n\nThe c4 GenAI Suite is designed for extensibility. Writing extensions is easy, as is using an already existing MCP server.\n\n* Model Context Protocol (MCP) servers\n* Custom systemprompt\n* Bing Search\n* Calculator\n\n",
    "ContentSha": "wvT4e6071qGfJDWClvklgUJiJRM71gUVIUQKaaDgVPo=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### Retrieval Augmented Generation (RAG)\n\nDie c4 GenAI Suite beinhaltet REI-S, einen Server zur Vorbereitung von Dateien für die Nutzung durch das LLM.\n\n* REI-S, ein individuell integrierter RAG-Server\n  * Vektorspeicher\n    * pgvector\n    * Azure AI Search\n  * Embedding-Modelle\n    * OpenAI-kompatible Embeddings\n    * Azure OpenAI Embeddings\n    * Ollama-kompatible Embeddings\n  * Dateiformate:\n    * pdf, docx, pptx, xlsx, ...\n    * Audio-Datei Spracherkennung (über Whisper)\n\n### Erweiterungen\n\nDie c4 GenAI Suite ist auf Erweiterbarkeit ausgelegt. Das Schreiben von Erweiterungen ist einfach, ebenso wie die Nutzung eines bereits existierenden MCP-Servers.\n\n* Model Context Protocol (MCP) Server\n* Benutzerdefinierter Systemprompt\n* Bing-Suche\n* Taschenrechner",
    "Status": "ok"
  },
  {
    "Id": 3,
    "Content": "## Getting Started\n\n### Using Docker-Compose\n\n- Run `docker compose up` in the project root.\n- Open the [application](http://localhost:3333) in a browser. The default login credentials are user `admin@example.com` and password `secret`.\n\n![video showing assistant configuration](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/assistants.webp)\n\n### Using Helm & Kubernetes\n\nFor deployment in Kubernetes environments, please refer to the [README of our Helm Chart](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/./helm-chart/README.md).\n\n### Setting up Assistants and Extensions\n\nThe c4 GenAI Suite revolves around *assistants*.\nEach assistant consists of a set of extensions, which determine the LLM model and which tools it can use.\n\n- In the admin area (click the username on the bottom left), go to the [assistants section](http://localhost:3333/admin/assistants).\n- Add an assistant with the green `+` button next to the section title. Choose a name and a description.\n- Select the created assistant and click the green `+ Add Extension`.\n- Select the model and fill in the credentials.\n- Use the `Test` Button to check that it works and `save`.\n\nNow you can return to the [chat page](http://localhost:3333/chat) (click on `c4 GenAI Suite` in the top left) and start a new conversation with your new assistant.\n\n> [!TIP]\n> Our `docker-compose` includes a local Ollama, which runs on the CPU. You can use this for quick testing. But it will be slow and you probably want to use another model. If you want to use it, just create the following model extension in your Assistant.\n> * Extension: `Dev: Ollama`\n> * Endpoint: `http://ollama:11434`\n> * Model: `llama3.2`\n",
    "ContentSha": "bLa8wjqGffqM6ThWqBWA15QpuTagUJbPEAqi4FPI4X4=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## Erste Schritte\n\n### Verwendung von Docker-Compose\n\n- Führen Sie `docker compose up` im Projektverzeichnis aus.\n- Öffnen Sie die [Anwendung](http://localhost:3333) in einem Browser. Die Standard-Login-Daten sind Benutzer `admin@example.com` und Passwort `secret`.\n\n![Video zur Assistenz-Konfiguration](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/assistants.webp)\n\n### Verwendung von Helm & Kubernetes\n\nFür die Bereitstellung in Kubernetes-Umgebungen beachten Sie bitte die [README unserer Helm-Chart](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/./helm-chart/README.md).\n\n### Einrichten von Assistenten und Erweiterungen\n\nDie c4 GenAI Suite dreht sich um *Assistenten*.\nJeder Assistent besteht aus einer Reihe von Erweiterungen, welche das LLM-Modell und die verwendbaren Tools bestimmen.\n\n- Im Admin-Bereich (klicken Sie auf den Benutzernamen unten links), gehen Sie zum [Assistenten-Bereich](http://localhost:3333/admin/assistants).\n- Fügen Sie einen Assistenten mit dem grünen `+`-Button neben dem Abschnittstitel hinzu. Wählen Sie einen Namen und eine Beschreibung.\n- Wählen Sie den erstellten Assistenten aus und klicken Sie auf das grüne `+ Erweiterung hinzufügen`.\n- Wählen Sie das Modell und geben Sie die Zugangsdaten ein.\n- Verwenden Sie den `Testen`-Button, um zu prüfen, ob alles funktioniert, und klicken Sie auf `Speichern`.\n\nNun können Sie zur [Chat-Seite](http://localhost:3333/chat) zurückkehren (klicken Sie auf `c4 GenAI Suite` oben links) und eine neue Konversation mit Ihrem neuen Assistenten starten.\n\n> [!TIPP]\n> Unser `docker-compose` beinhaltet ein lokales Ollama, das auf der CPU läuft. Sie können dies für schnelles Testen verwenden. Es wird jedoch langsam sein, und Sie möchten wahrscheinlich ein anderes Modell verwenden. Wenn Sie es verwenden möchten, erstellen Sie einfach die folgende Modellerweiterung in Ihrem Assistenten.\n> * Erweiterung: `Dev: Ollama`\n> * Endpunkt: `http://ollama:11434`\n> * Modell: `llama3.2`",
    "Status": "ok"
  },
  {
    "Id": 4,
    "Content": "### Model Context Protocol (MCP) [optional]\n\nUse any MCP server offering an `sse` interface with the `MCP Tools` Extension (or use our `mcp-tool-as-server` as a proxy in front of an `stdio` MCP server).\nEach MCP server can be configured in detail as an extension.\n\n### Retrieval Augmented Generation (RAG) / File Search [optional]\n\nUse our RAG server `REI-S` to search user provided files. Just configure a `Search Files` extension for the assistant.\nThis process is described in detail in [the `services/reis` subdirectory](services/reis/#example-configuration-in-c4).\n\n## Contributing & Development\n\n* See [CONTRIBUTING.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/CONTRIBUTING.md) for guidelines on how to contribute.\n* For developer onboarding, check [DEVELOPERS.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/DEVELOPERS.md).\n\n## Main Building Blocks\n\nThe application consists of a **Frontend** , a **Backend**  and a **REI-S**  service.\n\n```",
    "ContentSha": "X5WCsc3MMbiH/lsJf83tTEO6Cfl/16Nw2tPJwgCKYpc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### Model Context Protocol (MCP) [optional]\n\nVerwenden Sie jeden MCP-Server, der eine `sse`-Schnittstelle mit der Erweiterung `MCP Tools` anbietet (oder nutzen Sie unser `mcp-tool-as-server` als Proxy vor einem `stdio`-MCP-Server).\nJeder MCP-Server kann im Detail als Erweiterung konfiguriert werden.\n\n### Retrieval Augmented Generation (RAG) / Dateisuche [optional]\n\nVerwenden Sie unseren RAG-Server `REI-S`, um von Benutzern bereitgestellte Dateien zu durchsuchen. Konfigurieren Sie einfach eine `Search Files`-Erweiterung für den Assistenten.\nDieser Prozess wird ausführlich im [Unterverzeichnis `services/reis`](services/reis/#example-configuration-in-c4) beschrieben.\n\n## Beitrag & Entwicklung\n\n* Siehe [CONTRIBUTING.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/CONTRIBUTING.md) für Richtlinien zur Mitwirkung.\n* Für die Entwickler-Einführung siehe [DEVELOPERS.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/DEVELOPERS.md).\n\n## Hauptbausteine\n\nDie Anwendung besteht aus einem **Frontend**, einem **Backend** und einem **REI-S**-Dienst.\n\n```",
    "Status": "ok"
  },
  {
    "Id": 5,
    "Content": "┌──────────┐\n│   User   │\n└─────┬────┘\n      │ access\n      ▼\n┌──────────┐\n│ Frontend │\n└─────┬────┘\n      │ access\n      ▼\n┌──────────┐     ┌─────────────────┐\n│ Backend  │────►│      LLM        │\n└─────┬────┘     └─────────────────┘\n      │ access\n      ▼\n┌──────────┐     ┌─────────────────┐\n│  REI-S   │────►│ Embedding Model │\n│          │     └─────────────────┘\n│          │\n│          │     ┌─────────────────┐\n│          │────►│  Vector Store   │\n└──────────┘     └─────────────────┘\n```",
    "ContentSha": "IO09RXKALRyYzMSXIVzGJTmyROh56hbqbTTI4NQfxtQ=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "┌──────────┐\n│  Benutzer│\n└─────┬────┘\n      │ Zugriff\n      ▼\n┌──────────┐\n│ Frontend │\n└─────┬────┘\n      │ Zugriff\n      ▼\n┌──────────┐     ┌─────────────────┐\n│ Backend  │────►│      LLM        │\n└─────┬────┘     └─────────────────┘\n      │ Zugriff\n      ▼\n┌──────────┐     ┌─────────────────┐\n│  REI-S   │────►│ Embedding-Modell│\n│          │     └─────────────────┘\n│          │\n│          │     ┌─────────────────┐\n│          │────►│  Vektorspeicher │\n└──────────┘     └─────────────────┘\n```",
    "Status": "ok"
  },
  {
    "Id": 6,
    "Content": "\n\n### Frontend\n\nThe frontend is built with React and TypeScript, providing a user-friendly interface for interacting with the backend and REI-S service. It includes features for managing assistants, extensions, and chat functionalities.\n\n> Sources: `/frontend`\n\n### Backend\n\nThe backend is developed using NestJS and TypeScript, serving as the main API layer for the application. It handles requests from the frontend and interacts with llm providers to facilitate chat functionalities. The backend also manages assistants and their extensions, allowing users to configure and use various AI models for their chats.\n\nAdditionally, the backend manages user authentication, and communicates with the REI-S service for file indexing and retrieval.\n\nFor data persistence, the backend uses a **PostgreSQL** database.\n\n> Sources: `/backend`\n\n### REI-S\n\nThe REI-S (**R**etrieval **E**xtraction **I**ngestion **S**erver) is a Python-based server that provides basic RAG (Retrieval-Augmented Generation) capabilities. It allows for file content extraction, indexing and querying, enabling the application to handle large datasets efficiently. The REI-S service is designed to work seamlessly with the backend, providing necessary data for chat functionalities and file searches.\n\nThe REI-S supports Azure AI Search and pgvector for vector storage, allowing for flexible and scalable data retrieval options. The service can be configured using environment variables to specify the type of vector store and connection details.\n\n> Sources: `/services/reis`\n",
    "ContentSha": "Sceh4YX3ZYIBDEtdTMWIBUucYWH+hEUJMben1cxfrAc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### Frontend\n\nDas Frontend ist mit React und TypeScript entwickelt und bietet eine benutzerfreundliche Oberfläche zur Interaktion mit dem Backend und dem REI-S-Service. Es umfasst Funktionen zur Verwaltung von Assistenten, Erweiterungen und Chat-Funktionalitäten.\n\n> Quellen: `/frontend`\n\n### Backend\n\nDas Backend wurde mit NestJS und TypeScript entwickelt und dient als Haupt-API-Schicht der Anwendung. Es verarbeitet Anfragen vom Frontend und interagiert mit LLM-Anbietern, um Chat-Funktionalitäten bereitzustellen. Das Backend verwaltet außerdem Assistenten und deren Erweiterungen, sodass Benutzer verschiedene KI-Modelle für ihre Chats konfigurieren und nutzen können.\n\nZusätzlich verwaltet das Backend die Benutzerauthentifizierung und kommuniziert mit dem REI-S-Service zur Datei-Indizierung und -Abruf.\n\nFür die Datenpersistenz verwendet das Backend eine **PostgreSQL**-Datenbank.\n\n> Quellen: `/backend`\n\n### REI-S\n\nDer REI-S (**R**etrieval **E**xtraction **I**ngestion **S**erver) ist ein auf Python basierender Server, der grundlegende RAG-Funktionen (Retrieval-Augmented Generation) bereitstellt. Er ermöglicht die Extraktion, Indizierung und Abfrage von Dateiinhalten, wodurch die Anwendung große Datensätze effizient verarbeiten kann. Der REI-S-Service ist so konzipiert, dass er nahtlos mit dem Backend zusammenarbeitet und die notwendigen Daten für Chat-Funktionalitäten und Dateisuche bereitstellt.\n\nDer REI-S unterstützt Azure AI Search und pgvector für die Vektorspeicherung und ermöglicht so flexible und skalierbare Optionen für die Datenabfrage. Der Service kann über Umgebungsvariablen konfiguriert werden, um den Typ des Vektorspeichers und die Verbindungsdetails festzulegen.\n\n> Quellen: `/services/reis`\n",
    "Status": "ok"
  }
]