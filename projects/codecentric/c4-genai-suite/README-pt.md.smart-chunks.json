[
  {
    "Id": 1,
    "Content": "# c4 GenAI Suite\n\nAn AI chatbot application with Model Context Provider (MCP) integration, powered by Langchain and compatibility for all major Large Language Models (LLMs) and Embedding Models.\n\nAdministrators can create assistants with different capabilities by adding extensions, such as RAG (Retrieval-Augmented Generation) services or MCP servers. The application is built using a modern tech stack, including React, NestJS, and Python FastAPI for the REI-S service.\n\nUsers can interact with assistants through a user-friendly interface. Depending on the assistant's configuration, users may be able to ask questions, upload their own files, or use other features. The assistants interact with various LLM providers to provide responses based on the configured extensions. Contextual information provided by the configured extensions allows the assistants to answer domain-specific questions and provide relevant information.\n\nThe application is designed to be modular and extensible, allowing users to create assistants with different capabilities by adding extensions.\n\n![short demo video of basic usage](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/preview.webp)\n\n## Features\n\n### Large Language Models (LLM) and Multimodal Models\n\nThe c4 GenAI Suite supports already many models directly. And if your preferred model is not supported already, it should be easy to write an extension to support it.\n\n* OpenAI compatible models\n* Azure OpenAI models\n* Bedrock models\n* Google GenAI models\n* Ollama compatible models\n",
    "ContentSha": "VePwjxi+F5ttFJfuckWjhr5ujnZTyNWk2c6pu+FMFPM=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "# c4 GenAI Suite\n\nUm aplicativo de chatbot de IA com integração ao Model Context Provider (MCP), alimentado por Langchain e compatível com todos os principais Modelos de Linguagem de Grande Escala (LLMs) e Modelos de Embedding.\n\nAdministradores podem criar assistentes com diferentes capacidades adicionando extensões, como serviços RAG (Geração Aumentada por Recuperação) ou servidores MCP. O aplicativo é construído com uma stack tecnológica moderna, incluindo React, NestJS e Python FastAPI para o serviço REI-S.\n\nUsuários podem interagir com assistentes por meio de uma interface amigável. Dependendo da configuração do assistente, os usuários podem fazer perguntas, enviar seus próprios arquivos ou usar outros recursos. Os assistentes interagem com diversos provedores de LLM para fornecer respostas baseadas nas extensões configuradas. Informações contextuais fornecidas pelas extensões configuradas permitem que os assistentes respondam a perguntas específicas de domínio e forneçam informações relevantes.\n\nO aplicativo foi projetado para ser modular e extensível, permitindo que os usuários criem assistentes com diferentes capacidades ao adicionar extensões.\n\n![vídeo curto de demonstração de uso básico](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/preview.webp)\n\n## Funcionalidades\n\n### Modelos de Linguagem de Grande Escala (LLM) e Modelos Multimodais\n\nO c4 GenAI Suite já suporta muitos modelos diretamente. E se o seu modelo preferido ainda não for suportado, deve ser fácil escrever uma extensão para suportá-lo.\n\n* Modelos compatíveis com OpenAI\n* Modelos Azure OpenAI\n* Modelos Bedrock\n* Modelos Google GenAI\n* Modelos compatíveis com Ollama",
    "Status": "ok"
  },
  {
    "Id": 2,
    "Content": "### Retrieval Augmented Generation (RAG)\n\nThe c4 GenAI Suite includes REI-S, a server to prepare files for consumption by the LLM.\n\n* REI-S, a custom integrated RAG server\n  * Vector stores\n    * pgvector\n    * Azure AI Search\n  * Embedding models\n    * OpenAI compatible embeddings\n    * Azure OpenAI embeddings\n    * Ollama compatible embeddings\n  * File formats:\n    * pdf, docx, pptx, xlsx, ...\n    * audio file voice transcription (via Whisper)\n\n### Extensions\n\nThe c4 GenAI Suite is designed for extensibility. Writing extensions is easy, as is using an already existing MCP server.\n\n* Model Context Protocol (MCP) servers\n* Custom systemprompt\n* Bing Search\n* Calculator\n\n",
    "ContentSha": "wvT4e6071qGfJDWClvklgUJiJRM71gUVIUQKaaDgVPo=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### Geração Aumentada por Recuperação (RAG)\n\nO c4 GenAI Suite inclui o REI-S, um servidor para preparar arquivos para consumo pelo LLM.\n\n* REI-S, um servidor RAG integrado personalizado\n  * Armazenamento vetorial\n    * pgvector\n    * Azure AI Search\n  * Modelos de embedding\n    * Embeddings compatíveis com OpenAI\n    * Embeddings Azure OpenAI\n    * Embeddings compatíveis com Ollama\n  * Formatos de arquivo:\n    * pdf, docx, pptx, xlsx, ...\n    * transcrição de voz de arquivos de áudio (via Whisper)\n\n### Extensões\n\nO c4 GenAI Suite é projetado para ser extensível. Escrever extensões é fácil, assim como utilizar um servidor MCP já existente.\n\n* Servidores Model Context Protocol (MCP)\n* systemprompt personalizado\n* Pesquisa Bing\n* Calculadora",
    "Status": "ok"
  },
  {
    "Id": 3,
    "Content": "## Getting Started\n\n### Using Docker-Compose\n\n- Run `docker compose up` in the project root.\n- Open the [application](http://localhost:3333) in a browser. The default login credentials are user `admin@example.com` and password `secret`.\n\n![video showing assistant configuration](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/assistants.webp)\n\n### Using Helm & Kubernetes\n\nFor deployment in Kubernetes environments, please refer to the [README of our Helm Chart](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/./helm-chart/README.md).\n\n### Setting up Assistants and Extensions\n\nThe c4 GenAI Suite revolves around *assistants*.\nEach assistant consists of a set of extensions, which determine the LLM model and which tools it can use.\n\n- In the admin area (click the username on the bottom left), go to the [assistants section](http://localhost:3333/admin/assistants).\n- Add an assistant with the green `+` button next to the section title. Choose a name and a description.\n- Select the created assistant and click the green `+ Add Extension`.\n- Select the model and fill in the credentials.\n- Use the `Test` Button to check that it works and `save`.\n\nNow you can return to the [chat page](http://localhost:3333/chat) (click on `c4 GenAI Suite` in the top left) and start a new conversation with your new assistant.\n\n> [!TIP]\n> Our `docker-compose` includes a local Ollama, which runs on the CPU. You can use this for quick testing. But it will be slow and you probably want to use another model. If you want to use it, just create the following model extension in your Assistant.\n> * Extension: `Dev: Ollama`\n> * Endpoint: `http://ollama:11434`\n> * Model: `llama3.2`\n",
    "ContentSha": "bLa8wjqGffqM6ThWqBWA15QpuTagUJbPEAqi4FPI4X4=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## Primeiros Passos\n\n### Usando Docker-Compose\n\n- Execute `docker compose up` na raiz do projeto.\n- Abra o [aplicativo](http://localhost:3333) em um navegador. As credenciais padrão de login são usuário `admin@example.com` e senha `secret`.\n\n![vídeo mostrando a configuração do assistente](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/assistants.webp)\n\n### Usando Helm & Kubernetes\n\nPara implantação em ambientes Kubernetes, consulte o [README do nosso Helm Chart](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/./helm-chart/README.md).\n\n### Configurando Assistentes e Extensões\n\nO c4 GenAI Suite gira em torno de *assistentes*.\nCada assistente consiste em um conjunto de extensões, que determinam o modelo LLM e quais ferramentas ele pode usar.\n\n- Na área de administração (clique no nome de usuário no canto inferior esquerdo), vá para a [seção de assistentes](http://localhost:3333/admin/assistants).\n- Adicione um assistente com o botão verde `+` ao lado do título da seção. Escolha um nome e uma descrição.\n- Selecione o assistente criado e clique no verde `+ Adicionar Extensão`.\n- Selecione o modelo e preencha as credenciais.\n- Use o botão `Testar` para verificar se está funcionando e clique em `salvar`.\n\nAgora você pode retornar à [página de chat](http://localhost:3333/chat) (clique em `c4 GenAI Suite` no canto superior esquerdo) e iniciar uma nova conversa com seu novo assistente.\n\n> [!DICA]\n> Nosso `docker-compose` inclui um Ollama local, que roda na CPU. Você pode usar isso para testes rápidos. Mas será lento e provavelmente você vai querer usar outro modelo. Se quiser utilizá-lo, basta criar a seguinte extensão de modelo em seu Assistente.\n> * Extensão: `Dev: Ollama`\n> * Endpoint: `http://ollama:11434`\n> * Modelo: `llama3.2`\n",
    "Status": "ok"
  },
  {
    "Id": 4,
    "Content": "### Model Context Protocol (MCP) [optional]\n\nUse any MCP server offering an `sse` interface with the `MCP Tools` Extension (or use our `mcp-tool-as-server` as a proxy in front of an `stdio` MCP server).\nEach MCP server can be configured in detail as an extension.\n\n### Retrieval Augmented Generation (RAG) / File Search [optional]\n\nUse our RAG server `REI-S` to search user provided files. Just configure a `Search Files` extension for the assistant.\nThis process is described in detail in [the `services/reis` subdirectory](services/reis/#example-configuration-in-c4).\n\n## Contributing & Development\n\n* See [CONTRIBUTING.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/CONTRIBUTING.md) for guidelines on how to contribute.\n* For developer onboarding, check [DEVELOPERS.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/DEVELOPERS.md).\n\n## Main Building Blocks\n\nThe application consists of a **Frontend** , a **Backend**  and a **REI-S**  service.\n\n```",
    "ContentSha": "X5WCsc3MMbiH/lsJf83tTEO6Cfl/16Nw2tPJwgCKYpc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### Protocolo de Contexto de Modelo (MCP) [opcional]\n\nUtilize qualquer servidor MCP que ofereça uma interface `sse` com a extensão `MCP Tools` (ou use nosso `mcp-tool-as-server` como um proxy na frente de um servidor MCP `stdio`).\nCada servidor MCP pode ser configurado em detalhes como uma extensão.\n\n### Geração Aumentada por Recuperação (RAG) / Busca em Arquivos [opcional]\n\nUtilize nosso servidor RAG `REI-S` para pesquisar arquivos fornecidos pelo usuário. Basta configurar uma extensão `Search Files` para o assistente.\nEsse processo é descrito em detalhes no [subdiretório `services/reis`](services/reis/#example-configuration-in-c4).\n\n## Contribuindo & Desenvolvimento\n\n* Veja [CONTRIBUTING.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/CONTRIBUTING.md) para orientações sobre como contribuir.\n* Para integração de desenvolvedores, confira [DEVELOPERS.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/DEVELOPERS.md).\n\n## Principais Blocos de Construção\n\nA aplicação consiste em um **Frontend**, um **Backend** e um serviço **REI-S**.\n\n```",
    "Status": "ok"
  },
  {
    "Id": 5,
    "Content": "┌──────────┐\n│   User   │\n└─────┬────┘\n      │ access\n      ▼\n┌──────────┐\n│ Frontend │\n└─────┬────┘\n      │ access\n      ▼\n┌──────────┐     ┌─────────────────┐\n│ Backend  │────►│      LLM        │\n└─────┬────┘     └─────────────────┘\n      │ access\n      ▼\n┌──────────┐     ┌─────────────────┐\n│  REI-S   │────►│ Embedding Model │\n│          │     └─────────────────┘\n│          │\n│          │     ┌─────────────────┐\n│          │────►│  Vector Store   │\n└──────────┘     └─────────────────┘\n```",
    "ContentSha": "IO09RXKALRyYzMSXIVzGJTmyROh56hbqbTTI4NQfxtQ=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "┌──────────┐\n│  Usuário │\n└─────┬────┘\n      │ acesso\n      ▼\n┌──────────┐\n│ Frontend │\n└─────┬────┘\n      │ acesso\n      ▼\n┌──────────┐     ┌─────────────────┐\n│ Backend  │────►│      LLM        │\n└─────┬────┘     └─────────────────┘\n      │ acesso\n      ▼\n┌──────────┐     ┌─────────────────┐\n│  REI-S   │────►│ Modelo de Embedding │\n│          │     └─────────────────┘\n│          │\n│          │     ┌─────────────────┐\n│          │────►│  Armazenamento Vetorial │\n└──────────┘     └─────────────────┘\n```",
    "Status": "ok"
  },
  {
    "Id": 6,
    "Content": "\n\n### Frontend\n\nThe frontend is built with React and TypeScript, providing a user-friendly interface for interacting with the backend and REI-S service. It includes features for managing assistants, extensions, and chat functionalities.\n\n> Sources: `/frontend`\n\n### Backend\n\nThe backend is developed using NestJS and TypeScript, serving as the main API layer for the application. It handles requests from the frontend and interacts with llm providers to facilitate chat functionalities. The backend also manages assistants and their extensions, allowing users to configure and use various AI models for their chats.\n\nAdditionally, the backend manages user authentication, and communicates with the REI-S service for file indexing and retrieval.\n\nFor data persistence, the backend uses a **PostgreSQL** database.\n\n> Sources: `/backend`\n\n### REI-S\n\nThe REI-S (**R**etrieval **E**xtraction **I**ngestion **S**erver) is a Python-based server that provides basic RAG (Retrieval-Augmented Generation) capabilities. It allows for file content extraction, indexing and querying, enabling the application to handle large datasets efficiently. The REI-S service is designed to work seamlessly with the backend, providing necessary data for chat functionalities and file searches.\n\nThe REI-S supports Azure AI Search and pgvector for vector storage, allowing for flexible and scalable data retrieval options. The service can be configured using environment variables to specify the type of vector store and connection details.\n\n> Sources: `/services/reis`\n",
    "ContentSha": "Sceh4YX3ZYIBDEtdTMWIBUucYWH+hEUJMben1cxfrAc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### Frontend\n\nO frontend é construído com React e TypeScript, proporcionando uma interface amigável para interagir com o backend e o serviço REI-S. Inclui funcionalidades para gerenciamento de assistentes, extensões e chats.\n\n> Fontes: `/frontend`\n\n### Backend\n\nO backend é desenvolvido usando NestJS e TypeScript, servindo como a principal camada de API para a aplicação. Ele lida com as requisições do frontend e interage com provedores LLM para viabilizar funcionalidades de chat. O backend também gerencia assistentes e suas extensões, permitindo que os usuários configurem e utilizem diversos modelos de IA em seus chats.\n\nAlém disso, o backend gerencia a autenticação de usuários e se comunica com o serviço REI-S para indexação e recuperação de arquivos.\n\nPara persistência de dados, o backend utiliza um banco de dados **PostgreSQL**.\n\n> Fontes: `/backend`\n\n### REI-S\n\nO REI-S (**R**etrieval **E**xtraction **I**ngestion **S**erver) é um servidor baseado em Python que fornece capacidades básicas de RAG (Geração Aumentada por Recuperação). Ele permite a extração de conteúdo de arquivos, indexação e consulta, possibilitando que a aplicação manipule grandes conjuntos de dados de forma eficiente. O serviço REI-S foi projetado para funcionar perfeitamente com o backend, fornecendo os dados necessários para funcionalidades de chat e buscas de arquivos.\n\nO REI-S suporta Azure AI Search e pgvector para armazenamento vetorial, permitindo opções flexíveis e escaláveis de recuperação de dados. O serviço pode ser configurado usando variáveis de ambiente para especificar o tipo de armazenamento vetorial e detalhes de conexão.\n\n> Fontes: `/services/reis`\n",
    "Status": "ok"
  }
]