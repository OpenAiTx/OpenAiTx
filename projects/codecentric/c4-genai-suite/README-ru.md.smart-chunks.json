[
  {
    "Id": 1,
    "Content": "# c4 GenAI Suite\n\nAn AI chatbot application with Model Context Provider (MCP) integration, powered by Langchain and compatibility for all major Large Language Models (LLMs) and Embedding Models.\n\nAdministrators can create assistants with different capabilities by adding extensions, such as RAG (Retrieval-Augmented Generation) services or MCP servers. The application is built using a modern tech stack, including React, NestJS, and Python FastAPI for the REI-S service.\n\nUsers can interact with assistants through a user-friendly interface. Depending on the assistant's configuration, users may be able to ask questions, upload their own files, or use other features. The assistants interact with various LLM providers to provide responses based on the configured extensions. Contextual information provided by the configured extensions allows the assistants to answer domain-specific questions and provide relevant information.\n\nThe application is designed to be modular and extensible, allowing users to create assistants with different capabilities by adding extensions.\n\n![short demo video of basic usage](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/preview.webp)\n\n## Features\n\n### Large Language Models (LLM) and Multimodal Models\n\nThe c4 GenAI Suite supports already many models directly. And if your preferred model is not supported already, it should be easy to write an extension to support it.\n\n* OpenAI compatible models\n* Azure OpenAI models\n* Bedrock models\n* Google GenAI models\n* Ollama compatible models\n",
    "ContentSha": "VePwjxi+F5ttFJfuckWjhr5ujnZTyNWk2c6pu+FMFPM=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "# c4 GenAI Suite\n\nПриложение чат-бота с интеграцией Model Context Provider (MCP), построенное на базе Langchain и совместимое со всеми основными Большими Языковыми Моделями (LLM) и моделями эмбеддинга.\n\nАдминистраторы могут создавать ассистентов с различными возможностями, добавляя расширения, такие как сервисы RAG (генерация с расширенным поиском) или MCP-серверы. Приложение построено с использованием современного стека технологий, включая React, NestJS и Python FastAPI для сервиса REI-S.\n\nПользователи могут взаимодействовать с ассистентами через удобный пользовательский интерфейс. В зависимости от конфигурации ассистента пользователи могут задавать вопросы, загружать собственные файлы или использовать другие функции. Ассистенты взаимодействуют с различными LLM-провайдерами для предоставления ответов на основе настроенных расширений. Контекстная информация, предоставляемая настроенными расширениями, позволяет ассистентам отвечать на вопросы, относящиеся к определённой предметной области, и предоставлять актуальную информацию.\n\nПриложение разработано модульным и расширяемым, что позволяет пользователям создавать ассистентов с различными возможностями, добавляя расширения.\n\n![короткое демонстрационное видео базового использования](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/preview.webp)\n\n## Возможности\n\n### Большие языковые модели (LLM) и мультимодальные модели\n\nc4 GenAI Suite уже поддерживает множество моделей напрямую. Если ваша предпочитаемая модель ещё не поддерживается, написать расширение для её поддержки должно быть просто.\n\n* Модели, совместимые с OpenAI\n* Модели Azure OpenAI\n* Модели Bedrock\n* Модели Google GenAI\n* Модели, совместимые с Ollama",
    "Status": "ok"
  },
  {
    "Id": 2,
    "Content": "### Retrieval Augmented Generation (RAG)\n\nThe c4 GenAI Suite includes REI-S, a server to prepare files for consumption by the LLM.\n\n* REI-S, a custom integrated RAG server\n  * Vector stores\n    * pgvector\n    * Azure AI Search\n  * Embedding models\n    * OpenAI compatible embeddings\n    * Azure OpenAI embeddings\n    * Ollama compatible embeddings\n  * File formats:\n    * pdf, docx, pptx, xlsx, ...\n    * audio file voice transcription (via Whisper)\n\n### Extensions\n\nThe c4 GenAI Suite is designed for extensibility. Writing extensions is easy, as is using an already existing MCP server.\n\n* Model Context Protocol (MCP) servers\n* Custom systemprompt\n* Bing Search\n* Calculator\n\n",
    "ContentSha": "wvT4e6071qGfJDWClvklgUJiJRM71gUVIUQKaaDgVPo=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### Генерация с дополнением извлечённой информации (RAG)\n\nПакет c4 GenAI включает REI-S — сервер для подготовки файлов к использованию LLM.\n\n* REI-S, специализированный интегрированный RAG-сервер\n  * Векторные хранилища\n    * pgvector\n    * Azure AI Search\n  * Модели эмбеддингов\n    * эмбеддинги, совместимые с OpenAI\n    * эмбеддинги Azure OpenAI\n    * эмбеддинги, совместимые с Ollama\n  * Форматы файлов:\n    * pdf, docx, pptx, xlsx, ...\n    * транскрипция голосовых аудиофайлов (через Whisper)\n\n### Расширения\n\nПакет c4 GenAI разработан с учётом расширяемости. Создавать расширения просто, как и использовать уже существующий MCP-сервер.\n\n* Серверы протокола Model Context Protocol (MCP)\n* Пользовательский systemprompt\n* Bing Search\n* Калькулятор",
    "Status": "ok"
  },
  {
    "Id": 3,
    "Content": "## Getting Started\n\n### Using Docker-Compose\n\n- Run `docker compose up` in the project root.\n- Open the [application](http://localhost:3333) in a browser. The default login credentials are user `admin@example.com` and password `secret`.\n\n![video showing assistant configuration](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/assistants.webp)\n\n### Using Helm & Kubernetes\n\nFor deployment in Kubernetes environments, please refer to the [README of our Helm Chart](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/./helm-chart/README.md).\n\n### Setting up Assistants and Extensions\n\nThe c4 GenAI Suite revolves around *assistants*.\nEach assistant consists of a set of extensions, which determine the LLM model and which tools it can use.\n\n- In the admin area (click the username on the bottom left), go to the [assistants section](http://localhost:3333/admin/assistants).\n- Add an assistant with the green `+` button next to the section title. Choose a name and a description.\n- Select the created assistant and click the green `+ Add Extension`.\n- Select the model and fill in the credentials.\n- Use the `Test` Button to check that it works and `save`.\n\nNow you can return to the [chat page](http://localhost:3333/chat) (click on `c4 GenAI Suite` in the top left) and start a new conversation with your new assistant.\n\n> [!TIP]\n> Our `docker-compose` includes a local Ollama, which runs on the CPU. You can use this for quick testing. But it will be slow and you probably want to use another model. If you want to use it, just create the following model extension in your Assistant.\n> * Extension: `Dev: Ollama`\n> * Endpoint: `http://ollama:11434`\n> * Model: `llama3.2`\n",
    "ContentSha": "bLa8wjqGffqM6ThWqBWA15QpuTagUJbPEAqi4FPI4X4=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## Начало работы\n\n### Использование Docker-Compose\n\n- Запустите `docker compose up` в корневой папке проекта.\n- Откройте [приложение](http://localhost:3333) в браузере. Стандартные учетные данные для входа: пользователь `admin@example.com` и пароль `secret`.\n\n![видео, показывающее настройку ассистента](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/assistants.webp)\n\n### Использование Helm и Kubernetes\n\nДля развертывания в средах Kubernetes, пожалуйста, обратитесь к [README нашего Helm Chart](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/./helm-chart/README.md).\n\n### Настройка ассистентов и расширений\n\nC4 GenAI Suite строится вокруг *ассистентов*.\nКаждый ассистент состоит из набора расширений, которые определяют LLM-модель и доступные инструменты.\n\n- В административной панели (нажмите на имя пользователя в левом нижнем углу) перейдите в [раздел ассистентов](http://localhost:3333/admin/assistants).\n- Добавьте ассистента с помощью зеленой кнопки `+` рядом с заголовком раздела. Выберите имя и описание.\n- Выберите созданного ассистента и нажмите зеленую кнопку `+ Добавить расширение`.\n- Выберите модель и заполните учетные данные.\n- Используйте кнопку `Test`, чтобы проверить работоспособность, и затем `сохранить`.\n\nТеперь вы можете вернуться на [страницу чата](http://localhost:3333/chat) (нажмите на `c4 GenAI Suite` в левом верхнем углу) и начать новый разговор с вашим новым ассистентом.\n\n> [!ПОДСКАЗКА]\n> Наш `docker-compose` включает локальный Ollama, который работает на CPU. Вы можете использовать его для быстрого тестирования. Однако он будет работать медленно, и, вероятно, вы захотите использовать другую модель. Если вы хотите воспользоваться им, просто создайте следующее расширение модели в вашем Ассистенте.\n> * Расширение: `Dev: Ollama`\n> * Endpoint: `http://ollama:11434`\n> * Модель: `llama3.2`",
    "Status": "ok"
  },
  {
    "Id": 4,
    "Content": "### Model Context Protocol (MCP) [optional]\n\nUse any MCP server offering an `sse` interface with the `MCP Tools` Extension (or use our `mcp-tool-as-server` as a proxy in front of an `stdio` MCP server).\nEach MCP server can be configured in detail as an extension.\n\n### Retrieval Augmented Generation (RAG) / File Search [optional]\n\nUse our RAG server `REI-S` to search user provided files. Just configure a `Search Files` extension for the assistant.\nThis process is described in detail in [the `services/reis` subdirectory](services/reis/#example-configuration-in-c4).\n\n## Contributing & Development\n\n* See [CONTRIBUTING.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/CONTRIBUTING.md) for guidelines on how to contribute.\n* For developer onboarding, check [DEVELOPERS.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/DEVELOPERS.md).\n\n## Main Building Blocks\n\nThe application consists of a **Frontend** , a **Backend**  and a **REI-S**  service.\n\n```",
    "ContentSha": "X5WCsc3MMbiH/lsJf83tTEO6Cfl/16Nw2tPJwgCKYpc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### Протокол Контекста Модели (MCP) [необязательно]\n\nИспользуйте любой MCP-сервер, предоставляющий интерфейс `sse` с расширением `MCP Tools` (или используйте наш `mcp-tool-as-server` как прокси перед MCP-сервером с интерфейсом `stdio`).\nКаждый MCP-сервер может быть детально настроен как расширение.\n\n### Генерация с поддержкой поиска (RAG) / Поиск по файлам [необязательно]\n\nИспользуйте наш RAG-сервер `REI-S` для поиска по предоставленным пользователем файлам. Просто настройте расширение `Search Files` для ассистента.\nЭтот процесс подробно описан в [подкаталоге `services/reis`](services/reis/#example-configuration-in-c4).\n\n## Вклад и разработка\n\n* См. [CONTRIBUTING.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/CONTRIBUTING.md) для получения рекомендаций по внесению вклада.\n* Для ознакомления разработчиков смотрите [DEVELOPERS.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/DEVELOPERS.md).\n\n## Основные строительные блоки\n\nПриложение состоит из **Frontend** , **Backend**  и сервиса **REI-S** .\n\n```",
    "Status": "ok"
  },
  {
    "Id": 5,
    "Content": "┌──────────┐\n│   User   │\n└─────┬────┘\n      │ access\n      ▼\n┌──────────┐\n│ Frontend │\n└─────┬────┘\n      │ access\n      ▼\n┌──────────┐     ┌─────────────────┐\n│ Backend  │────►│      LLM        │\n└─────┬────┘     └─────────────────┘\n      │ access\n      ▼\n┌──────────┐     ┌─────────────────┐\n│  REI-S   │────►│ Embedding Model │\n│          │     └─────────────────┘\n│          │\n│          │     ┌─────────────────┐\n│          │────►│  Vector Store   │\n└──────────┘     └─────────────────┘\n```",
    "ContentSha": "IO09RXKALRyYzMSXIVzGJTmyROh56hbqbTTI4NQfxtQ=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "┌──────────┐\n│  Пользователь  │\n└─────┬────┘\n      │ доступ\n      ▼\n┌──────────┐\n│  Фронтенд │\n└─────┬────┘\n      │ доступ\n      ▼\n┌──────────┐     ┌─────────────────┐\n│ Бэкенд   │────►│      LLM        │\n└─────┬────┘     └─────────────────┘\n      │ доступ\n      ▼\n┌──────────┐     ┌─────────────────┐\n│  REI-S   │────►│ Embedding Model │\n│          │     └─────────────────┘\n│          │\n│          │     ┌─────────────────┐\n│          │────►│  Vector Store   │\n└──────────┘     └─────────────────┘\n```",
    "Status": "ok"
  },
  {
    "Id": 6,
    "Content": "\n\n### Frontend\n\nThe frontend is built with React and TypeScript, providing a user-friendly interface for interacting with the backend and REI-S service. It includes features for managing assistants, extensions, and chat functionalities.\n\n> Sources: `/frontend`\n\n### Backend\n\nThe backend is developed using NestJS and TypeScript, serving as the main API layer for the application. It handles requests from the frontend and interacts with llm providers to facilitate chat functionalities. The backend also manages assistants and their extensions, allowing users to configure and use various AI models for their chats.\n\nAdditionally, the backend manages user authentication, and communicates with the REI-S service for file indexing and retrieval.\n\nFor data persistence, the backend uses a **PostgreSQL** database.\n\n> Sources: `/backend`\n\n### REI-S\n\nThe REI-S (**R**etrieval **E**xtraction **I**ngestion **S**erver) is a Python-based server that provides basic RAG (Retrieval-Augmented Generation) capabilities. It allows for file content extraction, indexing and querying, enabling the application to handle large datasets efficiently. The REI-S service is designed to work seamlessly with the backend, providing necessary data for chat functionalities and file searches.\n\nThe REI-S supports Azure AI Search and pgvector for vector storage, allowing for flexible and scalable data retrieval options. The service can be configured using environment variables to specify the type of vector store and connection details.\n\n> Sources: `/services/reis`\n",
    "ContentSha": "Sceh4YX3ZYIBDEtdTMWIBUucYWH+hEUJMben1cxfrAc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### Фронтенд\n\nФронтенд построен с использованием React и TypeScript, обеспечивая удобный пользовательский интерфейс для взаимодействия с бекендом и сервисом REI-S. Включает функции управления ассистентами, расширениями и чат-возможностями.\n\n> Источники: `/frontend`\n\n### Бекенд\n\nБекенд разработан с использованием NestJS и TypeScript и служит основным API-слоем приложения. Он обрабатывает запросы от фронтенда и взаимодействует с провайдерами LLM для обеспечения чат-функциональности. Бекенд также управляет ассистентами и их расширениями, позволяя пользователям настраивать и использовать различные AI-модели для своих чатов.\n\nКроме того, бекенд управляет аутентификацией пользователей и взаимодействует с сервисом REI-S для индексации и получения файлов.\n\nДля хранения данных бекенд использует базу данных **PostgreSQL**.\n\n> Источники: `/backend`\n\n### REI-S\n\nREI-S (**R**etrieval **E**xtraction **I**ngestion **S**erver) — сервер на базе Python, обеспечивающий базовые возможности RAG (Retrieval-Augmented Generation). Он позволяет извлекать содержимое файлов, индексировать и выполнять запросы, что позволяет приложению эффективно работать с большими наборами данных. Сервис REI-S спроектирован для бесшовной работы с бекендом, предоставляя необходимые данные для чатов и поиска по файлам.\n\nREI-S поддерживает Azure AI Search и pgvector для хранения векторов, что обеспечивает гибкие и масштабируемые варианты поиска данных. Сервис можно настроить с помощью переменных окружения для указания типа векторного хранилища и параметров подключения.\n\n> Источники: `/services/reis`",
    "Status": "ok"
  }
]