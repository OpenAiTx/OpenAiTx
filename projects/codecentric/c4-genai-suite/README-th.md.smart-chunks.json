[
  {
    "Id": 1,
    "Content": "# c4 GenAI Suite\n\nAn AI chatbot application with Model Context Provider (MCP) integration, powered by Langchain and compatibility for all major Large Language Models (LLMs) and Embedding Models.\n\nAdministrators can create assistants with different capabilities by adding extensions, such as RAG (Retrieval-Augmented Generation) services or MCP servers. The application is built using a modern tech stack, including React, NestJS, and Python FastAPI for the REI-S service.\n\nUsers can interact with assistants through a user-friendly interface. Depending on the assistant's configuration, users may be able to ask questions, upload their own files, or use other features. The assistants interact with various LLM providers to provide responses based on the configured extensions. Contextual information provided by the configured extensions allows the assistants to answer domain-specific questions and provide relevant information.\n\nThe application is designed to be modular and extensible, allowing users to create assistants with different capabilities by adding extensions.\n\n![short demo video of basic usage](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/preview.webp)\n\n## Features\n\n### Large Language Models (LLM) and Multimodal Models\n\nThe c4 GenAI Suite supports already many models directly. And if your preferred model is not supported already, it should be easy to write an extension to support it.\n\n* OpenAI compatible models\n* Azure OpenAI models\n* Bedrock models\n* Google GenAI models\n* Ollama compatible models\n",
    "ContentSha": "VePwjxi+F5ttFJfuckWjhr5ujnZTyNWk2c6pu+FMFPM=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "# c4 GenAI Suite\n\nแอปพลิเคชันแชทบอท AI พร้อมการผสานรวม Model Context Provider (MCP) ขับเคลื่อนด้วย Langchain และรองรับการใช้งานกับโมเดลภาษาขนาดใหญ่ (LLMs) และโมเดล Embedding ทุกรูปแบบ\n\nผู้ดูแลระบบสามารถสร้างผู้ช่วยที่มีความสามารถแตกต่างกันได้โดยเพิ่มส่วนขยาย เช่น บริการ RAG (Retrieval-Augmented Generation) หรือเซิร์ฟเวอร์ MCP แอปพลิเคชันนี้สร้างขึ้นด้วยเทคโนโลยีสมัยใหม่ รวมถึง React, NestJS และ Python FastAPI สำหรับบริการ REI-S\n\nผู้ใช้สามารถโต้ตอบกับผู้ช่วยได้ผ่านอินเทอร์เฟซที่ใช้งานง่าย ขึ้นอยู่กับการกำหนดค่าของผู้ช่วย ผู้ใช้อาจสามารถถามคำถาม อัปโหลดไฟล์ของตนเอง หรือใช้ฟีเจอร์อื่น ๆ ผู้ช่วยจะโต้ตอบกับผู้ให้บริการ LLM ต่าง ๆ เพื่อให้ตอบกลับตามส่วนขยายที่กำหนดไว้ ข้อมูลบริบทที่ได้จากส่วนขยายที่กำหนดค่าจะช่วยให้ผู้ช่วยสามารถตอบคำถามเฉพาะทางและให้ข้อมูลที่เกี่ยวข้องได้\n\nแอปพลิเคชันนี้ถูกออกแบบมาให้เป็นโมดูลาร์และขยายต่อได้ง่าย ช่วยให้ผู้ใช้สามารถสร้างผู้ช่วยที่มีความสามารถต่าง ๆ ได้ด้วยการเพิ่มส่วนขยาย\n\n![short demo video of basic usage](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/preview.webp)\n\n## คุณสมบัติ\n\n### โมเดลภาษาขนาดใหญ่ (LLM) และโมเดลมัลติโหมด\n\nc4 GenAI Suite รองรับโมเดลหลากหลายรูปแบบโดยตรงอยู่แล้ว และหากยังไม่รองรับโมเดลที่คุณต้องการ ก็สามารถเขียนส่วนขยายเพื่อรองรับได้อย่างง่ายดาย\n\n* โมเดลที่รองรับ OpenAI\n* โมเดล Azure OpenAI\n* โมเดล Bedrock\n* โมเดล Google GenAI\n* โมเดลที่รองรับ Ollama",
    "Status": "ok"
  },
  {
    "Id": 2,
    "Content": "### Retrieval Augmented Generation (RAG)\n\nThe c4 GenAI Suite includes REI-S, a server to prepare files for consumption by the LLM.\n\n* REI-S, a custom integrated RAG server\n  * Vector stores\n    * pgvector\n    * Azure AI Search\n  * Embedding models\n    * OpenAI compatible embeddings\n    * Azure OpenAI embeddings\n    * Ollama compatible embeddings\n  * File formats:\n    * pdf, docx, pptx, xlsx, ...\n    * audio file voice transcription (via Whisper)\n\n### Extensions\n\nThe c4 GenAI Suite is designed for extensibility. Writing extensions is easy, as is using an already existing MCP server.\n\n* Model Context Protocol (MCP) servers\n* Custom systemprompt\n* Bing Search\n* Calculator\n\n",
    "ContentSha": "wvT4e6071qGfJDWClvklgUJiJRM71gUVIUQKaaDgVPo=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### Retrieval Augmented Generation (RAG)\n\nc4 GenAI Suite ประกอบด้วย REI-S ซึ่งเป็นเซิร์ฟเวอร์สำหรับเตรียมไฟล์เพื่อให้ LLM ใช้งาน\n\n* REI-S, เซิร์ฟเวอร์ RAG ที่ผสานรวมแบบกำหนดเอง\n  * Vector stores\n    * pgvector\n    * Azure AI Search\n  * Embedding models\n    * Embeddings ที่เข้ากันได้กับ OpenAI\n    * Azure OpenAI embeddings\n    * Embeddings ที่เข้ากันได้กับ Ollama\n  * รูปแบบไฟล์:\n    * pdf, docx, pptx, xlsx, ...\n    * ถอดเสียงไฟล์เสียง (ผ่าน Whisper)\n\n### Extensions\n\nc4 GenAI Suite ถูกออกแบบมาเพื่อรองรับการขยายฟังก์ชัน การเขียนส่วนขยายทำได้ง่าย รวมถึงการใช้งานเซิร์ฟเวอร์ MCP ที่มีอยู่แล้วก็ทำได้ง่ายเช่นกัน\n\n* เซิร์ฟเวอร์ Model Context Protocol (MCP)\n* ระบบ systemprompt ที่กำหนดเอง\n* Bing Search\n* เครื่องคิดเลข",
    "Status": "ok"
  },
  {
    "Id": 3,
    "Content": "## Getting Started\n\n### Using Docker-Compose\n\n- Run `docker compose up` in the project root.\n- Open the [application](http://localhost:3333) in a browser. The default login credentials are user `admin@example.com` and password `secret`.\n\n![video showing assistant configuration](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/assistants.webp)\n\n### Using Helm & Kubernetes\n\nFor deployment in Kubernetes environments, please refer to the [README of our Helm Chart](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/./helm-chart/README.md).\n\n### Setting up Assistants and Extensions\n\nThe c4 GenAI Suite revolves around *assistants*.\nEach assistant consists of a set of extensions, which determine the LLM model and which tools it can use.\n\n- In the admin area (click the username on the bottom left), go to the [assistants section](http://localhost:3333/admin/assistants).\n- Add an assistant with the green `+` button next to the section title. Choose a name and a description.\n- Select the created assistant and click the green `+ Add Extension`.\n- Select the model and fill in the credentials.\n- Use the `Test` Button to check that it works and `save`.\n\nNow you can return to the [chat page](http://localhost:3333/chat) (click on `c4 GenAI Suite` in the top left) and start a new conversation with your new assistant.\n\n> [!TIP]\n> Our `docker-compose` includes a local Ollama, which runs on the CPU. You can use this for quick testing. But it will be slow and you probably want to use another model. If you want to use it, just create the following model extension in your Assistant.\n> * Extension: `Dev: Ollama`\n> * Endpoint: `http://ollama:11434`\n> * Model: `llama3.2`\n",
    "ContentSha": "bLa8wjqGffqM6ThWqBWA15QpuTagUJbPEAqi4FPI4X4=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## เริ่มต้นใช้งาน\n\n### การใช้ Docker-Compose\n\n- รันคำสั่ง `docker compose up` ใน root ของโปรเจค\n- เปิด [แอปพลิเคชัน](http://localhost:3333) ในเบราว์เซอร์ โดยข้อมูลเข้าสู่ระบบเริ่มต้นคือ user `admin@example.com` และรหัสผ่าน `secret`\n\n![video showing assistant configuration](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/assistants.webp)\n\n### การใช้ Helm & Kubernetes\n\nสำหรับการปรับใช้งานในสภาพแวดล้อม Kubernetes โปรดดูที่ [README ของ Helm Chart ของเรา](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/./helm-chart/README.md)\n\n### การตั้งค่า Assistants และ Extensions\n\nc4 GenAI Suite จะเน้นที่ *assistants*\nแต่ละ assistant จะประกอบด้วยชุดของ extension ซึ่งกำหนดโมเดล LLM และเครื่องมือที่สามารถใช้งานได้\n\n- ในพื้นที่ผู้ดูแลระบบ (คลิกที่ชื่อผู้ใช้ด้านซ้ายล่าง) ไปที่ [ส่วน assistants](http://localhost:3333/admin/assistants)\n- เพิ่ม assistant ด้วยปุ่มสีเขียว `+` ข้างชื่อหัวข้อ เลือกชื่อและคำอธิบาย\n- เลือก assistant ที่สร้างขึ้นแล้วคลิกปุ่มสีเขียว `+ Add Extension`\n- เลือกโมเดลและกรอกข้อมูล credential\n- ใช้ปุ่ม `Test` เพื่อตรวจสอบการทำงาน และ `save`\n\nตอนนี้คุณสามารถกลับไปที่ [หน้าสนทนา](http://localhost:3333/chat) (คลิกที่ `c4 GenAI Suite` มุมบนซ้าย) และเริ่มต้นการสนทนาใหม่กับ assistant ของคุณ\n\n> [!TIP]\n> `docker-compose` ของเรามี Ollama แบบ local ซึ่งทำงานบน CPU คุณสามารถใช้สำหรับการทดสอบอย่างรวดเร็ว แต่จะทำงานช้าและคุณอาจต้องการใช้โมเดลอื่น หากคุณต้องการใช้ ให้สร้าง extension ของโมเดลดังนี้ใน Assistant ของคุณ\n> * Extension: `Dev: Ollama`\n> * Endpoint: `http://ollama:11434`\n> * Model: `llama3.2`\n",
    "Status": "ok"
  },
  {
    "Id": 4,
    "Content": "### Model Context Protocol (MCP) [optional]\n\nUse any MCP server offering an `sse` interface with the `MCP Tools` Extension (or use our `mcp-tool-as-server` as a proxy in front of an `stdio` MCP server).\nEach MCP server can be configured in detail as an extension.\n\n### Retrieval Augmented Generation (RAG) / File Search [optional]\n\nUse our RAG server `REI-S` to search user provided files. Just configure a `Search Files` extension for the assistant.\nThis process is described in detail in [the `services/reis` subdirectory](services/reis/#example-configuration-in-c4).\n\n## Contributing & Development\n\n* See [CONTRIBUTING.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/CONTRIBUTING.md) for guidelines on how to contribute.\n* For developer onboarding, check [DEVELOPERS.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/DEVELOPERS.md).\n\n## Main Building Blocks\n\nThe application consists of a **Frontend** , a **Backend**  and a **REI-S**  service.\n\n```",
    "ContentSha": "X5WCsc3MMbiH/lsJf83tTEO6Cfl/16Nw2tPJwgCKYpc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### โปรโตคอลบริบทโมเดล (MCP) [ไม่บังคับ]\n\nใช้เซิร์ฟเวอร์ MCP ใดๆ ที่มีอินเทอร์เฟซ `sse` พร้อมกับส่วนขยาย `MCP Tools` (หรือใช้ `mcp-tool-as-server` ของเราเป็นพร็อกซีหน้ากับเซิร์ฟเวอร์ MCP แบบ `stdio`)\nแต่ละเซิร์ฟเวอร์ MCP สามารถกำหนดค่ารายละเอียดได้เป็นส่วนขยาย\n\n### Retrieval Augmented Generation (RAG) / การค้นหาไฟล์ [ไม่บังคับ]\n\nใช้เซิร์ฟเวอร์ RAG `REI-S` ของเราเพื่อค้นหาไฟล์ที่ผู้ใช้ให้มา เพียงแค่กำหนดค่าส่วนขยาย `Search Files` สำหรับผู้ช่วย\nกระบวนการนี้อธิบายไว้โดยละเอียดใน [ไดเรกทอรีย่อย `services/reis`](services/reis/#example-configuration-in-c4)\n\n## การมีส่วนร่วม & การพัฒนา\n\n* ดู [CONTRIBUTING.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/CONTRIBUTING.md) สำหรับแนวทางการมีส่วนร่วม\n* สำหรับการเริ่มต้นใช้งานสำหรับนักพัฒนา ดู [DEVELOPERS.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/DEVELOPERS.md)\n\n## ส่วนประกอบหลัก\n\nแอปพลิเคชันนี้ประกอบด้วย **Frontend** , **Backend** และบริการ **REI-S**",
    "Status": "ok"
  },
  {
    "Id": 5,
    "Content": "┌──────────┐\n│   User   │\n└─────┬────┘\n      │ access\n      ▼\n┌──────────┐\n│ Frontend │\n└─────┬────┘\n      │ access\n      ▼\n┌──────────┐     ┌─────────────────┐\n│ Backend  │────►│      LLM        │\n└─────┬────┘     └─────────────────┘\n      │ access\n      ▼\n┌──────────┐     ┌─────────────────┐\n│  REI-S   │────►│ Embedding Model │\n│          │     └─────────────────┘\n│          │\n│          │     ┌─────────────────┐\n│          │────►│  Vector Store   │\n└──────────┘     └─────────────────┘\n```",
    "ContentSha": "IO09RXKALRyYzMSXIVzGJTmyROh56hbqbTTI4NQfxtQ=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "┌──────────┐\n│   ผู้ใช้   │\n└─────┬────┘\n      │ เข้าถึง\n      ▼\n┌──────────┐\n│ ส่วนหน้า │\n└─────┬────┘\n      │ เข้าถึง\n      ▼\n┌──────────┐     ┌─────────────────┐\n│ ส่วนหลัง │────►│      LLM        │\n└─────┬────┘     └─────────────────┘\n      │ เข้าถึง\n      ▼\n┌──────────┐     ┌─────────────────┐\n│  REI-S   │────►│ แบบฝังโมเดล     │\n│          │     └─────────────────┘\n│          │\n│          │     ┌─────────────────┐\n│          │────►│  เวกเตอร์สโตร์  │\n└──────────┘     └─────────────────┘\n```",
    "Status": "ok"
  },
  {
    "Id": 6,
    "Content": "\n\n### Frontend\n\nThe frontend is built with React and TypeScript, providing a user-friendly interface for interacting with the backend and REI-S service. It includes features for managing assistants, extensions, and chat functionalities.\n\n> Sources: `/frontend`\n\n### Backend\n\nThe backend is developed using NestJS and TypeScript, serving as the main API layer for the application. It handles requests from the frontend and interacts with llm providers to facilitate chat functionalities. The backend also manages assistants and their extensions, allowing users to configure and use various AI models for their chats.\n\nAdditionally, the backend manages user authentication, and communicates with the REI-S service for file indexing and retrieval.\n\nFor data persistence, the backend uses a **PostgreSQL** database.\n\n> Sources: `/backend`\n\n### REI-S\n\nThe REI-S (**R**etrieval **E**xtraction **I**ngestion **S**erver) is a Python-based server that provides basic RAG (Retrieval-Augmented Generation) capabilities. It allows for file content extraction, indexing and querying, enabling the application to handle large datasets efficiently. The REI-S service is designed to work seamlessly with the backend, providing necessary data for chat functionalities and file searches.\n\nThe REI-S supports Azure AI Search and pgvector for vector storage, allowing for flexible and scalable data retrieval options. The service can be configured using environment variables to specify the type of vector store and connection details.\n\n> Sources: `/services/reis`\n",
    "ContentSha": "Sceh4YX3ZYIBDEtdTMWIBUucYWH+hEUJMben1cxfrAc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### Frontend\n\nFrontend ถูกสร้างขึ้นด้วย React และ TypeScript โดยมีอินเทอร์เฟซที่ใช้งานง่ายสำหรับการโต้ตอบกับ backend และบริการ REI-S ประกอบด้วยฟีเจอร์สำหรับจัดการผู้ช่วย (assistants), ส่วนขยาย (extensions) และฟังก์ชันแชท\n\n> แหล่งที่มา: `/frontend`\n\n### Backend\n\nBackend พัฒนาด้วย NestJS และ TypeScript ทำหน้าที่เป็นชั้น API หลักของแอปพลิเคชัน รับและประมวลผลคำขอจาก frontend และโต้ตอบกับผู้ให้บริการ llm เพื่อรองรับฟังก์ชันแชท Backend ยังจัดการผู้ช่วยและส่วนขยายของพวกเขา ช่วยให้ผู้ใช้สามารถกำหนดค่าและใช้งานโมเดล AI ต่าง ๆ สำหรับการแชทได้\n\nนอกจากนี้ Backend ยังจัดการการยืนยันตัวผู้ใช้ และสื่อสารกับบริการ REI-S สำหรับการจัดทำดัชนีไฟล์และดึงข้อมูลไฟล์\n\nสำหรับการจัดเก็บข้อมูล Backend ใช้ฐานข้อมูล **PostgreSQL**\n\n> แหล่งที่มา: `/backend`\n\n### REI-S\n\nREI-S (**R**etrieval **E**xtraction **I**ngestion **S**erver) เป็นเซิร์ฟเวอร์ที่พัฒนาด้วย Python ซึ่งให้ความสามารถ RAG (Retrieval-Augmented Generation) เบื้องต้น โดยรองรับการแยกเนื้อหาไฟล์, การจัดทำดัชนี และการค้นหา ช่วยให้แอปพลิเคชันสามารถจัดการชุดข้อมูลขนาดใหญ่ได้อย่างมีประสิทธิภาพ บริการ REI-S ถูกออกแบบมาให้ทำงานร่วมกับ backend ได้อย่างไร้รอยต่อ โดยให้ข้อมูลที่จำเป็นสำหรับฟังก์ชันแชทและการค้นหาไฟล์\n\nREI-S รองรับ Azure AI Search และ pgvector สำหรับการจัดเก็บเวกเตอร์ ช่วยให้สามารถเลือกใช้รูปแบบการดึงข้อมูลที่ยืดหยุ่นและขยายขนาดได้ บริการนี้สามารถกำหนดค่าได้โดยใช้ environment variables เพื่อระบุประเภทของ vector store และรายละเอียดการเชื่อมต่อ\n\n> แหล่งที่มา: `/services/reis`",
    "Status": "ok"
  }
]