[
  {
    "Id": 1,
    "Content": "# c4 GenAI Suite\n\nAn AI chatbot application with Model Context Provider (MCP) integration, powered by Langchain and compatibility for all major Large Language Models (LLMs) and Embedding Models.\n\nAdministrators can create assistants with different capabilities by adding extensions, such as RAG (Retrieval-Augmented Generation) services or MCP servers. The application is built using a modern tech stack, including React, NestJS, and Python FastAPI for the REI-S service.\n\nUsers can interact with assistants through a user-friendly interface. Depending on the assistant's configuration, users may be able to ask questions, upload their own files, or use other features. The assistants interact with various LLM providers to provide responses based on the configured extensions. Contextual information provided by the configured extensions allows the assistants to answer domain-specific questions and provide relevant information.\n\nThe application is designed to be modular and extensible, allowing users to create assistants with different capabilities by adding extensions.\n\n![short demo video of basic usage](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/preview.webp)\n\n## Features\n\n### Large Language Models (LLM) and Multimodal Models\n\nThe c4 GenAI Suite supports already many models directly. And if your preferred model is not supported already, it should be easy to write an extension to support it.\n\n* OpenAI compatible models\n* Azure OpenAI models\n* Bedrock models\n* Google GenAI models\n* Ollama compatible models\n",
    "ContentSha": "VePwjxi+F5ttFJfuckWjhr5ujnZTyNWk2c6pu+FMFPM=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "# c4 GenAI Suite\n\n一款具備模型上下文提供者（MCP）整合的 AI 聊天機器人應用程式，採用 Langchain 驅動，並兼容所有主流大型語言模型（LLMs）及嵌入模型。\n\n管理員可以透過新增擴充功能（例如 RAG（檢索增強生成）服務或 MCP 伺服器）來建立具備不同能力的助手。此應用程式採用現代技術組合構建，包括 React、NestJS 以及用於 REI-S 服務的 Python FastAPI。\n\n使用者可透過友善的介面與助手互動。根據助手的設定，使用者可能可以提問、上傳自己的檔案，或使用其他功能。助手會依據所配置的擴充功能，與各種 LLM 供應商互動，並提供回應。由擴充功能提供的情境資訊，讓助手能夠回答特定領域的問題並提供相關資訊。\n\n本應用程式設計為模組化且可擴充，允許使用者透過新增擴充功能來建立具備不同能力的助手。\n\n![short demo video of basic usage](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/preview.webp)\n\n## 功能特色\n\n### 大型語言模型（LLM）及多模態模型\n\nc4 GenAI Suite 已直接支援多種模型。若您的首選模型尚未支援，亦可輕鬆撰寫擴充功能以支援。\n\n* 相容 OpenAI 的模型\n* Azure OpenAI 模型\n* Bedrock 模型\n* Google GenAI 模型\n* 相容 Ollama 的模型",
    "Status": "ok"
  },
  {
    "Id": 2,
    "Content": "### Retrieval Augmented Generation (RAG)\n\nThe c4 GenAI Suite includes REI-S, a server to prepare files for consumption by the LLM.\n\n* REI-S, a custom integrated RAG server\n  * Vector stores\n    * pgvector\n    * Azure AI Search\n  * Embedding models\n    * OpenAI compatible embeddings\n    * Azure OpenAI embeddings\n    * Ollama compatible embeddings\n  * File formats:\n    * pdf, docx, pptx, xlsx, ...\n    * audio file voice transcription (via Whisper)\n\n### Extensions\n\nThe c4 GenAI Suite is designed for extensibility. Writing extensions is easy, as is using an already existing MCP server.\n\n* Model Context Protocol (MCP) servers\n* Custom systemprompt\n* Bing Search\n* Calculator\n\n",
    "ContentSha": "wvT4e6071qGfJDWClvklgUJiJRM71gUVIUQKaaDgVPo=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### 檢索增強生成（RAG）\n\nc4 GenAI 套件包括 REI-S，一個用於為 LLM 準備檔案的伺服器。\n\n* REI-S，自訂整合的 RAG 伺服器\n  * 向量儲存\n    * pgvector\n    * Azure AI Search\n  * 嵌入模型\n    * 相容於 OpenAI 的嵌入\n    * Azure OpenAI 嵌入\n    * 相容於 Ollama 的嵌入\n  * 檔案格式：\n    * pdf、docx、pptx、xlsx，...\n    * 音訊檔語音轉錄（透過 Whisper）\n\n### 擴充功能\n\nc4 GenAI 套件設計具有可擴充性。撰寫擴充功能非常容易，使用現有的 MCP 伺服器也同樣簡單。\n\n* 模型上下文協議（MCP）伺服器\n* 自訂 systemprompt\n* Bing 搜尋\n* 計算機",
    "Status": "ok"
  },
  {
    "Id": 3,
    "Content": "## Getting Started\n\n### Using Docker-Compose\n\n- Run `docker compose up` in the project root.\n- Open the [application](http://localhost:3333) in a browser. The default login credentials are user `admin@example.com` and password `secret`.\n\n![video showing assistant configuration](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/assistants.webp)\n\n### Using Helm & Kubernetes\n\nFor deployment in Kubernetes environments, please refer to the [README of our Helm Chart](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/./helm-chart/README.md).\n\n### Setting up Assistants and Extensions\n\nThe c4 GenAI Suite revolves around *assistants*.\nEach assistant consists of a set of extensions, which determine the LLM model and which tools it can use.\n\n- In the admin area (click the username on the bottom left), go to the [assistants section](http://localhost:3333/admin/assistants).\n- Add an assistant with the green `+` button next to the section title. Choose a name and a description.\n- Select the created assistant and click the green `+ Add Extension`.\n- Select the model and fill in the credentials.\n- Use the `Test` Button to check that it works and `save`.\n\nNow you can return to the [chat page](http://localhost:3333/chat) (click on `c4 GenAI Suite` in the top left) and start a new conversation with your new assistant.\n\n> [!TIP]\n> Our `docker-compose` includes a local Ollama, which runs on the CPU. You can use this for quick testing. But it will be slow and you probably want to use another model. If you want to use it, just create the following model extension in your Assistant.\n> * Extension: `Dev: Ollama`\n> * Endpoint: `http://ollama:11434`\n> * Model: `llama3.2`\n",
    "ContentSha": "bLa8wjqGffqM6ThWqBWA15QpuTagUJbPEAqi4FPI4X4=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## 入門指南\n\n### 使用 Docker-Compose\n\n- 在專案根目錄執行 `docker compose up`。\n- 在瀏覽器中打開[應用程式](http://localhost:3333)。預設登入憑證為使用者名稱 `admin@example.com` 和密碼 `secret`。\n\n![展示助理設定的影片](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/assistants.webp)\n\n### 使用 Helm 與 Kubernetes\n\n若要在 Kubernetes 環境中部署，請參閱我們 [Helm Chart 的 README](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/./helm-chart/README.md)。\n\n### 設定助理與擴充套件\n\nc4 GenAI Suite 以*助理*為核心。\n每個助理由一組擴充套件組成，這些擴充套件決定了 LLM 模型以及可使用的工具。\n\n- 進入管理區（點擊左下方的使用者名稱），前往 [助理區段](http://localhost:3333/admin/assistants)。\n- 點擊區段標題旁的綠色 `+` 按鈕新增助理。輸入名稱和描述。\n- 選取剛建立的助理並點擊綠色的 `+ 新增擴充套件`。\n- 選擇模型並填寫憑證。\n- 使用 `測試` 按鈕檢查是否可用，然後點擊 `儲存`。\n\n現在你可以返回[聊天頁面](http://localhost:3333/chat)（點擊左上角的 `c4 GenAI Suite`），用你的新助理開始新對話。\n\n> [!TIP]\n> 我們的 `docker-compose` 內建本地 Ollama，會在 CPU 上執行。你可以用它進行快速測試，但速度會比較慢，而且你可能會想要使用其他模型。如果你要使用它，只需在你的助理中建立下列模型擴充套件即可。\n> * 擴充套件：`Dev: Ollama`\n> * 端點：`http://ollama:11434`\n> * 模型：`llama3.2`\n",
    "Status": "ok"
  },
  {
    "Id": 4,
    "Content": "### Model Context Protocol (MCP) [optional]\n\nUse any MCP server offering an `sse` interface with the `MCP Tools` Extension (or use our `mcp-tool-as-server` as a proxy in front of an `stdio` MCP server).\nEach MCP server can be configured in detail as an extension.\n\n### Retrieval Augmented Generation (RAG) / File Search [optional]\n\nUse our RAG server `REI-S` to search user provided files. Just configure a `Search Files` extension for the assistant.\nThis process is described in detail in [the `services/reis` subdirectory](services/reis/#example-configuration-in-c4).\n\n## Contributing & Development\n\n* See [CONTRIBUTING.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/CONTRIBUTING.md) for guidelines on how to contribute.\n* For developer onboarding, check [DEVELOPERS.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/DEVELOPERS.md).\n\n## Main Building Blocks\n\nThe application consists of a **Frontend** , a **Backend**  and a **REI-S**  service.\n\n```",
    "ContentSha": "X5WCsc3MMbiH/lsJf83tTEO6Cfl/16Nw2tPJwgCKYpc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### 模型上下文協定（MCP） [可選]\n\n使用任何提供 `sse` 介面的 MCP 伺服器，並搭配 `MCP Tools` 擴充套件（或使用我們的 `mcp-tool-as-server` 作為 `stdio` MCP 伺服器前端的代理伺服器）。\n每個 MCP 伺服器都可以作為擴充套件進行詳細配置。\n\n### 檢索增強生成（RAG）／檔案搜尋 [可選]\n\n使用我們的 RAG 伺服器 `REI-S` 來搜尋使用者提供的檔案。只需為助手配置一個 `搜尋檔案` 擴充套件即可。\n此過程在 [ `services/reis` 子目錄](services/reis/#example-configuration-in-c4) 中有詳細描述。\n\n## 貢獻與開發\n\n* 請參閱 [CONTRIBUTING.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/CONTRIBUTING.md) 以瞭解如何貢獻的指引。\n* 關於開發人員入門，請查閱 [DEVELOPERS.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/DEVELOPERS.md)。\n\n## 主要組件\n\n本應用程式包含 **前端**、**後端** 和 **REI-S** 服務。\n\n```",
    "Status": "ok"
  },
  {
    "Id": 5,
    "Content": "┌──────────┐\n│   User   │\n└─────┬────┘\n      │ access\n      ▼\n┌──────────┐\n│ Frontend │\n└─────┬────┘\n      │ access\n      ▼\n┌──────────┐     ┌─────────────────┐\n│ Backend  │────►│      LLM        │\n└─────┬────┘     └─────────────────┘\n      │ access\n      ▼\n┌──────────┐     ┌─────────────────┐\n│  REI-S   │────►│ Embedding Model │\n│          │     └─────────────────┘\n│          │\n│          │     ┌─────────────────┐\n│          │────►│  Vector Store   │\n└──────────┘     └─────────────────┘\n```",
    "ContentSha": "IO09RXKALRyYzMSXIVzGJTmyROh56hbqbTTI4NQfxtQ=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "┌──────────┐\n│   使用者  │\n└─────┬────┘\n      │ 存取\n      ▼\n┌──────────┐\n│ 前端介面 │\n└─────┬────┘\n      │ 存取\n      ▼\n┌──────────┐     ┌─────────────────┐\n│ 後端服務 │────►│      LLM        │\n└─────┬────┘     └─────────────────┘\n      │ 存取\n      ▼\n┌──────────┐     ┌─────────────────┐\n│  REI-S   │────►│ 嵌入模型        │\n│          │     └─────────────────┘\n│          │\n│          │     ┌─────────────────┐\n│          │────►│ 向量儲存庫      │\n└──────────┘     └─────────────────┘\n```",
    "Status": "ok"
  },
  {
    "Id": 6,
    "Content": "\n\n### Frontend\n\nThe frontend is built with React and TypeScript, providing a user-friendly interface for interacting with the backend and REI-S service. It includes features for managing assistants, extensions, and chat functionalities.\n\n> Sources: `/frontend`\n\n### Backend\n\nThe backend is developed using NestJS and TypeScript, serving as the main API layer for the application. It handles requests from the frontend and interacts with llm providers to facilitate chat functionalities. The backend also manages assistants and their extensions, allowing users to configure and use various AI models for their chats.\n\nAdditionally, the backend manages user authentication, and communicates with the REI-S service for file indexing and retrieval.\n\nFor data persistence, the backend uses a **PostgreSQL** database.\n\n> Sources: `/backend`\n\n### REI-S\n\nThe REI-S (**R**etrieval **E**xtraction **I**ngestion **S**erver) is a Python-based server that provides basic RAG (Retrieval-Augmented Generation) capabilities. It allows for file content extraction, indexing and querying, enabling the application to handle large datasets efficiently. The REI-S service is designed to work seamlessly with the backend, providing necessary data for chat functionalities and file searches.\n\nThe REI-S supports Azure AI Search and pgvector for vector storage, allowing for flexible and scalable data retrieval options. The service can be configured using environment variables to specify the type of vector store and connection details.\n\n> Sources: `/services/reis`\n",
    "ContentSha": "Sceh4YX3ZYIBDEtdTMWIBUucYWH+hEUJMben1cxfrAc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### 前端\n\n前端使用 React 和 TypeScript 構建，提供了一個用戶友善的介面，用於與後端和 REI-S 服務互動。它包含了管理助手、擴充功能以及聊天功能的介面。\n\n> 原始碼位置：`/frontend`\n\n### 後端\n\n後端採用 NestJS 和 TypeScript 開發，作為應用程式的主要 API 層。它處理來自前端的請求，並與 llm 提供者互動以實現聊天功能。後端同時管理助手及其擴充功能，讓使用者可以配置並使用各種 AI 模型進行聊天。\n\n此外，後端還負責用戶驗證，並與 REI-S 服務進行檔案索引與檢索的通訊。\n\n為了資料持久化，後端使用 **PostgreSQL** 資料庫。\n\n> 原始碼位置：`/backend`\n\n### REI-S\n\nREI-S（**R**etrieval **E**xtraction **I**ngestion **S**erver）是一個以 Python 為基礎的伺服器，提供基本的 RAG（檢索增強生成）能力。它支援檔案內容擷取、索引與查詢，使應用程式能有效處理大量資料集。REI-S 服務設計上能與後端無縫協作，為聊天功能和檔案搜尋提供所需資料。\n\nREI-S 支援 Azure AI Search 及 pgvector 作為向量儲存，提供彈性且可擴充的資料檢索選項。該服務可以透過環境變數進行配置，以指定向量儲存的類型及連線細節。\n\n> 原始碼位置：`/services/reis`\n",
    "Status": "ok"
  }
]