[
  {
    "Id": 1,
    "Content": "# c4 GenAI Suite\n\nAn AI chatbot application with Model Context Provider (MCP) integration, powered by Langchain and compatibility for all major Large Language Models (LLMs) and Embedding Models.\n\nAdministrators can create assistants with different capabilities by adding extensions, such as RAG (Retrieval-Augmented Generation) services or MCP servers. The application is built using a modern tech stack, including React, NestJS, and Python FastAPI for the REI-S service.\n\nUsers can interact with assistants through a user-friendly interface. Depending on the assistant's configuration, users may be able to ask questions, upload their own files, or use other features. The assistants interact with various LLM providers to provide responses based on the configured extensions. Contextual information provided by the configured extensions allows the assistants to answer domain-specific questions and provide relevant information.\n\nThe application is designed to be modular and extensible, allowing users to create assistants with different capabilities by adding extensions.\n\n![short demo video of basic usage](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/preview.webp)\n\n## Features\n\n### Large Language Models (LLM) and Multimodal Models\n\nThe c4 GenAI Suite supports already many models directly. And if your preferred model is not supported already, it should be easy to write an extension to support it.\n\n* OpenAI compatible models\n* Azure OpenAI models\n* Bedrock models\n* Google GenAI models\n* Ollama compatible models\n",
    "ContentSha": "VePwjxi+F5ttFJfuckWjhr5ujnZTyNWk2c6pu+FMFPM=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "# c4 GenAI Suite\n\nAplikasi chatbot AI dengan integrasi Model Context Provider (MCP), didukung oleh Langchain dan kompatibel dengan semua Large Language Models (LLM) utama serta Model Embedding.\n\nAdministrator dapat membuat asisten dengan kemampuan berbeda dengan menambahkan ekstensi, seperti layanan RAG (Retrieval-Augmented Generation) atau server MCP. Aplikasi ini dibangun menggunakan tumpukan teknologi modern, termasuk React, NestJS, dan Python FastAPI untuk layanan REI-S.\n\nPengguna dapat berinteraksi dengan asisten melalui antarmuka yang ramah pengguna. Tergantung pada konfigurasi asisten, pengguna dapat mengajukan pertanyaan, mengunggah file mereka sendiri, atau menggunakan fitur lainnya. Para asisten berinteraksi dengan berbagai penyedia LLM untuk memberikan respons berdasarkan ekstensi yang dikonfigurasi. Informasi kontekstual yang disediakan oleh ekstensi yang dikonfigurasi memungkinkan asisten menjawab pertanyaan spesifik domain dan memberikan informasi yang relevan.\n\nAplikasi ini dirancang secara modular dan dapat diperluas, memungkinkan pengguna membuat asisten dengan kemampuan berbeda dengan menambahkan ekstensi.\n\n![video demo singkat penggunaan dasar](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/preview.webp)\n\n## Fitur\n\n### Large Language Models (LLM) dan Model Multimodal\n\nc4 GenAI Suite sudah mendukung banyak model secara langsung. Jika model pilihan Anda belum didukung, seharusnya mudah untuk menulis ekstensi guna mendukungnya.\n\n* Model yang kompatibel dengan OpenAI\n* Model Azure OpenAI\n* Model Bedrock\n* Model Google GenAI\n* Model yang kompatibel dengan Ollama",
    "Status": "ok"
  },
  {
    "Id": 2,
    "Content": "### Retrieval Augmented Generation (RAG)\n\nThe c4 GenAI Suite includes REI-S, a server to prepare files for consumption by the LLM.\n\n* REI-S, a custom integrated RAG server\n  * Vector stores\n    * pgvector\n    * Azure AI Search\n  * Embedding models\n    * OpenAI compatible embeddings\n    * Azure OpenAI embeddings\n    * Ollama compatible embeddings\n  * File formats:\n    * pdf, docx, pptx, xlsx, ...\n    * audio file voice transcription (via Whisper)\n\n### Extensions\n\nThe c4 GenAI Suite is designed for extensibility. Writing extensions is easy, as is using an already existing MCP server.\n\n* Model Context Protocol (MCP) servers\n* Custom systemprompt\n* Bing Search\n* Calculator\n\n",
    "ContentSha": "wvT4e6071qGfJDWClvklgUJiJRM71gUVIUQKaaDgVPo=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### Retrieval Augmented Generation (RAG)\n\nc4 GenAI Suite mencakup REI-S, sebuah server untuk menyiapkan file agar dapat digunakan oleh LLM.\n\n* REI-S, server RAG terintegrasi khusus\n  * Penyimpanan vektor\n    * pgvector\n    * Azure AI Search\n  * Model embedding\n    * Embedding kompatibel OpenAI\n    * Embedding Azure OpenAI\n    * Embedding kompatibel Ollama\n  * Format file:\n    * pdf, docx, pptx, xlsx, ...\n    * transkripsi suara file audio (melalui Whisper)\n\n### Ekstensi\n\nc4 GenAI Suite dirancang untuk dapat diperluas. Menulis ekstensi sangat mudah, begitu juga menggunakan server MCP yang sudah ada.\n\n* Server Model Context Protocol (MCP)\n* Systemprompt kustom\n* Pencarian Bing\n* Kalkulator",
    "Status": "ok"
  },
  {
    "Id": 3,
    "Content": "## Getting Started\n\n### Using Docker-Compose\n\n- Run `docker compose up` in the project root.\n- Open the [application](http://localhost:3333) in a browser. The default login credentials are user `admin@example.com` and password `secret`.\n\n![video showing assistant configuration](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/assistants.webp)\n\n### Using Helm & Kubernetes\n\nFor deployment in Kubernetes environments, please refer to the [README of our Helm Chart](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/./helm-chart/README.md).\n\n### Setting up Assistants and Extensions\n\nThe c4 GenAI Suite revolves around *assistants*.\nEach assistant consists of a set of extensions, which determine the LLM model and which tools it can use.\n\n- In the admin area (click the username on the bottom left), go to the [assistants section](http://localhost:3333/admin/assistants).\n- Add an assistant with the green `+` button next to the section title. Choose a name and a description.\n- Select the created assistant and click the green `+ Add Extension`.\n- Select the model and fill in the credentials.\n- Use the `Test` Button to check that it works and `save`.\n\nNow you can return to the [chat page](http://localhost:3333/chat) (click on `c4 GenAI Suite` in the top left) and start a new conversation with your new assistant.\n\n> [!TIP]\n> Our `docker-compose` includes a local Ollama, which runs on the CPU. You can use this for quick testing. But it will be slow and you probably want to use another model. If you want to use it, just create the following model extension in your Assistant.\n> * Extension: `Dev: Ollama`\n> * Endpoint: `http://ollama:11434`\n> * Model: `llama3.2`\n",
    "ContentSha": "bLa8wjqGffqM6ThWqBWA15QpuTagUJbPEAqi4FPI4X4=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## Memulai\n\n### Menggunakan Docker-Compose\n\n- Jalankan `docker compose up` di root proyek.\n- Buka [aplikasi](http://localhost:3333) di browser. Kredensial login default adalah user `admin@example.com` dan password `secret`.\n\n![video menunjukkan konfigurasi asisten](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/assistants.webp)\n\n### Menggunakan Helm & Kubernetes\n\nUntuk deployment di lingkungan Kubernetes, silakan merujuk ke [README dari Helm Chart kami](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/./helm-chart/README.md).\n\n### Menyiapkan Asisten dan Ekstensi\n\nc4 GenAI Suite berpusat pada *asisten*.\nSetiap asisten terdiri dari satu set ekstensi, yang menentukan model LLM dan alat apa yang dapat digunakannya.\n\n- Di area admin (klik nama pengguna di kiri bawah), pergi ke [bagian asisten](http://localhost:3333/admin/assistants).\n- Tambahkan asisten dengan tombol hijau `+` di sebelah judul bagian. Pilih nama dan deskripsi.\n- Pilih asisten yang telah dibuat dan klik hijau `+ Add Extension`.\n- Pilih model dan isi kredensial.\n- Gunakan tombol `Test` untuk memeriksa apakah sudah berfungsi dan `save`.\n\nSekarang Anda dapat kembali ke [halaman chat](http://localhost:3333/chat) (klik pada `c4 GenAI Suite` di kiri atas) dan memulai percakapan baru dengan asisten baru Anda.\n\n> [!TIP]\n> `docker-compose` kami sudah termasuk Ollama lokal, yang berjalan di CPU. Anda dapat menggunakannya untuk pengujian cepat. Namun kecepatannya lambat dan Anda mungkin ingin menggunakan model lain. Jika ingin menggunakannya, cukup buat ekstensi model berikut di Asisten Anda.\n> * Ekstensi: `Dev: Ollama`\n> * Endpoint: `http://ollama:11434`\n> * Model: `llama3.2`\n",
    "Status": "ok"
  },
  {
    "Id": 4,
    "Content": "### Model Context Protocol (MCP) [optional]\n\nUse any MCP server offering an `sse` interface with the `MCP Tools` Extension (or use our `mcp-tool-as-server` as a proxy in front of an `stdio` MCP server).\nEach MCP server can be configured in detail as an extension.\n\n### Retrieval Augmented Generation (RAG) / File Search [optional]\n\nUse our RAG server `REI-S` to search user provided files. Just configure a `Search Files` extension for the assistant.\nThis process is described in detail in [the `services/reis` subdirectory](services/reis/#example-configuration-in-c4).\n\n## Contributing & Development\n\n* See [CONTRIBUTING.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/CONTRIBUTING.md) for guidelines on how to contribute.\n* For developer onboarding, check [DEVELOPERS.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/DEVELOPERS.md).\n\n## Main Building Blocks\n\nThe application consists of a **Frontend** , a **Backend**  and a **REI-S**  service.\n\n```",
    "ContentSha": "X5WCsc3MMbiH/lsJf83tTEO6Cfl/16Nw2tPJwgCKYpc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### Model Context Protocol (MCP) [opsional]\n\nGunakan server MCP apa pun yang menyediakan antarmuka `sse` dengan Ekstensi `MCP Tools` (atau gunakan `mcp-tool-as-server` kami sebagai proxy di depan server MCP `stdio`).\nSetiap server MCP dapat dikonfigurasi secara detail sebagai ekstensi.\n\n### Retrieval Augmented Generation (RAG) / Pencarian Berkas [opsional]\n\nGunakan server RAG kami `REI-S` untuk mencari berkas yang disediakan pengguna. Cukup konfigurasikan ekstensi `Search Files` untuk asisten.\nProses ini dijelaskan secara rinci di [subdirektori `services/reis`](services/reis/#example-configuration-in-c4).\n\n## Kontribusi & Pengembangan\n\n* Lihat [CONTRIBUTING.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/CONTRIBUTING.md) untuk panduan cara berkontribusi.\n* Untuk onboarding pengembang, periksa [DEVELOPERS.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/DEVELOPERS.md).\n\n## Blok Bangunan Utama\n\nAplikasi ini terdiri dari **Frontend**, **Backend**, dan layanan **REI-S**.\n\n```",
    "Status": "ok"
  },
  {
    "Id": 5,
    "Content": "┌──────────┐\n│   User   │\n└─────┬────┘\n      │ access\n      ▼\n┌──────────┐\n│ Frontend │\n└─────┬────┘\n      │ access\n      ▼\n┌──────────┐     ┌─────────────────┐\n│ Backend  │────►│      LLM        │\n└─────┬────┘     └─────────────────┘\n      │ access\n      ▼\n┌──────────┐     ┌─────────────────┐\n│  REI-S   │────►│ Embedding Model │\n│          │     └─────────────────┘\n│          │\n│          │     ┌─────────────────┐\n│          │────►│  Vector Store   │\n└──────────┘     └─────────────────┘\n```",
    "ContentSha": "IO09RXKALRyYzMSXIVzGJTmyROh56hbqbTTI4NQfxtQ=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "┌──────────┐\n│  Pengguna│\n└─────┬────┘\n      │ akses\n      ▼\n┌──────────┐\n│ Frontend │\n└─────┬────┘\n      │ akses\n      ▼\n┌──────────┐     ┌─────────────────┐\n│ Backend  │────►│      LLM        │\n└─────┬────┘     └─────────────────┘\n      │ akses\n      ▼\n┌──────────┐     ┌─────────────────┐\n│  REI-S   │────►│ Embedding Model │\n│          │     └─────────────────┘\n│          │\n│          │     ┌─────────────────┐\n│          │────►│  Vector Store   │\n└──────────┘     └─────────────────┘\n```",
    "Status": "ok"
  },
  {
    "Id": 6,
    "Content": "\n\n### Frontend\n\nThe frontend is built with React and TypeScript, providing a user-friendly interface for interacting with the backend and REI-S service. It includes features for managing assistants, extensions, and chat functionalities.\n\n> Sources: `/frontend`\n\n### Backend\n\nThe backend is developed using NestJS and TypeScript, serving as the main API layer for the application. It handles requests from the frontend and interacts with llm providers to facilitate chat functionalities. The backend also manages assistants and their extensions, allowing users to configure and use various AI models for their chats.\n\nAdditionally, the backend manages user authentication, and communicates with the REI-S service for file indexing and retrieval.\n\nFor data persistence, the backend uses a **PostgreSQL** database.\n\n> Sources: `/backend`\n\n### REI-S\n\nThe REI-S (**R**etrieval **E**xtraction **I**ngestion **S**erver) is a Python-based server that provides basic RAG (Retrieval-Augmented Generation) capabilities. It allows for file content extraction, indexing and querying, enabling the application to handle large datasets efficiently. The REI-S service is designed to work seamlessly with the backend, providing necessary data for chat functionalities and file searches.\n\nThe REI-S supports Azure AI Search and pgvector for vector storage, allowing for flexible and scalable data retrieval options. The service can be configured using environment variables to specify the type of vector store and connection details.\n\n> Sources: `/services/reis`\n",
    "ContentSha": "Sceh4YX3ZYIBDEtdTMWIBUucYWH+hEUJMben1cxfrAc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### Frontend\n\nFrontend dibangun dengan React dan TypeScript, menyediakan antarmuka yang ramah pengguna untuk berinteraksi dengan backend dan layanan REI-S. Frontend ini mencakup fitur untuk mengelola asisten, ekstensi, dan fungsionalitas chat.\n\n> Sumber: `/frontend`\n\n### Backend\n\nBackend dikembangkan menggunakan NestJS dan TypeScript, berfungsi sebagai lapisan API utama untuk aplikasi. Backend menangani permintaan dari frontend dan berinteraksi dengan penyedia llm untuk memfasilitasi fungsionalitas chat. Backend juga mengelola asisten dan ekstensi mereka, memungkinkan pengguna untuk mengonfigurasi dan menggunakan berbagai model AI untuk chat mereka.\n\nSelain itu, backend mengelola autentikasi pengguna, dan berkomunikasi dengan layanan REI-S untuk pengindeksan dan pengambilan file.\n\nUntuk persistensi data, backend menggunakan database **PostgreSQL**.\n\n> Sumber: `/backend`\n\n### REI-S\n\nREI-S (**R**etrieval **E**xtraction **I**ngestion **S**erver) adalah server berbasis Python yang menyediakan kemampuan dasar RAG (Retrieval-Augmented Generation). Server ini memungkinkan ekstraksi konten file, pengindeksan, dan pencarian, sehingga aplikasi dapat menangani dataset besar secara efisien. Layanan REI-S dirancang agar dapat bekerja secara mulus dengan backend, menyediakan data yang diperlukan untuk fungsionalitas chat dan pencarian file.\n\nREI-S mendukung Azure AI Search dan pgvector untuk penyimpanan vektor, memungkinkan opsi pengambilan data yang fleksibel dan skalabel. Layanan ini dapat dikonfigurasi menggunakan variabel lingkungan untuk menentukan jenis penyimpanan vektor dan detail koneksi.\n\n> Sumber: `/services/reis`",
    "Status": "ok"
  }
]