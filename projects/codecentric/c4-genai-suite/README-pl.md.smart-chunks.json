[
  {
    "Id": 1,
    "Content": "# c4 GenAI Suite\n\nAn AI chatbot application with Model Context Provider (MCP) integration, powered by Langchain and compatibility for all major Large Language Models (LLMs) and Embedding Models.\n\nAdministrators can create assistants with different capabilities by adding extensions, such as RAG (Retrieval-Augmented Generation) services or MCP servers. The application is built using a modern tech stack, including React, NestJS, and Python FastAPI for the REI-S service.\n\nUsers can interact with assistants through a user-friendly interface. Depending on the assistant's configuration, users may be able to ask questions, upload their own files, or use other features. The assistants interact with various LLM providers to provide responses based on the configured extensions. Contextual information provided by the configured extensions allows the assistants to answer domain-specific questions and provide relevant information.\n\nThe application is designed to be modular and extensible, allowing users to create assistants with different capabilities by adding extensions.\n\n![short demo video of basic usage](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/preview.webp)\n\n## Features\n\n### Large Language Models (LLM) and Multimodal Models\n\nThe c4 GenAI Suite supports already many models directly. And if your preferred model is not supported already, it should be easy to write an extension to support it.\n\n* OpenAI compatible models\n* Azure OpenAI models\n* Bedrock models\n* Google GenAI models\n* Ollama compatible models\n",
    "ContentSha": "VePwjxi+F5ttFJfuckWjhr5ujnZTyNWk2c6pu+FMFPM=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "# c4 GenAI Suite\n\nAplikacja chatbot AI z integracją Model Context Provider (MCP), oparta na Langchain i kompatybilna ze wszystkimi głównymi dużymi modelami językowymi (LLM) oraz modelami osadzającymi.\n\nAdministratorzy mogą tworzyć asystentów o różnych możliwościach poprzez dodawanie rozszerzeń, takich jak usługi RAG (Retrieval-Augmented Generation) lub serwery MCP. Aplikacja jest zbudowana w oparciu o nowoczesny stos technologiczny, w tym React, NestJS oraz Python FastAPI dla usługi REI-S.\n\nUżytkownicy mogą wchodzić w interakcje z asystentami za pomocą przyjaznego interfejsu użytkownika. W zależności od konfiguracji asystenta, użytkownicy mogą zadawać pytania, przesyłać własne pliki lub korzystać z innych funkcji. Asystenci komunikują się z różnymi dostawcami LLM, aby udzielać odpowiedzi w oparciu o skonfigurowane rozszerzenia. Informacje kontekstowe dostarczane przez skonfigurowane rozszerzenia pozwalają asystentom odpowiadać na pytania specyficzne dla danej dziedziny i dostarczać odpowiednich informacji.\n\nAplikacja została zaprojektowana jako modułowa i rozszerzalna, umożliwiając użytkownikom tworzenie asystentów o różnych możliwościach poprzez dodawanie rozszerzeń.\n\n![krótki film demo z podstawowym użyciem](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/preview.webp)\n\n## Funkcje\n\n### Duże modele językowe (LLM) i modele multimodalne\n\nc4 GenAI Suite obsługuje już wiele modeli bezpośrednio. Jeśli jednak Twój preferowany model nie jest jeszcze obsługiwany, powinno być łatwo napisać rozszerzenie, aby go obsłużyć.\n\n* Modele kompatybilne z OpenAI\n* Modele Azure OpenAI\n* Modele Bedrock\n* Modele Google GenAI\n* Modele kompatybilne z Ollama",
    "Status": "ok"
  },
  {
    "Id": 2,
    "Content": "### Retrieval Augmented Generation (RAG)\n\nThe c4 GenAI Suite includes REI-S, a server to prepare files for consumption by the LLM.\n\n* REI-S, a custom integrated RAG server\n  * Vector stores\n    * pgvector\n    * Azure AI Search\n  * Embedding models\n    * OpenAI compatible embeddings\n    * Azure OpenAI embeddings\n    * Ollama compatible embeddings\n  * File formats:\n    * pdf, docx, pptx, xlsx, ...\n    * audio file voice transcription (via Whisper)\n\n### Extensions\n\nThe c4 GenAI Suite is designed for extensibility. Writing extensions is easy, as is using an already existing MCP server.\n\n* Model Context Protocol (MCP) servers\n* Custom systemprompt\n* Bing Search\n* Calculator\n\n",
    "ContentSha": "wvT4e6071qGfJDWClvklgUJiJRM71gUVIUQKaaDgVPo=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### Retrieval Augmented Generation (RAG)\n\nPakiet c4 GenAI Suite zawiera REI-S, serwer do przygotowywania plików do wykorzystania przez LLM.\n\n* REI-S, niestandardowy zintegrowany serwer RAG\n  * Magazyny wektorowe\n    * pgvector\n    * Azure AI Search\n  * Modele osadzania\n    * Osadzania kompatybilne z OpenAI\n    * Osadzania Azure OpenAI\n    * Osadzania kompatybilne z Ollama\n  * Format plików:\n    * pdf, docx, pptx, xlsx, ...\n    * transkrypcja głosu z pliku audio (przez Whisper)\n\n### Rozszerzenia\n\nPakiet c4 GenAI Suite został zaprojektowany z myślą o rozszerzalności. Pisanie rozszerzeń jest łatwe, podobnie jak korzystanie z już istniejącego serwera MCP.\n\n* Serwery Model Context Protocol (MCP)\n* Niestandardowy systemprompt\n* Wyszukiwanie Bing\n* Kalkulator",
    "Status": "ok"
  },
  {
    "Id": 3,
    "Content": "## Getting Started\n\n### Using Docker-Compose\n\n- Run `docker compose up` in the project root.\n- Open the [application](http://localhost:3333) in a browser. The default login credentials are user `admin@example.com` and password `secret`.\n\n![video showing assistant configuration](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/assistants.webp)\n\n### Using Helm & Kubernetes\n\nFor deployment in Kubernetes environments, please refer to the [README of our Helm Chart](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/./helm-chart/README.md).\n\n### Setting up Assistants and Extensions\n\nThe c4 GenAI Suite revolves around *assistants*.\nEach assistant consists of a set of extensions, which determine the LLM model and which tools it can use.\n\n- In the admin area (click the username on the bottom left), go to the [assistants section](http://localhost:3333/admin/assistants).\n- Add an assistant with the green `+` button next to the section title. Choose a name and a description.\n- Select the created assistant and click the green `+ Add Extension`.\n- Select the model and fill in the credentials.\n- Use the `Test` Button to check that it works and `save`.\n\nNow you can return to the [chat page](http://localhost:3333/chat) (click on `c4 GenAI Suite` in the top left) and start a new conversation with your new assistant.\n\n> [!TIP]\n> Our `docker-compose` includes a local Ollama, which runs on the CPU. You can use this for quick testing. But it will be slow and you probably want to use another model. If you want to use it, just create the following model extension in your Assistant.\n> * Extension: `Dev: Ollama`\n> * Endpoint: `http://ollama:11434`\n> * Model: `llama3.2`\n",
    "ContentSha": "bLa8wjqGffqM6ThWqBWA15QpuTagUJbPEAqi4FPI4X4=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## Pierwsze kroki\n\n### Używanie Docker-Compose\n\n- Uruchom `docker compose up` w katalogu głównym projektu.\n- Otwórz [aplikację](http://localhost:3333) w przeglądarce. Domyślne dane logowania to użytkownik `admin@example.com` i hasło `secret`.\n\n![wideo pokazujące konfigurację asystenta](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/assistants.webp)\n\n### Używanie Helm & Kubernetes\n\nAby wdrożyć w środowisku Kubernetes, zapoznaj się z [README naszego Helm Chart](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/./helm-chart/README.md).\n\n### Konfiguracja Asystentów i Rozszerzeń\n\nc4 GenAI Suite opiera się na *asystentach*.\nKażdy asystent składa się z zestawu rozszerzeń, które określają model LLM oraz narzędzia, z których może korzystać.\n\n- W obszarze administracyjnym (kliknij nazwę użytkownika w lewym dolnym rogu) przejdź do [sekcji asystentów](http://localhost:3333/admin/assistants).\n- Dodaj asystenta przyciskiem z zielonym `+` obok tytułu sekcji. Wybierz nazwę i opis.\n- Wybierz utworzonego asystenta i kliknij zielony przycisk `+ Dodaj rozszerzenie`.\n- Wybierz model i uzupełnij dane uwierzytelniające.\n- Użyj przycisku `Test`, aby sprawdzić działanie, a następnie `zapisz`.\n\nTeraz możesz wrócić do [strony czatu](http://localhost:3333/chat) (kliknij `c4 GenAI Suite` w lewym górnym rogu) i rozpocząć nową rozmowę z nowym asystentem.\n\n> [!TIP]\n> Nasz `docker-compose` zawiera lokalną instancję Ollama, która działa na CPU. Możesz użyć jej do szybkiego testowania, ale będzie wolna i prawdopodobnie będziesz chciał użyć innego modelu. Jeśli chcesz z niej skorzystać, utwórz poniższe rozszerzenie modelu w swoim Asystencie.\n> * Rozszerzenie: `Dev: Ollama`\n> * Endpoint: `http://ollama:11434`\n> * Model: `llama3.2`\n",
    "Status": "ok"
  },
  {
    "Id": 4,
    "Content": "### Model Context Protocol (MCP) [optional]\n\nUse any MCP server offering an `sse` interface with the `MCP Tools` Extension (or use our `mcp-tool-as-server` as a proxy in front of an `stdio` MCP server).\nEach MCP server can be configured in detail as an extension.\n\n### Retrieval Augmented Generation (RAG) / File Search [optional]\n\nUse our RAG server `REI-S` to search user provided files. Just configure a `Search Files` extension for the assistant.\nThis process is described in detail in [the `services/reis` subdirectory](services/reis/#example-configuration-in-c4).\n\n## Contributing & Development\n\n* See [CONTRIBUTING.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/CONTRIBUTING.md) for guidelines on how to contribute.\n* For developer onboarding, check [DEVELOPERS.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/DEVELOPERS.md).\n\n## Main Building Blocks\n\nThe application consists of a **Frontend** , a **Backend**  and a **REI-S**  service.\n\n```",
    "ContentSha": "X5WCsc3MMbiH/lsJf83tTEO6Cfl/16Nw2tPJwgCKYpc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### Protokół Model Context Protocol (MCP) [opcjonalnie]\n\nUżyj dowolnego serwera MCP oferującego interfejs `sse` z rozszerzeniem `MCP Tools` (lub użyj naszego `mcp-tool-as-server` jako proxy przed serwerem MCP korzystającym z `stdio`).\nKażdy serwer MCP może być skonfigurowany szczegółowo jako rozszerzenie.\n\n### Retrieval Augmented Generation (RAG) / Wyszukiwanie w plikach [opcjonalnie]\n\nUżyj naszego serwera RAG `REI-S`, aby przeszukiwać pliki udostępnione przez użytkownika. Wystarczy skonfigurować rozszerzenie `Search Files` dla asystenta.\nProces ten został szczegółowo opisany w [podkatalogu `services/reis`](services/reis/#example-configuration-in-c4).\n\n## Wkład i rozwój\n\n* Zobacz [CONTRIBUTING.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/CONTRIBUTING.md), aby zapoznać się z wytycznymi dotyczącymi wkładu.\n* Dla wdrożenia programistów sprawdź [DEVELOPERS.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/DEVELOPERS.md).\n\n## Główne elementy aplikacji\n\nAplikacja składa się z **Frontendu**, **Backendu** oraz usługi **REI-S**.\n\n```",
    "Status": "ok"
  },
  {
    "Id": 5,
    "Content": "┌──────────┐\n│   User   │\n└─────┬────┘\n      │ access\n      ▼\n┌──────────┐\n│ Frontend │\n└─────┬────┘\n      │ access\n      ▼\n┌──────────┐     ┌─────────────────┐\n│ Backend  │────►│      LLM        │\n└─────┬────┘     └─────────────────┘\n      │ access\n      ▼\n┌──────────┐     ┌─────────────────┐\n│  REI-S   │────►│ Embedding Model │\n│          │     └─────────────────┘\n│          │\n│          │     ┌─────────────────┐\n│          │────►│  Vector Store   │\n└──────────┘     └─────────────────┘\n```",
    "ContentSha": "IO09RXKALRyYzMSXIVzGJTmyROh56hbqbTTI4NQfxtQ=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "┌──────────┐\n│   Użytkownik   │\n└─────┬────┘\n      │ dostęp\n      ▼\n┌──────────┐\n│ Frontend │\n└─────┬────┘\n      │ dostęp\n      ▼\n┌──────────┐     ┌─────────────────┐\n│ Backend  │────►│      LLM        │\n└─────┬────┘     └─────────────────┘\n      │ dostęp\n      ▼\n┌──────────┐     ┌─────────────────┐\n│  REI-S   │────►│ Model osadzania │\n│          │     └─────────────────┘\n│          │\n│          │     ┌─────────────────┐\n│          │────►│  Magazyn wektorów │\n└──────────┘     └─────────────────┘\n```",
    "Status": "ok"
  },
  {
    "Id": 6,
    "Content": "\n\n### Frontend\n\nThe frontend is built with React and TypeScript, providing a user-friendly interface for interacting with the backend and REI-S service. It includes features for managing assistants, extensions, and chat functionalities.\n\n> Sources: `/frontend`\n\n### Backend\n\nThe backend is developed using NestJS and TypeScript, serving as the main API layer for the application. It handles requests from the frontend and interacts with llm providers to facilitate chat functionalities. The backend also manages assistants and their extensions, allowing users to configure and use various AI models for their chats.\n\nAdditionally, the backend manages user authentication, and communicates with the REI-S service for file indexing and retrieval.\n\nFor data persistence, the backend uses a **PostgreSQL** database.\n\n> Sources: `/backend`\n\n### REI-S\n\nThe REI-S (**R**etrieval **E**xtraction **I**ngestion **S**erver) is a Python-based server that provides basic RAG (Retrieval-Augmented Generation) capabilities. It allows for file content extraction, indexing and querying, enabling the application to handle large datasets efficiently. The REI-S service is designed to work seamlessly with the backend, providing necessary data for chat functionalities and file searches.\n\nThe REI-S supports Azure AI Search and pgvector for vector storage, allowing for flexible and scalable data retrieval options. The service can be configured using environment variables to specify the type of vector store and connection details.\n\n> Sources: `/services/reis`\n",
    "ContentSha": "Sceh4YX3ZYIBDEtdTMWIBUucYWH+hEUJMben1cxfrAc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### Frontend\n\nFrontend został zbudowany z użyciem React i TypeScript, zapewniając przyjazny dla użytkownika interfejs do interakcji z backendem oraz usługą REI-S. Obejmuje funkcje do zarządzania asystentami, rozszerzeniami oraz funkcjonalnościami czatu.\n\n> Źródła: `/frontend`\n\n### Backend\n\nBackend został opracowany przy użyciu NestJS i TypeScript, służąc jako główna warstwa API dla aplikacji. Obsługuje żądania z frontendu oraz współpracuje z dostawcami LLM w celu realizacji funkcji czatu. Backend zarządza także asystentami i ich rozszerzeniami, umożliwiając użytkownikom konfigurowanie i wykorzystywanie różnych modeli AI w ich czatach.\n\nDodatkowo backend zarządza uwierzytelnianiem użytkowników oraz komunikuje się z usługą REI-S w celu indeksowania i pobierania plików.\n\nDo trwałego przechowywania danych backend wykorzystuje bazę danych **PostgreSQL**.\n\n> Źródła: `/backend`\n\n### REI-S\n\nREI-S (**R**etrieval **E**xtraction **I**ngestion **S**erver) to serwer oparty na Pythonie, który zapewnia podstawowe możliwości RAG (Retrieval-Augmented Generation). Umożliwia ekstrakcję zawartości plików, indeksowanie i zapytania, pozwalając aplikacji na efektywną obsługę dużych zbiorów danych. Usługa REI-S została zaprojektowana do bezproblemowej współpracy z backendem, dostarczając niezbędnych danych do funkcjonalności czatu i wyszukiwania plików.\n\nREI-S obsługuje Azure AI Search oraz pgvector do przechowywania wektorów, umożliwiając elastyczne i skalowalne opcje wyszukiwania danych. Usługę można skonfigurować za pomocą zmiennych środowiskowych w celu określenia typu magazynu wektorów oraz szczegółów połączenia.\n\n> Źródła: `/services/reis`\n",
    "Status": "ok"
  }
]