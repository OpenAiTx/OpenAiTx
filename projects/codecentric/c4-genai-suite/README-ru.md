# c4 GenAI Suite

Приложение чат-бота с интеграцией Model Context Provider (MCP), построенное на базе Langchain и совместимое со всеми основными Большими Языковыми Моделями (LLM) и моделями эмбеддинга.

Администраторы могут создавать ассистентов с различными возможностями, добавляя расширения, такие как сервисы RAG (генерация с расширенным поиском) или MCP-серверы. Приложение построено с использованием современного стека технологий, включая React, NestJS и Python FastAPI для сервиса REI-S.

Пользователи могут взаимодействовать с ассистентами через удобный пользовательский интерфейс. В зависимости от конфигурации ассистента пользователи могут задавать вопросы, загружать собственные файлы или использовать другие функции. Ассистенты взаимодействуют с различными LLM-провайдерами для предоставления ответов на основе настроенных расширений. Контекстная информация, предоставляемая настроенными расширениями, позволяет ассистентам отвечать на вопросы, относящиеся к определённой предметной области, и предоставлять актуальную информацию.

Приложение разработано модульным и расширяемым, что позволяет пользователям создавать ассистентов с различными возможностями, добавляя расширения.

![короткое демонстрационное видео базового использования](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/preview.webp)

## Возможности

### Большие языковые модели (LLM) и мультимодальные модели

c4 GenAI Suite уже поддерживает множество моделей напрямую. Если ваша предпочитаемая модель ещё не поддерживается, написать расширение для её поддержки должно быть просто.

* Модели, совместимые с OpenAI
* Модели Azure OpenAI
* Модели Bedrock
* Модели Google GenAI
* Модели, совместимые с Ollama
### Генерация с дополнением извлечённой информации (RAG)

Пакет c4 GenAI включает REI-S — сервер для подготовки файлов к использованию LLM.

* REI-S, специализированный интегрированный RAG-сервер
  * Векторные хранилища
    * pgvector
    * Azure AI Search
  * Модели эмбеддингов
    * эмбеддинги, совместимые с OpenAI
    * эмбеддинги Azure OpenAI
    * эмбеддинги, совместимые с Ollama
  * Форматы файлов:
    * pdf, docx, pptx, xlsx, ...
    * транскрипция голосовых аудиофайлов (через Whisper)

### Расширения

Пакет c4 GenAI разработан с учётом расширяемости. Создавать расширения просто, как и использовать уже существующий MCP-сервер.

* Серверы протокола Model Context Protocol (MCP)
* Пользовательский systemprompt
* Bing Search
* Калькулятор
## Начало работы

### Использование Docker-Compose

- Запустите `docker compose up` в корневой папке проекта.
- Откройте [приложение](http://localhost:3333) в браузере. Стандартные учетные данные для входа: пользователь `admin@example.com` и пароль `secret`.

![видео, показывающее настройку ассистента](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/assistants.webp)

### Использование Helm и Kubernetes

Для развертывания в средах Kubernetes, пожалуйста, обратитесь к [README нашего Helm Chart](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/./helm-chart/README.md).

### Настройка ассистентов и расширений

C4 GenAI Suite строится вокруг *ассистентов*.
Каждый ассистент состоит из набора расширений, которые определяют LLM-модель и доступные инструменты.

- В административной панели (нажмите на имя пользователя в левом нижнем углу) перейдите в [раздел ассистентов](http://localhost:3333/admin/assistants).
- Добавьте ассистента с помощью зеленой кнопки `+` рядом с заголовком раздела. Выберите имя и описание.
- Выберите созданного ассистента и нажмите зеленую кнопку `+ Добавить расширение`.
- Выберите модель и заполните учетные данные.
- Используйте кнопку `Test`, чтобы проверить работоспособность, и затем `сохранить`.

Теперь вы можете вернуться на [страницу чата](http://localhost:3333/chat) (нажмите на `c4 GenAI Suite` в левом верхнем углу) и начать новый разговор с вашим новым ассистентом.

> [!ПОДСКАЗКА]
> Наш `docker-compose` включает локальный Ollama, который работает на CPU. Вы можете использовать его для быстрого тестирования. Однако он будет работать медленно, и, вероятно, вы захотите использовать другую модель. Если вы хотите воспользоваться им, просто создайте следующее расширение модели в вашем Ассистенте.
> * Расширение: `Dev: Ollama`
> * Endpoint: `http://ollama:11434`
> * Модель: `llama3.2`
### Протокол Контекста Модели (MCP) [необязательно]

Используйте любой MCP-сервер, предоставляющий интерфейс `sse` с расширением `MCP Tools` (или используйте наш `mcp-tool-as-server` как прокси перед MCP-сервером с интерфейсом `stdio`).
Каждый MCP-сервер может быть детально настроен как расширение.

### Генерация с поддержкой поиска (RAG) / Поиск по файлам [необязательно]

Используйте наш RAG-сервер `REI-S` для поиска по предоставленным пользователем файлам. Просто настройте расширение `Search Files` для ассистента.
Этот процесс подробно описан в [подкаталоге `services/reis`](services/reis/#example-configuration-in-c4).

## Вклад и разработка

* См. [CONTRIBUTING.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/CONTRIBUTING.md) для получения рекомендаций по внесению вклада.
* Для ознакомления разработчиков смотрите [DEVELOPERS.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/DEVELOPERS.md).

## Основные строительные блоки

Приложение состоит из **Frontend** , **Backend**  и сервиса **REI-S** .

```
┌──────────┐
│  Пользователь  │
└─────┬────┘
      │ доступ
      ▼
┌──────────┐
│  Фронтенд │
└─────┬────┘
      │ доступ
      ▼
┌──────────┐     ┌─────────────────┐
│ Бэкенд   │────►│      LLM        │
└─────┬────┘     └─────────────────┘
      │ доступ
      ▼
┌──────────┐     ┌─────────────────┐
│  REI-S   │────►│ Embedding Model │
│          │     └─────────────────┘
│          │
│          │     ┌─────────────────┐
│          │────►│  Vector Store   │
└──────────┘     └─────────────────┘
```
### Фронтенд

Фронтенд построен с использованием React и TypeScript, обеспечивая удобный пользовательский интерфейс для взаимодействия с бекендом и сервисом REI-S. Включает функции управления ассистентами, расширениями и чат-возможностями.

> Источники: `/frontend`

### Бекенд

Бекенд разработан с использованием NestJS и TypeScript и служит основным API-слоем приложения. Он обрабатывает запросы от фронтенда и взаимодействует с провайдерами LLM для обеспечения чат-функциональности. Бекенд также управляет ассистентами и их расширениями, позволяя пользователям настраивать и использовать различные AI-модели для своих чатов.

Кроме того, бекенд управляет аутентификацией пользователей и взаимодействует с сервисом REI-S для индексации и получения файлов.

Для хранения данных бекенд использует базу данных **PostgreSQL**.

> Источники: `/backend`

### REI-S

REI-S (**R**etrieval **E**xtraction **I**ngestion **S**erver) — сервер на базе Python, обеспечивающий базовые возможности RAG (Retrieval-Augmented Generation). Он позволяет извлекать содержимое файлов, индексировать и выполнять запросы, что позволяет приложению эффективно работать с большими наборами данных. Сервис REI-S спроектирован для бесшовной работы с бекендом, предоставляя необходимые данные для чатов и поиска по файлам.

REI-S поддерживает Azure AI Search и pgvector для хранения векторов, что обеспечивает гибкие и масштабируемые варианты поиска данных. Сервис можно настроить с помощью переменных окружения для указания типа векторного хранилища и параметров подключения.

> Источники: `/services/reis`

---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-09

---