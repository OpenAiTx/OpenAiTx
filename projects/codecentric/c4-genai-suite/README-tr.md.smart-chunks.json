[
  {
    "Id": 1,
    "Content": "# c4 GenAI Suite\n\nAn AI chatbot application with Model Context Provider (MCP) integration, powered by Langchain and compatibility for all major Large Language Models (LLMs) and Embedding Models.\n\nAdministrators can create assistants with different capabilities by adding extensions, such as RAG (Retrieval-Augmented Generation) services or MCP servers. The application is built using a modern tech stack, including React, NestJS, and Python FastAPI for the REI-S service.\n\nUsers can interact with assistants through a user-friendly interface. Depending on the assistant's configuration, users may be able to ask questions, upload their own files, or use other features. The assistants interact with various LLM providers to provide responses based on the configured extensions. Contextual information provided by the configured extensions allows the assistants to answer domain-specific questions and provide relevant information.\n\nThe application is designed to be modular and extensible, allowing users to create assistants with different capabilities by adding extensions.\n\n![short demo video of basic usage](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/preview.webp)\n\n## Features\n\n### Large Language Models (LLM) and Multimodal Models\n\nThe c4 GenAI Suite supports already many models directly. And if your preferred model is not supported already, it should be easy to write an extension to support it.\n\n* OpenAI compatible models\n* Azure OpenAI models\n* Bedrock models\n* Google GenAI models\n* Ollama compatible models\n",
    "ContentSha": "VePwjxi+F5ttFJfuckWjhr5ujnZTyNWk2c6pu+FMFPM=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "# c4 GenAI Suite\n\nModel Context Provider (MCP) entegrasyonuna sahip, Langchain ile güçlendirilmiş ve tüm büyük Büyük Dil Modelleri (LLM'ler) ve Gömme Modelleri ile uyumlu bir yapay zeka sohbet botu uygulaması.\n\nYöneticiler, RAG (Retrieval-Augmented Generation) servisleri veya MCP sunucuları gibi uzantılar ekleyerek farklı yeteneklere sahip asistanlar oluşturabilirler. Uygulama, modern bir teknoloji yığını kullanılarak geliştirilmiştir; bunlar arasında React, NestJS ve REI-S servisi için Python FastAPI bulunmaktadır.\n\nKullanıcılar, kullanıcı dostu bir arayüz üzerinden asistanlarla etkileşime girebilirler. Asistanın yapılandırmasına bağlı olarak, kullanıcılar soru sorabilir, kendi dosyalarını yükleyebilir veya diğer özellikleri kullanabilirler. Asistanlar, yanıtlarını yapılandırılmış uzantılara göre çeşitli LLM sağlayıcılarıyla etkileşime girerek sunar. Yapılandırılmış uzantılar tarafından sağlanan bağlamsal bilgiler, asistanların alanına özgü soruları yanıtlamasına ve ilgili bilgileri sunmasına olanak tanır.\n\nUygulama, modüler ve genişletilebilir olacak şekilde tasarlanmıştır; böylece kullanıcılar, uzantılar ekleyerek farklı yeteneklere sahip asistanlar oluşturabilirler.\n\n![kısa temel kullanım demosu videosu](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/preview.webp)\n\n## Özellikler\n\n### Büyük Dil Modelleri (LLM) ve Çok Modlu Modeller\n\nc4 GenAI Suite zaten birçok modeli doğrudan desteklemektedir. Eğer tercih ettiğiniz model henüz desteklenmiyorsa, onu destekleyecek bir uzantı yazmak kolay olmalıdır.\n\n* OpenAI uyumlu modeller\n* Azure OpenAI modelleri\n* Bedrock modelleri\n* Google GenAI modelleri\n* Ollama uyumlu modeller",
    "Status": "ok"
  },
  {
    "Id": 2,
    "Content": "### Retrieval Augmented Generation (RAG)\n\nThe c4 GenAI Suite includes REI-S, a server to prepare files for consumption by the LLM.\n\n* REI-S, a custom integrated RAG server\n  * Vector stores\n    * pgvector\n    * Azure AI Search\n  * Embedding models\n    * OpenAI compatible embeddings\n    * Azure OpenAI embeddings\n    * Ollama compatible embeddings\n  * File formats:\n    * pdf, docx, pptx, xlsx, ...\n    * audio file voice transcription (via Whisper)\n\n### Extensions\n\nThe c4 GenAI Suite is designed for extensibility. Writing extensions is easy, as is using an already existing MCP server.\n\n* Model Context Protocol (MCP) servers\n* Custom systemprompt\n* Bing Search\n* Calculator\n\n",
    "ContentSha": "wvT4e6071qGfJDWClvklgUJiJRM71gUVIUQKaaDgVPo=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### Retrieval Augmented Generation (RAG)\n\nc4 GenAI Suite, LLM tarafından kullanılmak üzere dosyaları hazırlayan bir sunucu olan REI-S'i içerir.\n\n* REI-S, özel entegre edilmiş bir RAG sunucusu\n  * Vektör depoları\n    * pgvector\n    * Azure AI Search\n  * Gömme modelleri\n    * OpenAI uyumlu gömmeler\n    * Azure OpenAI gömmeleri\n    * Ollama uyumlu gömmeler\n  * Dosya formatları:\n    * pdf, docx, pptx, xlsx, ...\n    * ses dosyası sesli transkripsiyon (Whisper aracılığıyla)\n\n### Uzantılar\n\nc4 GenAI Suite, genişletilebilirlik için tasarlanmıştır. Uzantı yazmak ve mevcut bir MCP sunucusunu kullanmak kolaydır.\n\n* Model Context Protocol (MCP) sunucuları\n* Özel systemprompt\n* Bing Arama\n* Hesap makinesi",
    "Status": "ok"
  },
  {
    "Id": 3,
    "Content": "## Getting Started\n\n### Using Docker-Compose\n\n- Run `docker compose up` in the project root.\n- Open the [application](http://localhost:3333) in a browser. The default login credentials are user `admin@example.com` and password `secret`.\n\n![video showing assistant configuration](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/assistants.webp)\n\n### Using Helm & Kubernetes\n\nFor deployment in Kubernetes environments, please refer to the [README of our Helm Chart](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/./helm-chart/README.md).\n\n### Setting up Assistants and Extensions\n\nThe c4 GenAI Suite revolves around *assistants*.\nEach assistant consists of a set of extensions, which determine the LLM model and which tools it can use.\n\n- In the admin area (click the username on the bottom left), go to the [assistants section](http://localhost:3333/admin/assistants).\n- Add an assistant with the green `+` button next to the section title. Choose a name and a description.\n- Select the created assistant and click the green `+ Add Extension`.\n- Select the model and fill in the credentials.\n- Use the `Test` Button to check that it works and `save`.\n\nNow you can return to the [chat page](http://localhost:3333/chat) (click on `c4 GenAI Suite` in the top left) and start a new conversation with your new assistant.\n\n> [!TIP]\n> Our `docker-compose` includes a local Ollama, which runs on the CPU. You can use this for quick testing. But it will be slow and you probably want to use another model. If you want to use it, just create the following model extension in your Assistant.\n> * Extension: `Dev: Ollama`\n> * Endpoint: `http://ollama:11434`\n> * Model: `llama3.2`\n",
    "ContentSha": "bLa8wjqGffqM6ThWqBWA15QpuTagUJbPEAqi4FPI4X4=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## Başlarken\n\n### Docker-Compose Kullanımı\n\n- Proje kök dizininde `docker compose up` komutunu çalıştırın.\n- Bir tarayıcıda [uygulamayı](http://localhost:3333) açın. Varsayılan giriş bilgileri kullanıcı `admin@example.com` ve şifre `secret` şeklindedir.\n\n![asistan yapılandırmasını gösteren video](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/assistants.webp)\n\n### Helm & Kubernetes Kullanımı\n\nKubernetes ortamlarında dağıtım için lütfen [Helm Chart'ımızın README dosyasına](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/./helm-chart/README.md) bakın.\n\n### Asistanlar ve Uzantıların Kurulumu\n\nc4 GenAI Suite, *asistanlar* etrafında döner.\nHer asistan, LLM modelini ve hangi araçları kullanabileceğini belirleyen bir dizi uzantıdan oluşur.\n\n- Yönetici alanında (sol altta kullanıcı adına tıklayın), [asistanlar bölümüne](http://localhost:3333/admin/assistants) gidin.\n- Bölüm başlığının yanındaki yeşil `+` düğmesiyle bir asistan ekleyin. Bir isim ve açıklama seçin.\n- Oluşturulan asistanı seçin ve yeşil `+ Uzantı Ekle`ye tıklayın.\n- Modeli seçin ve kimlik bilgilerini doldurun.\n- Çalıştığını kontrol etmek için `Test` düğmesini kullanın ve ardından `kaydedin`.\n\nArtık [sohbet sayfasına](http://localhost:3333/chat) (sol üstteki `c4 GenAI Suite`e tıklayın) dönebilir ve yeni asistanınızla yeni bir konuşma başlatabilirsiniz.\n\n> [!IPUCU]\n> `docker-compose` dosyamız, CPU üzerinde çalışan yerel bir Ollama içerir. Bunu hızlı testler için kullanabilirsiniz. Ancak yavaş olacaktır ve muhtemelen başka bir model kullanmak istersiniz. Bunu kullanmak isterseniz, Asistanınızda aşağıdaki model uzantısını oluşturmanız yeterlidir.\n> * Uzantı: `Dev: Ollama`\n> * Uç Nokta: `http://ollama:11434`\n> * Model: `llama3.2`",
    "Status": "ok"
  },
  {
    "Id": 4,
    "Content": "### Model Context Protocol (MCP) [optional]\n\nUse any MCP server offering an `sse` interface with the `MCP Tools` Extension (or use our `mcp-tool-as-server` as a proxy in front of an `stdio` MCP server).\nEach MCP server can be configured in detail as an extension.\n\n### Retrieval Augmented Generation (RAG) / File Search [optional]\n\nUse our RAG server `REI-S` to search user provided files. Just configure a `Search Files` extension for the assistant.\nThis process is described in detail in [the `services/reis` subdirectory](services/reis/#example-configuration-in-c4).\n\n## Contributing & Development\n\n* See [CONTRIBUTING.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/CONTRIBUTING.md) for guidelines on how to contribute.\n* For developer onboarding, check [DEVELOPERS.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/DEVELOPERS.md).\n\n## Main Building Blocks\n\nThe application consists of a **Frontend** , a **Backend**  and a **REI-S**  service.\n\n```",
    "ContentSha": "X5WCsc3MMbiH/lsJf83tTEO6Cfl/16Nw2tPJwgCKYpc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### Model Context Protocol (MCP) [isteğe bağlı]\n\n`MCP Tools` Eklentisi ile `sse` arayüzü sunan herhangi bir MCP sunucusunu kullanın (veya bir `stdio` MCP sunucusunun önünde vekil olarak bizim `mcp-tool-as-server` uygulamamızı kullanın).\nHer MCP sunucusu, bir eklenti olarak ayrıntılı şekilde yapılandırılabilir.\n\n### Retrieval Augmented Generation (RAG) / Dosya Arama [isteğe bağlı]\n\nKullanıcı tarafından sağlanan dosyaları aramak için RAG sunucumuz `REI-S`'i kullanın. Asistan için sadece bir `Dosyaları Ara` eklentisi yapılandırın.\nBu süreç [services/reis alt dizininde](services/reis/#example-configuration-in-c4) ayrıntılı olarak açıklanmıştır.\n\n## Katkı & Geliştirme\n\n* Katkıda bulunma yönergeleri için [CONTRIBUTING.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/CONTRIBUTING.md) dosyasına bakın.\n* Geliştirici olarak başlamak için [DEVELOPERS.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/DEVELOPERS.md) dosyasını kontrol edin.\n\n## Ana Bileşenler\n\nUygulama bir **Ön Yüz** , bir **Arka Uç**  ve bir **REI-S**  hizmetinden oluşmaktadır.\n\n```",
    "Status": "ok"
  },
  {
    "Id": 5,
    "Content": "┌──────────┐\n│   User   │\n└─────┬────┘\n      │ access\n      ▼\n┌──────────┐\n│ Frontend │\n└─────┬────┘\n      │ access\n      ▼\n┌──────────┐     ┌─────────────────┐\n│ Backend  │────►│      LLM        │\n└─────┬────┘     └─────────────────┘\n      │ access\n      ▼\n┌──────────┐     ┌─────────────────┐\n│  REI-S   │────►│ Embedding Model │\n│          │     └─────────────────┘\n│          │\n│          │     ┌─────────────────┐\n│          │────►│  Vector Store   │\n└──────────┘     └─────────────────┘\n```",
    "ContentSha": "IO09RXKALRyYzMSXIVzGJTmyROh56hbqbTTI4NQfxtQ=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "┌──────────┐\n│   Kullanıcı │\n└─────┬────┘\n      │ erişim\n      ▼\n┌──────────┐\n│ Arayüz   │\n└─────┬────┘\n      │ erişim\n      ▼\n┌──────────┐     ┌─────────────────┐\n│ Arka Uç  │────►│      LLM        │\n└─────┬────┘     └─────────────────┘\n      │ erişim\n      ▼\n┌──────────┐     ┌─────────────────┐\n│  REI-S   │────►│ Gömme Modeli    │\n│          │     └─────────────────┘\n│          │\n│          │     ┌─────────────────┐\n│          │────►│  Vektör Deposu  │\n└──────────┘     └─────────────────┘\n```",
    "Status": "ok"
  },
  {
    "Id": 6,
    "Content": "\n\n### Frontend\n\nThe frontend is built with React and TypeScript, providing a user-friendly interface for interacting with the backend and REI-S service. It includes features for managing assistants, extensions, and chat functionalities.\n\n> Sources: `/frontend`\n\n### Backend\n\nThe backend is developed using NestJS and TypeScript, serving as the main API layer for the application. It handles requests from the frontend and interacts with llm providers to facilitate chat functionalities. The backend also manages assistants and their extensions, allowing users to configure and use various AI models for their chats.\n\nAdditionally, the backend manages user authentication, and communicates with the REI-S service for file indexing and retrieval.\n\nFor data persistence, the backend uses a **PostgreSQL** database.\n\n> Sources: `/backend`\n\n### REI-S\n\nThe REI-S (**R**etrieval **E**xtraction **I**ngestion **S**erver) is a Python-based server that provides basic RAG (Retrieval-Augmented Generation) capabilities. It allows for file content extraction, indexing and querying, enabling the application to handle large datasets efficiently. The REI-S service is designed to work seamlessly with the backend, providing necessary data for chat functionalities and file searches.\n\nThe REI-S supports Azure AI Search and pgvector for vector storage, allowing for flexible and scalable data retrieval options. The service can be configured using environment variables to specify the type of vector store and connection details.\n\n> Sources: `/services/reis`\n",
    "ContentSha": "Sceh4YX3ZYIBDEtdTMWIBUucYWH+hEUJMben1cxfrAc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### Frontend\n\nFrontend, kullanıcıların backend ve REI-S servisi ile etkileşime geçmesini sağlayan kullanıcı dostu bir arayüz sunmak için React ve TypeScript ile geliştirilmiştir. Asistanları, uzantıları ve sohbet işlevlerini yönetmek için özellikler içerir.\n\n> Kaynaklar: `/frontend`\n\n### Backend\n\nBackend, uygulamanın ana API katmanı olarak hizmet veren NestJS ve TypeScript kullanılarak geliştirilmiştir. Frontend'den gelen istekleri işler ve sohbet işlevlerini kolaylaştırmak için llm sağlayıcıları ile etkileşime geçer. Backend ayrıca asistanları ve bunların uzantılarını yönetir, kullanıcıların sohbetlerinde çeşitli AI modellerini yapılandırmasına ve kullanmasına olanak tanır.\n\nEk olarak, backend kullanıcı kimlik doğrulamasını yönetir ve dosya indeksleme ve geri alma işlemleri için REI-S servisi ile iletişim kurar.\n\nVeri kalıcılığı için backend, **PostgreSQL** veritabanı kullanır.\n\n> Kaynaklar: `/backend`\n\n### REI-S\n\nREI-S (**R**etrieval **E**xtraction **I**ngestion **S**erver), temel RAG (Retrieval-Augmented Generation) yetenekleri sunan Python tabanlı bir sunucudur. Dosya içeriği çıkarma, indeksleme ve sorgulama işlemlerine olanak tanır, böylece uygulamanın büyük veri setlerini verimli bir şekilde işlemesini sağlar. REI-S servisi, backend ile sorunsuz çalışacak şekilde tasarlanmıştır ve sohbet işlevleri ile dosya aramaları için gerekli verileri sağlar.\n\nREI-S, vektör depolama için Azure AI Search ve pgvector'u destekler, böylece esnek ve ölçeklenebilir veri erişim seçenekleri sunar. Servis, vektör depolama tipi ve bağlantı detaylarını belirtmek için ortam değişkenleri ile yapılandırılabilir.\n\n> Kaynaklar: `/services/reis`\n",
    "Status": "ok"
  }
]