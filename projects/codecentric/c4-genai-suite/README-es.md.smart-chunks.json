[
  {
    "Id": 1,
    "Content": "# c4 GenAI Suite\n\nAn AI chatbot application with Model Context Provider (MCP) integration, powered by Langchain and compatibility for all major Large Language Models (LLMs) and Embedding Models.\n\nAdministrators can create assistants with different capabilities by adding extensions, such as RAG (Retrieval-Augmented Generation) services or MCP servers. The application is built using a modern tech stack, including React, NestJS, and Python FastAPI for the REI-S service.\n\nUsers can interact with assistants through a user-friendly interface. Depending on the assistant's configuration, users may be able to ask questions, upload their own files, or use other features. The assistants interact with various LLM providers to provide responses based on the configured extensions. Contextual information provided by the configured extensions allows the assistants to answer domain-specific questions and provide relevant information.\n\nThe application is designed to be modular and extensible, allowing users to create assistants with different capabilities by adding extensions.\n\n![short demo video of basic usage](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/preview.webp)\n\n## Features\n\n### Large Language Models (LLM) and Multimodal Models\n\nThe c4 GenAI Suite supports already many models directly. And if your preferred model is not supported already, it should be easy to write an extension to support it.\n\n* OpenAI compatible models\n* Azure OpenAI models\n* Bedrock models\n* Google GenAI models\n* Ollama compatible models\n",
    "ContentSha": "VePwjxi+F5ttFJfuckWjhr5ujnZTyNWk2c6pu+FMFPM=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "# c4 GenAI Suite\n\nUna aplicación de chatbot de IA con integración de Model Context Provider (MCP), impulsada por Langchain y compatible con todos los principales Modelos de Lenguaje de Gran Escala (LLMs) y Modelos de Embedding.\n\nLos administradores pueden crear asistentes con diferentes capacidades añadiendo extensiones, como servicios RAG (Generación Aumentada por Recuperación) o servidores MCP. La aplicación está construida utilizando una pila tecnológica moderna, incluyendo React, NestJS y Python FastAPI para el servicio REI-S.\n\nLos usuarios pueden interactuar con los asistentes a través de una interfaz fácil de usar. Dependiendo de la configuración del asistente, los usuarios pueden hacer preguntas, subir sus propios archivos o utilizar otras funciones. Los asistentes interactúan con varios proveedores de LLM para proporcionar respuestas basadas en las extensiones configuradas. La información contextual proporcionada por las extensiones configuradas permite a los asistentes responder preguntas específicas de dominio y ofrecer información relevante.\n\nLa aplicación está diseñada para ser modular y extensible, permitiendo a los usuarios crear asistentes con diferentes capacidades mediante la adición de extensiones.\n\n![vídeo corto de demostración del uso básico](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/preview.webp)\n\n## Características\n\n### Modelos de Lenguaje de Gran Escala (LLM) y Modelos Multimodales\n\nLa c4 GenAI Suite ya es compatible con muchos modelos directamente. Y si tu modelo preferido aún no está soportado, debería ser fácil escribir una extensión para soportarlo.\n\n* Modelos compatibles con OpenAI\n* Modelos Azure OpenAI\n* Modelos Bedrock\n* Modelos Google GenAI\n* Modelos compatibles con Ollama",
    "Status": "ok"
  },
  {
    "Id": 2,
    "Content": "### Retrieval Augmented Generation (RAG)\n\nThe c4 GenAI Suite includes REI-S, a server to prepare files for consumption by the LLM.\n\n* REI-S, a custom integrated RAG server\n  * Vector stores\n    * pgvector\n    * Azure AI Search\n  * Embedding models\n    * OpenAI compatible embeddings\n    * Azure OpenAI embeddings\n    * Ollama compatible embeddings\n  * File formats:\n    * pdf, docx, pptx, xlsx, ...\n    * audio file voice transcription (via Whisper)\n\n### Extensions\n\nThe c4 GenAI Suite is designed for extensibility. Writing extensions is easy, as is using an already existing MCP server.\n\n* Model Context Protocol (MCP) servers\n* Custom systemprompt\n* Bing Search\n* Calculator\n\n",
    "ContentSha": "wvT4e6071qGfJDWClvklgUJiJRM71gUVIUQKaaDgVPo=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### Generación Aumentada por Recuperación (RAG)\n\nLa Suite c4 GenAI incluye REI-S, un servidor para preparar archivos para su consumo por el LLM.\n\n* REI-S, un servidor RAG integrado personalizado\n  * Almacenes vectoriales\n    * pgvector\n    * Azure AI Search\n  * Modelos de embedding\n    * Embeddings compatibles con OpenAI\n    * Embeddings de Azure OpenAI\n    * Embeddings compatibles con Ollama\n  * Formatos de archivo:\n    * pdf, docx, pptx, xlsx, ...\n    * transcripción de voz de archivos de audio (vía Whisper)\n\n### Extensiones\n\nLa Suite c4 GenAI está diseñada para ser extensible. Es fácil escribir extensiones, así como utilizar un servidor MCP ya existente.\n\n* Servidores de Protocolo de Contexto de Modelo (MCP)\n* Systemprompt personalizado\n* Búsqueda de Bing\n* Calculadora",
    "Status": "ok"
  },
  {
    "Id": 3,
    "Content": "## Getting Started\n\n### Using Docker-Compose\n\n- Run `docker compose up` in the project root.\n- Open the [application](http://localhost:3333) in a browser. The default login credentials are user `admin@example.com` and password `secret`.\n\n![video showing assistant configuration](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/assistants.webp)\n\n### Using Helm & Kubernetes\n\nFor deployment in Kubernetes environments, please refer to the [README of our Helm Chart](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/./helm-chart/README.md).\n\n### Setting up Assistants and Extensions\n\nThe c4 GenAI Suite revolves around *assistants*.\nEach assistant consists of a set of extensions, which determine the LLM model and which tools it can use.\n\n- In the admin area (click the username on the bottom left), go to the [assistants section](http://localhost:3333/admin/assistants).\n- Add an assistant with the green `+` button next to the section title. Choose a name and a description.\n- Select the created assistant and click the green `+ Add Extension`.\n- Select the model and fill in the credentials.\n- Use the `Test` Button to check that it works and `save`.\n\nNow you can return to the [chat page](http://localhost:3333/chat) (click on `c4 GenAI Suite` in the top left) and start a new conversation with your new assistant.\n\n> [!TIP]\n> Our `docker-compose` includes a local Ollama, which runs on the CPU. You can use this for quick testing. But it will be slow and you probably want to use another model. If you want to use it, just create the following model extension in your Assistant.\n> * Extension: `Dev: Ollama`\n> * Endpoint: `http://ollama:11434`\n> * Model: `llama3.2`\n",
    "ContentSha": "bLa8wjqGffqM6ThWqBWA15QpuTagUJbPEAqi4FPI4X4=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## Comenzando\n\n### Usando Docker-Compose\n\n- Ejecuta `docker compose up` en la raíz del proyecto.\n- Abre la [aplicación](http://localhost:3333) en un navegador. Las credenciales de inicio de sesión por defecto son usuario `admin@example.com` y contraseña `secret`.\n\n![video mostrando la configuración del asistente](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/assistants.webp)\n\n### Usando Helm y Kubernetes\n\nPara el despliegue en entornos Kubernetes, por favor consulta el [README de nuestro Helm Chart](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/./helm-chart/README.md).\n\n### Configuración de Asistentes y Extensiones\n\nLa c4 GenAI Suite gira en torno a los *asistentes*.\nCada asistente consiste en un conjunto de extensiones, que determinan el modelo LLM y qué herramientas puede utilizar.\n\n- En el área de administración (haz clic en el nombre de usuario en la parte inferior izquierda), ve a la [sección de asistentes](http://localhost:3333/admin/assistants).\n- Agrega un asistente con el botón verde `+` junto al título de la sección. Elige un nombre y una descripción.\n- Selecciona el asistente creado y haz clic en el botón verde `+ Agregar Extensión`.\n- Selecciona el modelo y completa las credenciales.\n- Usa el botón `Probar` para verificar que funciona y `guardar`.\n\nAhora puedes regresar a la [página de chat](http://localhost:3333/chat) (haz clic en `c4 GenAI Suite` en la parte superior izquierda) y comenzar una nueva conversación con tu nuevo asistente.\n\n> [!TIP]\n> Nuestro `docker-compose` incluye un Ollama local, que se ejecuta en la CPU. Puedes utilizarlo para pruebas rápidas. Pero será lento y probablemente querrás usar otro modelo. Si deseas utilizarlo, simplemente crea la siguiente extensión de modelo en tu Asistente.\n> * Extensión: `Dev: Ollama`\n> * Endpoint: `http://ollama:11434`\n> * Modelo: `llama3.2`\n",
    "Status": "ok"
  },
  {
    "Id": 4,
    "Content": "### Model Context Protocol (MCP) [optional]\n\nUse any MCP server offering an `sse` interface with the `MCP Tools` Extension (or use our `mcp-tool-as-server` as a proxy in front of an `stdio` MCP server).\nEach MCP server can be configured in detail as an extension.\n\n### Retrieval Augmented Generation (RAG) / File Search [optional]\n\nUse our RAG server `REI-S` to search user provided files. Just configure a `Search Files` extension for the assistant.\nThis process is described in detail in [the `services/reis` subdirectory](services/reis/#example-configuration-in-c4).\n\n## Contributing & Development\n\n* See [CONTRIBUTING.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/CONTRIBUTING.md) for guidelines on how to contribute.\n* For developer onboarding, check [DEVELOPERS.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/DEVELOPERS.md).\n\n## Main Building Blocks\n\nThe application consists of a **Frontend** , a **Backend**  and a **REI-S**  service.\n\n```",
    "ContentSha": "X5WCsc3MMbiH/lsJf83tTEO6Cfl/16Nw2tPJwgCKYpc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### Protocolo de Contexto de Modelo (MCP) [opcional]\n\nUtilice cualquier servidor MCP que ofrezca una interfaz `sse` con la Extensión `MCP Tools` (o use nuestro `mcp-tool-as-server` como proxy frente a un servidor MCP `stdio`).\nCada servidor MCP puede configurarse en detalle como una extensión.\n\n### Generación Aumentada por Recuperación (RAG) / Búsqueda de Archivos [opcional]\n\nUtilice nuestro servidor RAG `REI-S` para buscar en archivos proporcionados por el usuario. Solo necesita configurar una extensión `Buscar Archivos` para el asistente.\nEste proceso se describe en detalle en [el subdirectorio `services/reis`](services/reis/#example-configuration-in-c4).\n\n## Contribuir y Desarrollo\n\n* Consulte [CONTRIBUTING.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/CONTRIBUTING.md) para obtener pautas sobre cómo contribuir.\n* Para la incorporación de desarrolladores, revise [DEVELOPERS.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/DEVELOPERS.md).\n\n## Bloques Constructivos Principales\n\nLa aplicación consiste en un **Frontend**, un **Backend** y un servicio **REI-S**.\n\n```",
    "Status": "ok"
  },
  {
    "Id": 5,
    "Content": "┌──────────┐\n│   User   │\n└─────┬────┘\n      │ access\n      ▼\n┌──────────┐\n│ Frontend │\n└─────┬────┘\n      │ access\n      ▼\n┌──────────┐     ┌─────────────────┐\n│ Backend  │────►│      LLM        │\n└─────┬────┘     └─────────────────┘\n      │ access\n      ▼\n┌──────────┐     ┌─────────────────┐\n│  REI-S   │────►│ Embedding Model │\n│          │     └─────────────────┘\n│          │\n│          │     ┌─────────────────┐\n│          │────►│  Vector Store   │\n└──────────┘     └─────────────────┘\n```",
    "ContentSha": "IO09RXKALRyYzMSXIVzGJTmyROh56hbqbTTI4NQfxtQ=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "┌──────────┐\n│  Usuario │\n└─────┬────┘\n      │ acceso\n      ▼\n┌──────────┐\n│ Frontend │\n└─────┬────┘\n      │ acceso\n      ▼\n┌──────────┐     ┌─────────────────┐\n│ Backend  │────►│      LLM        │\n└─────┬────┘     └─────────────────┘\n      │ acceso\n      ▼\n┌──────────┐     ┌─────────────────┐\n│  REI-S   │────►│ Modelo de Embedding │\n│          │     └─────────────────┘\n│          │\n│          │     ┌─────────────────┐\n│          │────►│  Almacén Vectorial │\n└──────────┘     └─────────────────┘\n```",
    "Status": "ok"
  },
  {
    "Id": 6,
    "Content": "\n\n### Frontend\n\nThe frontend is built with React and TypeScript, providing a user-friendly interface for interacting with the backend and REI-S service. It includes features for managing assistants, extensions, and chat functionalities.\n\n> Sources: `/frontend`\n\n### Backend\n\nThe backend is developed using NestJS and TypeScript, serving as the main API layer for the application. It handles requests from the frontend and interacts with llm providers to facilitate chat functionalities. The backend also manages assistants and their extensions, allowing users to configure and use various AI models for their chats.\n\nAdditionally, the backend manages user authentication, and communicates with the REI-S service for file indexing and retrieval.\n\nFor data persistence, the backend uses a **PostgreSQL** database.\n\n> Sources: `/backend`\n\n### REI-S\n\nThe REI-S (**R**etrieval **E**xtraction **I**ngestion **S**erver) is a Python-based server that provides basic RAG (Retrieval-Augmented Generation) capabilities. It allows for file content extraction, indexing and querying, enabling the application to handle large datasets efficiently. The REI-S service is designed to work seamlessly with the backend, providing necessary data for chat functionalities and file searches.\n\nThe REI-S supports Azure AI Search and pgvector for vector storage, allowing for flexible and scalable data retrieval options. The service can be configured using environment variables to specify the type of vector store and connection details.\n\n> Sources: `/services/reis`\n",
    "ContentSha": "Sceh4YX3ZYIBDEtdTMWIBUucYWH+hEUJMben1cxfrAc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### Frontend\n\nEl frontend está construido con React y TypeScript, proporcionando una interfaz fácil de usar para interactuar con el backend y el servicio REI-S. Incluye funcionalidades para la gestión de asistentes, extensiones y funciones de chat.\n\n> Fuentes: `/frontend`\n\n### Backend\n\nEl backend está desarrollado con NestJS y TypeScript, sirviendo como la capa principal de API para la aplicación. Gestiona las solicitudes del frontend e interactúa con los proveedores de LLM para facilitar las funcionalidades de chat. El backend también administra los asistentes y sus extensiones, permitiendo a los usuarios configurar y utilizar diversos modelos de IA para sus chats.\n\nAdemás, el backend gestiona la autenticación de usuarios y se comunica con el servicio REI-S para la indexación y recuperación de archivos.\n\nPara la persistencia de datos, el backend utiliza una base de datos **PostgreSQL**.\n\n> Fuentes: `/backend`\n\n### REI-S\n\nEl REI-S (**R**etrieval **E**xtraction **I**ngestion **S**erver) es un servidor basado en Python que proporciona capacidades básicas de RAG (Generación Aumentada por Recuperación). Permite la extracción de contenido de archivos, indexación y consultas, lo que habilita a la aplicación para manejar grandes conjuntos de datos de manera eficiente. El servicio REI-S está diseñado para funcionar sin problemas con el backend, proporcionando los datos necesarios para las funcionalidades de chat y búsquedas de archivos.\n\nEl REI-S es compatible con Azure AI Search y pgvector para el almacenamiento vectorial, permitiendo opciones de recuperación de datos flexibles y escalables. El servicio puede configurarse mediante variables de entorno para especificar el tipo de almacenamiento vectorial y los detalles de conexión.\n\n> Fuentes: `/services/reis`\n",
    "Status": "ok"
  }
]