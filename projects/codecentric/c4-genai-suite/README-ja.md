# c4 GenAI Suite

Langchainによって強化され、すべての主要な大規模言語モデル（LLM）および埋め込みモデルに対応した、Model Context Provider（MCP）統合のAIチャットボットアプリケーションです。

管理者は、RAG（検索拡張生成）サービスやMCPサーバーなどの拡張機能を追加することで、異なる機能を持つアシスタントを作成できます。このアプリケーションは、React、NestJS、REI-Sサービス用のPython FastAPIなど、モダンな技術スタックで構築されています。

ユーザーは、使いやすいインターフェースを通じてアシスタントと対話できます。アシスタントの設定に応じて、ユーザーは質問をしたり、自分のファイルをアップロードしたり、その他の機能を利用したりすることができます。アシスタントは、さまざまなLLMプロバイダーと連携し、設定された拡張機能に基づいて応答を提供します。設定された拡張機能が提供するコンテキスト情報により、アシスタントはドメイン固有の質問に答えたり、関連情報を提供したりできます。

このアプリケーションはモジュール式かつ拡張性を持つように設計されており、ユーザーは拡張機能を追加することで、さまざまな機能を持つアシスタントを作成できます。

![short demo video of basic usage](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/preview.webp)

## 特徴

### 大規模言語モデル（LLM）およびマルチモーダルモデル

c4 GenAI Suiteは、多くのモデルをすでに直接サポートしています。また、お好みのモデルがまだサポートされていない場合でも、それをサポートする拡張機能を簡単に作成できるはずです。

* OpenAI互換モデル
* Azure OpenAIモデル
* Bedrockモデル
* Google GenAIモデル
* Ollama互換モデル
### 検索拡張生成（RAG）

c4 GenAI Suite には、LLM で利用するファイルを準備するためのサーバー REI-S が含まれています。

* REI-S：カスタム統合型RAGサーバー
  * ベクトルストア
    * pgvector
    * Azure AI Search
  * 埋め込みモデル
    * OpenAI 互換埋め込み
    * Azure OpenAI 埋め込み
    * Ollama 互換埋め込み
  * ファイルフォーマット：
    * pdf、docx、pptx、xlsx、…
    * 音声ファイル音声書き起こし（Whisper 経由）

### 拡張機能

c4 GenAI Suite は拡張性を考慮して設計されています。拡張機能の作成は簡単で、既存の MCP サーバーの利用も容易です。

* モデルコンテキストプロトコル（MCP）サーバー
* カスタム systemprompt
* Bing 検索
* 電卓
## はじめに

### Docker-Composeの使用

- プロジェクトのルートディレクトリで`docker compose up`を実行します。
- ブラウザで[アプリケーション](http://localhost:3333)を開きます。デフォルトのログイン認証情報は、ユーザー名が`admin@example.com`、パスワードが`secret`です。

![アシスタント設定の動画](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/assistants.webp)

### HelmとKubernetesの使用

Kubernetes環境でのデプロイについては、[HelmチャートのREADME](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/./helm-chart/README.md)をご参照ください。

### アシスタントと拡張機能の設定

c4 GenAI Suiteは*アシスタント*を中心に構成されています。
各アシスタントは一連の拡張機能で構成されており、どのLLMモデルを使用し、どのツールが利用可能かを決定します。

- 管理エリア（左下のユーザー名をクリック）で、[アシスタントセクション](http://localhost:3333/admin/assistants)に移動します。
- セクションタイトルの横にある緑色の`+`ボタンでアシスタントを追加します。名前と説明を入力してください。
- 作成したアシスタントを選択し、緑色の`+ 拡張機能を追加`をクリックします。
- モデルを選択し、認証情報を入力します。
- `テスト`ボタンで動作確認し、`保存`してください。

これで[チャットページ](http://localhost:3333/chat)（左上の`c4 GenAI Suite`をクリック）に戻り、新しいアシスタントとの会話を開始できます。

> [!TIP]
> `docker-compose`にはローカルOllamaが含まれており、CPU上で動作します。これを使って簡単にテストできますが、動作は遅くなるため、他のモデルの利用をおすすめします。使用したい場合は、以下のモデル拡張機能をアシスタントに作成してください。
> * 拡張機能: `Dev: Ollama`
> * エンドポイント: `http://ollama:11434`
> * モデル: `llama3.2`

### モデルコンテキストプロトコル (MCP) [オプション]

`sse` インターフェースを提供する任意の MCP サーバーを、`MCP Tools` 拡張機能とともに使用できます（または、`stdio` MCP サーバーの前にプロキシとして当社の `mcp-tool-as-server` を使用できます）。
各 MCP サーバーは、拡張機能として詳細に設定可能です。

### 検索拡張生成 (RAG) / ファイル検索 [オプション]

ユーザーが提供したファイルを検索するために、当社の RAG サーバー `REI-S` を使用します。アシスタント用に `Search Files` 拡張機能を設定するだけです。
このプロセスの詳細は [ `services/reis` サブディレクトリ](services/reis/#example-configuration-in-c4) で説明されています。

## 貢献と開発

* 貢献方法のガイドラインについては、[CONTRIBUTING.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/CONTRIBUTING.md) を参照してください。
* 開発者向けのオンボーディングについては、[DEVELOPERS.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/DEVELOPERS.md) を確認してください。

## 主な構成要素

本アプリケーションは、**フロントエンド**、**バックエンド**、および **REI-S** サービスで構成されています。

```
┌──────────┐
│   ユーザー │
└─────┬────┘
      │ アクセス
      ▼
┌──────────┐
│ フロントエンド │
└─────┬────┘
      │ アクセス
      ▼
┌──────────┐     ┌─────────────────┐
│ バックエンド │────►│      LLM        │
└─────┬────┘     └─────────────────┘
      │ アクセス
      ▼
┌──────────┐     ┌─────────────────┐
│  REI-S   │────►│ 埋め込みモデル    │
│          │     └─────────────────┘
│          │
│          │     ┌─────────────────┐
│          │────►│ ベクターストア   │
└──────────┘     └─────────────────┘
```
### フロントエンド

フロントエンドはReactとTypeScriptで構築されており、バックエンドおよびREI-Sサービスと連携するためのユーザーフレンドリーなインターフェースを提供します。アシスタント、拡張機能、チャット機能の管理機能が含まれています。

> ソース: `/frontend`

### バックエンド

バックエンドはNestJSとTypeScriptを使用して開発されており、アプリケーションの主要なAPIレイヤーとして機能します。フロントエンドからのリクエストを処理し、llmプロバイダーと連携してチャット機能を提供します。バックエンドはまた、アシスタントおよびその拡張機能の管理も行い、ユーザーがさまざまなAIモデルをチャットで設定・利用できるようにします。

さらに、バックエンドはユーザー認証を管理し、ファイルのインデックス作成および取得のためにREI-Sサービスと通信します。

データ永続化のために、バックエンドは**PostgreSQL**データベースを使用しています。

> ソース: `/backend`

### REI-S

REI-S（**R**etrieval **E**xtraction **I**ngestion **S**erver）は、Pythonベースのサーバーで、基本的なRAG（Retrieval-Augmented Generation）機能を提供します。ファイル内容の抽出、インデックス作成、クエリを可能にし、アプリケーションが大規模なデータセットを効率的に扱えるようにします。REI-Sサービスはバックエンドとシームレスに連携するよう設計されており、チャット機能やファイル検索に必要なデータを提供します。

REI-Sは、ベクトルストレージとしてAzure AI Searchおよびpgvectorをサポートしており、柔軟かつスケーラブルなデータ検索オプションを実現します。サービスは環境変数によってベクトルストアの種類や接続情報を設定できます。

> ソース: `/services/reis`


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-09

---