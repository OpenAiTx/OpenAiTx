[
  {
    "Id": 1,
    "Content": "# c4 GenAI Suite\n\nAn AI chatbot application with Model Context Provider (MCP) integration, powered by Langchain and compatibility for all major Large Language Models (LLMs) and Embedding Models.\n\nAdministrators can create assistants with different capabilities by adding extensions, such as RAG (Retrieval-Augmented Generation) services or MCP servers. The application is built using a modern tech stack, including React, NestJS, and Python FastAPI for the REI-S service.\n\nUsers can interact with assistants through a user-friendly interface. Depending on the assistant's configuration, users may be able to ask questions, upload their own files, or use other features. The assistants interact with various LLM providers to provide responses based on the configured extensions. Contextual information provided by the configured extensions allows the assistants to answer domain-specific questions and provide relevant information.\n\nThe application is designed to be modular and extensible, allowing users to create assistants with different capabilities by adding extensions.\n\n![short demo video of basic usage](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/preview.webp)\n\n## Features\n\n### Large Language Models (LLM) and Multimodal Models\n\nThe c4 GenAI Suite supports already many models directly. And if your preferred model is not supported already, it should be easy to write an extension to support it.\n\n* OpenAI compatible models\n* Azure OpenAI models\n* Bedrock models\n* Google GenAI models\n* Ollama compatible models\n",
    "ContentSha": "VePwjxi+F5ttFJfuckWjhr5ujnZTyNWk2c6pu+FMFPM=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "# c4 GenAI Suite\n\nتطبيق دردشة ذكاء اصطناعي مع تكامل مزود سياق النماذج (MCP)، مدعوم بـ Langchain ومتوافق مع جميع نماذج اللغة الكبيرة (LLMs) ونماذج التضمين الرئيسية.\n\nيمكن للمسؤولين إنشاء مساعدين بقدرات مختلفة عن طريق إضافة الامتدادات، مثل خدمات RAG (توليد معزز بالاسترجاع) أو خوادم MCP. تم بناء التطبيق باستخدام حزمة تقنيات حديثة، تشمل React وNestJS وPython FastAPI لخدمة REI-S.\n\nيمكن للمستخدمين التفاعل مع المساعدين من خلال واجهة سهلة الاستخدام. وبحسب تكوين المساعد، قد يتمكن المستخدمون من طرح الأسئلة، أو رفع ملفاتهم الخاصة، أو استخدام ميزات أخرى. يتفاعل المساعدون مع مزودي LLM مختلفين لتقديم الردود بناءً على الامتدادات المُكوَّنة. تتيح المعلومات السياقية المقدمة من الامتدادات المُكوَّنة للمساعدين الإجابة على الأسئلة المتخصصة في المجالات وتقديم معلومات ذات صلة.\n\nتم تصميم التطبيق ليكون معياريًا وقابلًا للتوسعة، مما يسمح للمستخدمين بإنشاء مساعدين بقدرات مختلفة عن طريق إضافة الامتدادات.\n\n![فيديو عرض قصير للاستخدام الأساسي](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/preview.webp)\n\n## الميزات\n\n### نماذج اللغة الكبيرة (LLM) والنماذج متعددة الوسائط\n\nيدعم c4 GenAI Suite بالفعل العديد من النماذج مباشرة. وإذا كان النموذج المفضل لديك غير مدعوم حتى الآن، يجب أن يكون من السهل كتابة امتداد لدعمه.\n\n* نماذج متوافقة مع OpenAI\n* نماذج Azure OpenAI\n* نماذج Bedrock\n* نماذج Google GenAI\n* نماذج متوافقة مع Ollama",
    "Status": "ok"
  },
  {
    "Id": 2,
    "Content": "### Retrieval Augmented Generation (RAG)\n\nThe c4 GenAI Suite includes REI-S, a server to prepare files for consumption by the LLM.\n\n* REI-S, a custom integrated RAG server\n  * Vector stores\n    * pgvector\n    * Azure AI Search\n  * Embedding models\n    * OpenAI compatible embeddings\n    * Azure OpenAI embeddings\n    * Ollama compatible embeddings\n  * File formats:\n    * pdf, docx, pptx, xlsx, ...\n    * audio file voice transcription (via Whisper)\n\n### Extensions\n\nThe c4 GenAI Suite is designed for extensibility. Writing extensions is easy, as is using an already existing MCP server.\n\n* Model Context Protocol (MCP) servers\n* Custom systemprompt\n* Bing Search\n* Calculator\n\n",
    "ContentSha": "wvT4e6071qGfJDWClvklgUJiJRM71gUVIUQKaaDgVPo=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### توليد المعرفة المعزز بالاسترجاع (RAG)\n\nتتضمن حزمة c4 GenAI خادم REI-S لتحضير الملفات للاستهلاك من قبل نموذج اللغة الكبير (LLM).\n\n* REI-S، خادم RAG مدمج مخصص\n  * مخازن المتجهات\n    * pgvector\n    * بحث Azure AI\n  * نماذج تضمين\n    * تضمينات متوافقة مع OpenAI\n    * تضمينات Azure OpenAI\n    * تضمينات متوافقة مع Ollama\n  * صيغ الملفات:\n    * pdf، docx، pptx، xlsx، ...\n    * نسخ صوتي لملفات الصوت (عبر Whisper)\n\n### الامتدادات\n\nتم تصميم حزمة c4 GenAI لتكون قابلة للتوسعة. كتابة الامتدادات سهلة، كما هو الحال مع استخدام خادم MCP موجود بالفعل.\n\n* خوادم بروتوكول سياق النموذج (MCP)\n* systemprompt مخصص\n* بحث Bing\n* الآلة الحاسبة",
    "Status": "ok"
  },
  {
    "Id": 3,
    "Content": "## Getting Started\n\n### Using Docker-Compose\n\n- Run `docker compose up` in the project root.\n- Open the [application](http://localhost:3333) in a browser. The default login credentials are user `admin@example.com` and password `secret`.\n\n![video showing assistant configuration](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/assistants.webp)\n\n### Using Helm & Kubernetes\n\nFor deployment in Kubernetes environments, please refer to the [README of our Helm Chart](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/./helm-chart/README.md).\n\n### Setting up Assistants and Extensions\n\nThe c4 GenAI Suite revolves around *assistants*.\nEach assistant consists of a set of extensions, which determine the LLM model and which tools it can use.\n\n- In the admin area (click the username on the bottom left), go to the [assistants section](http://localhost:3333/admin/assistants).\n- Add an assistant with the green `+` button next to the section title. Choose a name and a description.\n- Select the created assistant and click the green `+ Add Extension`.\n- Select the model and fill in the credentials.\n- Use the `Test` Button to check that it works and `save`.\n\nNow you can return to the [chat page](http://localhost:3333/chat) (click on `c4 GenAI Suite` in the top left) and start a new conversation with your new assistant.\n\n> [!TIP]\n> Our `docker-compose` includes a local Ollama, which runs on the CPU. You can use this for quick testing. But it will be slow and you probably want to use another model. If you want to use it, just create the following model extension in your Assistant.\n> * Extension: `Dev: Ollama`\n> * Endpoint: `http://ollama:11434`\n> * Model: `llama3.2`\n",
    "ContentSha": "bLa8wjqGffqM6ThWqBWA15QpuTagUJbPEAqi4FPI4X4=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## البدء\n\n### استخدام Docker-Compose\n\n- شغّل الأمر `docker compose up` في جذر المشروع.\n- افتح [التطبيق](http://localhost:3333) في المتصفح. بيانات الدخول الافتراضية هي اسم المستخدم `admin@example.com` وكلمة المرور `secret`.\n\n![فيديو يوضح تهيئة المساعد](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/assistants.webp)\n\n### استخدام Helm و Kubernetes\n\nللنشر في بيئات Kubernetes، يرجى الرجوع إلى [ملف README الخاص بمخطط Helm لدينا](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/./helm-chart/README.md).\n\n### إعداد المساعدين والإضافات\n\nتدور Suite c4 GenAI حول *المساعدين*.\nيتكون كل مساعد من مجموعة من الإضافات، التي تحدد نموذج LLM والأدوات التي يمكنه استخدامها.\n\n- في منطقة الإدارة (انقر على اسم المستخدم في الأسفل يساراً)، انتقل إلى [قسم المساعدين](http://localhost:3333/admin/assistants).\n- أضف مساعداً باستخدام زر `+` الأخضر بجانب عنوان القسم. اختر اسماً ووصفاً.\n- حدد المساعد الذي أنشأته واضغط على `+ إضافة إضافة` الأخضر.\n- اختر النموذج واملأ بيانات الاعتماد.\n- استخدم زر `اختبار` للتحقق من أن كل شيء يعمل ثم اضغط `حفظ`.\n\nالآن يمكنك العودة إلى [صفحة المحادثة](http://localhost:3333/chat) (انقر على `c4 GenAI Suite` في أعلى اليسار) وبدء محادثة جديدة مع مساعدك الجديد.\n\n> [!TIP]\n> يتضمن `docker-compose` الخاص بنا خدمة Ollama محلية تعمل على وحدة المعالجة المركزية (CPU). يمكنك استخدام هذه الخدمة للاختبار السريع، لكنها ستكون بطيئة وربما ترغب باستخدام نموذج آخر. إذا أردت استخدامها، أنشئ إضافة النموذج التالية في مساعدك:\n> * الإضافة: `Dev: Ollama`\n> * نقطة النهاية: `http://ollama:11434`\n> * النموذج: `llama3.2`\n",
    "Status": "ok"
  },
  {
    "Id": 4,
    "Content": "### Model Context Protocol (MCP) [optional]\n\nUse any MCP server offering an `sse` interface with the `MCP Tools` Extension (or use our `mcp-tool-as-server` as a proxy in front of an `stdio` MCP server).\nEach MCP server can be configured in detail as an extension.\n\n### Retrieval Augmented Generation (RAG) / File Search [optional]\n\nUse our RAG server `REI-S` to search user provided files. Just configure a `Search Files` extension for the assistant.\nThis process is described in detail in [the `services/reis` subdirectory](services/reis/#example-configuration-in-c4).\n\n## Contributing & Development\n\n* See [CONTRIBUTING.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/CONTRIBUTING.md) for guidelines on how to contribute.\n* For developer onboarding, check [DEVELOPERS.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/DEVELOPERS.md).\n\n## Main Building Blocks\n\nThe application consists of a **Frontend** , a **Backend**  and a **REI-S**  service.\n\n```",
    "ContentSha": "X5WCsc3MMbiH/lsJf83tTEO6Cfl/16Nw2tPJwgCKYpc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### بروتوكول سياق النموذج (MCP) [اختياري]\n\nاستخدم أي خادم MCP يقدم واجهة `sse` مع امتداد `MCP Tools` (أو استخدم أداتنا `mcp-tool-as-server` كوكيل أمام خادم MCP بنمط `stdio`).\nيمكن تكوين كل خادم MCP بالتفصيل كامتداد.\n\n### التوليد المعزز بالاسترجاع (RAG) / البحث في الملفات [اختياري]\n\nاستخدم خادم RAG الخاص بنا `REI-S` للبحث في الملفات التي يقدمها المستخدم. فقط قم بتكوين امتداد `Search Files` للمساعد.\nتم وصف هذه العملية بالتفصيل في [دليل `services/reis` الفرعي](services/reis/#example-configuration-in-c4).\n\n## المساهمة والتطوير\n\n* راجع [CONTRIBUTING.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/CONTRIBUTING.md) للاطلاع على إرشادات المساهمة.\n* لمطوري البرامج الجدد، اطلع على [DEVELOPERS.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/DEVELOPERS.md).\n\n## اللبنات الأساسية الرئيسية\n\nيتكون التطبيق من **الواجهة الأمامية**، و**الواجهة الخلفية**، وخدمة **REI-S**.\n\n```",
    "Status": "ok"
  },
  {
    "Id": 5,
    "Content": "┌──────────┐\n│   User   │\n└─────┬────┘\n      │ access\n      ▼\n┌──────────┐\n│ Frontend │\n└─────┬────┘\n      │ access\n      ▼\n┌──────────┐     ┌─────────────────┐\n│ Backend  │────►│      LLM        │\n└─────┬────┘     └─────────────────┘\n      │ access\n      ▼\n┌──────────┐     ┌─────────────────┐\n│  REI-S   │────►│ Embedding Model │\n│          │     └─────────────────┘\n│          │\n│          │     ┌─────────────────┐\n│          │────►│  Vector Store   │\n└──────────┘     └─────────────────┘\n```",
    "ContentSha": "IO09RXKALRyYzMSXIVzGJTmyROh56hbqbTTI4NQfxtQ=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "┌──────────┐\n│   المستخدم  │\n└─────┬────┘\n      │ وصول\n      ▼\n┌──────────┐\n│ الواجهة الأمامية │\n└─────┬────┘\n      │ وصول\n      ▼\n┌──────────┐     ┌─────────────────┐\n│ الواجهة الخلفية │────►│    نموذج اللغة الضخم    │\n└─────┬────┘     └─────────────────┘\n      │ وصول\n      ▼\n┌──────────┐     ┌─────────────────┐\n│   REI-S  │────►│ نموذج التضمين   │\n│          │     └─────────────────┘\n│          │\n│          │     ┌─────────────────┐\n│          │────►│   مخزن المتجهات  │\n└──────────┘     └─────────────────┘\n```",
    "Status": "ok"
  },
  {
    "Id": 6,
    "Content": "\n\n### Frontend\n\nThe frontend is built with React and TypeScript, providing a user-friendly interface for interacting with the backend and REI-S service. It includes features for managing assistants, extensions, and chat functionalities.\n\n> Sources: `/frontend`\n\n### Backend\n\nThe backend is developed using NestJS and TypeScript, serving as the main API layer for the application. It handles requests from the frontend and interacts with llm providers to facilitate chat functionalities. The backend also manages assistants and their extensions, allowing users to configure and use various AI models for their chats.\n\nAdditionally, the backend manages user authentication, and communicates with the REI-S service for file indexing and retrieval.\n\nFor data persistence, the backend uses a **PostgreSQL** database.\n\n> Sources: `/backend`\n\n### REI-S\n\nThe REI-S (**R**etrieval **E**xtraction **I**ngestion **S**erver) is a Python-based server that provides basic RAG (Retrieval-Augmented Generation) capabilities. It allows for file content extraction, indexing and querying, enabling the application to handle large datasets efficiently. The REI-S service is designed to work seamlessly with the backend, providing necessary data for chat functionalities and file searches.\n\nThe REI-S supports Azure AI Search and pgvector for vector storage, allowing for flexible and scalable data retrieval options. The service can be configured using environment variables to specify the type of vector store and connection details.\n\n> Sources: `/services/reis`\n",
    "ContentSha": "Sceh4YX3ZYIBDEtdTMWIBUucYWH+hEUJMben1cxfrAc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### الواجهة الأمامية\n\nتم بناء الواجهة الأمامية باستخدام React وTypeScript، لتوفير واجهة مستخدم سهلة الاستخدام للتفاعل مع الواجهة الخلفية وخدمة REI-S. تتضمن ميزات لإدارة المساعدين، والامتدادات، ووظائف الدردشة.\n\n> المصادر: `/frontend`\n\n### الواجهة الخلفية\n\nتم تطوير الواجهة الخلفية باستخدام NestJS وTypeScript، وتعمل كطبقة API رئيسية للتطبيق. تتعامل مع الطلبات القادمة من الواجهة الأمامية وتتفاعل مع مزودي llm لتسهيل وظائف الدردشة. كما تدير الواجهة الخلفية المساعدين وامتداداتهم، مما يسمح للمستخدمين بتكوين واستخدام نماذج الذكاء الاصطناعي المختلفة في دردشاتهم.\n\nبالإضافة إلى ذلك، تدير الواجهة الخلفية مصادقة المستخدمين وتتواصل مع خدمة REI-S لفهرسة الملفات واسترجاعها.\n\nمن أجل استمرارية البيانات، تستخدم الواجهة الخلفية قاعدة بيانات **PostgreSQL**.\n\n> المصادر: `/backend`\n\n### REI-S\n\nخادم REI-S (**خ**ادم **اس**تخراج **ال**استرجاع **ال**ادخال) هو خادم مبني بلغة بايثون يوفر إمكانيات RAG (التوليد المدعوم بالاسترجاع) الأساسية. يسمح باستخراج محتوى الملفات، والفهرسة، والاستعلام، مما يمكن التطبيق من التعامل مع مجموعات بيانات كبيرة بكفاءة. تم تصميم خدمة REI-S للعمل بسلاسة مع الواجهة الخلفية، وتوفير البيانات اللازمة لوظائف الدردشة والبحث في الملفات.\n\nيدعم REI-S كلًا من Azure AI Search وpgvector لتخزين المتجهات، مما يوفر خيارات استرجاع بيانات مرنة وقابلة للتوسع. يمكن تهيئة الخدمة باستخدام متغيرات البيئة لتحديد نوع مخزن المتجهات وتفاصيل الاتصال.\n\n> المصادر: `/services/reis`",
    "Status": "ok"
  }
]