[
  {
    "Id": 1,
    "Content": "# c4 GenAI Suite\n\nAn AI chatbot application with Model Context Provider (MCP) integration, powered by Langchain and compatibility for all major Large Language Models (LLMs) and Embedding Models.\n\nAdministrators can create assistants with different capabilities by adding extensions, such as RAG (Retrieval-Augmented Generation) services or MCP servers. The application is built using a modern tech stack, including React, NestJS, and Python FastAPI for the REI-S service.\n\nUsers can interact with assistants through a user-friendly interface. Depending on the assistant's configuration, users may be able to ask questions, upload their own files, or use other features. The assistants interact with various LLM providers to provide responses based on the configured extensions. Contextual information provided by the configured extensions allows the assistants to answer domain-specific questions and provide relevant information.\n\nThe application is designed to be modular and extensible, allowing users to create assistants with different capabilities by adding extensions.\n\n![short demo video of basic usage](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/preview.webp)\n\n## Features\n\n### Large Language Models (LLM) and Multimodal Models\n\nThe c4 GenAI Suite supports already many models directly. And if your preferred model is not supported already, it should be easy to write an extension to support it.\n\n* OpenAI compatible models\n* Azure OpenAI models\n* Bedrock models\n* Google GenAI models\n* Ollama compatible models\n",
    "ContentSha": "VePwjxi+F5ttFJfuckWjhr5ujnZTyNWk2c6pu+FMFPM=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "# c4 GenAI 套件\n\n一个集成了模型上下文提供者（MCP）的 AI 聊天机器人应用程序，由 Langchain 提供支持，并兼容所有主流的大型语言模型（LLM）和嵌入模型。\n\n管理员可以通过添加扩展（如 RAG（检索增强生成）服务或 MCP 服务器）来创建具备不同能力的助手。该应用采用现代技术栈构建，包括用于 REI-S 服务的 React、NestJS 和 Python FastAPI。\n\n用户可以通过用户友好的界面与助手进行交互。根据助手的配置，用户可以提问、上传自己的文件或使用其他功能。助手通过与各种 LLM 提供商进行交互，根据已配置的扩展提供响应。由已配置扩展提供的上下文信息使助手能够回答特定领域的问题并提供相关信息。\n\n该应用程序采用模块化和可扩展的设计，允许用户通过添加扩展来创建具备不同能力的助手。\n\n![基本用法的简短演示视频](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/preview.webp)\n\n## 功能特性\n\n### 大型语言模型（LLM）及多模态模型\n\nc4 GenAI 套件已经直接支持多种模型。如果您的首选模型尚未支持，也可以很容易地编写扩展来支持它。\n\n* 兼容 OpenAI 的模型\n* Azure OpenAI 模型\n* Bedrock 模型\n* Google GenAI 模型\n* 兼容 Ollama 的模型",
    "Status": "ok"
  },
  {
    "Id": 2,
    "Content": "### Retrieval Augmented Generation (RAG)\n\nThe c4 GenAI Suite includes REI-S, a server to prepare files for consumption by the LLM.\n\n* REI-S, a custom integrated RAG server\n  * Vector stores\n    * pgvector\n    * Azure AI Search\n  * Embedding models\n    * OpenAI compatible embeddings\n    * Azure OpenAI embeddings\n    * Ollama compatible embeddings\n  * File formats:\n    * pdf, docx, pptx, xlsx, ...\n    * audio file voice transcription (via Whisper)\n\n### Extensions\n\nThe c4 GenAI Suite is designed for extensibility. Writing extensions is easy, as is using an already existing MCP server.\n\n* Model Context Protocol (MCP) servers\n* Custom systemprompt\n* Bing Search\n* Calculator\n\n",
    "ContentSha": "wvT4e6071qGfJDWClvklgUJiJRM71gUVIUQKaaDgVPo=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### 检索增强生成（RAG）\n\nc4 GenAI 套件包括 REI-S，这是一个为 LLM 准备文件的服务器。\n\n* REI-S，一个定制集成的 RAG 服务器\n  * 向量存储\n    * pgvector\n    * Azure AI Search\n  * 嵌入模型\n    * 兼容 OpenAI 的嵌入\n    * Azure OpenAI 嵌入\n    * 兼容 Ollama 的嵌入\n  * 文件格式：\n    * pdf、docx、pptx、xlsx 等\n    * 音频文件语音转录（通过 Whisper）\n\n### 扩展\n\nc4 GenAI 套件专为可扩展性设计。编写扩展非常简单，使用已有的 MCP 服务器也同样方便。\n\n* 模型上下文协议（MCP）服务器\n* 自定义 systemprompt\n* 必应搜索\n* 计算器",
    "Status": "ok"
  },
  {
    "Id": 3,
    "Content": "## Getting Started\n\n### Using Docker-Compose\n\n- Run `docker compose up` in the project root.\n- Open the [application](http://localhost:3333) in a browser. The default login credentials are user `admin@example.com` and password `secret`.\n\n![video showing assistant configuration](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/assistants.webp)\n\n### Using Helm & Kubernetes\n\nFor deployment in Kubernetes environments, please refer to the [README of our Helm Chart](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/./helm-chart/README.md).\n\n### Setting up Assistants and Extensions\n\nThe c4 GenAI Suite revolves around *assistants*.\nEach assistant consists of a set of extensions, which determine the LLM model and which tools it can use.\n\n- In the admin area (click the username on the bottom left), go to the [assistants section](http://localhost:3333/admin/assistants).\n- Add an assistant with the green `+` button next to the section title. Choose a name and a description.\n- Select the created assistant and click the green `+ Add Extension`.\n- Select the model and fill in the credentials.\n- Use the `Test` Button to check that it works and `save`.\n\nNow you can return to the [chat page](http://localhost:3333/chat) (click on `c4 GenAI Suite` in the top left) and start a new conversation with your new assistant.\n\n> [!TIP]\n> Our `docker-compose` includes a local Ollama, which runs on the CPU. You can use this for quick testing. But it will be slow and you probably want to use another model. If you want to use it, just create the following model extension in your Assistant.\n> * Extension: `Dev: Ollama`\n> * Endpoint: `http://ollama:11434`\n> * Model: `llama3.2`\n",
    "ContentSha": "bLa8wjqGffqM6ThWqBWA15QpuTagUJbPEAqi4FPI4X4=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## 入门指南\n\n### 使用 Docker-Compose\n\n- 在项目根目录下运行 `docker compose up`。\n- 在浏览器中打开[应用程序](http://localhost:3333)。默认登录凭证为用户 `admin@example.com`，密码为 `secret`。\n\n![视频展示助手配置](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/demo/assistants.webp)\n\n### 使用 Helm & Kubernetes\n\n如需在 Kubernetes 环境中部署，请参考我们 Helm Chart 的 [README 文档](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/./helm-chart/README.md)。\n\n### 设置助手和扩展\n\nc4 GenAI Suite 以*助手*为核心。\n每个助手由一组扩展组成，这些扩展决定了所用的 LLM 模型以及可用的工具。\n\n- 在管理区域（点击左下角的用户名），进入[助手管理页面](http://localhost:3333/admin/assistants)。\n- 通过板块标题旁的绿色 `+` 按钮添加助手，选择名称和描述。\n- 选择创建的助手，点击绿色的 `+ 添加扩展`。\n- 选择模型并填写凭证信息。\n- 使用 `测试` 按钮检查是否正常工作，然后点击 `保存`。\n\n现在您可以返回到[聊天页面](http://localhost:3333/chat)（点击左上角的 `c4 GenAI Suite`）并开始与新助手的对话。\n\n> [!TIP]\n> 我们的 `docker-compose` 集成了本地 Ollama，运行于 CPU 上。您可以用它进行快速测试。但它会比较慢，建议使用其他模型。如果您想使用 Ollama，只需在助手中创建如下模型扩展：\n> * 扩展：`Dev: Ollama`\n> * 端点：`http://ollama:11434`\n> * 模型：`llama3.2`",
    "Status": "ok"
  },
  {
    "Id": 4,
    "Content": "### Model Context Protocol (MCP) [optional]\n\nUse any MCP server offering an `sse` interface with the `MCP Tools` Extension (or use our `mcp-tool-as-server` as a proxy in front of an `stdio` MCP server).\nEach MCP server can be configured in detail as an extension.\n\n### Retrieval Augmented Generation (RAG) / File Search [optional]\n\nUse our RAG server `REI-S` to search user provided files. Just configure a `Search Files` extension for the assistant.\nThis process is described in detail in [the `services/reis` subdirectory](services/reis/#example-configuration-in-c4).\n\n## Contributing & Development\n\n* See [CONTRIBUTING.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/CONTRIBUTING.md) for guidelines on how to contribute.\n* For developer onboarding, check [DEVELOPERS.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/DEVELOPERS.md).\n\n## Main Building Blocks\n\nThe application consists of a **Frontend** , a **Backend**  and a **REI-S**  service.\n\n```",
    "ContentSha": "X5WCsc3MMbiH/lsJf83tTEO6Cfl/16Nw2tPJwgCKYpc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### 模型上下文协议（MCP）[可选]\n\n可使用任何提供 `sse` 接口并配备 `MCP Tools` 扩展的 MCP 服务器（或者在 `stdio` MCP 服务器前端使用我们的 `mcp-tool-as-server` 作为代理）。\n每个 MCP 服务器都可以作为扩展进行详细配置。\n\n### 检索增强生成（RAG）/ 文件搜索 [可选]\n\n使用我们的 RAG 服务器 `REI-S` 来搜索用户提供的文件。只需为助手配置一个 `搜索文件` 扩展即可。\n该过程在 [ `services/reis` 子目录](services/reis/#example-configuration-in-c4) 中有详细描述。\n\n## 贡献与开发\n\n* 有关贡献指南，请参见 [CONTRIBUTING.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/CONTRIBUTING.md)。\n* 开发者入门指南，请查阅 [DEVELOPERS.md](https://raw.githubusercontent.com/codecentric/c4-genai-suite/main/DEVELOPERS.md)。\n\n## 主要构建模块\n\n该应用程序由**前端**、**后端**和**REI-S**服务组成。\n\n```",
    "Status": "ok"
  },
  {
    "Id": 5,
    "Content": "┌──────────┐\n│   User   │\n└─────┬────┘\n      │ access\n      ▼\n┌──────────┐\n│ Frontend │\n└─────┬────┘\n      │ access\n      ▼\n┌──────────┐     ┌─────────────────┐\n│ Backend  │────►│      LLM        │\n└─────┬────┘     └─────────────────┘\n      │ access\n      ▼\n┌──────────┐     ┌─────────────────┐\n│  REI-S   │────►│ Embedding Model │\n│          │     └─────────────────┘\n│          │\n│          │     ┌─────────────────┐\n│          │────►│  Vector Store   │\n└──────────┘     └─────────────────┘\n```",
    "ContentSha": "IO09RXKALRyYzMSXIVzGJTmyROh56hbqbTTI4NQfxtQ=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "┌──────────┐\n│   用户   │\n└─────┬────┘\n      │ 访问\n      ▼\n┌──────────┐\n│ 前端     │\n└─────┬────┘\n      │ 访问\n      ▼\n┌──────────┐     ┌─────────────────┐\n│ 后端     │────►│     LLM         │\n└─────┬────┘     └─────────────────┘\n      │ 访问\n      ▼\n┌──────────┐     ┌─────────────────┐\n│ REI-S    │────►│ 嵌入模型        │\n│          │     └─────────────────┘\n│          │\n│          │     ┌─────────────────┐\n│          │────►│ 向量存储        │\n└──────────┘     └─────────────────┘\n```",
    "Status": "ok"
  },
  {
    "Id": 6,
    "Content": "\n\n### Frontend\n\nThe frontend is built with React and TypeScript, providing a user-friendly interface for interacting with the backend and REI-S service. It includes features for managing assistants, extensions, and chat functionalities.\n\n> Sources: `/frontend`\n\n### Backend\n\nThe backend is developed using NestJS and TypeScript, serving as the main API layer for the application. It handles requests from the frontend and interacts with llm providers to facilitate chat functionalities. The backend also manages assistants and their extensions, allowing users to configure and use various AI models for their chats.\n\nAdditionally, the backend manages user authentication, and communicates with the REI-S service for file indexing and retrieval.\n\nFor data persistence, the backend uses a **PostgreSQL** database.\n\n> Sources: `/backend`\n\n### REI-S\n\nThe REI-S (**R**etrieval **E**xtraction **I**ngestion **S**erver) is a Python-based server that provides basic RAG (Retrieval-Augmented Generation) capabilities. It allows for file content extraction, indexing and querying, enabling the application to handle large datasets efficiently. The REI-S service is designed to work seamlessly with the backend, providing necessary data for chat functionalities and file searches.\n\nThe REI-S supports Azure AI Search and pgvector for vector storage, allowing for flexible and scalable data retrieval options. The service can be configured using environment variables to specify the type of vector store and connection details.\n\n> Sources: `/services/reis`\n",
    "ContentSha": "Sceh4YX3ZYIBDEtdTMWIBUucYWH+hEUJMben1cxfrAc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### 前端\n\n前端使用 React 和 TypeScript 构建，提供了一个用户友好的界面，用于与后端和 REI-S 服务进行交互。它包含助手管理、扩展管理和聊天功能等特性。\n\n> 源码位置：`/frontend`\n\n### 后端\n\n后端使用 NestJS 和 TypeScript 开发，作为应用程序的主要 API 层。它处理来自前端的请求，并与 LLM 提供商进行交互，以实现聊天功能。后端还负责管理助手及其扩展，允许用户配置和使用各种 AI 模型进行聊天。\n\n此外，后端还负责用户身份验证，并与 REI-S 服务通信以实现文件索引和检索。\n\n为实现数据持久化，后端使用了 **PostgreSQL** 数据库。\n\n> 源码位置：`/backend`\n\n### REI-S\n\nREI-S（**R**etrieval **E**xtraction **I**ngestion **S**erver，检索提取摄取服务器）是一个基于 Python 的服务器，提供基础的 RAG（检索增强生成）能力。它支持文件内容提取、索引和查询，使应用能够高效处理大型数据集。REI-S 服务设计与后端无缝协作，为聊天功能和文件搜索提供所需数据。\n\nREI-S 支持 Azure AI Search 和 pgvector 用于向量存储，提供灵活且可扩展的数据检索选项。该服务可通过环境变量配置，指定向量存储类型及连接详情。\n\n> 源码位置：`/services/reis`\n",
    "Status": "ok"
  }
]