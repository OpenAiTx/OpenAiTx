# SongGen: Un Transformador Auto-regresivo de Etapa √önica para Generaci√≥n de Canciones a partir de Texto

üöÄüöÄüöÄ Implementaci√≥n oficial de **SongGen: Un Transformador Auto-regresivo de Etapa √önica para Generaci√≥n de Canciones a partir de Texto**
<p align="center" style="font-size: 1 em; margin-top: -1em">
<a href="https://scholar.google.com/citations?user=iELd-Q0AAAAJ">Zihan Liu</a>,  
<a href="https://mark12ding.github.io/">Shuangrui Ding</a>,  
<a href="https://github.com/rookiexiong7/">Zhixiong Zhang</a>, 
<a href="https://lightdxy.github.io/">Xiaoyi Dong</a>,  
<a href="https://panzhang0212.github.io/">Pan Zhang</a>,
<a href="https://yuhangzang.github.io/">Yuhang Zang</a>,  
<a href="https://scholar.google.com/citations?user=sJkqsqkAAAAJ">Yuhang Cao</a>, </br>  
<a href="http://dahua.site/">Dahua Lin</a>,  
<a href="https://myownskyw7.github.io/">Jiaqi Wang</a> 
</p>

<p align="center" style="font-size: 5 em; margin-top: 0.5em">
<a href="https://arxiv.org/abs/2502.13128"><img src="https://img.shields.io/badge/arXiv-<color>"></a>
<a href="https://github.com/LiuZH-19/SongGen"><img src="https://img.shields.io/badge/Code-red"></a>
<a href="https://liuzh-19.github.io/SongGen/"><img src="https://img.shields.io/badge/Demo-20d67c"></a>
<a href="https://huggingface.co/collections/LiuZH-19/songgen-a-single-stage-auto-regressive-transformer-for-text-6867ec21169d808034f6d252">
    <img src="https://img.shields.io/badge/HF-Collection-yellow"></a>
</p>





## üìú Noticias
üöÄ [2025/7/4] Hemos publicado el c√≥digo de entrenamiento junto con una detallada [gu√≠a de entrenamiento](https://raw.githubusercontent.com/LiuZH-19/SongGen/master/./training/README.md) .

üöÄ [2025/6/30] El conjunto de prueba MusicCaps ya est√° disponible en [Huggingfaceü§ó](https://huggingface.co/datasets/LiuZH-19/MusicCaps_Test_Song) para evaluaci√≥n de texto a canci√≥n.

üöÄ [2025/6/27] Hemos publicado el punto de control de SongGen Interleaving (A-V) en [Huggingfaceü§ó](https://huggingface.co/LiuZH-19/SongGen_interleaving_A_V).

üéâ [2025/5/1] ¬°SongGen ha sido aceptado en ICML 2025!

üöÄ [2025/3/18] Hemos publicado el punto de control de SongGen Mixed_Pro en [Huggingfaceü§ó](https://huggingface.co/LiuZH-19/SongGen_mixed_pro).

üöÄ [2025/2/19] El [art√≠culo](https://arxiv.org/abs/2502.13128) y la [p√°gina demo](https://liuzh-19.github.io/SongGen/) han sido publicados!

## üí° Destacados
- üî•Presentamos SongGen, un transformador auto-regresivo de **etapa √∫nica** para generaci√≥n de **texto a canci√≥n**, que ofrece control vers√°til mediante letras, texto descriptivo y una voz de referencia opcional.
- üî•SongGen soporta tanto el modo **mixto** como el modo **de dos pistas** para adaptarse a diferentes requerimientos. Nuestros experimentos proporcionan **valiosos insights** para optimizar ambos modos.
- üî•Al publicar los **pesos del modelo**, el **c√≥digo**, los **datos anotados** y la **cadena de preprocesamiento**, buscamos establecer una l√≠nea base simple pero efectiva para la investigaci√≥n futura en generaci√≥n de canciones.
<!-- <img align="center" src="https://raw.githubusercontent.com/LiuZH-19/SongGen/master/assets/imgs/motivation1.jpg" style="  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 50%;" /> -->

## üë®‚Äçüíª Por hacer
- [ ] Publicar datos anotados y cadena de preprocesamiento
- [x] Publicar conjunto de prueba Musiccaps
- [x] Publicar c√≥digo de entrenamiento de SongGen
- [x] Publicar punto de control de SongGen (Interleaving A-V)
- [x] Publicar punto de control de SongGen Mixed_pro
- [x] Publicar c√≥digo de inferencia de SongGen 
- [x] Demo de SongGen

## üõ†Ô∏è Uso

### 1. Instalar entorno y dependencias
```bash
git clone https://github.com/LiuZH-19/SongGen.git
cd SongGen
# We recommend using conda to create a new environment.
conda create -n songgen_env python=3.9.18 
conda activate songgen_env
# Install CUDA >= 11.8 and PyTorch, e.g.,
pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu118
pip install flash-attn==2.6.1 --no-build-isolation
```
Para usar SongGen solo en modo de inferencia, inst√°lelo usando:
```bash
pip install -e .
```
### 2. Descargar el xcodec

Descargue el punto de control X-Codec desde [ü§ó](https://raw.githubusercontent.com/LiuZH-19/SongGen/master/
https://huggingface.co/ZhenYe234/xcodec/blob/main/xcodec_hubert_general_audio_v2.pth) y col√≥quelo en el siguiente directorio : SongGen/songgen/xcodec_wrapper/xcodec_infer/ckpts/general_more

```
xcodec_infer
    ‚îú‚îÄ‚îÄ ckpts
    ‚îÇ   ‚îî‚îÄ‚îÄ general_more
    ‚îÇ       ‚îú‚îÄ‚îÄ config_hubert_general.yaml
    ‚îÇ       ‚îî‚îÄ‚îÄ xcodec_hubert_general_audio_v2.pth

```

### 3. Ejecutar la inferencia

#### (1). Modo Pro Mixto

```python
import torch
import os
from songgen import (
    VoiceBpeTokenizer,
    SongGenMixedForConditionalGeneration,
    SongGenProcessor
)
import soundfile as sf

ckpt_path = "LiuZH-19/SongGen_mixed_pro" # Path to the pretrained model
device = "cuda:0" if torch.cuda.is_available() else "cpu"
model = SongGenMixedForConditionalGeneration.from_pretrained(
    ckpt_path,
    attn_implementation='sdpa').to(device)
processor = SongGenProcessor(ckpt_path, device)

# Define input text and lyrics
lyrics = "..." # The lyrics text
text = "..." # The music description text
ref_voice_path = 'path/to/your/reference_audio.wav' # Path to reference audio, optional
separate= True # Whether to separate the vocal track from the reference voice audio

model_inputs = processor(text=text, lyrics=lyrics, ref_voice_path=ref_voice_path, separate=separate) 
generation = model.generate(**model_inputs,
                do_sample=True,
            )
audio_arr = generation.cpu().numpy().squeeze()
sf.write("songgen_out.wav", audio_arr, model.config.sampling_rate)
```



#### (2). Interleaving A-V  (Dual-track mode)
```python
import torch
import os
from songgen import (
    VoiceBpeTokenizer,
    SongGenDualTrackForConditionalGeneration,
    SongGenProcessor
)
import soundfile as sf

ckpt_path = "LiuZH-19/SongGen_interleaving_A_V" # Path to the pretrained model
device = "cuda:0" if torch.cuda.is_available() else "cpu"
model = SongGenDualTrackForConditionalGeneration.from_pretrained(
    ckpt_path,
    attn_implementation='sdpa').to(device)
processor = SongGenProcessor(ckpt_path, device)

# Define input text and lyrics
lyrics = "..." # The lyrics text
text = "..." # The music description text
ref_voice_path = 'path/to/your/reference_audio.wav' # Path to reference audio, optional
separate= True # Whether to separate the vocal track from the reference voice audio

model_inputs = processor(text=text, lyrics=lyrics, ref_voice_path=ref_voice_path, separate=True) 
generation = model.generate(**model_inputs,
                do_sample=True,
            )

acc_array = generation[0].cpu().numpy()
vocal_array = generation[1].cpu().numpy()
min_len =min(vocal_array.shape[0], acc_array.shape[0])
acc_array = acc_array[:min_len]
vocal_array = vocal_array[:min_len]
audio_arr = vocal_array + acc_array
sf.write("songgen_out.wav", audio_arr, model.config.sampling_rate)
```

### 4. Entrenamiento

La [carpeta de entrenamiento](./training) contiene toda la informaci√≥n para entrenar o ajustar finamente tu propio modelo SongGen. Consulta la [gu√≠a de entrenamiento](https://raw.githubusercontent.com/LiuZH-19/SongGen/master/./training/README.md) para instrucciones paso a paso.



## ‚ù§Ô∏è Agradecimientos
Esta biblioteca se construye sobre una serie de gigantes de c√≥digo abierto, a quienes queremos extender nuestro m√°s c√°lido agradecimiento por proporcionar estas herramientas.

Agradecimientos especiales a:

- [Parler-tts](https://github.com/huggingface/parler-tts): La base de c√≥digo sobre la que construimos.
- [X-Codec](https://github.com/zhenye234/xcodec): El c√≥dec de audio utilizado en nuestra investigaci√≥n.
- [lp-music-caps](https://github.com/seungheondoh/lp-music-caps): Un proyecto enfocado en generar subt√≠tulos para m√∫sica.

Apreciamos profundamente todo el apoyo recibido a lo largo del camino.

## ‚òéÔ∏è Limitaciones y Trabajo Futuro

Este es un **trabajo de investigaci√≥n** enfocado en la generaci√≥n **de texto a canci√≥n**. Debido a las limitaciones del conjunto de datos actual, nuestro modelo est√° restringido actualmente a generar canciones en ingl√©s con una duraci√≥n m√°xima de 30 segundos.
Sin embargo, a pesar de haber sido entrenado con solo **2k horas** de datos y un modelo de **1.3B** par√°metros, nuestro enfoque ha demostrado una fuerte efectividad y un potencial prometedor para generar canciones coherentes y expresivas. Creemos que aumentar tanto los datos como el tama√±o del modelo mejorar√° a√∫n m√°s la alineaci√≥n de las letras y la musicalidad.
Dicho esto, ampliar el conjunto de datos consume tiempo y es un desaf√≠o. Invitamos a colaboraciones y discusiones para explorar nuevas formas de mejorar el modelo y ampliar sus capacidades.
Para cualquier consulta o colaboraci√≥n potencial, no dudes en contactar a: Zihan Liu (liuzihan@pjlab.org.cn) y Jiaqi Wang (wangjiaqi@pjlab.org.cn).

## ‚úíÔ∏è Citaci√≥n
Si encuentras √∫til nuestro trabajo para tu investigaci√≥n, considera dar una estrella ‚≠ê y una citaci√≥n üìù
```bibtex
@misc{liu2025songgen,
      title={SongGen: A Single Stage Auto-regressive Transformer for Text-to-Song Generation}, 
      author={Zihan Liu and Shuangrui Ding and Zhixiong Zhang and Xiaoyi Dong and Pan Zhang and Yuhang Zang and Yuhang Cao and Dahua Lin and Jiaqi Wang},
      year={2025},
      eprint={2502.13128},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2502.13128}, 
}

```







---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-08-07

---