<div align="center">
<h1>
ë¬´ì—‡ì´ë“  ì¸ì§€í•˜ê¸°: ì´ë¯¸ì§€ ë° ë¹„ë””ì˜¤ ë‚´ ë¬´ì—‡ì´ë“  ì¸ì‹, ì„¤ëª…, ìº¡ì…˜ ì‘ì„±, ë¶„í• í•˜ê¸° (PAM)
</h1>

</div>

<div align="center">

[Weifeng Lin](), [Xinyu Wei](), [Ruichuan An](), [Tianhe Ren](), [Tingwei Chen](), [Renrui Zhang](), [Ziyu Guo]() <br>
[Wentao Zhang](), [Lei Zhang](), [Hongsheng Li]() <br>
CUHK, HKU, PolyU, PekingU

</div>

<p align="center">
  <a href="https://Perceive-Anything.github.io"><b>ğŸŒ í”„ë¡œì íŠ¸ ì›¹ì‚¬ì´íŠ¸</b></a> |
  <a href="https://arxiv.org/abs/2506.05302"><b>ğŸ“• ë…¼ë¬¸</b></a> |
  <a href="https://huggingface.co/Perceive-Anything/PAM-3B"><b>ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ</b></a> |
  <a href="https://huggingface.co/datasets/Perceive-Anything/PAM-data"><b>ğŸ¤— ë°ì´í„°ì…‹</b></a> |
  <a href="#quick-start"><b>âš¡ë¹ ë¥¸ ì‹œì‘</b></a> <br>
  <a href="#license"><b>ğŸ“œ ë¼ì´ì„ ìŠ¤</b></a> |
  <a href="#citation"><b>ğŸ“– ì¸ìš© (BibTeX)</b></a> <br>
</p>

<p align="center">
    <img src="https://raw.githubusercontent.com/Perceive-Anything/PAM/main/assets/teaser_img.jpg" width="95%"> <br>
    <img src="https://raw.githubusercontent.com/Perceive-Anything/PAM/main/assets/teaser_video.jpg" width="95%"> <br>
</p>

## ë‰´ìŠ¤

<!-- **2025.06.20**: Release Gradio demo ([online demo]() and [local](#gradio-demo)) -->

<!-- **2025.06.05**: Evaluation code Please refer to [this link](). -->

**2025.06.08**: ëª¨ë¸ ê°€ì¤‘ì¹˜(1.5B / 3B) ë° í•™ìŠµ ë°ì´í„°ì…‹ì´ ê³µê°œë˜ì—ˆìŠµë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [PAM-1.5B](https://huggingface.co/Perceive-Anything/PAM-1.5B), [PAM-3B](https://huggingface.co/Perceive-Anything/PAM-3B) ë° [ë°ì´í„°ì…‹](https://huggingface.co/datasets/Perceive-Anything/PAM-data)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

**2025.06.08**: ê°ì²´ ë¶„í•  ë° ì´í•´ë¥¼ ìœ„í•œ ê°„ë‹¨í•œ ì¢…ë‹¨ê°„(region-level) VLMì¸ PAMì´ ê³µê°œë˜ì—ˆìŠµë‹ˆë‹¤. ë…¼ë¬¸ì€ [ì—¬ê¸°](https://arxiv.org/abs/2506.05302)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.


## ì†Œê°œ

**ë¬´ì—‡ì´ë“  ì¸ì§€í•˜ê¸° ëª¨ë¸(PAM)**ì€ ì´ë¯¸ì§€ ë° ë¹„ë””ì˜¤ì—ì„œ í¬ê´„ì ì¸ ì˜ì—­ ìˆ˜ì¤€ ì‹œê° ì´í•´ë¥¼ ìœ„í•œ ê°œë…ì ìœ¼ë¡œ ê°„ë‹¨í•˜ê³  íš¨ìœ¨ì ì¸ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ë³¸ ì ‘ê·¼ë²•ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ í†µí•©í•˜ì—¬ SAM 2ë¥¼ í™•ì¥í•˜ë©°, ê°ì²´ ë¶„í• ê³¼ ë™ì‹œì— ë²”ì£¼, ë¼ë²¨ ì •ì˜, ê¸°ëŠ¥ì  ì„¤ëª… ë° ìƒì„¸ ìº¡ì…˜ ë“± ë‹¤ì–‘í•œ ì˜ì—­ë³„ ì˜ë¯¸ë¡ ì  ì¶œë ¥ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì¼ë°˜ì ì¸ ì‹œê° ì •ë³´, ìœ„ì¹˜ ë° ì˜ë¯¸ë¡ ì  ì‚¬ì „ ì§€ì‹ì„ ë‚´í¬í•œ SAM 2ì˜ í’ë¶€í•œ ì‹œê°ì  íŠ¹ì§•ì„ LLM ì´í•´ë¥¼ ìœ„í•œ ë‹¤ì¤‘ ëª¨ë‹¬ í† í°ìœ¼ë¡œ íš¨ìœ¨ì ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë°©ì‹ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê²¬ê³ í•œ ë‹¤ì¤‘ ì„¸ë¶„í™” ì´í•´ë¥¼ ì§€ì›í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ê³ í’ˆì§ˆì˜ ì´ë¯¸ì§€ ë° ë¹„ë””ì˜¤ ì˜ì—­-ì˜ë¯¸ ì£¼ì„ ë°ì´í„°ì…‹ [**dataset**](https://huggingface.co/datasets/Perceive-Anything/PAM-data)ê³¼ ìƒˆë¡œìš´ ì˜ì—­ ìˆ˜ì¤€ ìŠ¤íŠ¸ë¦¬ë° ë¹„ë””ì˜¤ ìº¡ì…˜ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ì „ìš© ë°ì´í„° ì •ì œ ë° ì¦ê°• íŒŒì´í”„ë¼ì¸ì„ ê°œë°œí–ˆìŠµë‹ˆë‹¤.


<p align="center">
    <img src="https://raw.githubusercontent.com/Perceive-Anything/PAM/main/assets/PAM_comp.jpg" width="95%"> <br>
    <img src="https://raw.githubusercontent.com/Perceive-Anything/PAM/main/assets/PAM_arch.jpg" width="95%"> <br>
</p>

## ì„¤ì¹˜

1. ì´ ì €ì¥ì†Œë¥¼ í´ë¡ í•˜ê³  ê¸°ë³¸ í´ë”ë¡œ ì´ë™í•©ë‹ˆë‹¤</translate-content>

```bash
git clone https://github.com/Afeng-x/PAM.git
cd PAM
```
<translate-content>
2. íŒ¨í‚¤ì§€ ì„¤ì¹˜</translate-content>
```bash
### packages for base
conda create -n PAM python=3.10 -y
conda activate PAM
pip install --upgrade pip
pip install -e ".[train]"
### packages for sam2
cd sam2
pip install -e ".[notebooks]"
```
3. í”Œë˜ì‹œ ì–´í…ì…˜ ì„¤ì¹˜í•˜ê¸°

```bash
pip install flash-attn --no-build-isolation
### (If the method mentioned above donâ€™t work for you, try the following one)
git clone https://github.com/Dao-AILab/flash-attention.git
cd flash-attention
python setup.py install
```
4. SAM2.1-h-large ì²´í¬í¬ì¸íŠ¸ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”:

```bash
cd llava/model/multimodal_encoder
bash download_ckpts.sh
```
## ë¹ ë¥¸ ì‹œì‘

- ì´ë¯¸ì§€: [image_infer_example.ipynb](https://raw.githubusercontent.com/Perceive-Anything/PAM/main/./notebooks/image_infer_example.ipynb)ì˜ ì˜ˆì œë¥¼ ì°¸ì¡°í•˜ì„¸ìš”  
- ë¹„ë””ì˜¤: [video_infer_example.ipynb](https://raw.githubusercontent.com/Perceive-Anything/PAM/main/./notebooks/video_infer_example.ipynb) ì˜ ì˜ˆì œë¥¼ ì°¸ì¡°í•˜ì„¸ìš”  
- ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼: [video_stream_infer_example.ipynb](https://raw.githubusercontent.com/Perceive-Anything/PAM/main/./notebooks/video_stream_infer_example.ipynb) ì˜ ì˜ˆì œë¥¼ ì°¸ì¡°í•˜ì„¸ìš”  

## ë°ì´í„°ì…‹

ì •ì œë˜ê³  ì¦ê°•ëœ ë°ì´í„° ì£¼ì„ì„ ë‹¤ìš´ë¡œë“œí•˜ë ¤ë©´ [ì´ ë§í¬](https://huggingface.co/datasets/Perceive-Anything/PAM-data)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.  

**ì°¸ê³ :** ì›ë³¸ ì´ë¯¸ì§€ëŠ” ì§ì ‘ ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê° ë°ì´í„°ì…‹ì— ëŒ€í•´ ê´€ë ¨ ë‹¤ìš´ë¡œë“œ ë§í¬ ë˜ëŠ” ê³µì‹ ì›¹ì‚¬ì´íŠ¸ ì£¼ì†Œë¥¼ ì œê³µí•˜ì—¬ ì‚¬ìš©ìê°€ ë‹¤ìš´ë¡œë“œ ë°©ë²•ì„ ì•ˆë‚´ë°›ì„ ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. [DATA_README](https://raw.githubusercontent.com/Perceive-Anything/PAM/main/data/README.md)  

<!-- ## Training PAM

You can train or fine-tune PAM on custom datasets of images, videos, or both. Please check the training [README](https://raw.githubusercontent.com/Perceive-Anything/PAM/main/training/README.md) on how to get started. -->

## PAMì„ ìœ„í•œ ë¡œì»¬ ê·¸ë¼ë””ì˜¤ ë°ëª¨
ì§„í–‰ ì¤‘ ......  
<!-- ### Simple Gradio Demo for Image

[`pam_image.py`](https://raw.githubusercontent.com/Perceive-Anything/PAM/main/pam_image.py) - ì´ë¯¸ì§€ì—ì„œ ë§ˆìŠ¤í¬ë¥¼ ê·¸ë¦¬ê³  ì˜ë¯¸ë¥¼ ì–»ê¸° ìœ„í•œ ëŒ€í™”í˜• Gradio ì›¹ ì¸í„°í˜ì´ìŠ¤ì…ë‹ˆë‹¤. **ì´ ë°ëª¨ëŠ” `gradio` 5.5.0ì—ì„œ í…ŒìŠ¤íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.**

### Simple Gradio Demo for Video

[`pam_video.py`](https://raw.githubusercontent.com/Perceive-Anything/PAM/main/pam_video.py) - ë¹„ë””ì˜¤ì—ì„œ ë§ˆìŠ¤í¬ë¥¼ ê·¸ë¦¬ê³  ì˜ë¯¸ë¥¼ ì–»ê¸° ìœ„í•œ ëŒ€í™”í˜• Gradio ì›¹ ì¸í„°í˜ì´ìŠ¤ì…ë‹ˆë‹¤. **ì´ ë°ëª¨ëŠ” `gradio` 5.5.0ì—ì„œ í…ŒìŠ¤íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.** -->

## ë¼ì´ì„ ìŠ¤

ì´ ì½”ë“œ ì €ì¥ì†ŒëŠ” [Apache 2.0](./LICENSE) ë¼ì´ì„ ìŠ¤ í•˜ì— ìˆìŠµë‹ˆë‹¤.  

## ê°ì‚¬ì˜ ê¸€
ë³¸ ì‘ì—…ì— ê¸°ì—¬í•œ ë‹¤ìŒ í”„ë¡œì íŠ¸ë“¤ì— ê°ì‚¬ë“œë¦½ë‹ˆë‹¤:

- [LLaVA-Next](https://github.com/LLaVA-VL/LLaVA-NeXT)  
- [SAM](https://github.com/facebookresearch/segment-anything)  
- [SAM 2](https://github.com/facebookresearch/sam2)  

## ì¸ìš©

PAMì´ ì—°êµ¬ë‚˜ ì‘ìš©ì— ìœ ìš©í•˜ê±°ë‚˜, ë°ì´í„°ì…‹ì„ ì—°êµ¬ì— ì‚¬ìš©í•˜ì…¨ë‹¤ë©´, ë‹¤ìŒ BibTeX í•­ëª©ì„ ì‚¬ìš©í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.


```bibtex
@misc{lin2025perceiveanythingrecognizeexplain,
      title={Perceive Anything: Recognize, Explain, Caption, and Segment Anything in Images and Videos}, 
      author={Weifeng Lin and Xinyu Wei and Ruichuan An and Tianhe Ren and Tingwei Chen and Renrui Zhang and Ziyu Guo and Wentao Zhang and Lei Zhang and Hongsheng Li},
      year={2025},
      eprint={2506.05302},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2506.05302}, 
}
```



---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-19

---