{
  "id": 2,
  "origin": "\n| Method | FID (Extreme) â†“ | FVD (Extreme) â†“ | VBench Score â†‘ |\n|--------|-----------------|-----------------|----------------|\n| ReCamMaster | 64.68 | 943.45 | 0.434 |\n| TrajectoryCrafter | 65.33 | 893.80 | 0.447 |\n| TrajectoryAttention | 62.49 | 912.14 | 0.389 |\n| **EX-4D (Ours)** | **55.42** | **823.61** | **0.450** | -->\n\n### User Study Results\n\n- **70.7%** of participants preferred EX-4D over baseline methods\n- Superior performance in physical consistency and extreme viewpoint quality\n- Significant improvement as camera angles become more extreme\n\n\n## ğŸ¯ Applications\n\n- **ğŸ® Gaming**: Create immersive 3D game cinematics from 2D footage\n- **ğŸ¬ Film Production**: Generate novel camera angles for post-production\n- **ğŸ¥½ VR/AR**: Create free-viewpoint video experiences\n- **ğŸ“± Social Media**: Generate dynamic camera movements for content creation\n- **ğŸ¢ Architecture**: Visualize spaces from multiple viewpoints\n\n<!-- ## ğŸ“ˆ Benchmarks -->\n\n<!-- ### Viewpoint Range Evaluation\n\n| Range | Small (0Â°â†’30Â°) | Large (0Â°â†’60Â°) | Extreme (0Â°â†’90Â°) | Full (-90Â°â†’90Â°) |\n|-------|----------------|----------------|------------------|-----------------|\n| FID Score | 44.19 | 50.30 | 55.42 | - |\n| Performance Gap | +9.1% better | +8.9% better | +11.3% better | +15.5% better | -->\n\n<!-- *Performance gap compared to the second-best method in each category.* -->\n\n## âš ï¸ Limitations\n\n- **Depth Dependency**: Performance relies on monocular depth estimation quality\n- **Computational Cost**: Requires significant computation for high-resolution videos\n- **Reflective Surfaces**: Challenges with reflective or transparent materials\n\n## ğŸ”® Future Work\n- [ ] Real-time inference optimization (3DGS / 4DGS)\n- [ ] Support for higher resolutions (1K, 2K)\n- [ ] Neural mesh refinement techniques\n\n## ğŸ™ Acknowledgments\n\nWe would like to thank the [DiffSynth-Studio v1.1.1](https://github.com/modelscope/DiffSynth-Studio/tree/v1.1.1) project for providing the foundational diffusion framework.\n\n## ğŸ“š Citation\n\nIf you find our work useful, please consider citing:\n\n```bibtex\n@misc{hu2025ex4dextremeviewpoint4d,\n      title={EX-4D: EXtreme Viewpoint 4D Video Synthesis via Depth Watertight Mesh}, \n      author={Tao Hu and Haoyang Peng and Xiao Liu and Yuewen Ma},\n      year={2025},\n      eprint={2506.05554},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2506.05554}, \n}\n```\n",
  "origin_sha": "MqODIYJGtSeyW7gMiMBJ9jTkV4ymtK2QYcLxg6W8I6k=",
  "translate": "| Metoda | FID (Ekstremalny) â†“ | FVD (Ekstremalny) â†“ | Wynik VBench â†‘ |\n|--------|---------------------|---------------------|----------------|\n| ReCamMaster | 64.68 | 943.45 | 0.434 |\n| TrajectoryCrafter | 65.33 | 893.80 | 0.447 |\n| TrajectoryAttention | 62.49 | 912.14 | 0.389 |\n| **EX-4D (Nasza metoda)** | **55.42** | **823.61** | **0.450** | -->\n\n### Wyniki badaÅ„ uÅ¼ytkownikÃ³w\n\n- **70,7%** uczestnikÃ³w preferowaÅ‚o EX-4D wzglÄ™dem metod bazowych\n- Lepsza spÃ³jnoÅ›Ä‡ fizyczna i jakoÅ›Ä‡ przy ekstremalnych kÄ…tach widzenia\n- ZnaczÄ…ca poprawa wraz ze wzrostem ekstremalnoÅ›ci kÄ…tÃ³w kamery\n\n\n## ğŸ¯ Zastosowania\n\n- **ğŸ® Gry**: Tworzenie immersyjnych filmowych scen 3D na podstawie materiaÅ‚u 2D\n- **ğŸ¬ Produkcja filmowa**: Generowanie nowych ujÄ™Ä‡ kamery do postprodukcji\n- **ğŸ¥½ VR/AR**: Tworzenie doÅ›wiadczeÅ„ wideo z dowolnego punktu widzenia\n- **ğŸ“± Media spoÅ‚ecznoÅ›ciowe**: Generowanie dynamicznych ruchÃ³w kamery do tworzenia treÅ›ci\n- **ğŸ¢ Architektura**: Wizualizacja przestrzeni z wielu punktÃ³w widzenia\n\n<!-- ## ğŸ“ˆ Benchmarki -->\n\n<!-- ### Ocena zakresu punktÃ³w widzenia\n\n| Zakres | MaÅ‚y (0Â°â†’30Â°) | DuÅ¼y (0Â°â†’60Â°) | Ekstremalny (0Â°â†’90Â°) | PeÅ‚ny (-90Â°â†’90Â°) |\n|--------|---------------|---------------|----------------------|------------------|\n| Wynik FID | 44.19 | 50.30 | 55.42 | - |\n| RÃ³Å¼nica wydajnoÅ›ci | +9,1% lepiej | +8,9% lepiej | +11,3% lepiej | +15,5% lepiej | -->\n\n<!-- *RÃ³Å¼nica wydajnoÅ›ci wzglÄ™dem drugiej najlepszej metody w kaÅ¼dej kategorii.* -->\n\n## âš ï¸ Ograniczenia\n\n- **ZaleÅ¼noÅ›Ä‡ od gÅ‚Ä™bi**: WydajnoÅ›Ä‡ zaleÅ¼y od jakoÅ›ci estymacji gÅ‚Ä™bi z jednego obrazu\n- **Koszt obliczeniowy**: Wymaga duÅ¼ej mocy obliczeniowej dla filmÃ³w w wysokiej rozdzielczoÅ›ci\n- **Powierzchnie refleksyjne**: TrudnoÅ›ci z materiaÅ‚ami odblaskowymi lub przezroczystymi\n\n## ğŸ”® PrzyszÅ‚e prace\n- [ ] Optymalizacja wnioskowania w czasie rzeczywistym (3DGS / 4DGS)\n- [ ] Wsparcie dla wyÅ¼szych rozdzielczoÅ›ci (1K, 2K)\n- [ ] Techniki neuronowego udoskonalania siatek (mesh refinement)\n\n## ğŸ™ PodziÄ™kowania\n\nChcielibyÅ›my podziÄ™kowaÄ‡ projektowi [DiffSynth-Studio v1.1.1](https://github.com/modelscope/DiffSynth-Studio/tree/v1.1.1) za udostÄ™pnienie podstawowego frameworka dyfuzyjnego.\n\n## ğŸ“š Cytowanie\n\nJeÅ›li nasza praca okazaÅ‚a siÄ™ przydatna, prosimy o cytowanie:\n\n```bibtex\n@misc{hu2025ex4dextremeviewpoint4d,\n      title={EX-4D: EXtreme Viewpoint 4D Video Synthesis via Depth Watertight Mesh}, \n      author={Tao Hu and Haoyang Peng and Xiao Liu and Yuewen Ma},\n      year={2025},\n      eprint={2506.05554},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2506.05554}, \n}\n```",
  "status": "ok"
}