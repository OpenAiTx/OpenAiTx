# EX-4D: Sintesi Video 4D da Punti di Vista Estremi tramite Depth Watertight Mesh

<div align="center">

<img src="https://raw.githubusercontent.com/tau-yihouxiang/EX-4D/main/docs/Logo.png" alt="EX-4D Logo" width="250">

[ğŸ“„ Paper](https://arxiv.org/abs/2506.05554)  |  [ğŸ¥ Homepage](https://tau-yihouxiang.github.io/projects/EX-4D/EX-4D.html)  |  [ğŸ’» Codice](https://github.com/tau-yihouxiang/EX-4D)

</div>



## ğŸŒŸ Punti Salienti

- **ğŸ¯ Sintesi da Punti di Vista Estremi**: Genera video 4D di alta qualitÃ  con movimenti di camera da -90Â° a 90Â°
- **ğŸ”§ Depth Watertight Mesh**: Nuova rappresentazione geometrica che modella sia le regioni visibili che quelle occluse
- **âš¡ Architettura Leggera**: Solo l'1% dei parametri addestrabili (140M) rispetto alla backbone di diffusione video da 14B
- **ğŸ­ Nessun Addestramento Multi-view**: Innovativa strategia di mascheramento che elimina la necessitÃ  di costosi dataset multi-view
- **ğŸ† Prestazioni all'Avanguardia**: Supera i metodi esistenti, soprattutto su angoli di ripresa estremi

## ğŸ¬ Risultati Demo

<div align="center">
<img src="https://raw.githubusercontent.com/tau-yihouxiang/EX-4D/main/docs/teaser.png" alt="EX-4D Demo Results" width="800">
</div>

*EX-4D trasforma video monoculari in esperienze 4D controllabili dalla camera con risultati fisicamente coerenti anche da punti di vista estremi.*

## ğŸ—ï¸ Panoramica del Framework

<div align="center">
<img src="https://raw.githubusercontent.com/tau-yihouxiang/EX-4D/main/docs/overview.png" alt="EX-4D Architecture">
</div>

Il nostro framework Ã¨ composto da tre componenti chiave:

1. **ğŸ”º Costruzione Depth Watertight Mesh**: Crea un robusto prior geometrico che modella esplicitamente sia le regioni visibili che quelle occluse
2. **ğŸ­ Strategia di Mascheramento Simulato**: Genera dati di addestramento efficaci da video monoculari senza dataset multi-view
3. **âš™ï¸ Lightweight LoRA Adapter**: Integra in modo efficiente le informazioni geometriche con modelli pre-addestrati di video diffusion

## ğŸš€ Quick Start

### Installazione

```bash
# Clona il repository
git clone https://github.com/tau-yihouxiang/EX-4D.git
cd EX-4D

# Crea un ambiente conda
conda create -n ex4d python=3.10
conda activate ex4d
# Installa PyTorch (consigliato 2.x)
pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu124
# Installa Nvdiffrast
pip install git+https://github.com/NVlabs/nvdiffrast.git
# Installa le dipendenze e diffsynth
pip install -e .
# Installa depthcrafter per la stima della profonditÃ . (Segui le istruzioni di DepthCrafter per la preparazione dei checkpoint.)
git clone https://github.com/Tencent/DepthCrafter.git
```

### Scarica il Modello Preaddestrato
```bash
huggingface-cli download Wan-AI/Wan2.1-I2V-14B-480P --local-dir ./models/Wan-AI
huggingface-cli download yihouxiang/EX-4D --local-dir ./models/EX-4D
```

### Esempio di Utilizzo
#### 1. Ricostruzione DW-Mesh
```bash
# --cam 180 (30 / 60 / 90 / zoom_in / zoom_out )
python recon.py --input_video examples/flower/input.mp4 --cam 180 --output_dir outputs/flower --save_mesh
```
#### 2. Generazione EX-4D (richiesti 48GB VRAM)
```bash
python generate.py --color_video outputs/flower/color_180.mp4 --mask_video outputs/flower/mask_180.mp4 --output_video outputs/flower/output.mp4
```

<table>
<tr>
<td width="45%" align="center">
<img src="https://raw.githubusercontent.com/tau-yihouxiang/EX-4D/main/examples/flower/input.gif" width="100%">
<br><b>Video di Input</b>
</td>
<td align="center">
<div style="font-size: 2em; color: #4A90E2; padding: 0 0px;">
  âœ
</div>
</td>
<td width="45%" align="center">
<img src="https://raw.githubusercontent.com/tau-yihouxiang/EX-4D/main/examples/flower/output.gif" width="100%">
<br><b>Video di Output</b>
</td>
</tr> 
</table>

<!-- ## ğŸ“Š Performance

### Quantitative Results
| Metodo | FID (Estremo) â†“ | FVD (Estremo) â†“ | VBench Score â†‘ |
|--------|-----------------|-----------------|----------------|
| ReCamMaster | 64.68 | 943.45 | 0.434 |
| TrajectoryCrafter | 65.33 | 893.80 | 0.447 |
| TrajectoryAttention | 62.49 | 912.14 | 0.389 |
| **EX-4D (Nostro)** | **55.42** | **823.61** | **0.450** | -->

### Risultati dello Studio Utente

- **70.7%** dei partecipanti ha preferito EX-4D rispetto ai metodi di base
- Prestazioni superiori in coerenza fisica e qualitÃ  da punti di vista estremi
- Miglioramento significativo man mano che gli angoli di ripresa diventano piÃ¹ estremi


## ğŸ¯ Applicazioni

- **ğŸ® Gaming**: Creazione di cinematiche di gioco 3D immersive da filmati 2D
- **ğŸ¬ Produzione Cinematografica**: Generazione di angolazioni di camera innovative per la post-produzione
- **ğŸ¥½ VR/AR**: Creazione di esperienze video a punto di vista libero
- **ğŸ“± Social Media**: Generazione di movimenti dinamici di camera per la creazione di contenuti
- **ğŸ¢ Architettura**: Visualizzazione di spazi da molteplici punti di vista

<!-- ## ğŸ“ˆ Benchmark -->

<!-- ### Valutazione Intervallo Angolazione

| Intervallo | Piccolo (0Â°â†’30Â°) | Grande (0Â°â†’60Â°) | Estremo (0Â°â†’90Â°) | Completo (-90Â°â†’90Â°) |
|------------|------------------|------------------|------------------|---------------------|
| FID Score | 44.19 | 50.30 | 55.42 | - |
| Vantaggio Prestazionale | +9.1% meglio | +8.9% meglio | +11.3% meglio | +15.5% meglio | -->

<!-- *Vantaggio prestazionale rispetto al secondo miglior metodo in ciascuna categoria.* -->

## âš ï¸ Limitazioni

- **Dipendenza dalla ProfonditÃ **: Le prestazioni dipendono dalla qualitÃ  della stima di profonditÃ  monoculare
- **Costo Computazionale**: Richiede notevoli risorse computazionali per video ad alta risoluzione
- **Superfici Riflettenti**: DifficoltÃ  con materiali riflettenti o trasparenti

## ğŸ”® Lavori Futuri
- [ ] Ottimizzazione per inferenza in tempo reale (3DGS / 4DGS)
- [ ] Supporto per risoluzioni piÃ¹ elevate (1K, 2K)
- [ ] Tecniche di raffinamento di mesh neurali

## ğŸ™ Ringraziamenti

Desideriamo ringraziare il progetto [DiffSynth-Studio v1.1.1](https://github.com/modelscope/DiffSynth-Studio/tree/v1.1.1) per aver fornito il framework di diffusione di base.

## ğŸ“š Citazione

Se trovi utile il nostro lavoro, considera di citarlo:

```bibtex
@misc{hu2025ex4dextremeviewpoint4d,
      title={EX-4D: EXtreme Viewpoint 4D Video Synthesis via Depth Watertight Mesh}, 
      author={Tao Hu and Haoyang Peng and Xiao Liu and Yuewen Ma},
      year={2025},
      eprint={2506.05554},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2506.05554}, 
}
```

---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-08

---