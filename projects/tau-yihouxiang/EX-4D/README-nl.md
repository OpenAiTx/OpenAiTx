# EX-4D: EXtreme Viewpoint 4D Video Synthese via Depth Watertight Mesh

<div align="center">

<img src="https://raw.githubusercontent.com/tau-yihouxiang/EX-4D/main/docs/Logo.png" alt="EX-4D Logo" width="250">

[ğŸ“„ Paper](https://arxiv.org/abs/2506.05554)  |  [ğŸ¥ Homepage](https://tau-yihouxiang.github.io/projects/EX-4D/EX-4D.html)  |  [ğŸ’» Code](https://github.com/tau-yihouxiang/EX-4D)

</div>



## ğŸŒŸ Hoogtepunten

- **ğŸ¯ Extreme Standpunt Synthese**: Genereer hoogwaardige 4D-videoâ€™s met camerabewegingen van -90Â° tot 90Â°
- **ğŸ”§ Depth Watertight Mesh**: Nieuwe geometrische representatie die zowel zichtbare als verborgen gebieden modelleert
- **âš¡ Lichtgewicht Architectuur**: Slechts 1% trainbare parameters (140M) van de 14B video diffusie-backbone
- **ğŸ­ Geen Multi-view Training**: Innovatieve maskeringsstrategie elimineert de noodzaak voor dure multi-view datasets
- **ğŸ† State-of-the-art Prestaties**: Overtreft bestaande methoden, vooral bij extreme camerahoeken

## ğŸ¬ Demo Resultaten

<div align="center">
<img src="https://raw.githubusercontent.com/tau-yihouxiang/EX-4D/main/docs/teaser.png" alt="EX-4D Demo Results" width="800">
</div>

*EX-4D transformeert monoculaire videoâ€™s in camera-controleerbare 4D-ervaringen met fysiek consistente resultaten onder extreme standpunten.*

## ğŸ—ï¸ Framework Overzicht

<div align="center">
<img src="https://raw.githubusercontent.com/tau-yihouxiang/EX-4D/main/docs/overview.png" alt="EX-4D Architecture">
</div>

Ons framework bestaat uit drie belangrijke componenten:

1. **ğŸ”º Depth Watertight Mesh Constructie**: CreÃ«ert een robuuste geometrische prior die zowel zichtbare als verborgen gebieden expliciet modelleert
2. **ğŸ­ Gesimuleerde Maskeringsstrategie**: Genereert effectieve trainingsdata uit monoculaire videoâ€™s zonder multi-view datasets
3. **âš™ï¸ Lichtgewicht LoRA Adapter**: Integreert efficiÃ«nt geometrische informatie met voorgetrainde video diffusie-modellen

## ğŸš€ Snelstart

### Installatie

```bash
# Clone de repository
git clone https://github.com/tau-yihouxiang/EX-4D.git
cd EX-4D

# Maak conda-omgeving aan
conda create -n ex4d python=3.10
conda activate ex4d
# Installeer PyTorch (2.x aanbevolen)
pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu124
# Installeer Nvdiffrast
pip install git+https://github.com/NVlabs/nvdiffrast.git
# Installeer afhankelijkheden en diffsynth
pip install -e .
# Installeer depthcrafter voor diepte-schatting. (Volg DepthCrafter's installatie-instructie voor de voorbereiding van checkpoints.)
git clone https://github.com/Tencent/DepthCrafter.git
```

### Download Voorgetraind Model
```bash
huggingface-cli download Wan-AI/Wan2.1-I2V-14B-480P --local-dir ./models/Wan-AI
huggingface-cli download yihouxiang/EX-4D --local-dir ./models/EX-4D
```

### Voorbeeld Gebruik
#### 1. DW-Mesh Reconstructie
```bash
# --cam 180 (30 / 60 / 90 / zoom_in / zoom_out )
python recon.py --input_video examples/flower/input.mp4 --cam 180 --output_dir outputs/flower --save_mesh
```
#### 2. EX-4D Generatie (48GB VRAM vereist)
```bash
python generate.py --color_video outputs/flower/color_180.mp4 --mask_video outputs/flower/mask_180.mp4 --output_video outputs/flower/output.mp4
```

<table>
<tr>
<td width="45%" align="center">
<img src="https://raw.githubusercontent.com/tau-yihouxiang/EX-4D/main/examples/flower/input.gif" width="100%">
<br><b>Invoervideo</b>
</td>
<td align="center">
<div style="font-size: 2em; color: #4A90E2; padding: 0 0px;">
  âœ
</div>
</td>
<td width="45%" align="center">
<img src="https://raw.githubusercontent.com/tau-yihouxiang/EX-4D/main/examples/flower/output.gif" width="100%">
<br><b>Uitvoervideo</b>
</td>
</tr> 
</table>

<!-- ## ğŸ“Š Prestaties

### Kwantitatieve Resultaten
| Methode | FID (Extreme) â†“ | FVD (Extreme) â†“ | VBench Score â†‘ |
|---------|-----------------|-----------------|----------------|
| ReCamMaster | 64.68 | 943.45 | 0.434 |
| TrajectoryCrafter | 65.33 | 893.80 | 0.447 |
| TrajectoryAttention | 62.49 | 912.14 | 0.389 |
| **EX-4D (Ours)** | **55.42** | **823.61** | **0.450** | -->

### Resultaten Gebruikersonderzoek

- **70,7%** van de deelnemers gaf de voorkeur aan EX-4D boven de basismethoden
- Superieure prestaties in fysieke consistentie en kwaliteit bij extreme gezichtspunten
- Significante verbetering naarmate de camerahoeken extremer worden


## ğŸ¯ Toepassingen

- **ğŸ® Gaming**: Maak meeslepende 3D-gamecinematografie van 2D-beelden
- **ğŸ¬ Filmproductie**: Genereer nieuwe camerahoeken voor postproductie
- **ğŸ¥½ VR/AR**: Maak vrij-uitzicht video-ervaringen
- **ğŸ“± Sociale media**: Genereer dynamische camerabewegingen voor contentcreatie
- **ğŸ¢ Architectuur**: Visualiseer ruimtes vanuit meerdere gezichtspunten

<!-- ## ğŸ“ˆ Benchmarks -->

<!-- ### Viewpoint Range Evaluation

| Bereik | Klein (0Â°â†’30Â°) | Groot (0Â°â†’60Â°) | Extreem (0Â°â†’90Â°) | Volledig (-90Â°â†’90Â°) |
|--------|----------------|----------------|------------------|---------------------|
| FID Score | 44.19 | 50.30 | 55.42 | - |
| Prestatiekloof | +9,1% beter | +8,9% beter | +11,3% beter | +15,5% beter | -->

<!-- *Prestatiekloof ten opzichte van de op Ã©Ã©n na beste methode in elke categorie.* -->

## âš ï¸ Beperkingen

- **Diepteafhankelijkheid**: Prestaties zijn afhankelijk van de kwaliteit van monoculaire diepteschatting
- **Rekenkosten**: Vereist aanzienlijke rekenkracht voor video's met hoge resolutie
- **Reflecterende oppervlakken**: Uitdagingen met reflecterende of transparante materialen

## ğŸ”® Toekomstig Werk
- [ ] Real-time inferentie-optimalisatie (3DGS / 4DGS)
- [ ] Ondersteuning voor hogere resoluties (1K, 2K)
- [ ] Neurale mesh-verfijningstechnieken

## ğŸ™ Dankwoord

Wij willen het [DiffSynth-Studio v1.1.1](https://github.com/modelscope/DiffSynth-Studio/tree/v1.1.1) project bedanken voor het leveren van het fundamentele diffusiemodel.

## ğŸ“š Referentie

Als u ons werk nuttig vindt, overweeg dan om te citeren:

```bibtex
@misc{hu2025ex4dextremeviewpoint4d,
      title={EX-4D: EXtreme Viewpoint 4D Video Synthesis via Depth Watertight Mesh}, 
      author={Tao Hu and Haoyang Peng and Xiao Liu and Yuewen Ma},
      year={2025},
      eprint={2506.05554},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2506.05554}, 
}
```

---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-08

---