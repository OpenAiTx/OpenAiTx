# EX-4D: SÃ­ntese de VÃ­deo 4D com Pontos de Vista Extremos via Malha Ã  Prova dâ€™Ãgua de Profundidade

<div align="center">

<img src="https://raw.githubusercontent.com/tau-yihouxiang/EX-4D/main/docs/Logo.png" alt="EX-4D Logo" width="250">

[ğŸ“„ Artigo](https://arxiv.org/abs/2506.05554)  |  [ğŸ¥ PÃ¡gina Principal](https://tau-yihouxiang.github.io/projects/EX-4D/EX-4D.html)  |  [ğŸ’» CÃ³digo](https://github.com/tau-yihouxiang/EX-4D)

</div>



## ğŸŒŸ Destaques

- **ğŸ¯ SÃ­ntese de Pontos de Vista Extremos**: Gere vÃ­deos 4D de alta qualidade com movimentos de cÃ¢mera de -90Â° a 90Â°
- **ğŸ”§ Malha Ã  Prova dâ€™Ãgua de Profundidade**: Nova representaÃ§Ã£o geomÃ©trica que modela regiÃµes visÃ­veis e ocultas
- **âš¡ Arquitetura Leve**: Apenas 1% dos parÃ¢metros treinÃ¡veis (140M) do backbone de difusÃ£o de vÃ­deo de 14B
- **ğŸ­ Sem Treinamento Multi-view**: EstratÃ©gia inovadora de mascaramento elimina a necessidade de datasets multi-view caros
- **ğŸ† Desempenho de Estado da Arte**: Supera mÃ©todos existentes, especialmente em Ã¢ngulos de cÃ¢mera extremos

## ğŸ¬ Resultados de DemonstraÃ§Ã£o

<div align="center">
<img src="https://raw.githubusercontent.com/tau-yihouxiang/EX-4D/main/docs/teaser.png" alt="EX-4D Demo Results" width="800">
</div>

*EX-4D transforma vÃ­deos monoculares em experiÃªncias 4D controlÃ¡veis pela cÃ¢mera, com resultados fisicamente consistentes sob pontos de vista extremos.*

## ğŸ—ï¸ VisÃ£o Geral do Framework

<div align="center">
<img src="https://raw.githubusercontent.com/tau-yihouxiang/EX-4D/main/docs/overview.png" alt="EX-4D Architecture">
</div>

Nosso framework consiste em trÃªs componentes principais:

1. **ğŸ”º ConstruÃ§Ã£o de Malha Ã  Prova dâ€™Ãgua de Profundidade**: Cria um prior geomÃ©trico robusto que modela explicitamente regiÃµes visÃ­veis e ocultas
2. **ğŸ­ EstratÃ©gia de Mascaramento Simulado**: Gera dados de treinamento eficazes a partir de vÃ­deos monoculares sem datasets multi-view
3. **âš™ï¸ Adaptador LoRA Leve**: Integra eficientemente informaÃ§Ãµes geomÃ©tricas com modelos de difusÃ£o de vÃ­deo prÃ©-treinados

## ğŸš€ InÃ­cio RÃ¡pido

### InstalaÃ§Ã£o

```bash
# Clone o repositÃ³rio
git clone https://github.com/tau-yihouxiang/EX-4D.git
cd EX-4D

# Crie o ambiente conda
conda create -n ex4d python=3.10
conda activate ex4d
# Instale o PyTorch (recomendado 2.x)
pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu124
# Instale o Nvdiffrast
pip install git+https://github.com/NVlabs/nvdiffrast.git
# Instale dependÃªncias e diffsynth
pip install -e .
# Instale depthcrafter para estimativa de profundidade. (Siga as instruÃ§Ãµes de instalaÃ§Ã£o do DepthCrafter para preparaÃ§Ã£o dos checkpoints.)
git clone https://github.com/Tencent/DepthCrafter.git
```

### Baixar Modelo PrÃ©-treinado
```bash
huggingface-cli download Wan-AI/Wan2.1-I2V-14B-480P --local-dir ./models/Wan-AI
huggingface-cli download yihouxiang/EX-4D --local-dir ./models/EX-4D
```

### Exemplo de Uso
#### 1. ReconstruÃ§Ã£o DW-Mesh
```bash
# --cam 180 (30 / 60 / 90 / zoom_in / zoom_out )
python recon.py --input_video examples/flower/input.mp4 --cam 180 --output_dir outputs/flower --save_mesh
```
#### 2. GeraÃ§Ã£o EX-4D (Requer 48GB VRAM)
```bash
python generate.py --color_video outputs/flower/color_180.mp4 --mask_video outputs/flower/mask_180.mp4 --output_video outputs/flower/output.mp4
```

<table>
<tr>
<td width="45%" align="center">
<img src="https://raw.githubusercontent.com/tau-yihouxiang/EX-4D/main/examples/flower/input.gif" width="100%">
<br><b>VÃ­deo de Entrada</b>
</td>
<td align="center">
<div style="font-size: 2em; color: #4A90E2; padding: 0 0px;">
  âœ
</div>
</td>
<td width="45%" align="center">
<img src="https://raw.githubusercontent.com/tau-yihouxiang/EX-4D/main/examples/flower/output.gif" width="100%">
<br><b>VÃ­deo de SaÃ­da</b>
</td>
</tr> 
</table>

<!-- ## ğŸ“Š Performance

### Quantitative Results
| MÃ©todo | FID (Extremo) â†“ | FVD (Extremo) â†“ | PontuaÃ§Ã£o VBench â†‘ |
|--------|-----------------|-----------------|--------------------|
| ReCamMaster | 64.68 | 943.45 | 0.434 |
| TrajectoryCrafter | 65.33 | 893.80 | 0.447 |
| TrajectoryAttention | 62.49 | 912.14 | 0.389 |
| **EX-4D (Nosso)** | **55.42** | **823.61** | **0.450** | -->

### Resultados do Estudo com UsuÃ¡rios

- **70,7%** dos participantes preferiram o EX-4D em relaÃ§Ã£o aos mÃ©todos de referÃªncia
- Desempenho superior em consistÃªncia fÃ­sica e qualidade de pontos de vista extremos
- Melhora significativa Ã  medida que os Ã¢ngulos de cÃ¢mera se tornam mais extremos


## ğŸ¯ AplicaÃ§Ãµes

- **ğŸ® Games**: CriaÃ§Ã£o de cenas cinematogrÃ¡ficas 3D imersivas a partir de vÃ­deos 2D
- **ğŸ¬ ProduÃ§Ã£o de Filmes**: GeraÃ§Ã£o de novos Ã¢ngulos de cÃ¢mera para pÃ³s-produÃ§Ã£o
- **ğŸ¥½ VR/AR**: CriaÃ§Ã£o de experiÃªncias de vÃ­deo com ponto de vista livre
- **ğŸ“± MÃ­dias Sociais**: GeraÃ§Ã£o de movimentos dinÃ¢micos de cÃ¢mera para criaÃ§Ã£o de conteÃºdo
- **ğŸ¢ Arquitetura**: VisualizaÃ§Ã£o de espaÃ§os a partir de mÃºltiplos pontos de vista

<!-- ## ğŸ“ˆ Benchmarks -->

<!-- ### AvaliaÃ§Ã£o do Alcance do Ponto de Vista

| Alcance | Pequeno (0Â°â†’30Â°) | Grande (0Â°â†’60Â°) | Extremo (0Â°â†’90Â°) | Completo (-90Â°â†’90Â°) |
|---------|------------------|------------------|------------------|---------------------|
| PontuaÃ§Ã£o FID | 44.19 | 50.30 | 55.42 | - |
| DiferenÃ§a de desempenho | +9,1% melhor | +8,9% melhor | +11,3% melhor | +15,5% melhor | -->

<!-- *DiferenÃ§a de desempenho em comparaÃ§Ã£o ao segundo melhor mÃ©todo em cada categoria.* -->

## âš ï¸ LimitaÃ§Ãµes

- **DependÃªncia de Profundidade**: O desempenho depende da qualidade da estimativa de profundidade monocular
- **Custo Computacional**: Exige computaÃ§Ã£o significativa para vÃ­deos em alta resoluÃ§Ã£o
- **SuperfÃ­cies Reflexivas**: Desafios com materiais reflexivos ou transparentes

## ğŸ”® Trabalhos Futuros
- [ ] OtimizaÃ§Ã£o para inferÃªncia em tempo real (3DGS / 4DGS)
- [ ] Suporte para resoluÃ§Ãµes mais altas (1K, 2K)
- [ ] TÃ©cnicas de refinamento de malha neural

## ğŸ™ Agradecimentos

GostarÃ­amos de agradecer ao projeto [DiffSynth-Studio v1.1.1](https://github.com/modelscope/DiffSynth-Studio/tree/v1.1.1) por fornecer a base do framework de difusÃ£o.

## ğŸ“š CitaÃ§Ã£o

Se vocÃª achar nosso trabalho Ãºtil, por favor, considere citar:

```bibtex
@misc{hu2025ex4dextremeviewpoint4d,
      title={EX-4D: EXtreme Viewpoint 4D Video Synthesis via Depth Watertight Mesh}, 
      author={Tao Hu and Haoyang Peng and Xiao Liu and Yuewen Ma},
      year={2025},
      eprint={2506.05554},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2506.05554}, 
}
```

---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-08

---