# EX-4D: SÃ­ntesis de Video 4D de Puntos de Vista Extremos mediante Malla Estanca de Profundidad

<div align="center">

<img src="https://raw.githubusercontent.com/tau-yihouxiang/EX-4D/main/docs/Logo.png" alt="EX-4D Logo" width="250">

[ğŸ“„ ArtÃ­culo](https://arxiv.org/abs/2506.05554)  |  [ğŸ¥ PÃ¡gina principal](https://tau-yihouxiang.github.io/projects/EX-4D/EX-4D.html)  |  [ğŸ’» CÃ³digo](https://github.com/tau-yihouxiang/EX-4D)

</div>



## ğŸŒŸ Destacados

- **ğŸ¯ SÃ­ntesis de Puntos de Vista Extremos**: Genera videos 4D de alta calidad con movimientos de cÃ¡mara desde -90Â° hasta 90Â°
- **ğŸ”§ Malla Estanca de Profundidad**: Nueva representaciÃ³n geomÃ©trica que modela tanto regiones visibles como ocultas
- **âš¡ Arquitectura Liviana**: Solo el 1% de los parÃ¡metros entrenables (140M) de la columna vertebral de difusiÃ³n de video de 14B
- **ğŸ­ Sin Entrenamiento Multi-vista**: Estrategia innovadora de enmascaramiento que elimina la necesidad de costosos conjuntos de datos multi-vista
- **ğŸ† Rendimiento de Ãšltima GeneraciÃ³n**: Supera a los mÃ©todos existentes, especialmente en Ã¡ngulos extremos de cÃ¡mara

## ğŸ¬ Resultados de DemostraciÃ³n

<div align="center">
<img src="https://raw.githubusercontent.com/tau-yihouxiang/EX-4D/main/docs/teaser.png" alt="EX-4D Demo Results" width="800">
</div>

*EX-4D transforma videos monoculares en experiencias 4D controlables por cÃ¡mara con resultados fÃ­sicamente consistentes bajo puntos de vista extremos.*

## ğŸ—ï¸ DescripciÃ³n General del Framework

<div align="center">
<img src="https://raw.githubusercontent.com/tau-yihouxiang/EX-4D/main/docs/overview.png" alt="EX-4D Architecture">
</div>

Nuestro framework consta de tres componentes clave:

1. **ğŸ”º ConstrucciÃ³n de Malla Estanca de Profundidad**: Crea un robusto priorde geomÃ©trico que modela explÃ­citamente regiones visibles y ocultas
2. **ğŸ­ Estrategia de Enmascaramiento Simulado**: Genera datos de entrenamiento efectivos a partir de videos monoculares sin conjuntos de datos multi-vista
3. **âš™ï¸ Adaptador LoRA Liviano**: Integra eficientemente la informaciÃ³n geomÃ©trica con modelos de difusiÃ³n de video preentrenados

## ğŸš€ Inicio RÃ¡pido

### InstalaciÃ³n

```bash
# Clona el repositorio
git clone https://github.com/tau-yihouxiang/EX-4D.git
cd EX-4D

# Crea el entorno conda
conda create -n ex4d python=3.10
conda activate ex4d
# Instala PyTorch (se recomienda 2.x)
pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu124
# Instala Nvdiffrast
pip install git+https://github.com/NVlabs/nvdiffrast.git
# Instala dependencias y diffsynth
pip install -e .
# Instala depthcrafter para la estimaciÃ³n de profundidad. (Sigue las instrucciones de instalaciÃ³n de DepthCrafter para la preparaciÃ³n de los checkpoints.)
git clone https://github.com/Tencent/DepthCrafter.git
```

### Descargar el Modelo Preentrenado
```bash
huggingface-cli download Wan-AI/Wan2.1-I2V-14B-480P --local-dir ./models/Wan-AI
huggingface-cli download yihouxiang/EX-4D --local-dir ./models/EX-4D
```

### Ejemplo de Uso
#### 1. ReconstrucciÃ³n DW-Mesh
```bash
# --cam 180 (30 / 60 / 90 / zoom_in / zoom_out )
python recon.py --input_video examples/flower/input.mp4 --cam 180 --output_dir outputs/flower --save_mesh
```
#### 2. GeneraciÃ³n EX-4D (se requieren 48GB de VRAM)
```bash
python generate.py --color_video outputs/flower/color_180.mp4 --mask_video outputs/flower/mask_180.mp4 --output_video outputs/flower/output.mp4
```

<table>
<tr>
<td width="45%" align="center">
<img src="https://raw.githubusercontent.com/tau-yihouxiang/EX-4D/main/examples/flower/input.gif" width="100%">
<br><b>Video de Entrada</b>
</td>
<td align="center">
<div style="font-size: 2em; color: #4A90E2; padding: 0 0px;">
  âœ
</div>
</td>
<td width="45%" align="center">
<img src="https://raw.githubusercontent.com/tau-yihouxiang/EX-4D/main/examples/flower/output.gif" width="100%">
<br><b>Video de Salida</b>
</td>
</tr> 
</table>

<!-- ## ğŸ“Š Rendimiento

### Resultados Cuantitativos
| MÃ©todo | FID (Extremo) â†“ | FVD (Extremo) â†“ | PuntuaciÃ³n VBench â†‘ |
|--------|-----------------|-----------------|---------------------|
| ReCamMaster | 64.68 | 943.45 | 0.434 |
| TrajectoryCrafter | 65.33 | 893.80 | 0.447 |
| TrajectoryAttention | 62.49 | 912.14 | 0.389 |
| **EX-4D (Nuestro)** | **55.42** | **823.61** | **0.450** | -->

### Resultados del Estudio de Usuario

- **70.7%** de los participantes prefirieron EX-4D sobre los mÃ©todos base
- Rendimiento superior en consistencia fÃ­sica y calidad en puntos de vista extremos
- Mejora significativa a medida que los Ã¡ngulos de la cÃ¡mara se vuelven mÃ¡s extremos


## ğŸ¯ Aplicaciones

- **ğŸ® Videojuegos**: Crear cinemÃ¡ticas de juegos 3D inmersivas a partir de grabaciones 2D
- **ğŸ¬ ProducciÃ³n CinematogrÃ¡fica**: Generar Ã¡ngulos de cÃ¡mara novedosos para postproducciÃ³n
- **ğŸ¥½ VR/AR**: Crear experiencias de video con puntos de vista libres
- **ğŸ“± Redes Sociales**: Generar movimientos dinÃ¡micos de cÃ¡mara para creaciÃ³n de contenido
- **ğŸ¢ Arquitectura**: Visualizar espacios desde mÃºltiples puntos de vista

<!-- ## ğŸ“ˆ Benchmarks -->

<!-- ### EvaluaciÃ³n del Rango de Puntos de Vista

| Rango | PequeÃ±o (0Â°â†’30Â°) | Grande (0Â°â†’60Â°) | Extremo (0Â°â†’90Â°) | Completo (-90Â°â†’90Â°) |
|-------|-------------------|------------------|-------------------|---------------------|
| PuntuaciÃ³n FID | 44.19 | 50.30 | 55.42 | - |
| Diferencia de Rendimiento | +9.1% mejor | +8.9% mejor | +11.3% mejor | +15.5% mejor | -->

<!-- *Diferencia de rendimiento en comparaciÃ³n con el segundo mejor mÃ©todo en cada categorÃ­a.* -->

## âš ï¸ Limitaciones

- **Dependencia de Profundidad**: El rendimiento depende de la calidad de la estimaciÃ³n de profundidad monocular
- **Costo Computacional**: Requiere cÃ³mputo significativo para videos de alta resoluciÃ³n
- **Superficies Reflectantes**: DesafÃ­os con materiales reflectantes o transparentes

## ğŸ”® Trabajo Futuro
- [ ] OptimizaciÃ³n de inferencia en tiempo real (3DGS / 4DGS)
- [ ] Soporte para resoluciones mÃ¡s altas (1K, 2K)
- [ ] TÃ©cnicas de refinamiento de malla neuronal

## ğŸ™ Agradecimientos

Queremos agradecer al proyecto [DiffSynth-Studio v1.1.1](https://github.com/modelscope/DiffSynth-Studio/tree/v1.1.1) por proveer el marco de difusiÃ³n fundamental.

## ğŸ“š CitaciÃ³n

Si encuentra Ãºtil nuestro trabajo, por favor considere citarlo:

```bibtex
@misc{hu2025ex4dextremeviewpoint4d,
      title={EX-4D: EXtreme Viewpoint 4D Video Synthesis via Depth Watertight Mesh}, 
      author={Tao Hu and Haoyang Peng and Xiao Liu y Yuewen Ma},
      year={2025},
      eprint={2506.05554},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2506.05554}, 
}
```

---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-08

---