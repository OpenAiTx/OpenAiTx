{
  "id": 1,
  "origin": "# EX-4D: EXtreme Viewpoint 4D Video Synthesis via Depth Watertight Mesh\n\n<div align=\"center\">\n\n<img src=\"https://raw.githubusercontent.com/tau-yihouxiang/EX-4D/main/docs/Logo.png\" alt=\"EX-4D Logo\" width=\"250\">\n\n[ğŸ“„ Paper](https://arxiv.org/abs/2506.05554)  |  [ğŸ¥ Homepage](https://tau-yihouxiang.github.io/projects/EX-4D/EX-4D.html)  |  [ğŸ’» Code](https://github.com/tau-yihouxiang/EX-4D)\n\n</div>\n\n\n\n## ğŸŒŸ Highlights\n\n- **ğŸ¯ Extreme Viewpoint Synthesis**: Generate high-quality 4D videos with camera movements ranging from -90Â° to 90Â°\n- **ğŸ”§ Depth Watertight Mesh**: Novel geometric representation that models both visible and occluded regions\n- **âš¡ Lightweight Architecture**: Only 1% trainable parameters (140M) of the 14B video diffusion backbone\n- **ğŸ­ No Multi-view Training**: Innovative masking strategy eliminates the need for expensive multi-view datasets\n- **ğŸ† State-of-the-art Performance**: Outperforms existing methods, especially on extreme camera angles\n\n## ğŸ¬ Demo Results\n\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/tau-yihouxiang/EX-4D/main/docs/teaser.png\" alt=\"EX-4D Demo Results\" width=\"800\">\n</div>\n\n*EX-4D transforms monocular videos into camera-controllable 4D experiences with physically consistent results under extreme viewpoints.*\n\n## ğŸ—ï¸ Framework Overview\n\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/tau-yihouxiang/EX-4D/main/docs/overview.png\" alt=\"EX-4D Architecture\">\n</div>\n\nOur framework consists of three key components:\n\n1. **ğŸ”º Depth Watertight Mesh Construction**: Creates a robust geometric prior that explicitly models both visible and occluded regions\n2. **ğŸ­ Simulated Masking Strategy**: Generates effective training data from monocular videos without multi-view datasets\n3. **âš™ï¸ Lightweight LoRA Adapter**: Efficiently integrates geometric information with pre-trained video diffusion models\n\n## ğŸš€ Quick Start\n\n### Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/tau-yihouxiang/EX-4D.git\ncd EX-4D\n\n# Create conda environment\nconda create -n ex4d python=3.10\nconda activate ex4d\n# Install PyTorch (2.x recommended)\npip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu124\n# Install Nvdiffrast\npip install git+https://github.com/NVlabs/nvdiffrast.git\n# Install dependencies and diffsynth\npip install -e .\n# Install depthcrafter for depth estimation. (Follow DepthCrafter's installing instruction for checkpoints preparation.)\ngit clone https://github.com/Tencent/DepthCrafter.git\n```\n\n### Download Pretrained Model\n```bash\nhuggingface-cli download Wan-AI/Wan2.1-I2V-14B-480P --local-dir ./models/Wan-AI\nhuggingface-cli download yihouxiang/EX-4D --local-dir ./models/EX-4D\n```\n\n### Example Usage\n#### 1. DW-Mesh Reconstruction\n```bash\n# --cam 180 (30 / 60 / 90 / zoom_in / zoom_out )\npython recon.py --input_video examples/flower/input.mp4 --cam 180 --output_dir outputs/flower --save_mesh\n```\n#### 2. EX-4D Generation (48GB VRAM required)\n```bash\npython generate.py --color_video outputs/flower/color_180.mp4 --mask_video outputs/flower/mask_180.mp4 --output_video outputs/flower/output.mp4\n```\n\n<table>\n<tr>\n<td width=\"45%\" align=\"center\">\n<img src=\"https://raw.githubusercontent.com/tau-yihouxiang/EX-4D/main/examples/flower/input.gif\" width=\"100%\">\n<br><b>Input Video</b>\n</td>\n<td align=\"center\">\n<div style=\"font-size: 2em; color: #4A90E2; padding: 0 0px;\">\n  âœ\n</div>\n</td>\n<td width=\"45%\" align=\"center\">\n<img src=\"https://raw.githubusercontent.com/tau-yihouxiang/EX-4D/main/examples/flower/output.gif\" width=\"100%\">\n<br><b>Output Video</b>\n</td>\n</tr> \n</table>\n\n<!-- ## ğŸ“Š Performance\n\n### Quantitative Results",
  "origin_sha": "z/SjAGRVytkbxDboQkPKdL4sDyxhY8jlopCaz23Kuro=",
  "translate": "# EX-4D : SynthÃ¨se vidÃ©o 4D Ã  points de vue extrÃªmes via un maillage Ã©tanche Ã  la profondeur\n\n<div align=\"center\">\n\n<img src=\"https://raw.githubusercontent.com/tau-yihouxiang/EX-4D/main/docs/Logo.png\" alt=\"EX-4D Logo\" width=\"250\">\n\n[ğŸ“„ Article](https://arxiv.org/abs/2506.05554)  |  [ğŸ¥ Page d'accueil](https://tau-yihouxiang.github.io/projects/EX-4D/EX-4D.html)  |  [ğŸ’» Code](https://github.com/tau-yihouxiang/EX-4D)\n\n</div>\n\n\n\n## ğŸŒŸ Points forts\n\n- **ğŸ¯ SynthÃ¨se Ã  points de vue extrÃªmes** : GÃ©nÃ¨re des vidÃ©os 4D de haute qualitÃ© avec des mouvements de camÃ©ra de -90Â° Ã  90Â°\n- **ğŸ”§ Maillage Ã©tanche Ã  la profondeur** : Nouvelle reprÃ©sentation gÃ©omÃ©trique modÃ©lisant Ã  la fois les rÃ©gions visibles et occultÃ©es\n- **âš¡ Architecture lÃ©gÃ¨re** : Seulement 1 % des paramÃ¨tres entraÃ®nables (140M) du backbone de diffusion vidÃ©o 14B\n- **ğŸ­ Pas d'entraÃ®nement multi-vues** : StratÃ©gie de masquage innovante Ã©liminant le besoin de jeux de donnÃ©es multi-vues coÃ»teux\n- **ğŸ† Performances Ã  l'Ã©tat de l'art** : Surpasse les mÃ©thodes existantes, notamment sur les angles de camÃ©ra extrÃªmes\n\n## ğŸ¬ RÃ©sultats de dÃ©monstration\n\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/tau-yihouxiang/EX-4D/main/docs/teaser.png\" alt=\"EX-4D Demo Results\" width=\"800\">\n</div>\n\n*EX-4D transforme les vidÃ©os monoculaires en expÃ©riences 4D contrÃ´lables par la camÃ©ra avec des rÃ©sultats physiquement cohÃ©rents sous des points de vue extrÃªmes.*\n\n## ğŸ—ï¸ Vue d'ensemble du framework\n\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/tau-yihouxiang/EX-4D/main/docs/overview.png\" alt=\"EX-4D Architecture\">\n</div>\n\nNotre framework se compose de trois Ã©lÃ©ments clÃ©s :\n\n1. **ğŸ”º Construction du maillage Ã©tanche Ã  la profondeur** : CrÃ©e un a priori gÃ©omÃ©trique robuste modÃ©lisant explicitement les rÃ©gions visibles et occultÃ©es\n2. **ğŸ­ StratÃ©gie de masquage simulÃ©e** : GÃ©nÃ¨re des donnÃ©es d'entraÃ®nement efficaces Ã  partir de vidÃ©os monoculaires sans jeux de donnÃ©es multi-vues\n3. **âš™ï¸ Adaptateur LoRA lÃ©ger** : IntÃ¨gre efficacement les informations gÃ©omÃ©triques avec des modÃ¨les de diffusion vidÃ©o prÃ©-entraÃ®nÃ©s\n\n## ğŸš€ DÃ©marrage rapide\n\n### Installation\n\n```bash\n# Cloner le dÃ©pÃ´t\ngit clone https://github.com/tau-yihouxiang/EX-4D.git\ncd EX-4D\n\n# CrÃ©er un environnement conda\nconda create -n ex4d python=3.10\nconda activate ex4d\n# Installer PyTorch (2.x recommandÃ©)\npip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu124\n# Installer Nvdiffrast\npip install git+https://github.com/NVlabs/nvdiffrast.git\n# Installer les dÃ©pendances et diffsynth\npip install -e .\n# Installer depthcrafter pour l'estimation de profondeur. (Suivez les instructions d'installation de DepthCrafter pour la prÃ©paration des checkpoints.)\ngit clone https://github.com/Tencent/DepthCrafter.git\n```\n\n### TÃ©lÃ©charger le modÃ¨le prÃ©-entraÃ®nÃ©\n```bash\nhuggingface-cli download Wan-AI/Wan2.1-I2V-14B-480P --local-dir ./models/Wan-AI\nhuggingface-cli download yihouxiang/EX-4D --local-dir ./models/EX-4D\n```\n\n### Exemple d'utilisation\n#### 1. Reconstruction du maillage DW\n```bash\n# --cam 180 (30 / 60 / 90 / zoom_in / zoom_out )\npython recon.py --input_video examples/flower/input.mp4 --cam 180 --output_dir outputs/flower --save_mesh\n```\n#### 2. GÃ©nÃ©ration EX-4D (48GB VRAM requis)\n```bash\npython generate.py --color_video outputs/flower/color_180.mp4 --mask_video outputs/flower/mask_180.mp4 --output_video outputs/flower/output.mp4\n```\n\n<table>\n<tr>\n<td width=\"45%\" align=\"center\">\n<img src=\"https://raw.githubusercontent.com/tau-yihouxiang/EX-4D/main/examples/flower/input.gif\" width=\"100%\">\n<br><b>VidÃ©o d'entrÃ©e</b>\n</td>\n<td align=\"center\">\n<div style=\"font-size: 2em; color: #4A90E2; padding: 0 0px;\">\n  âœ\n</div>\n</td>\n<td width=\"45%\" align=\"center\">\n<img src=\"https://raw.githubusercontent.com/tau-yihouxiang/EX-4D/main/examples/flower/output.gif\" width=\"100%\">\n<br><b>VidÃ©o de sortie</b>\n</td>\n</tr> \n</table>\n\n<!-- ## ğŸ“Š Performance\n\n### Quantitative Results",
  "status": "ok"
}