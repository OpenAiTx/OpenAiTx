[
  {
    "Id": 1,
    "Content": "# OneTwoVLA: A Unified Vision-Language-Action Model with Adaptive Reasoning\n\n[[Project Page]](https://one-two-vla.github.io/) [[Paper]](https://arxiv.org/abs/2505.11917) [[Processed Datasets]](https://huggingface.co/datasets/Richard-Nai/onetwovla-dataset)\n\n[Fanqi Lin](https://fanqi-lin.github.io/)<sup>1,2,3,5\\*</sup>,\n[Ruiqian Nai](https://richard-coder-nai.github.io/)<sup>1,2,3,5\\*</sup>,\n[Yingdong Hu](https://yingdong-hu.github.io/)<sup>1,2,3\\*</sup>,\n[Jiacheng You](https://scholar.google.com/citations?user=FiP-TVUAAAAJ)<sup>1,2,3</sup>,\nJunming Zhao<sup>1,4</sup>,\n[Yang Gao](https://yang-gao.weebly.com/)<sup>1,2,3,5</sup>\n\n<sup>1</sup>Tsinghua University,\n<sup>2</sup>Shanghai Qi Zhi Institute,\n<sup>3</sup>Shanghai AI Lab, \n<sup>4</sup>Fudan University,\n<sup>5</sup>Spirit AI\n\n<sup>\\*</sup> indicates equal contributions\n\n\n## 🛠️ Installation\n\nWe manage Python dependencies with [uv](https://docs.astral.sh/uv/). If you haven't installed `uv`, please follow [uv installation instructions](https://docs.astral.sh/uv/getting-started/installation/) to set it up. \n\nRun the following to set up the environment:\n",
    "ContentSha": "IuOtcNu/UFy/IDKQKBIFss9jAlyu3e8nCzu3F7A4tc0=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "# OneTwoVLA: 적응형 추론을 갖춘 통합 비전-언어-행동 모델\n\n[[프로젝트 페이지]](https://one-two-vla.github.io/) [[논문]](https://arxiv.org/abs/2505.11917) [[처리된 데이터셋]](https://huggingface.co/datasets/Richard-Nai/onetwovla-dataset)\n\n[Fanqi Lin](https://fanqi-lin.github.io/)<sup>1,2,3,5\\*</sup>,\n[Ruiqian Nai](https://richard-coder-nai.github.io/)<sup>1,2,3,5\\*</sup>,\n[Yingdong Hu](https://yingdong-hu.github.io/)<sup>1,2,3\\*</sup>,\n[Jiacheng You](https://scholar.google.com/citations?user=FiP-TVUAAAAJ)<sup>1,2,3</sup>,\nJunming Zhao<sup>1,4</sup>,\n[Yang Gao](https://yang-gao.weebly.com/)<sup>1,2,3,5</sup>\n\n<sup>1</sup>칭화대학,\n<sup>2</sup>상하이 치즈 연구소,\n<sup>3</sup>상하이 AI 연구소,\n<sup>4</sup>복단대학,\n<sup>5</sup>Spirit AI\n\n<sup>\\*</sup>는 동등한 기여를 나타냅니다\n\n\n## 🛠️ 설치\n\n우리는 Python 의존성을 [uv](https://docs.astral.sh/uv/)로 관리합니다. `uv`를 설치하지 않았다면, [uv 설치 안내](https://docs.astral.sh/uv/getting-started/installation/)를 따라 설치해 주세요.\n\n다음 명령어로 환경을 설정합니다:\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "2WIOmZu45wCgKkG4TyoIVD1BkqIeUOdOGOoeSAyR3Qw=",
        "originContent": "# OneTwoVLA: A Unified Vision-Language-Action Model with Adaptive Reasoning",
        "translatedContent": "# OneTwoVLA: 적응형 추론을 갖춘 통합 비전-언어-행동 모델"
      },
      {
        "row": 2,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 3,
        "rowsha": "+QysMWdUHiBfPrTZK1a9Swte21DwlwYQEknRJZPck6Y=",
        "originContent": "[[Project Page]](https://one-two-vla.github.io/) [[Paper]](https://arxiv.org/abs/2505.11917) [[Processed Datasets]](https://huggingface.co/datasets/Richard-Nai/onetwovla-dataset)",
        "translatedContent": "[[프로젝트 페이지]](https://one-two-vla.github.io/) [[논문]](https://arxiv.org/abs/2505.11917) [[처리된 데이터셋]](https://huggingface.co/datasets/Richard-Nai/onetwovla-dataset)"
      },
      {
        "row": 4,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 5,
        "rowsha": "i5TSD/dqatxhfvnmaJHjS9NHHkznAywROLL3Qy70G0U=",
        "originContent": "[Fanqi Lin](https://fanqi-lin.github.io/)<sup>1,2,3,5\\*</sup>,",
        "translatedContent": "[Fanqi Lin](https://fanqi-lin.github.io/)<sup>1,2,3,5\\*</sup>,"
      },
      {
        "row": 6,
        "rowsha": "FxUa3ah1sou8tk335XF+FtBO6NRJzrsZKcHUDuFOKK4=",
        "originContent": "[Ruiqian Nai](https://richard-coder-nai.github.io/)<sup>1,2,3,5\\*</sup>,",
        "translatedContent": "[Ruiqian Nai](https://richard-coder-nai.github.io/)<sup>1,2,3,5\\*</sup>,"
      },
      {
        "row": 7,
        "rowsha": "Kyh2SB/dSXd1TOBx95WXgSMmNGusg08rkii8XNm/Qzg=",
        "originContent": "[Yingdong Hu](https://yingdong-hu.github.io/)<sup>1,2,3\\*</sup>,",
        "translatedContent": "[Yingdong Hu](https://yingdong-hu.github.io/)<sup>1,2,3\\*</sup>,"
      },
      {
        "row": 8,
        "rowsha": "SJhnPiiPA8mAiUOzd8VyBii7SSEAzzTXtipLYK6eCzI=",
        "originContent": "[Jiacheng You](https://scholar.google.com/citations?user=FiP-TVUAAAAJ)<sup>1,2,3</sup>,",
        "translatedContent": "[Jiacheng You](https://scholar.google.com/citations?user=FiP-TVUAAAAJ)<sup>1,2,3</sup>,"
      },
      {
        "row": 9,
        "rowsha": "uY0Fn41b88ikx8++6/ItKnEBoywD94pDZbQvkO9ONzU=",
        "originContent": "Junming Zhao<sup>1,4</sup>,",
        "translatedContent": "Junming Zhao<sup>1,4</sup>,"
      },
      {
        "row": 10,
        "rowsha": "i1BqPt/V8MNVUbqNukA9ISq5peipx6UFIIXNhHKozFI=",
        "originContent": "[Yang Gao](https://yang-gao.weebly.com/)<sup>1,2,3,5</sup>",
        "translatedContent": "[Yang Gao](https://yang-gao.weebly.com/)<sup>1,2,3,5</sup>"
      },
      {
        "row": 11,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 12,
        "rowsha": "Hz6ceffxmT8jtvRhf0Z1IbwiG8+FjeAwPz2P8NSNC9g=",
        "originContent": "<sup>1</sup>Tsinghua University,",
        "translatedContent": "<sup>1</sup>칭화대학,"
      },
      {
        "row": 13,
        "rowsha": "b721h29eZHLJuKUUT+bX4FU9zjuViAsTqJPuxzPC9uI=",
        "originContent": "<sup>2</sup>Shanghai Qi Zhi Institute,",
        "translatedContent": "<sup>2</sup>상하이 치즈 연구소,"
      },
      {
        "row": 14,
        "rowsha": "cc7gUzpbR3nSarXPpuv62DsPi9wxZBkN/Wjz2Ws918k=",
        "originContent": "<sup>3</sup>Shanghai AI Lab, ",
        "translatedContent": "<sup>3</sup>상하이 AI 연구소,"
      },
      {
        "row": 15,
        "rowsha": "BimtuE/6aSPxmUrW0Vb8WxNS44HdeV8Z2pRw1BUkcms=",
        "originContent": "<sup>4</sup>Fudan University,",
        "translatedContent": "<sup>4</sup>복단대학,"
      },
      {
        "row": 16,
        "rowsha": "GTm/5HSeekOEt0508AMbrpRvB+DYrNlhpvYmRv8ECeo=",
        "originContent": "<sup>5</sup>Spirit AI",
        "translatedContent": "<sup>5</sup>Spirit AI"
      },
      {
        "row": 17,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 18,
        "rowsha": "7enxRP0HfndSc6nvZnfWJIkp+czQME4Ms6sN+Q5We9M=",
        "originContent": "<sup>\\*</sup> indicates equal contributions",
        "translatedContent": "<sup>\\*</sup>는 동등한 기여를 나타냅니다"
      },
      {
        "row": 19,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 20,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 21,
        "rowsha": "mjxiuRZ1/CyihNdVofQBPNtawI9pDP9M+RFrU+so5kM=",
        "originContent": "## 🛠️ Installation",
        "translatedContent": "## 🛠️ 설치"
      },
      {
        "row": 22,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 23,
        "rowsha": "wv+eaFoOBZDaNfOZyzq003jRgOeBUP7Q6eZYNmWp3CE=",
        "originContent": "We manage Python dependencies with [uv](https://docs.astral.sh/uv/). If you haven't installed `uv`, please follow [uv installation instructions](https://docs.astral.sh/uv/getting-started/installation/) to set it up. ",
        "translatedContent": "우리는 Python 의존성을 [uv](https://docs.astral.sh/uv/)로 관리합니다. `uv`를 설치하지 않았다면, [uv 설치 안내](https://docs.astral.sh/uv/getting-started/installation/)를 따라 설치해 주세요."
      },
      {
        "row": 24,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 25,
        "rowsha": "Zc6BdVNXXvJ0ijqFa19vglAoECBAGJAzkNOJ7dAgvAQ=",
        "originContent": "Run the following to set up the environment:",
        "translatedContent": "다음 명령어로 환경을 설정합니다:"
      },
      {
        "row": 26,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 2,
    "Content": "```bash\nGIT_LFS_SKIP_SMUDGE=1 uv sync\nGIT_LFS_SKIP_SMUDGE=1 uv pip install -e .\n```",
    "ContentSha": "a10ht77Cvq1nwDAVsg13OMmvhz4r3lmBVVzXe+jJabY=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bash\nGIT_LFS_SKIP_SMUDGE=1 uv sync\nGIT_LFS_SKIP_SMUDGE=1 uv pip install -e .\n```",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "rTMjNc+qNIxb1xXDR5hEYiJw1fc4muBwuBsTXVIaIho=",
        "originContent": "```bash",
        "translatedContent": "```bash"
      },
      {
        "row": 2,
        "rowsha": "0RrwraRExvZM/tIl6NSHMieibtEHub8x+wKvZzvJyqE=",
        "originContent": "GIT_LFS_SKIP_SMUDGE=1 uv sync",
        "translatedContent": "GIT_LFS_SKIP_SMUDGE=1 uv sync"
      },
      {
        "row": 3,
        "rowsha": "GngV3TJXyPL+mAiC8RBy0mjtarvJus/OQYw2nJd7PV8=",
        "originContent": "GIT_LFS_SKIP_SMUDGE=1 uv pip install -e .",
        "translatedContent": "GIT_LFS_SKIP_SMUDGE=1 uv pip install -e ."
      },
      {
        "row": 4,
        "rowsha": "8bkBhHOQsO1+N058HkZOwXtGpCfEh6WtbL0pBkBQg9U=",
        "originContent": "```",
        "translatedContent": "```"
      }
    ],
    "IsCodeBlock": true
  },
  {
    "Id": 3,
    "Content": "\n> NOTE: `GIT_LFS_SKIP_SMUDGE=1` is needed to pull LeRobot as a dependency.\n\nFor more details, refer to the original [openpi repository](https://github.com/Physical-Intelligence/openpi.git). \n\n## 🚀 Training OneTwoVLA\nDownload the dataset and place them under `$LEROBOT_HOME/umi/`.\n\nTo train a OneTwoVLA model, run:",
    "ContentSha": "glhX1nhSiG/NZtJVpm8UBbK7ogQnSNgWWBPJEsQTUcQ=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "> 참고: LeRobot을 의존성으로 가져오기 위해 `GIT_LFS_SKIP_SMUDGE=1`이 필요합니다.\n\n자세한 내용은 원본 [openpi 저장소](https://github.com/Physical-Intelligence/openpi.git)를 참조하세요.\n\n## 🚀 OneTwoVLA 훈련하기\n데이터셋을 다운로드하여 `$LEROBOT_HOME/umi/` 아래에 위치시킵니다.\n\nOneTwoVLA 모델을 훈련하려면, 다음을 실행하세요:\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "> 참고: LeRobot을 의존성으로 가져오기 위해 `GIT_LFS_SKIP_SMUDGE=1`이 필요합니다."
      },
      {
        "row": 2,
        "rowsha": "KdhzsdyISdu+IrbKF/C7B1pFus4YLeY30bRtwAl2WdE=",
        "originContent": "> NOTE: `GIT_LFS_SKIP_SMUDGE=1` is needed to pull LeRobot as a dependency.",
        "translatedContent": ""
      },
      {
        "row": 3,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "자세한 내용은 원본 [openpi 저장소](https://github.com/Physical-Intelligence/openpi.git)를 참조하세요."
      },
      {
        "row": 4,
        "rowsha": "zBCLyhRpJTP+WeD1J6QvJtfjNGb5IOgYRrqmIajfB9k=",
        "originContent": "For more details, refer to the original [openpi repository](https://github.com/Physical-Intelligence/openpi.git). ",
        "translatedContent": ""
      },
      {
        "row": 5,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## 🚀 OneTwoVLA 훈련하기"
      },
      {
        "row": 6,
        "rowsha": "8wED7outGFpEeDoM2KWV68k4m4SVV3rp6i/5n6rvwg8=",
        "originContent": "## 🚀 Training OneTwoVLA",
        "translatedContent": "데이터셋을 다운로드하여 `$LEROBOT_HOME/umi/` 아래에 위치시킵니다."
      },
      {
        "row": 7,
        "rowsha": "AIuLjqU+GdwKoqvewfwGO0e3tbuXR19t8nOIqtvBOIo=",
        "originContent": "Download the dataset and place them under `$LEROBOT_HOME/umi/`.",
        "translatedContent": ""
      },
      {
        "row": 8,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "OneTwoVLA 모델을 훈련하려면, 다음을 실행하세요:"
      },
      {
        "row": 9,
        "rowsha": "lggefBMBMdbLxxIRks4Dw2q0v7ScHv+41MxotJrHhmk=",
        "originContent": "To train a OneTwoVLA model, run:",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 4,
    "Content": "```bash\nbash train_scripts/train_<task_name>.sh\n```",
    "ContentSha": "YBJq/IHV+XjBo0792nS93nRirIU3Q8iGNWXiEcXAeRQ=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bash\nbash train_scripts/train_<task_name>.sh\n```",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "rTMjNc+qNIxb1xXDR5hEYiJw1fc4muBwuBsTXVIaIho=",
        "originContent": "```bash",
        "translatedContent": "```bash"
      },
      {
        "row": 2,
        "rowsha": "Arct76t49ZdzHvK0sdbfBJSwRAWeb0tOm0NlmqvXDuA=",
        "originContent": "bash train_scripts/train_<task_name>.sh",
        "translatedContent": "bash train_scripts/train_<task_name>.sh"
      },
      {
        "row": 3,
        "rowsha": "8bkBhHOQsO1+N058HkZOwXtGpCfEh6WtbL0pBkBQg9U=",
        "originContent": "```",
        "translatedContent": "```"
      }
    ],
    "IsCodeBlock": true
  },
  {
    "Id": 5,
    "Content": "Available tasks are:",
    "ContentSha": "+77T8O3iSnZItf8I/0rc7yQAyYwADEYdfZXmbC6CFUE=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "사용 가능한 작업은 다음과 같습니다:",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "+77T8O3iSnZItf8I/0rc7yQAyYwADEYdfZXmbC6CFUE=",
        "originContent": "Available tasks are:",
        "translatedContent": "사용 가능한 작업은 다음과 같습니다:"
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 6,
    "Content": "```bash\ntrain_scripts\n|-- train_onetwovla_cocktail.sh\n|-- train_onetwovla_visual_grounding.sh\n|-- train_pi0_cocktail.sh\n|-- train_pi0_visual_grounding.sh\n```",
    "ContentSha": "lppw/43rAOm+6uhUJl+Q7oGlMFcw0ZHhFCQjdJDUrzw=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bash\ntrain_scripts\n|-- train_onetwovla_cocktail.sh\n|-- train_onetwovla_visual_grounding.sh\n|-- train_pi0_cocktail.sh\n|-- train_pi0_visual_grounding.sh\n```",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "rTMjNc+qNIxb1xXDR5hEYiJw1fc4muBwuBsTXVIaIho=",
        "originContent": "```bash",
        "translatedContent": "```bash"
      },
      {
        "row": 2,
        "rowsha": "czNQEF3zGa0wsVGbnjXB4n2tJz+7VfWS8uXwOhE2m1I=",
        "originContent": "train_scripts",
        "translatedContent": "train_scripts"
      },
      {
        "row": 3,
        "rowsha": "R/YO11xBh2lba8GY+2rGdbOCf5PzeVOt0y3XstCKtbU=",
        "originContent": "|-- train_onetwovla_cocktail.sh",
        "translatedContent": "|-- train_onetwovla_cocktail.sh"
      },
      {
        "row": 4,
        "rowsha": "u4sDmZz7BEVbQGJHfO82YYA0fM5moIFXp5rgsz1BqsY=",
        "originContent": "|-- train_onetwovla_visual_grounding.sh",
        "translatedContent": "|-- train_onetwovla_visual_grounding.sh"
      },
      {
        "row": 5,
        "rowsha": "rcQzIwIR/kqPfgxWXT/4NcxOaqPsQ0lkpT/hW9HF/yk=",
        "originContent": "|-- train_pi0_cocktail.sh",
        "translatedContent": "|-- train_pi0_cocktail.sh"
      },
      {
        "row": 6,
        "rowsha": "TuhMCJ6uKb6IKZH/jnbmrPd9D4x19nakpR4OVn8sx3w=",
        "originContent": "|-- train_pi0_visual_grounding.sh",
        "translatedContent": "|-- train_pi0_visual_grounding.sh"
      },
      {
        "row": 7,
        "rowsha": "8bkBhHOQsO1+N058HkZOwXtGpCfEh6WtbL0pBkBQg9U=",
        "originContent": "```",
        "translatedContent": "```"
      }
    ],
    "IsCodeBlock": true
  },
  {
    "Id": 7,
    "Content": "\n## 🦾 Real-World Deployment\nWe run inference using a policy server and a hardware client. The instructions for running policy server can be found at [examples/umi/README.md](https://raw.githubusercontent.com/Fanqi-Lin/OneTwoVLA/main/examples/umi/README.md), and we provide the UMI hardware client code in this [repository](https://github.com/Fanqi-Lin/OneTwoVLA-UMI-Client).\n\n## 📷 Data\nWe provide access to the following datasets:\n\n- `Robot Datasets`: Datasets for the `cocktail` and `open-world visual grounding` tasks.\n- `Vision-Language Datasets`: Datasets contains synthetic images and annotated reasoning for the `open-world visual grounding` task.\n\nAll datasets are hosted on Hugging Face. You can find them [here](https://huggingface.co/datasets/Richard-Nai/onetwovla-dataset).\n\nWe provide code for converting UMI data format to LeRobot data format [here](https://raw.githubusercontent.com/Fanqi-Lin/OneTwoVLA/main/examples/umi/convert_umi_data_to_lerobot.py).\n\n### Synthetic Image Augmentation\n\nTo make the synthetic images more closely resemble real robot observations, we randomly apply several augmentations, including random fisheye distortion and compositing a robot gripper with adaptive brightness adjustments. The implementation is available in [scripts/augment_vl_data/augment.py](https://raw.githubusercontent.com/Fanqi-Lin/OneTwoVLA/main/scripts/augment_vl_data/augment.py).\n\nHere we show an example. From left to right, the images are: the original image, the image with fisheye distortion, the image compositing a robot gripper with adaptive brightness adjustments, and the image with both applied.\n\n<img width=\"90%\" src=\"https://raw.githubusercontent.com/Fanqi-Lin/OneTwoVLA/main/figures/fisheye-aug.png\">\n\n## 🙏 Acknowledgements\nWe express our sincere gratitude to the developers of the [openpi](https://github.com/Physical-Intelligence/openpi.git) for open-sourcing their code.\n",
    "ContentSha": "buZUnK2VwJSzP9BA3Pwb2pSLw2xMLOQO5jaSxGaJ+wM=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## 🦾 실제 배포  \n우리는 정책 서버와 하드웨어 클라이언트를 사용하여 추론을 수행합니다. 정책 서버 실행 지침은 [examples/umi/README.md](https://raw.githubusercontent.com/Fanqi-Lin/OneTwoVLA/main/examples/umi/README.md)에서 확인할 수 있으며, UMI 하드웨어 클라이언트 코드는 이 [저장소](https://github.com/Fanqi-Lin/OneTwoVLA-UMI-Client)에서 제공합니다.  \n\n## 📷 데이터  \n다음 데이터셋에 대한 접근을 제공합니다:  \n\n- `로봇 데이터셋`: `칵테일` 및 `오픈월드 비주얼 그라운딩` 작업용 데이터셋.  \n- `비전-언어 데이터셋`: `오픈월드 비주얼 그라운딩` 작업을 위한 합성 이미지 및 주석된 추론 포함 데이터셋.  \n\n모든 데이터셋은 Hugging Face에서 호스팅됩니다. [여기](https://huggingface.co/datasets/Richard-Nai/onetwovla-dataset)에서 확인할 수 있습니다.  \n\nUMI 데이터 포맷을 LeRobot 데이터 포맷으로 변환하는 코드는 [여기](https://raw.githubusercontent.com/Fanqi-Lin/OneTwoVLA/main/examples/umi/convert_umi_data_to_lerobot.py)에서 제공합니다.  \n\n### 합성 이미지 증강  \n\n합성 이미지가 실제 로봇 관측과 더 유사해지도록, 랜덤 어그멘테이션을 적용하며, 여기에는 랜덤 어안 렌즈 왜곡과 적응형 밝기 조정이 포함된 로봇 그리퍼 합성이 포함됩니다. 구현은 [scripts/augment_vl_data/augment.py](https://raw.githubusercontent.com/Fanqi-Lin/OneTwoVLA/main/scripts/augment_vl_data/augment.py)에서 확인할 수 있습니다.  \n\n다음은 예시입니다. 좌측부터 우측으로 원본 이미지, 어안 렌즈 왜곡 적용 이미지, 적응형 밝기 조정이 적용된 로봇 그리퍼 합성 이미지, 두 가지 모두 적용된 이미지입니다.  \n\n<img width=\"90%\" src=\"https://raw.githubusercontent.com/Fanqi-Lin/OneTwoVLA/main/figures/fisheye-aug.png\">  \n\n## 🙏 감사의 말씀  \n코드를 오픈 소스로 공개해주신 [openpi](https://github.com/Physical-Intelligence/openpi.git) 개발자분들께 진심으로 감사드립니다.  \n\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## 🦾 실제 배포  "
      },
      {
        "row": 2,
        "rowsha": "Jvy0N3uD30AKxvEPftR5pZnxc70QBh+o04yLfOqkRa0=",
        "originContent": "## 🦾 Real-World Deployment",
        "translatedContent": "우리는 정책 서버와 하드웨어 클라이언트를 사용하여 추론을 수행합니다. 정책 서버 실행 지침은 [examples/umi/README.md](https://raw.githubusercontent.com/Fanqi-Lin/OneTwoVLA/main/examples/umi/README.md)에서 확인할 수 있으며, UMI 하드웨어 클라이언트 코드는 이 [저장소](https://github.com/Fanqi-Lin/OneTwoVLA-UMI-Client)에서 제공합니다.  "
      },
      {
        "row": 3,
        "rowsha": "tDBYI05qiQzxTD+ZwP5OshD58N1Cm30gzN0KxD4aVdY=",
        "originContent": "We run inference using a policy server and a hardware client. The instructions for running policy server can be found at [examples/umi/README.md](https://raw.githubusercontent.com/Fanqi-Lin/OneTwoVLA/main/examples/umi/README.md), and we provide the UMI hardware client code in this [repository](https://github.com/Fanqi-Lin/OneTwoVLA-UMI-Client).",
        "translatedContent": ""
      },
      {
        "row": 4,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## 📷 데이터  "
      },
      {
        "row": 5,
        "rowsha": "o19pLNM7IsGPc2J86Ecbva2O/vncAi0dsl35YU588Mw=",
        "originContent": "## 📷 Data",
        "translatedContent": "다음 데이터셋에 대한 접근을 제공합니다:  "
      },
      {
        "row": 6,
        "rowsha": "IxFrMTkmacyCAA2t0xJHfBADHkwiE8/s9jawsb3wu4o=",
        "originContent": "We provide access to the following datasets:",
        "translatedContent": ""
      },
      {
        "row": 7,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "- `로봇 데이터셋`: `칵테일` 및 `오픈월드 비주얼 그라운딩` 작업용 데이터셋.  "
      },
      {
        "row": 8,
        "rowsha": "xsbWlva3r7hR/qYK1jfbEnXjnvJvbYXEyfT/aFs7ups=",
        "originContent": "- `Robot Datasets`: Datasets for the `cocktail` and `open-world visual grounding` tasks.",
        "translatedContent": "- `비전-언어 데이터셋`: `오픈월드 비주얼 그라운딩` 작업을 위한 합성 이미지 및 주석된 추론 포함 데이터셋.  "
      },
      {
        "row": 9,
        "rowsha": "05wNok5riKvy57MTMGqkZdA9tL2Z7oVXyCJXdOKqRV4=",
        "originContent": "- `Vision-Language Datasets`: Datasets contains synthetic images and annotated reasoning for the `open-world visual grounding` task.",
        "translatedContent": ""
      },
      {
        "row": 10,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "모든 데이터셋은 Hugging Face에서 호스팅됩니다. [여기](https://huggingface.co/datasets/Richard-Nai/onetwovla-dataset)에서 확인할 수 있습니다.  "
      },
      {
        "row": 11,
        "rowsha": "rKl6jLQZ8wSB5y95aPSz1sgQB0WklfXenToLvlaBtGk=",
        "originContent": "All datasets are hosted on Hugging Face. You can find them [here](https://huggingface.co/datasets/Richard-Nai/onetwovla-dataset).",
        "translatedContent": ""
      },
      {
        "row": 12,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "UMI 데이터 포맷을 LeRobot 데이터 포맷으로 변환하는 코드는 [여기](https://raw.githubusercontent.com/Fanqi-Lin/OneTwoVLA/main/examples/umi/convert_umi_data_to_lerobot.py)에서 제공합니다.  "
      },
      {
        "row": 13,
        "rowsha": "IQOUnWZO0zjNm7R+BRSndPyMbIM5uiQbDxEfgv62dn4=",
        "originContent": "We provide code for converting UMI data format to LeRobot data format [here](https://raw.githubusercontent.com/Fanqi-Lin/OneTwoVLA/main/examples/umi/convert_umi_data_to_lerobot.py).",
        "translatedContent": ""
      },
      {
        "row": 14,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "### 합성 이미지 증강  "
      },
      {
        "row": 15,
        "rowsha": "ricIuYJHWpS9xmRI24vrUfdOrPeN9ub0OQPsPf2kk7M=",
        "originContent": "### Synthetic Image Augmentation",
        "translatedContent": ""
      },
      {
        "row": 16,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "합성 이미지가 실제 로봇 관측과 더 유사해지도록, 랜덤 어그멘테이션을 적용하며, 여기에는 랜덤 어안 렌즈 왜곡과 적응형 밝기 조정이 포함된 로봇 그리퍼 합성이 포함됩니다. 구현은 [scripts/augment_vl_data/augment.py](https://raw.githubusercontent.com/Fanqi-Lin/OneTwoVLA/main/scripts/augment_vl_data/augment.py)에서 확인할 수 있습니다.  "
      },
      {
        "row": 17,
        "rowsha": "/KPp+/1fDGyV/3JdOlBcC+TlK4N5TeJgaXaskZd7wx0=",
        "originContent": "To make the synthetic images more closely resemble real robot observations, we randomly apply several augmentations, including random fisheye distortion and compositing a robot gripper with adaptive brightness adjustments. The implementation is available in [scripts/augment_vl_data/augment.py](https://raw.githubusercontent.com/Fanqi-Lin/OneTwoVLA/main/scripts/augment_vl_data/augment.py).",
        "translatedContent": ""
      },
      {
        "row": 18,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "다음은 예시입니다. 좌측부터 우측으로 원본 이미지, 어안 렌즈 왜곡 적용 이미지, 적응형 밝기 조정이 적용된 로봇 그리퍼 합성 이미지, 두 가지 모두 적용된 이미지입니다.  "
      },
      {
        "row": 19,
        "rowsha": "bORcuW0u+BUFNi1ZU/iRjrbZc+b5nBkLWmv78Tyu8I0=",
        "originContent": "Here we show an example. From left to right, the images are: the original image, the image with fisheye distortion, the image compositing a robot gripper with adaptive brightness adjustments, and the image with both applied.",
        "translatedContent": ""
      },
      {
        "row": 20,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "<img width=\"90%\" src=\"https://raw.githubusercontent.com/Fanqi-Lin/OneTwoVLA/main/figures/fisheye-aug.png\">  "
      },
      {
        "row": 21,
        "rowsha": "XiBVoXjD+k384YpyAb6cStKCTtCZkb6F/C15juwaQ4k=",
        "originContent": "<img width=\"90%\" src=\"https://raw.githubusercontent.com/Fanqi-Lin/OneTwoVLA/main/figures/fisheye-aug.png\">",
        "translatedContent": ""
      },
      {
        "row": 22,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## 🙏 감사의 말씀  "
      },
      {
        "row": 23,
        "rowsha": "xu64CG5WWAk6HrV6oWvSAQFXHyBWkQmb01SHddeshOI=",
        "originContent": "## 🙏 Acknowledgements",
        "translatedContent": "코드를 오픈 소스로 공개해주신 [openpi](https://github.com/Physical-Intelligence/openpi.git) 개발자분들께 진심으로 감사드립니다.  "
      },
      {
        "row": 24,
        "rowsha": "NleJXkkPuTroCIeprHoLQy0+J1Kp5vqTi0BSBF7PoTI=",
        "originContent": "We express our sincere gratitude to the developers of the [openpi](https://github.com/Physical-Intelligence/openpi.git) for open-sourcing their code.",
        "translatedContent": ""
      },
      {
        "row": 25,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  }
]