<div align="center">
<img src=assets/yume.png width="20%"/>
</div>

# Yume: Un Modelo de Generaci√≥n de Mundos Interactivo

Yume es un proyecto a largo plazo que tiene como objetivo crear un mundo interactivo, realista y din√°mico mediante la entrada de texto, im√°genes o videos.


<div align="center">




[![project page](https://img.shields.io/badge/Project-Page-2ea44f)](https://stdstu12.github.io/YUME-Project/)&nbsp;
[![arXiv](https://img.shields.io/badge/arXiv%20paper-2507.17744-b31b1b.svg)](https://arxiv.org/abs/2507.17744)&nbsp;
[![model](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Model-blue)](https://huggingface.co/stdstu123/Yume-I2V-540P)&nbsp;
[![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://www.youtube.com/watch?v=51VII_iJ1EM)&nbsp;

</div>

- Recetas de destilaci√≥n para video DiT.
- C√≥digo de entrenamiento tipo [FramePack-Like](https://github.com/lllyasviel/FramePack).
- M√©todo de generaci√≥n de video largo con soporte para muestreo DDP/FSDP.



## üîß Instalaci√≥n
El c√≥digo ha sido probado en Python 3.10.0, CUDA 12.1 y A100.
```
./env_setup.sh fastvideo
pip install -r requirements.txt
```
Necesita ejecutar `pip install .` despu√©s de cada modificaci√≥n del c√≥digo, o alternativamente, puede copiar los archivos modificados directamente en su entorno virtual. Por ejemplo, si modifiqu√© `wan/image2video.py` y mi entorno virtual es `yume`, puedo copiar el archivo a:  
`envs/yume/lib/python3.10/site-packages/wan/image2video.py`.

## üöÄ Inferencia

### EDO  
Para la generaci√≥n de im√°genes a video, usamos `--jpg_dir="./jpg"` para especificar el directorio de im√°genes de entrada y `--caption_path="./caption.txt"` para proporcionar entradas de condicionamiento de texto, donde cada l√≠nea corresponde a una instancia de generaci√≥n que controla una salida de video de 2 segundos.
```bash
# Download the model weights and place them in Path_To_Yume.
bash scripts/inference/sample_jpg.sh 
```
Tambi√©n consideramos generar videos utilizando los datos de `./val`, donde `--test_data_dir="./val"` especifica la ubicaci√≥n de los datos de ejemplo.
```bash
# Download the model weights and place them in Path_To_Yume.
bash scripts/inference/sample.sh 
```
### SDE
Realizamos muestreo TTS, donde `args.sde` controla si se utiliza el muestreo basado en SDE.
```bash
# Download the model weights and place them in Path_To_Yume.
bash scripts/inference/sample_tts.sh 
```
Para obtener resultados √≥ptimos, recomendamos mantener la distancia real, la tasa de cambio angular (velocidad de giro) y la velocidad de rotaci√≥n de la vista dentro del rango de 0.1 a 10.

Pautas clave de ajuste:
1. Al ejecutar C√°mara permanece quieta (¬∑), reduzca el valor de distancia real
2. Al ejecutar Persona permanece quieta, disminuya tanto la tasa de cambio angular como los valores de velocidad de rotaci√≥n de la vista

Tenga en cuenta que estos par√°metros (distancia real, tasa de cambio angular y velocidad de rotaci√≥n de la vista) s√≠ afectan los resultados de la generaci√≥n. Como enfoque alternativo, puede considerar eliminar estos par√°metros por completo para una operaci√≥n simplificada.

## üéØ Entrenamiento y destilaci√≥n
Para el entrenamiento del modelo, usamos `args.MVDT` para lanzar el marco MVDT, que requiere al menos 16 GPUs A100. Cargar T5 en la CPU puede ayudar a conservar la memoria de la GPU. Empleamos `args.Distil` para habilitar la destilaci√≥n adversarial.



```bash
# Download the model weights and place them in Path_To_Yume.
bash scripts/finetune/finetune.sh
```

## üß± Preparaci√≥n del Conjunto de Datos
Por favor, consulte https://github.com/Lixsp11/sekai-codebase para descargar el conjunto de datos. Para el formato de datos procesados, consulte `./test_video`.
```
path_to_processed_dataset_folder/
‚îú‚îÄ‚îÄ Keys_None_Mouse_Down/ 
‚îÇ   ‚îú‚îÄ‚îÄ video_id.mp4
‚îÇ   ‚îú‚îÄ‚îÄ video_id.txt
‚îú‚îÄ‚îÄ Keys_None_Mouse_Up
‚îÇ‚îÄ‚îÄ  ...
‚îî‚îÄ‚îÄ Keys_S_Mouse_¬∑
```
El contenido proporcionado en el archivo TXT registra ya sea par√°metros de control de movimiento de c√°mara o datos de fotogramas clave de animaci√≥n, con las siguientes definiciones de campos:
```
Start Frame: 2 #Starting frame number (begins at frame 2 at origin video)

End Frame: 50 #Ending frame number

Duration: 49 frames #Total duration

Keys: W #Keyboard input

Mouse: ‚Üì #Mouse action
```
En `scripts/finetune/finetune.sh`, `args.root_dir` representa la `ruta_a_la_carpeta_del_dataset_procesado`, y `args.root_dir` representa la ruta completa al dataset Sekai.


## üìë Plan de Desarrollo
- Procesamiento del dataset
  - [ ] Proveer datasets procesados
- Actualizaci√≥n de c√≥digo
  - [ ] Soporte fp8
  - [ ] Mejores m√©todos de destilaci√≥n
- Actualizaci√≥n del modelo
  - [ ] Modelos cuantizados y destilados
  - [ ] Modelos para generaci√≥n en resoluci√≥n 720p‚Äã

## ü§ù Contribuciones
Damos la bienvenida a todas las contribuciones.


## Agradecimientos
Aprendimos y reutilizamos c√≥digo de los siguientes proyectos:
- [FastVideo](https://github.com/hao-ai-lab/FastVideo)
- [diffusers](https://github.com/huggingface/diffusers)
- [HunyuanVideo-I2V](https://github.com/Tencent-Hunyuan/HunyuanVideo-I2V)
- [Wan2.1](https://github.com/Wan-Video/Wan2.1)
- [Skywork-Reward-V2](https://github.com/SkyworkAI/Skywork-Reward-V2)
- [MDT](https://github.com/sail-sg/MDT)
- [AddSR](https://github.com/NJU-PCALab/AddSR)

## Citaci√≥n
Si utiliza Yume para su investigaci√≥n, por favor cite nuestro art√≠culo:

```bibtex
@article{mao2025yume,
  title={Yume: An Interactive World Generation Model},
  author={Mao, Xiaofeng and Lin, Shaoheng and Li, Zhen and Li, Chuanhao and Peng, Wenshuo and He, Tong and Pang, Jiangmiao and Chi, Mingmin and Qiao, Yu and Zhang, Kaipeng},
  journal={arXiv preprint arXiv:2507.17744},
  year={2025}
}

```


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-10-09

---