<div align="center">
<img src=assets/yume.png width="20%"/>
</div>

# Yume : Un mod√®le de g√©n√©ration de monde interactif

Yume est un projet √† long terme visant √† cr√©er un monde interactif, r√©aliste et dynamique √† partir de texte, d‚Äôimages ou de vid√©os.


<div align="center">




[![page du projet](https://img.shields.io/badge/Project-Page-2ea44f)](https://stdstu12.github.io/YUME-Project/)&nbsp;
[![arXiv](https://img.shields.io/badge/arXiv%20paper-2507.17744-b31b1b.svg)](https://arxiv.org/abs/2507.17744)&nbsp;
[![mod√®le](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Model-blue)](https://huggingface.co/stdstu123/Yume-I2V-540P)&nbsp;
[![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://www.youtube.com/watch?v=51VII_iJ1EM)&nbsp;

</div>

- Une recette de distillation pour video DiT.
- Code d‚Äôentra√Ænement de type [FramePack-Like](https://github.com/lllyasviel/FramePack).
- M√©thode de g√©n√©ration vid√©o longue avec support de l‚Äô√©chantillonnage DDP/FSDP.



## üîß Installation
Le code est test√© sur Python 3.10.0, CUDA 12.1 et A100.
```
./env_setup.sh fastvideo
pip install -r requirements.txt
```
Vous devez ex√©cuter `pip install .` apr√®s chaque modification de code, ou alternativement, vous pouvez copier les fichiers modifi√©s directement dans votre environnement virtuel. Par exemple, si j'ai modifi√© `wan/image2video.py` et que mon environnement virtuel est `yume`, je peux copier le fichier dans :  
`envs/yume/lib/python3.10/site-packages/wan/image2video.py`.  

## üöÄ Inf√©rence  

### ODE  
Pour la g√©n√©ration image-vid√©o, nous utilisons `--jpg_dir="./jpg"` pour sp√©cifier le r√©pertoire des images d'entr√©e et `--caption_path="./caption.txt"` pour fournir les entr√©es de conditionnement textuel, o√π chaque ligne correspond √† une instance de g√©n√©ration contr√¥lant une sortie vid√©o de 2 secondes.
```bash
# Download the model weights and place them in Path_To_Yume.
bash scripts/inference/sample_jpg.sh 
```
Nous envisageons √©galement de g√©n√©rer des vid√©os en utilisant les donn√©es de `./val`, o√π `--test_data_dir="./val"` sp√©cifie l'emplacement des donn√©es d'exemple.
```bash
# Download the model weights and place them in Path_To_Yume.
bash scripts/inference/sample.sh 
```
### SDE
Nous effectuons un √©chantillonnage TTS, o√π `args.sde` contr√¥le l'utilisation de l'√©chantillonnage bas√© sur la SDE.
```bash
# Download the model weights and place them in Path_To_Yume.
bash scripts/inference/sample_tts.sh 
```
Pour des r√©sultats optimaux, nous recommandons de maintenir la distance r√©elle, le taux de changement angulaire (vitesse de rotation) et la vitesse de rotation de la vue dans une plage de 0,1 √† 10.

Lignes directrices cl√©s pour l‚Äôajustement :
1. Lors de l‚Äôex√©cution de la cam√©ra immobile (¬∑), r√©duisez la valeur de la distance r√©elle
2. Lors de l‚Äôex√©cution de la personne immobile, diminuez les valeurs du taux de changement angulaire et de la vitesse de rotation de la vue

Notez que ces param√®tres (distance r√©elle, taux de changement angulaire et vitesse de rotation de la vue) impactent les r√©sultats de g√©n√©ration. Comme approche alternative, vous pouvez envisager de supprimer enti√®rement ces param√®tres pour une op√©ration simplifi√©e.



## üéØ Entra√Ænement & Distillation  
Pour l‚Äôentra√Ænement du mod√®le, nous utilisons `args.MVDT` pour lancer le cadre MVDT, qui n√©cessite au moins 16 GPU A100. Charger T5 sur le CPU peut aider √† √©conomiser la m√©moire GPU. Nous utilisons `args.Distil` pour activer la distillation adversariale.

```bash
# Download the model weights and place them in Path_To_Yume.
bash scripts/finetune/finetune.sh
```

## üß± Pr√©paration du jeu de donn√©es
Veuillez consulter https://github.com/Lixsp11/sekai-codebase pour t√©l√©charger le jeu de donn√©es. Pour le format des donn√©es trait√©es, r√©f√©rez-vous √† `./test_video`.
```
path_to_processed_dataset_folder/
‚îú‚îÄ‚îÄ Keys_None_Mouse_Down/ 
‚îÇ   ‚îú‚îÄ‚îÄ video_id.mp4
‚îÇ   ‚îú‚îÄ‚îÄ video_id.txt
‚îú‚îÄ‚îÄ Keys_None_Mouse_Up
‚îÇ‚îÄ‚îÄ  ...
‚îî‚îÄ‚îÄ Keys_S_Mouse_¬∑
```
Le contenu du fichier TXT fourni enregistre soit les param√®tres de contr√¥le du mouvement de la cam√©ra, soit les donn√©es de keyframe d‚Äôanimation, avec les d√©finitions de champ suivantes :
```
Start Frame: 2 #Starting frame number (begins at frame 2 at origin video)

End Frame: 50 #Ending frame number

Duration: 49 frames #Total duration

Keys: W #Keyboard input

Mouse: ‚Üì #Mouse action
```
Dans `scripts/finetune/finetune.sh`, `args.root_dir` repr√©sente le `path_to_processed_dataset_folder`, et `args.root_dir` repr√©sente le chemin complet vers le dataset Sekai.


## üìë Plan de D√©veloppement
- Traitement du dataset
  - [ ] Fournir des datasets trait√©s
- Mise √† jour du code
  - [ ] Support fp8
  - [ ] Meilleures m√©thodes de distillation
- ‚Äã‚ÄãMise √† jour du mod√®le
  - [ ] Mod√®les quantifi√©s et distill√©s
  - [ ] Mod√®les pour g√©n√©ration en r√©solution 720p‚Äã

## ü§ù Contribution
Nous accueillons toutes les contributions.


## Remerciements
Nous avons appris et r√©utilis√© du code des projets suivants :
- [FastVideo](https://github.com/hao-ai-lab/FastVideo)
- [diffusers](https://github.com/huggingface/diffusers)
- [HunyuanVideo-I2V](https://github.com/Tencent-Hunyuan/HunyuanVideo-I2V)
- [Wan2.1](https://github.com/Wan-Video/Wan2.1)
- [Skywork-Reward-V2](https://github.com/SkyworkAI/Skywork-Reward-V2)
- [MDT](https://github.com/sail-sg/MDT)
- [AddSR](https://github.com/NJU-PCALab/AddSR)

## Citation
Si vous utilisez Yume pour vos recherches, veuillez citer notre article :

```bibtex
@article{mao2025yume,
  title={Yume: An Interactive World Generation Model},
  author={Mao, Xiaofeng and Lin, Shaoheng and Li, Zhen and Li, Chuanhao and Peng, Wenshuo and He, Tong and Pang, Jiangmiao and Chi, Mingmin and Qiao, Yu and Zhang, Kaipeng},
  journal={arXiv preprint arXiv:2507.17744},
  year={2025}
}

```


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-10-09

---