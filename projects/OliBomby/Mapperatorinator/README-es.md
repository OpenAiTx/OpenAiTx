
<div align="right">
  <details>
    <summary >üåê Idioma</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=OliBomby&project=Mapperatorinator&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=OliBomby&project=Mapperatorinator&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="https://openaitx.github.io/view.html?user=OliBomby&project=Mapperatorinator&lang=zh-TW">ÁπÅÈ´î‰∏≠Êñá</a>
        | <a href="https://openaitx.github.io/view.html?user=OliBomby&project=Mapperatorinator&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=OliBomby&project=Mapperatorinator&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="https://openaitx.github.io/view.html?user=OliBomby&project=Mapperatorinator&lang=hi">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä</a>
        | <a href="https://openaitx.github.io/view.html?user=OliBomby&project=Mapperatorinator&lang=th">‡πÑ‡∏ó‡∏¢</a>
        | <a href="https://openaitx.github.io/view.html?user=OliBomby&project=Mapperatorinator&lang=fr">Fran√ßais</a>
        | <a href="https://openaitx.github.io/view.html?user=OliBomby&project=Mapperatorinator&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=OliBomby&project=Mapperatorinator&lang=es">Espa√±ol</a>
        | <a href="https://openaitx.github.io/view.html?user=OliBomby&project=Mapperatorinator&lang=it">Italiano</a>
        | <a href="https://openaitx.github.io/view.html?user=OliBomby&project=Mapperatorinator&lang=ru">–†—É—Å—Å–∫–∏–π</a>
        | <a href="https://openaitx.github.io/view.html?user=OliBomby&project=Mapperatorinator&lang=pt">Portugu√™s</a>
        | <a href="https://openaitx.github.io/view.html?user=OliBomby&project=Mapperatorinator&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=OliBomby&project=Mapperatorinator&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=OliBomby&project=Mapperatorinator&lang=ar">ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</a>
        | <a href="https://openaitx.github.io/view.html?user=OliBomby&project=Mapperatorinator&lang=fa">ŸÅÿßÿ±ÿ≥€å</a>
        | <a href="https://openaitx.github.io/view.html?user=OliBomby&project=Mapperatorinator&lang=tr">T√ºrk√ße</a>
        | <a href="https://openaitx.github.io/view.html?user=OliBomby&project=Mapperatorinator&lang=vi">Ti·∫øng Vi·ªát</a>
        | <a href="https://openaitx.github.io/view.html?user=OliBomby&project=Mapperatorinator&lang=id">Bahasa Indonesia</a>
        | <a href="https://openaitx.github.io/view.html?user=OliBomby&project=Mapperatorinator&lang=as">‡¶Ö‡¶∏‡¶Æ‡ßÄ‡¶Ø‡¶º‡¶æ</
      </div>
    </div>
  </details>
</div>

# Mapperatorinator

Prueba el modelo generativo [aqu√≠](https://colab.research.google.com/github/OliBomby/Mapperatorinator/blob/main/colab/mapperatorinator_inference.ipynb), o MaiMod [aqu√≠](https://colab.research.google.com/github/OliBomby/Mapperatorinator/blob/main/colab/mai_mod_inference.ipynb). Mira una demostraci√≥n en video [aqu√≠](https://youtu.be/FEr7t1L2EoA).

Mapperatorinator es un marco multi-modelo que utiliza entradas de espectrograma para generar mapas de osu! completamente funcionales para todos los modos de juego y [asistir en el modding de mapas](#maimod-the-ai-driven-modding-tool).
El objetivo de este proyecto es generar autom√°ticamente mapas de osu! de calidad rankeable para cualquier canci√≥n, con un alto grado de personalizaci√≥n.

Este proyecto est√° construido sobre [osuT5](https://github.com/gyataro/osuT5) y [osu-diffusion](https://github.com/OliBomby/osu-diffusion). Para desarrollarlo, dediqu√© aproximadamente 2500 horas de c√≥mputo GPU en 142 ejecuciones en mi 4060 Ti y alquilando instancias 4090 en vast.ai.

#### Utiliza esta herramienta de manera responsable. Siempre divulga el uso de IA en tus beatmaps.

## Instalaci√≥n

La siguiente instrucci√≥n te permite generar beatmaps en tu m√°quina local, alternativamente puedes ejecutarlo en la nube con el [colab notebook](https://colab.research.google.com/github/OliBomby/Mapperatorinator/blob/main/colab/mapperatorinator_inference.ipynb).

### 1. Clona el repositorio

```sh
git clone https://github.com/OliBomby/Mapperatorinator.git
cd Mapperatorinator
```

### 2. (Opcional) Crear entorno virtual

Utilice Python 3.10, versiones posteriores podr√≠an no ser compatibles con las dependencias.

```sh
python -m venv .venv

# In cmd.exe
.venv\Scripts\activate.bat
# In PowerShell
.venv\Scripts\Activate.ps1
# In Linux or MacOS
source .venv/bin/activate
```
### 3. Instalar dependencias

- Python 3.10
- [Git](https://git-scm.com/downloads)
- [ffmpeg](http://www.ffmpeg.org/)
- [CUDA](https://developer.nvidia.com/cuda-zone) (Para GPUs NVIDIA) o [ROCm](https://rocmdocs.amd.com/en/latest/Installation_Guide/Installation-Guide.html) (Para GPUs AMD en Linux)
- [PyTorch](https://pytorch.org/get-started/locally/): Aseg√∫rate de seguir la gu√≠a para comenzar para instalar `torch` y `torchaudio` con soporte para GPU. Selecciona la versi√≥n correcta de la Plataforma de C√≥mputo que instalaste en el paso anterior.

- y las dependencias restantes de Python:


```sh
pip install -r requirements.txt
```

## Interfaz Web (Recomendada)

Para una experiencia m√°s amigable, considere usar la Interfaz Web. Proporciona una interfaz gr√°fica para configurar los par√°metros de generaci√≥n, iniciar el proceso y monitorear la salida.

### Iniciar la Interfaz

Navegue al directorio clonado `Mapperatorinator` en su terminal y ejecute:

```sh
python web-ui.py
```
Esto iniciar√° un servidor web local y abrir√° autom√°ticamente la interfaz en una nueva ventana.

### Uso de la GUI

- **Configurar:** Establezca las rutas de entrada/salida usando los campos del formulario y los botones "Examinar". Ajuste los par√°metros de generaci√≥n como modo de juego, dificultad, estilo (a√±o, ID del creador, descriptores), sincronizaci√≥n, caracter√≠sticas espec√≠ficas (hitsounds, sincronizaci√≥n avanzada), y m√°s, reflejando las opciones de l√≠nea de comandos. (Nota: Si proporciona una `beatmap_path`, la interfaz determinar√° autom√°ticamente la `audio_path` y la `output_path` a partir de ella, por lo que puede dejar esos campos en blanco)
- **Iniciar:** Haga clic en el bot√≥n "Iniciar Inferencia" para comenzar la generaci√≥n del beatmap.
- **Cancelar:** Puede detener el proceso en curso usando el bot√≥n "Cancelar Inferencia".
- **Abrir Salida:** Una vez finalizado, use el bot√≥n "Abrir Carpeta de Salida" para acceder r√°pidamente a los archivos generados.

La interfaz web act√∫a como un envoltorio conveniente alrededor del script `inference.py`. Para opciones avanzadas o soluci√≥n de problemas, consulte las instrucciones de l√≠nea de comandos.

![python_u3zyW0S3Vs](https://github.com/user-attachments/assets/5312a45f-d51c-4b37-9389-da3258ddd0a1)

## Inferencia por L√≠nea de Comandos

Para usuarios que prefieren la l√≠nea de comandos o necesitan acceso a configuraciones avanzadas, siga los pasos a continuaci√≥n. **Nota:** Para una interfaz gr√°fica m√°s sencilla, consulte la secci√≥n [Web UI (Recomendado)](#web-ui-recommended) arriba.

Ejecute `inference.py` y pase algunos argumentos para generar beatmaps. Para esto, use la [sintaxis de sobrescritura Hydra](https://hydra.cc/docs/advanced/override_grammar/basic/). Consulte `configs/inference_v29.yaml` para todos los par√°metros disponibles.

```
python inference.py \
  audio_path           [Path to input audio] \
  output_path          [Path to output directory] \
  beatmap_path         [Path to .osu file to autofill metadata, and output_path, or use as reference] \
  
  gamemode             [Game mode to generate 0=std, 1=taiko, 2=ctb, 3=mania] \
  difficulty           [Difficulty star rating to generate] \
  mapper_id            [Mapper user ID for style] \
  year                 [Upload year to simulate] \
  hitsounded           [Whether to add hitsounds] \
  slider_multiplier    [Slider velocity multiplier] \
  circle_size          [Circle size] \
  keycount             [Key count for mania] \
  hold_note_ratio      [Hold note ratio for mania 0-1] \
  scroll_speed_ratio   [Scroll speed ratio for mania and ctb 0-1] \
  descriptors          [List of beatmap user tags for style] \
  negative_descriptors [List of beatmap user tags for classifier-free guidance] \
  
  add_to_beatmap       [Whether to add generated content to the reference beatmap instead of making a new beatmap] \
  start_time           [Generation start time in milliseconds] \
  end_time             [Generation end time in milliseconds] \
  in_context           [List of additional context to provide to the model [NONE,TIMING,KIAI,MAP,GD,NO_HS]] \
  output_type          [List of content types to generate] \
  cfg_scale            [Scale of the classifier-free guidance] \
  super_timing         [Whether to use slow accurate variable BPM timing generator] \
  seed                 [Random seed for generation] \
```
Ejemplo:

```
python inference.py beatmap_path="'C:\Users\USER\AppData\Local\osu!\Songs\1 Kenji Ninuma - DISCO PRINCE\Kenji Ninuma - DISCOPRINCE (peppy) [Normal].osu'" gamemode=0 difficulty=5.5 year=2023 descriptors="['jump aim','clean']" in_context=[TIMING,KIAI]
```

## CLI interactiva
Para quienes prefieren un flujo de trabajo basado en terminal pero desean una configuraci√≥n guiada, el script CLI interactivo es una excelente alternativa a la interfaz web.

### Lanzar la CLI
Navega al directorio clonado. Es posible que necesites hacer el script ejecutable primero.

```sh
# Make the script executable (only needs to be done once)
chmod +x cli_inference.sh
```

```sh
# Run the script
./cli_inference.sh
```

### Uso de la CLI
El script te guiar√° a trav√©s de una serie de indicaciones para configurar todos los par√°metros de generaci√≥n, igual que la interfaz web.

Utiliza una interfaz codificada por colores para mayor claridad.
Ofrece un men√∫ avanzado de selecci√≥n m√∫ltiple para elegir descriptores de estilo usando las teclas de flecha y la barra espaciadora.
Despu√©s de responder todas las preguntas, mostrar√° el comando final para tu revisi√≥n.
Luego puedes confirmar para ejecutarlo directamente o cancelar y copiar el comando para uso manual.

## Consejos de Generaci√≥n

- Puedes editar `configs/inference_v29.yaml` y agregar tus argumentos all√≠ en lugar de escribirlos en la terminal cada vez.
- Todos los descriptores disponibles se pueden encontrar [aqu√≠](https://osu.ppy.sh/wiki/en/Beatmap/Beatmap_tags).
- Siempre proporciona un argumento de a√±o entre 2007 y 2023. Si lo dejas desconocido, el modelo podr√≠a generar con un estilo inconsistente.
- Siempre proporciona un argumento de dificultad. Si lo dejas desconocido, el modelo podr√≠a generar con una dificultad inconsistente.
- Incrementa el par√°metro `cfg_scale` para aumentar la efectividad de los argumentos `mapper_id` y `descriptors`.
- Puedes usar el argumento `negative_descriptors` para guiar al modelo alej√°ndolo de ciertos estilos. Esto solo funciona cuando `cfg_scale > 1`. Aseg√∫rate que el n√∫mero de descriptores negativos sea igual al n√∫mero de descriptores.
- Si el estilo de tu canci√≥n y el estilo deseado del mapa no coinciden bien, el modelo podr√≠a no seguir tus indicaciones. Por ejemplo, es dif√≠cil generar un mapa de SR alto y SV alto para una canci√≥n tranquila.
- Si ya tienes el timing y los tiempos de kiai hechos para una canci√≥n, puedes d√°rselos al modelo para aumentar mucho la velocidad y precisi√≥n de la inferencia: Usa los argumentos `beatmap_path` e `in_context=[TIMING,KIAI]`.
- Para remapear solo una parte de tu mapa, usa los argumentos `beatmap_path`, `start_time`, `end_time` y `add_to_beatmap=true`.
- Para generar una dificultad guest para un mapa, usa los argumentos `beatmap_path` e `in_context=[GD,TIMING,KIAI]`.
- Para generar hitsounds para un mapa, usa los argumentos `beatmap_path` e `in_context=[NO_HS,TIMING,KIAI]`.
- Para generar solo timing para una canci√≥n, usa los argumentos `super_timing=true` y `output_type=[TIMING]`.

## MaiMod: La Herramienta de Modding Impulsada por IA

MaiMod es una herramienta de modding para mapas de osu! que usa predicciones de Mapperatorinator para encontrar posibles fallos e inconsistencias que no pueden ser detectadas por otras herramientas autom√°ticas de modding como [Mapset Verifier](https://github.com/Naxesss/MapsetVerifier).
Puede detectar problemas como:
- Snapping o patrones r√≠tmicos incorrectos
- Puntos de timing inexactos
- Posiciones inconsistentes de objetos o colocaciones nuevas de combo
- Formas extra√±as de sliders
- Hitsounds o vol√∫menes inconsistentes

Puedes probar MaiMod [aqu√≠](https://colab.research.google.com/github/OliBomby/Mapperatorinator/blob/main/colab/mai_mod_inference.ipynb), o ejecutarlo localmente:
Para ejecutar MaiMod localmente, necesitar√°s instalar Mapperatorinator. Luego, ejecuta el script `mai_mod.py`, especificando la ruta de tu mapa con el argumento `beatmap_path`.
```sh
python mai_mod.py beatmap_path="'C:\Users\USER\AppData\Local\osu!\Songs\1 Kenji Ninuma - DISCO PRINCE\Kenji Ninuma - DISCOPRINCE (peppy) [Normal].osu'"
```
Esto imprimir√° las sugerencias de modificaci√≥n en la consola, que luego puedes aplicar manualmente a tu mapa de ritmo.  
Las sugerencias est√°n ordenadas cronol√≥gicamente y agrupadas en categor√≠as.  
El primer valor en el c√≠rculo indica la 'sorpresa', que es una medida de qu√© tan inesperado encontr√≥ el modelo el problema, para que puedas priorizar los problemas m√°s importantes.  

El modelo puede cometer errores, especialmente en problemas de baja sorpresa, as√≠ que siempre verifica dos veces las sugerencias antes de aplicarlas a tu mapa de ritmo.  
El objetivo principal es ayudarte a reducir el espacio de b√∫squeda de posibles problemas, para que no tengas que revisar manualmente cada objeto de golpe en tu mapa de ritmo.  

### MaiMod GUI  
Para ejecutar la interfaz web de MaiMod, necesitar√°s instalar Mapperatorinator.  
Luego, ejecuta el script `mai_mod_ui.py`. Esto iniciar√° un servidor web local y abrir√° autom√°ticamente la interfaz en una nueva ventana:

```sh
python mai_mod_ui.py
```

<img width="850" height="1019" alt="afbeelding" src="https://github.com/user-attachments/assets/67c03a43-a7bd-4265-a5b1-5e4d62aca1fa" />

## Visi√≥n general

### Tokenizaci√≥n

Mapperatorinator convierte los mapas de osu! en una representaci√≥n intermedia de eventos que puede ser convertida directamente hacia y desde tokens.
Incluye objetos de golpe, sonidos de golpe, velocidades de sliders, nuevos combos, puntos de sincronizaci√≥n, tiempos kiai y velocidades de desplazamiento para taiko/mania.

Aqu√≠ hay un peque√±o ejemplo del proceso de tokenizaci√≥n:

![mapperatorinator_parser](https://github.com/user-attachments/assets/84efde76-4c27-48a1-b8ce-beceddd9e695)

Para ahorrar en el tama√±o del vocabulario, los eventos de tiempo se cuantifican en intervalos de 10 ms y las coordenadas de posici√≥n se cuantifican en una cuadr√≠cula de 32 p√≠xeles.

### Arquitectura del modelo
El modelo es b√°sicamente un envoltorio alrededor del modelo [HF Transformers Whisper](https://huggingface.co/docs/transformers/en/model_doc/whisper#transformers.WhisperForConditionalGeneration), con embeddings de entrada personalizados y funci√≥n de p√©rdida.
El tama√±o del modelo es de 219 millones de par√°metros.
Se encontr√≥ que este modelo es m√°s r√°pido y preciso que T5 para esta tarea.

La visi√≥n general de alto nivel de la entrada y salida del modelo es la siguiente:

![Picture2](https://user-images.githubusercontent.com/28675590/201044116-1384ad72-c540-44db-a285-7319dd01caad.svg)

El modelo usa marcos de espectrograma Mel como entrada del codificador, con un marco por posici√≥n de entrada. La salida del decodificador en cada paso es una distribuci√≥n softmax sobre un vocabulario discreto y predefinido de eventos. Las salidas son escasas, los eventos solo se requieren cuando ocurre un objeto de golpe, en lugar de anotar cada marco de audio individual.

### Formato de entrenamiento multitarea

![Multitask training format](https://github.com/user-attachments/assets/62f490bc-a567-4671-a7ce-dbcc5f9cd6d9)

Antes del token SOS hay tokens adicionales que facilitan la generaci√≥n condicional. Estos tokens incluyen el modo de juego, la dificultad, el ID del mapper, el a√±o y otros metadatos.
Durante el entrenamiento, estos tokens no tienen etiquetas acompa√±antes, por lo que nunca son generados por el modelo. 
Tambi√©n durante el entrenamiento existe una probabilidad aleatoria de que un token de metadatos sea reemplazado por un token 'desconocido', de modo que durante la inferencia podemos usar estos tokens 'desconocidos' para reducir la cantidad de metadatos que debemos proporcionar al modelo.

### Generaci√≥n continua sin interrupciones

La longitud de contexto del modelo es de 8.192 segundos. Esto obviamente no es suficiente para generar un mapa completo, por lo que debemos dividir la canci√≥n en m√∫ltiples ventanas y generar el mapa en partes peque√±as.
Para asegurarnos de que el mapa generado no tenga uniones notorias entre ventanas, usamos una superposici√≥n del 90% y generamos las ventanas secuencialmente.
Cada ventana de generaci√≥n, excepto la primera, comienza con el decodificador prellenado hasta un 50% de la ventana de generaci√≥n con tokens de las ventanas anteriores.
Utilizamos un procesador logit para asegurarnos de que el modelo no pueda generar tokens de tiempo que est√©n en el primer 50% de la ventana de generaci√≥n.  
Adem√°s, el √∫ltimo 40% de la ventana de generaci√≥n est√° reservado para la siguiente ventana. Cualquier token de tiempo generado en ese rango se trata como tokens EOS.  
Esto asegura que cada token generado est√© condicionado por al menos 4 segundos de tokens previos y 3.3 segundos de audio futuro para anticipar.  

Para evitar el desplazamiento del offset durante una generaci√≥n larga, se han a√±adido offsets aleatorios a los eventos de tiempo en el decodificador durante el entrenamiento.  
Esto lo obliga a corregir errores de sincronizaci√≥n escuchando los inicios en el audio en su lugar, y resulta en un offset consistentemente preciso.  

### Coordenadas refinadas con difusi√≥n  

Las coordenadas de posici√≥n generadas por el decodificador se cuantizan en una cuadr√≠cula de 32 p√≠xeles, por lo que luego usamos difusi√≥n para eliminar el ruido de las coordenadas hasta las posiciones finales.  
Para esto entrenamos una versi√≥n modificada de [osu-diffusion](https://github.com/OliBomby/osu-diffusion) que est√° especializada solo en el √∫ltimo 10% del programa de ruido, y acepta los tokens de metadatos m√°s avanzados que Mapperatorinator usa para la generaci√≥n condicional.  

Dado que el modelo Mapperatorinator genera el SV de los sliders, la longitud requerida del slider es fija independientemente de la forma del camino del punto de control.  
Por lo tanto, tratamos de guiar el proceso de difusi√≥n para crear coordenadas que se ajusten a las longitudes requeridas del slider.  
Hacemos esto recalculando las posiciones finales del slider despu√©s de cada paso del proceso de difusi√≥n bas√°ndonos en la longitud requerida y el camino actual del punto de control.  
Esto significa que el proceso de difusi√≥n no tiene control directo sobre las posiciones finales del slider, pero a√∫n puede influenciarlas cambiando el camino del punto de control.  

### Post-procesamiento  

Mapperatorinator realiza un post-procesamiento adicional para mejorar la calidad del beatmap generado:  

- Refinar coordenadas de posici√≥n con difusi√≥n.  
- Reajustar eventos de tiempo al tick m√°s cercano usando los divisores de snap generados por el modelo.  
- Ajustar solapamientos posicionales casi perfectos.  
- Convertir eventos de columnas mania a coordenadas X.  
- Generar caminos de sliders para redobles de taiko.  
- Corregir grandes discrepancias en la longitud requerida del slider y la longitud del camino del punto de control.  

### Generador de super sincronizaci√≥n  

El generador de super sincronizaci√≥n es un algoritmo que mejora la precisi√≥n y exactitud de la sincronizaci√≥n generada inferiendo la sincronizaci√≥n para toda la canci√≥n 20 veces y promediando los resultados.  
Esto es √∫til para canciones con BPM variable, o canciones con cambios de BPM. El resultado es casi perfecto con solo a veces una secci√≥n que necesita ajuste manual.  

## Entrenamiento  

La instrucci√≥n a continuaci√≥n crea un entorno de entrenamiento en tu m√°quina local.  

### 1. Clona el repositorio  

```sh
git clone https://github.com/OliBomby/Mapperatorinator.git
cd Mapperatorinator
```

### 2. Crear conjunto de datos

Crea tu propio conjunto de datos usando la [aplicaci√≥n de consola Mapperator](https://github.com/mappingtools/Mapperator/blob/master/README.md#create-a-high-quality-dataset). Requiere un [token de cliente OAuth de osu!](https://osu.ppy.sh/home/account/edit) para verificar beatmaps y obtener metadatos adicionales. Coloca el conjunto de datos en un directorio `datasets` junto al directorio `Mapperatorinator`.

```sh
Mapperator.ConsoleApp.exe dataset2 -t "/Mapperatorinator/datasets/beatmap_descriptors.csv" -i "path/to/osz/files" -o "/datasets/cool_dataset"
```

### 3. (Opcional) Configurar Weight & Biases para el registro
Crea una cuenta en [Weight & Biases](https://wandb.ai/site) y obt√©n tu clave API desde la configuraci√≥n de tu cuenta.
Luego, establece la variable de entorno `WANDB_API_KEY`, para que el proceso de entrenamiento sepa registrar con esta clave.

```sh
export WANDB_API_KEY=<your_api_key>
```
### 4. Crear contenedor docker
El entrenamiento en tu entorno virtual tambi√©n es posible, pero recomendamos usar Docker en WSL para un mejor rendimiento.

```sh
docker compose up -d --force-recreate
docker attach mapperatorinator_space
cd Mapperatorinator
```

### 5. Configurar par√°metros y comenzar el entrenamiento

Todas las configuraciones se encuentran en `./configs/train/default.yaml`. 
Aseg√∫rese de establecer la ruta correcta `train_dataset_path` y `test_dataset_path` a su conjunto de datos, as√≠ como los √≠ndices de inicio y fin del conjunto de mapas para la divisi√≥n train/test.
La ruta es local al contenedor de docker, por lo que si coloc√≥ su conjunto de datos llamado `cool_dataset` en el directorio `datasets`, entonces deber√≠a ser `/workspace/datasets/cool_dataset`.

Recomiendo crear un archivo de configuraci√≥n personalizado que sobrescriba la configuraci√≥n predeterminada, para que tenga un registro de su configuraci√≥n de entrenamiento para reproducibilidad.

```yaml
data:
  train_dataset_path: "/workspace/datasets/cool_dataset"
  test_dataset_path: "/workspace/datasets/cool_dataset"
  train_dataset_start: 0
  train_dataset_end: 90
  test_dataset_start: 90
  test_dataset_end: 100
```

Begin training by calling `python osuT5/train.py` or `torchrun --nproc_per_node=NUM_GPUS osuT5/train.py` for multi-GPU training.


```sh
python osuT5/train.py -cn train_v29 train_dataset_path="/workspace/datasets/cool_dataset" test_dataset_path="/workspace/datasets/cool_dataset" train_dataset_end=90 test_dataset_start=90 test_dataset_end=100
```

### 6. Afinaci√≥n LoRA

Tambi√©n puedes afinar un modelo preentrenado con [LoRA](https://arxiv.org/abs/2106.09685) para adaptarlo a un estilo o modo de juego espec√≠fico.
Para ello, adapta `configs/train/lora.yaml` a tus necesidades y ejecuta la configuraci√≥n de entrenamiento `lora`:

```sh
python osuT5/train.py -cn lora train_dataset_path="/workspace/datasets/cool_dataset" test_dataset_path="/workspace/datasets/cool_dataset" train_dataset_end=90 test_dataset_start=90 test_dataset_end=100
```

Par√°metros importantes de LoRA a considerar:
- `pretrained_path`: Ruta o repositorio HF del modelo base para ajustar.
- `r`: Rango de las matrices LoRA. Valores m√°s altos aumentan la capacidad del modelo pero tambi√©n el uso de memoria.
- `lora_alpha`: Factor de escalado para las actualizaciones LoRA.
- `total_steps`: N√∫mero total de pasos de entrenamiento. Aj√∫stalo seg√∫n el tama√±o de tu conjunto de datos.
- `enable_lora`: Si usar LoRA o ajuste completo del modelo.

Durante la inferencia, puedes especificar los pesos LoRA a usar con el argumento `lora_path`.
Esto puede ser una ruta local o un repositorio de Hugging Face.

## Ver tambi√©n
- [Mapper Classifier](https://raw.githubusercontent.com/OliBomby/Mapperatorinator/main/./classifier/README.md)
- [RComplexion](https://raw.githubusercontent.com/OliBomby/Mapperatorinator/main/./rcomplexion/README.md)

## Cr√©ditos

Agradecimientos especiales a:
1. Los autores de [osuT5](https://github.com/gyataro/osuT5) por su c√≥digo de entrenamiento.
2. El equipo de Hugging Face por sus [herramientas](https://huggingface.co/docs/transformers/index).
3. [Jason Won](https://github.com/jaswon) y [Richard Nagyfi](https://github.com/sedthh) por intercambiar ideas.
4. [Marvin](https://github.com/minetoblend) por donar cr√©ditos de entrenamiento.
5. La comunidad de osu! por los beatmaps.

## Trabajos relacionados

1. [osu! Beatmap Generator](https://github.com/Syps/osu_beatmap_generator) por Syps (Nick Sypteras)
2. [osumapper](https://github.com/kotritrona/osumapper) por kotritrona, jyvden, Yoyolick (Ryan Zmuda)
3. [osu-diffusion](https://github.com/OliBomby/osu-diffusion) por OliBomby (Olivier Schipper), NiceAesth (Andrei Baciu)
4. [osuT5](https://github.com/gyataro/osuT5) por gyataro (Xiwen Teoh)
5. [Beat Learning](https://github.com/sedthh/BeatLearning) por sedthh (Richard Nagyfi)
6. [osu!dreamer](https://github.com/jaswon/osu-dreamer) por jaswon (Jason Won)


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2026-01-24

---