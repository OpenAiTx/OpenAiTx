
<div align="right">
  <details>
    <summary >üåê Sprache</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=OliBomby&project=Mapperatorinator&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=OliBomby&project=Mapperatorinator&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="https://openaitx.github.io/view.html?user=OliBomby&project=Mapperatorinator&lang=zh-TW">ÁπÅÈ´î‰∏≠Êñá</a>
        | <a href="https://openaitx.github.io/view.html?user=OliBomby&project=Mapperatorinator&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=OliBomby&project=Mapperatorinator&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="https://openaitx.github.io/view.html?user=OliBomby&project=Mapperatorinator&lang=hi">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä</a>
        | <a href="https://openaitx.github.io/view.html?user=OliBomby&project=Mapperatorinator&lang=th">‡πÑ‡∏ó‡∏¢</a>
        | <a href="https://openaitx.github.io/view.html?user=OliBomby&project=Mapperatorinator&lang=fr">Fran√ßais</a>
        | <a href="https://openaitx.github.io/view.html?user=OliBomby&project=Mapperatorinator&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=OliBomby&project=Mapperatorinator&lang=es">Espa√±ol</a>
        | <a href="https://openaitx.github.io/view.html?user=OliBomby&project=Mapperatorinator&lang=it">Italiano</a>
        | <a href="https://openaitx.github.io/view.html?user=OliBomby&project=Mapperatorinator&lang=ru">–†—É—Å—Å–∫–∏–π</a>
        | <a href="https://openaitx.github.io/view.html?user=OliBomby&project=Mapperatorinator&lang=pt">Portugu√™s</a>
        | <a href="https://openaitx.github.io/view.html?user=OliBomby&project=Mapperatorinator&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=OliBomby&project=Mapperatorinator&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=OliBomby&project=Mapperatorinator&lang=ar">ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</a>
        | <a href="https://openaitx.github.io/view.html?user=OliBomby&project=Mapperatorinator&lang=fa">ŸÅÿßÿ±ÿ≥€å</a>
        | <a href="https://openaitx.github.io/view.html?user=OliBomby&project=Mapperatorinator&lang=tr">T√ºrk√ße</a>
        | <a href="https://openaitx.github.io/view.html?user=OliBomby&project=Mapperatorinator&lang=vi">Ti·∫øng Vi·ªát</a>
        | <a href="https://openaitx.github.io/view.html?user=OliBomby&project=Mapperatorinator&lang=id">Bahasa Indonesia</a>
        | <a href="https://openaitx.github.io/view.html?user=OliBomby&project=Mapperatorinator&lang=as">‡¶Ö‡¶∏‡¶Æ‡ßÄ‡¶Ø‡¶º‡¶æ</
      </div>
    </div>
  </details>
</div>

# Mapperatorinator

Teste das generative Modell [hier](https://colab.research.google.com/github/OliBomby/Mapperatorinator/blob/main/colab/mapperatorinator_inference.ipynb), oder MaiMod [hier](https://colab.research.google.com/github/OliBomby/Mapperatorinator/blob/main/colab/mai_mod_inference.ipynb). Sieh dir eine Video-Demo [hier](https://youtu.be/FEr7t1L2EoA) an.

Mapperatorinator ist ein Multi-Model-Framework, das Spektrogramme als Eingabe nutzt, um voll ausgestattete osu!-Beatmaps f√ºr alle Spielmodi zu generieren und [beim Modden von Beatmaps zu unterst√ºtzen](#maimod-the-ai-driven-modding-tool).
Das Ziel dieses Projekts ist es, automatisch osu!-Beatmaps in rankbarer Qualit√§t aus jedem beliebigen Song mit einem hohen Ma√ü an Anpassbarkeit zu erstellen.

Dieses Projekt basiert auf [osuT5](https://github.com/gyataro/osuT5) und [osu-diffusion](https://github.com/OliBomby/osu-diffusion). F√ºr die Entwicklung habe ich etwa 2500 Stunden GPU-Rechenzeit in 142 Durchl√§ufen auf meiner 4060 Ti und gemieteten 4090-Instanzen auf vast.ai investiert.

#### Verwenden Sie dieses Tool verantwortungsbewusst. Geben Sie immer die Nutzung von KI in Ihren Beatmaps an.

## Installation

Die folgende Anleitung erm√∂glicht es Ihnen, Beatmaps auf Ihrem lokalen Rechner zu erstellen. Alternativ k√∂nnen Sie das Tool in der Cloud mit dem [Colab-Notebook](https://colab.research.google.com/github/OliBomby/Mapperatorinator/blob/main/colab/mapperatorinator_inference.ipynb) ausf√ºhren.

### 1. Klonen Sie das Repository

```sh
git clone https://github.com/OliBomby/Mapperatorinator.git
cd Mapperatorinator
```

### 2. (Optional) Virtuelle Umgebung erstellen

Verwenden Sie Python 3.10, sp√§tere Versionen sind m√∂glicherweise nicht mit den Abh√§ngigkeiten kompatibel.

```sh
python -m venv .venv

# In cmd.exe
.venv\Scripts\activate.bat
# In PowerShell
.venv\Scripts\Activate.ps1
# In Linux or MacOS
source .venv/bin/activate
```

### 3. Abh√§ngigkeiten installieren

- Python 3.10
- [Git](https://git-scm.com/downloads)
- [ffmpeg](http://www.ffmpeg.org/)
- [CUDA](https://developer.nvidia.com/cuda-zone) (F√ºr NVIDIA-GPUs) oder [ROCm](https://rocmdocs.amd.com/en/latest/Installation_Guide/Installation-Guide.html) (F√ºr AMD-GPUs unter Linux)
- [PyTorch](https://pytorch.org/get-started/locally/): Achten Sie darauf, dem Get Started-Guide zu folgen, damit Sie `torch` und `torchaudio` mit GPU-Unterst√ºtzung installieren. W√§hlen Sie die richtige Compute Platform-Version, die Sie im vorherigen Schritt installiert haben.

- und die restlichen Python-Abh√§ngigkeiten:

```sh
pip install -r requirements.txt
```

## Web-GUI (Empfohlen)

F√ºr eine benutzerfreundlichere Erfahrung empfiehlt sich die Nutzung der Web-Oberfl√§che. Sie bietet eine grafische Oberfl√§che zum Konfigurieren der Generierungsparameter, Starten des Prozesses und √úberwachen der Ausgabe.

### Starten der GUI

Navigieren Sie im Terminal zum geklonten Verzeichnis `Mapperatorinator` und f√ºhren Sie aus:

```sh
python web-ui.py
```
Dadurch wird ein lokaler Webserver gestartet und die Benutzeroberfl√§che automatisch in einem neuen Fenster ge√∂ffnet.

### Verwendung der GUI

- **Konfigurieren:** Legen Sie Eingabe-/Ausgabepfade √ºber die Formularfelder und ‚ÄûDurchsuchen‚Äú-Schaltfl√§chen fest. Passen Sie Generierungsparameter wie Spielmodus, Schwierigkeitsgrad, Stil (Jahr, Mapper-ID, Deskriptoren), Timing, spezifische Features (Hitsounds, Super Timing) und mehr an, analog zu den Kommandozeilenoptionen. (Hinweis: Wenn Sie einen `beatmap_path` angeben, ermittelt die UI automatisch den `audio_path` und `output_path`, sodass Sie diese Felder leer lassen k√∂nnen)
- **Starten:** Klicken Sie auf die Schaltfl√§che ‚ÄûInference starten‚Äú, um die Beatmap-Generierung zu beginnen.
- **Abbrechen:** Sie k√∂nnen den laufenden Prozess mit der Schaltfl√§che ‚ÄûInference abbrechen‚Äú stoppen.
- **Ausgabe √∂ffnen:** Nach Abschluss k√∂nnen Sie mit der Schaltfl√§che ‚ÄûAusgabeordner √∂ffnen‚Äú schnell auf die generierten Dateien zugreifen.

Die Web-UI dient als komfortable H√ºlle f√ºr das Skript `inference.py`. F√ºr erweiterte Optionen oder zur Fehlerbehebung beachten Sie bitte die Kommandozeilenanweisungen.

![python_u3zyW0S3Vs](https://github.com/user-attachments/assets/5312a45f-d51c-4b37-9389-da3258ddd0a1)

## Kommandozeilen-Inferenz

F√ºr Nutzer, die die Kommandozeile bevorzugen oder Zugriff auf erweiterte Konfigurationen ben√∂tigen, folgen Sie den untenstehenden Schritten. **Hinweis:** F√ºr eine einfachere grafische Oberfl√§che siehe bitte den Abschnitt [Web-UI (Empfohlen)](#web-ui-recommended) oben.

F√ºhren Sie `inference.py` aus und √ºbergeben Sie einige Argumente, um Beatmaps zu generieren. Verwenden Sie dazu die [Hydra-Override-Syntax](https://hydra.cc/docs/advanced/override_grammar/basic/). Alle verf√ºgbaren Parameter finden Sie in `configs/inference_v29.yaml`.

```
python inference.py \
  audio_path           [Path to input audio] \
  output_path          [Path to output directory] \
  beatmap_path         [Path to .osu file to autofill metadata, and output_path, or use as reference] \
  
  gamemode             [Game mode to generate 0=std, 1=taiko, 2=ctb, 3=mania] \
  difficulty           [Difficulty star rating to generate] \
  mapper_id            [Mapper user ID for style] \
  year                 [Upload year to simulate] \
  hitsounded           [Whether to add hitsounds] \
  slider_multiplier    [Slider velocity multiplier] \
  circle_size          [Circle size] \
  keycount             [Key count for mania] \
  hold_note_ratio      [Hold note ratio for mania 0-1] \
  scroll_speed_ratio   [Scroll speed ratio for mania and ctb 0-1] \
  descriptors          [List of beatmap user tags for style] \
  negative_descriptors [List of beatmap user tags for classifier-free guidance] \
  
  add_to_beatmap       [Whether to add generated content to the reference beatmap instead of making a new beatmap] \
  start_time           [Generation start time in milliseconds] \
  end_time             [Generation end time in milliseconds] \
  in_context           [List of additional context to provide to the model [NONE,TIMING,KIAI,MAP,GD,NO_HS]] \
  output_type          [List of content types to generate] \
  cfg_scale            [Scale of the classifier-free guidance] \
  super_timing         [Whether to use slow accurate variable BPM timing generator] \
  seed                 [Random seed for generation] \
```

Beispiel:
```
python inference.py beatmap_path="'C:\Users\USER\AppData\Local\osu!\Songs\1 Kenji Ninuma - DISCO PRINCE\Kenji Ninuma - DISCOPRINCE (peppy) [Normal].osu'" gamemode=0 difficulty=5.5 year=2023 descriptors="['jump aim','clean']" in_context=[TIMING,KIAI]
```

## Interaktive CLI
F√ºr diejenigen, die einen terminalbasierten Workflow bevorzugen, aber eine gef√ºhrte Einrichtung w√ºnschen, ist das interaktive CLI-Skript eine ausgezeichnete Alternative zur Web-Oberfl√§che.

### Starten der CLI
Navigieren Sie zum geklonten Verzeichnis. M√∂glicherweise m√ºssen Sie das Skript zuerst ausf√ºhrbar machen.

```sh
# Make the script executable (only needs to be done once)
chmod +x cli_inference.sh
```

```sh
# Run the script
./cli_inference.sh
```

### Verwendung der CLI
Das Skript f√ºhrt Sie durch eine Reihe von Eingabeaufforderungen, um alle Generierungsparameter zu konfigurieren, genau wie die Web-Oberfl√§che.

Es verwendet eine farbcodierte Oberfl√§che f√ºr bessere √úbersichtlichkeit.
Es stellt ein erweitertes Multi-Select-Men√º bereit, um Stil-Deskriptoren mit den Pfeiltasten und der Leertaste auszuw√§hlen.
Nachdem Sie alle Fragen beantwortet haben, zeigt es den endg√ºltigen Befehl zur √úberpr√ºfung an.
Sie k√∂nnen dann best√§tigen, um ihn direkt auszuf√ºhren, oder abbrechen und den Befehl f√ºr die manuelle Verwendung kopieren.

## Tipps zur Generierung

- Sie k√∂nnen `configs/inference_v29.yaml` bearbeiten und Ihre Argumente dort eintragen, anstatt sie jedes Mal im Terminal einzugeben.
- Alle verf√ºgbaren Deskriptoren finden Sie [hier](https://osu.ppy.sh/wiki/en/Beatmap/Beatmap_tags).
- Geben Sie immer ein Jahr zwischen 2007 und 2023 an. Wenn Sie dies offen lassen, kann das Modell mit uneinheitlichem Stil generieren.
- Geben Sie immer einen Schwierigkeitsgrad an. Wenn Sie dies offen lassen, kann das Modell mit uneinheitlicher Schwierigkeit generieren.
- Erh√∂hen Sie den Parameter `cfg_scale`, um die Wirksamkeit der Argumente `mapper_id` und `descriptors` zu steigern.
- Sie k√∂nnen das Argument `negative_descriptors` verwenden, um das Modell von bestimmten Stilen abzulenken. Dies funktioniert nur, wenn `cfg_scale > 1`. Achten Sie darauf, dass die Anzahl der negativen Deskriptoren der der Deskriptoren entspricht.
- Wenn sich der Songstil und der gew√ºnschte Beatmap-Stil nicht gut erg√§nzen, h√§lt sich das Modell m√∂glicherweise nicht an Ihre Vorgaben. Zum Beispiel ist es schwer, f√ºr ein ruhiges Lied eine Beatmap mit hohem SR und hohem SV zu generieren.
- Wenn Sie bereits Timing- und Kiai-Zeiten f√ºr ein Lied haben, k√∂nnen Sie diese dem Modell geben, um die Inferenzgeschwindigkeit und Genauigkeit stark zu erh√∂hen: Verwenden Sie die Argumente `beatmap_path` und `in_context=[TIMING,KIAI]`.
- Um nur einen Teil Ihrer Beatmap neu zu gestalten, verwenden Sie die Argumente `beatmap_path`, `start_time`, `end_time` und `add_to_beatmap=true`.
- Um eine Gast-Schwierigkeit f√ºr eine Beatmap zu generieren, verwenden Sie die Argumente `beatmap_path` und `in_context=[GD,TIMING,KIAI]`.
- Um Hitsounds f√ºr eine Beatmap zu generieren, verwenden Sie die Argumente `beatmap_path` und `in_context=[NO_HS,TIMING,KIAI]`.
- Um nur das Timing f√ºr ein Lied zu generieren, verwenden Sie die Argumente `super_timing=true` und `output_type=[TIMING]`.

## MaiMod: Das KI-gest√ºtzte Modding-Tool

MaiMod ist ein Modding-Tool f√ºr osu!-Beatmaps, das Mapperatorinator-Vorhersagen verwendet, um potenzielle Fehler und Inkonsistenzen zu finden, die von anderen automatischen Modding-Tools wie [Mapset Verifier](https://github.com/Naxesss/MapsetVerifier) nicht erkannt werden k√∂nnen.
Es kann Probleme wie folgende erkennen:
- Falsches Snapping oder rhythmische Muster
- Ungenaue Timing-Punkte
- Inkonsistente Hit-Objekt-Positionen oder neue Combo-Platzierungen
- Seltsame Slider-Formen
- Inkonsistente Hitsounds oder Lautst√§rken

Sie k√∂nnen MaiMod [hier](https://colab.research.google.com/github/OliBomby/Mapperatorinator/blob/main/colab/mai_mod_inference.ipynb) ausprobieren oder lokal ausf√ºhren:
Um MaiMod lokal auszuf√ºhren, m√ºssen Sie Mapperatorinator installieren. F√ºhren Sie dann das Skript `mai_mod.py` aus und geben Sie den Pfad zu Ihrer Beatmap mit dem Argument `beatmap_path` an.
```sh
python mai_mod.py beatmap_path="'C:\Users\USER\AppData\Local\osu!\Songs\1 Kenji Ninuma - DISCO PRINCE\Kenji Ninuma - DISCOPRINCE (peppy) [Normal].osu'"
```
Dadurch werden die Modding-Vorschl√§ge in der Konsole ausgegeben, die du dann manuell auf deine Beatmap anwenden kannst.
Die Vorschl√§ge sind chronologisch geordnet und in Kategorien gruppiert.
Der erste Wert im Kreis zeigt die ‚Äû√úberraschung‚Äú an, ein Ma√ü daf√ºr, wie unerwartet das Modell das Problem fand, sodass du die wichtigsten Probleme priorisieren kannst.

Das Modell kann Fehler machen, besonders bei Problemen mit niedriger √úberraschung, daher solltest du die Vorschl√§ge immer √ºberpr√ºfen, bevor du sie auf deine Beatmap anwendest.
Das Hauptziel ist, dir zu helfen, den Suchraum f√ºr potenzielle Probleme einzugrenzen, sodass du nicht jedes einzelne Hit-Objekt deiner Beatmap manuell pr√ºfen musst.

### MaiMod GUI
Um die MaiMod Web-Oberfl√§che auszuf√ºhren, musst du Mapperatorinator installieren.
Danach starte das Skript `mai_mod_ui.py`. Dadurch wird ein lokaler Webserver gestartet und die Oberfl√§che automatisch in einem neuen Fenster ge√∂ffnet:

```sh
python mai_mod_ui.py
```

<img width="850" height="1019" alt="afbeelding" src="https://github.com/user-attachments/assets/67c03a43-a7bd-4265-a5b1-5e4d62aca1fa" />

## √úbersicht

### Tokenisierung

Mapperatorinator konvertiert osu!-Beatmaps in eine Zwischenrepr√§sentation von Ereignissen, die direkt in Tokens umgewandelt werden kann und umgekehrt.
Sie umfasst Hit-Objekte, Hitsounds, Slider-Geschwindigkeiten, neue Combos, Timing-Punkte, Kiai-Zeiten sowie Taiko/Mania-Scroll-Geschwindigkeiten.

Hier ist ein kleines Beispiel des Tokenisierungsprozesses:

![mapperatorinator_parser](https://github.com/user-attachments/assets/84efde76-4c27-48a1-b8ce-beceddd9e695)

Um den Vokabularumfang zu reduzieren, werden Zeitereignisse auf 10ms-Intervalle quantisiert und Positionskoordinaten auf ein 32-Pixel-Raster.

### Modellarchitektur
Das Modell ist im Wesentlichen ein Wrapper um das [HF Transformers Whisper](https://huggingface.co/docs/transformers/en/model_doc/whisper#transformers.WhisperForConditionalGeneration)-Modell mit eigenen Eingabe-Embeddings und Loss-Funktion.
Die Modellgr√∂√üe betr√§gt 219 Mio. Parameter.
F√ºr diese Aufgabe erwies sich dieses Modell als schneller und genauer als T5.

Die √úbersicht des Modells auf hoher Ebene sieht wie folgt aus:

![Picture2](https://user-images.githubusercontent.com/28675590/201044116-1384ad72-c540-44db-a285-7319dd01caad.svg)

Das Modell verwendet Mel-Spektrogramm-Frames als Encoder-Eingabe, mit einem Frame pro Eingabeposition. Die Decoder-Ausgabe des Modells ist bei jedem Schritt eine Softmax-Verteilung √ºber einen diskreten, vordefinierten Ereignis-Wortschatz. Die Ausgaben sind sp√§rlich, Ereignisse werden nur dann ben√∂tigt, wenn ein Hit-Objekt auftritt, anstatt jeden einzelnen Audioframe zu annotieren.

### Multitasking-Trainingsformat

![Multitask training format](https://github.com/user-attachments/assets/62f490bc-a567-4671-a7ce-dbcc5f9cd6d9)

Vor dem SOS-Token befinden sich zus√§tzliche Tokens, die die bedingte Generierung erleichtern. Diese Tokens umfassen den Spielmodus, Schwierigkeitsgrad, Mapper-ID, Jahr und weitere Metadaten.
W√§hrend des Trainings haben diese Tokens keine zugeh√∂rigen Labels und werden daher nie vom Modell ausgegeben.
Au√üerdem besteht w√§hrend des Trainings eine zuf√§llige Chance, dass ein Metadaten-Token durch ein ‚ÄûUnbekannt‚Äú-Token ersetzt wird, sodass wir diese ‚ÄûUnbekannt‚Äú-Tokens zur Inferenz nutzen k√∂nnen, um den Umfang der an das Modell zu √ºbergebenden Metadaten zu reduzieren.

### Nahtlose lange Generierung

Die Kontextl√§nge des Modells betr√§gt 8,192 Sekunden. Das reicht offensichtlich nicht aus, um eine vollst√§ndige Beatmap zu erzeugen, daher m√ºssen wir das Lied in mehrere Fenster aufteilen und die Beatmap in kleinen Teilen generieren.
Um sicherzustellen, dass die generierte Beatmap keine auff√§lligen N√§hte zwischen den Fenstern aufweist, verwenden wir eine 90%ige √úberlappung und generieren die Fenster nacheinander.
Jedes Generierungsfenster au√üer dem ersten beginnt mit einem Decoder, der bis zu 50% des Fensters mit Tokens aus den vorherigen Fenstern vorbef√ºllt ist.
Wir verwenden einen Logit-Prozessor, um sicherzustellen, dass das Modell keine Zeit-Token generieren kann, die im ersten 50 % des Generierungsfensters liegen.
Zus√§tzlich sind die letzten 40 % des Generierungsfensters f√ºr das n√§chste Fenster reserviert. Alle generierten Zeit-Token in diesem Bereich werden als EOS-Token behandelt.
Dies stellt sicher, dass jedes generierte Token auf mindestens 4 Sekunden vorheriger Token und 3,3 Sekunden zuk√ºnftiger Audiodaten zum Antizipieren basiert.

Um ein Offset-Driften w√§hrend langer Generierung zu verhindern, wurden w√§hrend des Trainings zuf√§llige Offsets zu den Zeitereignissen im Decoder hinzugef√ºgt.
Dadurch wird das Modell gezwungen, Timing-Fehler zu korrigieren, indem es stattdessen auf die Onsets im Audio h√∂rt, was zu einem durchgehend genauen Offset f√ºhrt.

### Verfeinerte Koordinaten mit Diffusion

Vom Decoder generierte Positionskoordinaten werden auf 32-Pixel-Gitterpunkte quantisiert, daher verwenden wir anschlie√üend Diffusion, um die Koordinaten auf die endg√ºltigen Positionen zu entrauschen.
Daf√ºr haben wir eine modifizierte Version von [osu-diffusion](https://github.com/OliBomby/osu-diffusion) trainiert, die speziell auf die letzten 10 % des Rauschplans spezialisiert ist und die fortschrittlicheren Metadaten-Token akzeptiert, die Mapperatorinator f√ºr die bedingte Generierung verwendet.

Da das Mapperatorinator-Modell die SV von Slidern ausgibt, ist die erforderliche L√§nge des Sliders unabh√§ngig von der Form des Kontrollpunktpfads festgelegt.
Daher versuchen wir, den Diffusionsprozess so zu lenken, dass Koordinaten entstehen, die zu den erforderlichen Slider-L√§ngen passen.
Dies erreichen wir, indem wir nach jedem Schritt des Diffusionsprozesses die Slider-Endpositionen basierend auf der erforderlichen L√§nge und dem aktuellen Kontrollpunktpfad neu berechnen.
Das bedeutet, dass der Diffusionsprozess keinen direkten Einfluss auf die Slider-Endpositionen hat, diese aber durch √Ñnderung des Kontrollpunktpfads beeinflussen kann.

### Nachbearbeitung

Mapperatorinator f√ºhrt zus√§tzliche Nachbearbeitung durch, um die Qualit√§t der generierten Beatmap zu verbessern:

- Verfeinerung der Positionskoordinaten mit Diffusion.
- Zeitereignisse an das n√§chste Tick mithilfe der vom Modell generierten Snap-Divisoren snappen.
- Fast perfekte Positions√ºberlagerungen snappen.
- Mania-Spaltenereignisse in X-Koordinaten umwandeln.
- Sliderpfade f√ºr Taiko-Drumrolls generieren.
- Gro√üe Diskrepanzen zwischen erforderlicher Sliderl√§nge und L√§nge des Kontrollpunktpfads beheben.

### Super Timing Generator

Der Super Timing Generator ist ein Algorithmus, der die Pr√§zision und Genauigkeit der generierten Zeitwerte verbessert, indem er die Zeitwerte f√ºr das gesamte Lied 20-mal ermittelt und die Ergebnisse mittelt.
Dies ist n√ºtzlich f√ºr Lieder mit variablem BPM oder BPM-Wechseln. Das Ergebnis ist fast perfekt, nur gelegentlich muss ein Abschnitt manuell angepasst werden.

## Training

Die folgende Anleitung erstellt eine Trainingsumgebung auf Ihrem lokalen Rechner.

### 1. Klonen Sie das Repository

```sh
git clone https://github.com/OliBomby/Mapperatorinator.git
cd Mapperatorinator
```

### 2. Datensatz erstellen

Erstellen Sie Ihren eigenen Datensatz mit der [Mapperator-Konsolenanwendung](https://github.com/mappingtools/Mapperator/blob/master/README.md#create-a-high-quality-dataset). Daf√ºr wird ein [osu! OAuth-Client-Token](https://osu.ppy.sh/home/account/edit) ben√∂tigt, um Beatmaps zu verifizieren und zus√§tzliche Metadaten abzurufen. Legen Sie den Datensatz in einem `datasets`-Verzeichnis neben dem `Mapperatorinator`-Verzeichnis ab.

```sh
Mapperator.ConsoleApp.exe dataset2 -t "/Mapperatorinator/datasets/beatmap_descriptors.csv" -i "path/to/osz/files" -o "/datasets/cool_dataset"
```

### 3. (Optional) Einrichtung von Weight & Biases zum Logging
Erstellen Sie ein Konto bei [Weight & Biases](https://wandb.ai/site) und holen Sie sich Ihren API-Schl√ºssel aus den Kontoeinstellungen.
Setzen Sie dann die Umgebungsvariable `WANDB_API_KEY`, damit der Trainingsprozess wei√ü, dass zu diesem Schl√ºssel geloggt werden soll.

```sh
export WANDB_API_KEY=<your_api_key>
```

### 4. Docker-Container erstellen
Das Training in Ihrer venv ist ebenfalls m√∂glich, aber wir empfehlen die Verwendung von Docker unter WSL f√ºr eine bessere Leistung.
```sh
docker compose up -d --force-recreate
docker attach mapperatorinator_space
cd Mapperatorinator
```

### 5. Parameter konfigurieren und Training starten

Alle Konfigurationen befinden sich in `./configs/train/default.yaml`.
Stellen Sie sicher, dass Sie den korrekten `train_dataset_path` und `test_dataset_path` zu Ihrem Datensatz sowie die Start- und End-Mapset-Indizes f√ºr die Train/Test-Aufteilung festlegen.
Der Pfad ist lokal zum Docker-Container, also wenn Sie Ihren Datensatz namens `cool_dataset` in das Verzeichnis `datasets` gelegt haben, sollte er `/workspace/datasets/cool_dataset` lauten.

Ich empfehle, eine eigene Konfigurationsdatei zu erstellen, die die Standardkonfiguration √ºberschreibt, damit Sie eine Aufzeichnung Ihrer Trainingskonfiguration f√ºr die Reproduzierbarkeit haben.

```yaml
data:
  train_dataset_path: "/workspace/datasets/cool_dataset"
  test_dataset_path: "/workspace/datasets/cool_dataset"
  train_dataset_start: 0
  train_dataset_end: 90
  test_dataset_start: 90
  test_dataset_end: 100
```

Begin training by calling `python osuT5/train.py` or `torchrun --nproc_per_node=NUM_GPUS osuT5/train.py` for multi-GPU training.


```sh
python osuT5/train.py -cn train_v29 train_dataset_path="/workspace/datasets/cool_dataset" test_dataset_path="/workspace/datasets/cool_dataset" train_dataset_end=90 test_dataset_start=90 test_dataset_end=100
```

### 6. LoRA-Feinabstimmung

Sie k√∂nnen auch ein vortrainiertes Modell mit [LoRA](https://arxiv.org/abs/2106.09685) feinabstimmen, um es an einen bestimmten Stil oder Spielmodus anzupassen.
Passen Sie dazu `configs/train/lora.yaml` an Ihre Anforderungen an und f√ºhren Sie die `lora`-Trainingskonfiguration aus:

```sh
python osuT5/train.py -cn lora train_dataset_path="/workspace/datasets/cool_dataset" test_dataset_path="/workspace/datasets/cool_dataset" train_dataset_end=90 test_dataset_start=90 test_dataset_end=100
```

Wichtige LoRA-Parameter, die zu ber√ºcksichtigen sind:
- `pretrained_path`: Pfad oder HF-Repository des Basismodells zum Fine-Tuning.
- `r`: Rang der LoRA-Matrizen. H√∂here Werte erh√∂hen die Modellkapazit√§t, aber auch den Speicherverbrauch.
- `lora_alpha`: Skalierungsfaktor f√ºr die LoRA-Updates.
- `total_steps`: Gesamtanzahl der Trainingsschritte. Stimmen Sie dies auf die Gr√∂√üe Ihres Datensatzes ab.
- `enable_lora`: Ob LoRA oder vollst√§ndiges Modell-Fine-Tuning verwendet wird.

W√§hrend der Inferenz k√∂nnen Sie die zu verwendenden LoRA-Gewichte mit dem Argument `lora_path` angeben.
Dies kann ein lokaler Pfad oder ein Hugging Face-Repository sein.

## Siehe auch
- [Mapper Classifier](https://raw.githubusercontent.com/OliBomby/Mapperatorinator/main/./classifier/README.md)
- [RComplexion](https://raw.githubusercontent.com/OliBomby/Mapperatorinator/main/./rcomplexion/README.md)

## Danksagungen

Besonderer Dank an:
1. Die Autoren von [osuT5](https://github.com/gyataro/osuT5) f√ºr ihren Trainingscode.
2. Das Hugging Face-Team f√ºr ihre [Tools](https://huggingface.co/docs/transformers/index).
3. [Jason Won](https://github.com/jaswon) und [Richard Nagyfi](https://github.com/sedthh) f√ºr das Austauschen von Ideen.
4. [Marvin](https://github.com/minetoblend) f√ºr das Spenden von Trainings-Credits.
5. Die osu!-Community f√ºr die Beatmaps.

## Verwandte Arbeiten

1. [osu! Beatmap Generator](https://github.com/Syps/osu_beatmap_generator) von Syps (Nick Sypteras)
2. [osumapper](https://github.com/kotritrona/osumapper) von kotritrona, jyvden, Yoyolick (Ryan Zmuda)
3. [osu-diffusion](https://github.com/OliBomby/osu-diffusion) von OliBomby (Olivier Schipper), NiceAesth (Andrei Baciu)
4. [osuT5](https://github.com/gyataro/osuT5) von gyataro (Xiwen Teoh)
5. [Beat Learning](https://github.com/sedthh/BeatLearning) von sedthh (Richard Nagyfi)
6. [osu!dreamer](https://github.com/jaswon/osu-dreamer) von jaswon (Jason Won)


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2026-01-24

---