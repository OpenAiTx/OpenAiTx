[
  {
    "Id": 1,
    "Content": "# AudioStory: Generating Long-Form Narrative Audio with Large Language Models\n\n**[Yuxin Guo<sup>1,2</sup>](https://scholar.google.com/citations?user=x_0spxgAAAAJ&hl=en), \n[Teng Wang<sup>2,&#9993;</sup>](http://ttengwang.com/), \n[Yuying Ge<sup>2</sup>](https://geyuying.github.io/), \n[Shijie Ma<sup>1,2</sup>](https://mashijie1028.github.io/), \n[Yixiao Ge<sup>2</sup>](https://geyixiao.com/), \n[Wei Zou<sup>1</sup>](https://people.ucas.ac.cn/~zouwei),\n[Ying Shan<sup>2</sup>](https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en)**\n<br>\n<sup>1</sup>Institute of Automation, CAS\n<sup>2</sup>ARC Lab, Tencent PCG\n<br>\n\n\n\n## 📖 Release\n\n[2025/8/28] 🔥🔥 We release the inference code!\n\n[2025/8/28] 🔥🔥 We release our demo videos!\n\n\n\n## 🔎 Introduction\n\n![audiostory](https://raw.githubusercontent.com/TencentARC/AudioStory/main/audiostory.png)\n\n✨ **TL; DR: We propose a model for long-form narrative audio generation built upon a unified understanding–generation framework, capable of handling video dubbing, audio continuation, and long-form narrative audio synthesis.**\n\nRecent advances in text-to-audio (TTA) generation excel at synthesizing short audio clips but struggle with long-form narrative audio, which requires temporal coherence and compositional reasoning. To address this gap, we propose AudioStory, a unified framework that integrates large language models (LLMs) with TTA systems to generate structured, long-form audio narratives. AudioStory possesses strong instruction-following reasoning generation capabilities. It employs LLMs to decompose complex narrative queries into temporally ordered sub-tasks with contextual cues, enabling coherent scene transitions and emotional tone consistency. AudioStory has two appealing features: \n\n1) Decoupled bridging mechanism: AudioStory disentangles LLM-diffuser collaboration into two specialized components—a bridging query for intra-event semantic alignment and a consistency query for cross-event coherence preservation.\n2) End-to-end training: By unifying instruction comprehension and audio generation within a single end-to-end framework, AudioStory eliminates the need for modular training pipelines while enhancing synergy between components. \n    Furthermore, we establish a benchmark AudioStory-10K, encompassing diverse domains such as animated soundscapes and natural sound narratives.\n\nExtensive experiments show the superiority of AudioStory on both single-audio generation and narrative audio generation, surpassing prior TTA baselines in both instruction-following ability and audio fidelity.\n\n\n",
    "ContentSha": "IRBwHNPnjtSW48yqd+7r6sEmf9DaFT127infPDaIdwc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "# AudioStory: Generating Long-Form Narrative Audio with Large Language Models\n\n**[Yuxin Guo<sup>1,2</sup>](https://scholar.google.com/citations?user=x_0spxgAAAAJ&hl=en), \n[Teng Wang<sup>2,&#9993;</sup>](http://ttengwang.com/), \n[Yuying Ge<sup>2</sup>](https://geyuying.github.io/), \n[Shijie Ma<sup>1,2</sup>](https://mashijie1028.github.io/), \n[Yixiao Ge<sup>2</sup>](https://geyixiao.com/), \n[Wei Zou<sup>1</sup>](https://people.ucas.ac.cn/~zouwei),\n[Ying Shan<sup>2</sup>](https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en)**\n<br>\n<sup>1</sup>Institute of Automation, CAS\n<sup>2</sup>ARC Lab, Tencent PCG\n<br>\n\n\n\n## 📖 Release\n\n[2025/8/28] 🔥🔥 We release the inference code!\n\n[2025/8/28] 🔥🔥 We release our demo videos!\n\n\n\n## 🔎 Introduction\n\n![audiostory](https://raw.githubusercontent.com/TencentARC/AudioStory/main/audiostory.png)\n\n✨ **TL; DR: We propose a model for long-form narrative audio generation built upon a unified understanding–generation framework, capable of handling video dubbing, audio continuation, and long-form narrative audio synthesis.**\n\nRecent advances in text-to-audio (TTA) generation excel at synthesizing short audio clips but struggle with long-form narrative audio, which requires temporal coherence and compositional reasoning. To address this gap, we propose AudioStory, a unified framework that integrates large language models (LLMs) with TTA systems to generate structured, long-form audio narratives. AudioStory possesses strong instruction-following reasoning generation capabilities. It employs LLMs to decompose complex narrative queries into temporally ordered sub-tasks with contextual cues, enabling coherent scene transitions and emotional tone consistency. AudioStory has two appealing features: \n\n1) Decoupled bridging mechanism: AudioStory disentangles LLM-diffuser collaboration into two specialized components—a bridging query for intra-event semantic alignment and a consistency query for cross-event coherence preservation.\n2) End-to-end training: By unifying instruction comprehension and audio generation within a single end-to-end framework, AudioStory eliminates the need for modular training pipelines while enhancing synergy between components. \n    Furthermore, we establish a benchmark AudioStory-10K, encompassing diverse domains such as animated soundscapes and natural sound narratives.\n\nExtensive experiments show the superiority of AudioStory on both single-audio generation and narrative audio generation, surpassing prior TTA baselines in both instruction-following ability and audio fidelity.\n\n\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "FSlWJLuo6Mj/8UOqAMnfG0FeLewjwHFxahfOi9F7aXs=",
        "originContent": "# AudioStory: Generating Long-Form Narrative Audio with Large Language Models",
        "translatedContent": "# AudioStory: Generating Long-Form Narrative Audio with Large Language Models"
      },
      {
        "row": 2,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 3,
        "rowsha": "7pj5MxCr6a6JgMDyv96FwXC+eTQMNiHkELvRBSfLdCc=",
        "originContent": "**[Yuxin Guo<sup>1,2</sup>](https://scholar.google.com/citations?user=x_0spxgAAAAJ&hl=en), ",
        "translatedContent": "**[Yuxin Guo<sup>1,2</sup>](https://scholar.google.com/citations?user=x_0spxgAAAAJ&hl=en), "
      },
      {
        "row": 4,
        "rowsha": "I8/1UUE5gpzu9Xb3Q5CB33oVLk+LUWuyHO5K3kNeDqQ=",
        "originContent": "[Teng Wang<sup>2,&#9993;</sup>](http://ttengwang.com/), ",
        "translatedContent": "[Teng Wang<sup>2,&#9993;</sup>](http://ttengwang.com/), "
      },
      {
        "row": 5,
        "rowsha": "LLQhB41AUK9J3r1VCRpBRcEia2fI/o7MDG/fVzObYkc=",
        "originContent": "[Yuying Ge<sup>2</sup>](https://geyuying.github.io/), ",
        "translatedContent": "[Yuying Ge<sup>2</sup>](https://geyuying.github.io/), "
      },
      {
        "row": 6,
        "rowsha": "wtaTd0Ja9Uv/7/T6mpuKkSdm/A3+fh81hpB8nQObPD8=",
        "originContent": "[Shijie Ma<sup>1,2</sup>](https://mashijie1028.github.io/), ",
        "translatedContent": "[Shijie Ma<sup>1,2</sup>](https://mashijie1028.github.io/), "
      },
      {
        "row": 7,
        "rowsha": "wWudxOacsyCruyB5ku8i8KaYdwV5du8s5qJ+K0qfMpc=",
        "originContent": "[Yixiao Ge<sup>2</sup>](https://geyixiao.com/), ",
        "translatedContent": "[Yixiao Ge<sup>2</sup>](https://geyixiao.com/), "
      },
      {
        "row": 8,
        "rowsha": "FPZEFhOdadaxGtx5yM4lffPGGkx7gAxM+9sAn5ARaWM=",
        "originContent": "[Wei Zou<sup>1</sup>](https://people.ucas.ac.cn/~zouwei),",
        "translatedContent": "[Wei Zou<sup>1</sup>](https://people.ucas.ac.cn/~zouwei),"
      },
      {
        "row": 9,
        "rowsha": "/XwtaWm1xbCxCliGLcLK3zuGeMMuF+5XP1t/r0xFkoo=",
        "originContent": "[Ying Shan<sup>2</sup>](https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en)**",
        "translatedContent": "[Ying Shan<sup>2</sup>](https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en)**"
      },
      {
        "row": 10,
        "rowsha": "Za7xrbqGcqX+eTLZ+fDcNcJDWJP2URKsrGKNLcF6C88=",
        "originContent": "<br>",
        "translatedContent": "<br>"
      },
      {
        "row": 11,
        "rowsha": "nQpF63Jn9FCPUpA8HVx2mbqsqMxzTv8xAxeS1jMrgu4=",
        "originContent": "<sup>1</sup>Institute of Automation, CAS",
        "translatedContent": "<sup>1</sup>Institute of Automation, CAS"
      },
      {
        "row": 12,
        "rowsha": "N/ZhVTwOyG0vyNutSAHfKXyvVJklXWMgttqLXCizzGE=",
        "originContent": "<sup>2</sup>ARC Lab, Tencent PCG",
        "translatedContent": "<sup>2</sup>ARC Lab, Tencent PCG"
      },
      {
        "row": 13,
        "rowsha": "Za7xrbqGcqX+eTLZ+fDcNcJDWJP2URKsrGKNLcF6C88=",
        "originContent": "<br>",
        "translatedContent": "<br>"
      },
      {
        "row": 14,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 15,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 16,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 17,
        "rowsha": "VAliztezgn/ssMixKQZIXx0YRgi9SKETFrH/ywehwmM=",
        "originContent": "## 📖 Release",
        "translatedContent": "## 📖 Release"
      },
      {
        "row": 18,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 19,
        "rowsha": "AVq/FCM66s4bjizL3qgI01Qe8E6u8UbzxY0aDpZTzWc=",
        "originContent": "[2025/8/28] 🔥🔥 We release the inference code!",
        "translatedContent": "[2025/8/28] 🔥🔥 We release the inference code!"
      },
      {
        "row": 20,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 21,
        "rowsha": "iRSsrbzu7Kg7wogXrPePZYadG7GnGEgRsT7KdmyG+W0=",
        "originContent": "[2025/8/28] 🔥🔥 We release our demo videos!",
        "translatedContent": "[2025/8/28] 🔥🔥 We release our demo videos!"
      },
      {
        "row": 22,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 23,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 24,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 25,
        "rowsha": "9bwc96MXV71svorL6Wnrty1lSDppDxKmeyGe4Wj5fn8=",
        "originContent": "## 🔎 Introduction",
        "translatedContent": "## 🔎 Introduction"
      },
      {
        "row": 26,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 27,
        "rowsha": "PcywANjK1M4GDEjOoHORQ/c5jRpQZKP5F6P6TiSpF9E=",
        "originContent": "![audiostory](https://raw.githubusercontent.com/TencentARC/AudioStory/main/audiostory.png)",
        "translatedContent": "![audiostory](https://raw.githubusercontent.com/TencentARC/AudioStory/main/audiostory.png)"
      },
      {
        "row": 28,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 29,
        "rowsha": "q00ZP1xd2vElr43vj7gKmpvpsWsJBuCJNMCyAMyMPrc=",
        "originContent": "✨ **TL; DR: We propose a model for long-form narrative audio generation built upon a unified understanding–generation framework, capable of handling video dubbing, audio continuation, and long-form narrative audio synthesis.**",
        "translatedContent": "✨ **TL; DR: We propose a model for long-form narrative audio generation built upon a unified understanding–generation framework, capable of handling video dubbing, audio continuation, and long-form narrative audio synthesis.**"
      },
      {
        "row": 30,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 31,
        "rowsha": "s+AOmNtiFdwIChjGJAHn8zgEw+jeBT8PKsiT83fXzDQ=",
        "originContent": "Recent advances in text-to-audio (TTA) generation excel at synthesizing short audio clips but struggle with long-form narrative audio, which requires temporal coherence and compositional reasoning. To address this gap, we propose AudioStory, a unified framework that integrates large language models (LLMs) with TTA systems to generate structured, long-form audio narratives. AudioStory possesses strong instruction-following reasoning generation capabilities. It employs LLMs to decompose complex narrative queries into temporally ordered sub-tasks with contextual cues, enabling coherent scene transitions and emotional tone consistency. AudioStory has two appealing features: ",
        "translatedContent": "Recent advances in text-to-audio (TTA) generation excel at synthesizing short audio clips but struggle with long-form narrative audio, which requires temporal coherence and compositional reasoning. To address this gap, we propose AudioStory, a unified framework that integrates large language models (LLMs) with TTA systems to generate structured, long-form audio narratives. AudioStory possesses strong instruction-following reasoning generation capabilities. It employs LLMs to decompose complex narrative queries into temporally ordered sub-tasks with contextual cues, enabling coherent scene transitions and emotional tone consistency. AudioStory has two appealing features: "
      },
      {
        "row": 32,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 33,
        "rowsha": "XZUHI5mmiyNBevVzm/OkDO8EkkoJyWVA7rBQVhqnsto=",
        "originContent": "1) Decoupled bridging mechanism: AudioStory disentangles LLM-diffuser collaboration into two specialized components—a bridging query for intra-event semantic alignment and a consistency query for cross-event coherence preservation.",
        "translatedContent": "1) Decoupled bridging mechanism: AudioStory disentangles LLM-diffuser collaboration into two specialized components—a bridging query for intra-event semantic alignment and a consistency query for cross-event coherence preservation."
      },
      {
        "row": 34,
        "rowsha": "LR7DR8cVmI/lcvgNhEyRIkxgrBu8Wf1evvjFOEDTK/M=",
        "originContent": "2) End-to-end training: By unifying instruction comprehension and audio generation within a single end-to-end framework, AudioStory eliminates the need for modular training pipelines while enhancing synergy between components. ",
        "translatedContent": "2) End-to-end training: By unifying instruction comprehension and audio generation within a single end-to-end framework, AudioStory eliminates the need for modular training pipelines while enhancing synergy between components. "
      },
      {
        "row": 35,
        "rowsha": "CiD/Ews3JkyHETo7ElTT6cc6NoxBUUOZ5UVkesYTWt8=",
        "originContent": "    Furthermore, we establish a benchmark AudioStory-10K, encompassing diverse domains such as animated soundscapes and natural sound narratives.",
        "translatedContent": "    Furthermore, we establish a benchmark AudioStory-10K, encompassing diverse domains such as animated soundscapes and natural sound narratives."
      },
      {
        "row": 36,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 37,
        "rowsha": "hLpBif+fzMIMVGvpyrRBQWNm7pP0Jlud9b+uGbB0nBA=",
        "originContent": "Extensive experiments show the superiority of AudioStory on both single-audio generation and narrative audio generation, surpassing prior TTA baselines in both instruction-following ability and audio fidelity.",
        "translatedContent": "Extensive experiments show the superiority of AudioStory on both single-audio generation and narrative audio generation, surpassing prior TTA baselines in both instruction-following ability and audio fidelity."
      },
      {
        "row": 38,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 39,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 40,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 2,
    "Content": "## ⭐ Demos\n\n### 1. Video Dubbing (Tom & Jerry style)\n> Dubbing is achieved using AudioStory (trained on Tom & Jerry) with visual captions extracted from videos.\n\n<table class=\"center\">\n  <td><video src=\"https://github.com/user-attachments/assets/f06b5999-6649-44d3-af38-63fdcecd833c\"></video></td>\n  <td><video src=\"https://github.com/user-attachments/assets/17727c2a-bfea-4252-9aa8-48fc9ac33500\"></video></td>\n  <td><video src=\"https://github.com/user-attachments/assets/09589d82-62c9-47a6-838a-5a62319f35e2\"></video></td>\n  <tr>\n</table >\n\n\n### 2. Cross-domain Video Dubbing (Tom & Jerry style)\n\n<table class=\"center\">\n    <td><video src=\"https://github.com/user-attachments/assets/e62d0c09-cdf0-4e51-b550-0a2c23f8d68d\"></video></td>\n    <td><video src=\"https://github.com/user-attachments/assets/736d22ca-6636-4ef0-99f3-768e4dfb112a\"></video></td>\n    <td><video src=\"https://github.com/user-attachments/assets/f2f7c94c-7f72-4cc0-8edc-290910980b04\"></video></td>\n  <tr>\n  <td><video src=\"https://github.com/user-attachments/assets/d3e58dd4-31ae-4e32-aef1-03f1e649cb0c\"></video></td>\n  <td><video src=\"https://github.com/user-attachments/assets/4f68199f-e48a-4be7-b6dc-1acb8d377a6e\"></video></td>\n  <td><video src=\"https://github.com/user-attachments/assets/062236c3-1d26-4622-b843-cc0cd0c58053\"></video></td>\n\t<tr>\n  <td><video src=\"https://github.com/user-attachments/assets/8931f428-dd4d-430f-9927-068f2912dd36\"></video></td>\n  <td><video src=\"https://github.com/user-attachments/assets/ab7e46d5-f42c-472e-b66e-df786b658210\"></video></td>      \n  <td><video src=\"https://github.com/user-attachments/assets/9a0998ad-b5a4-42ac-bdaf-ceaf796fc586\"></video></td>\n  <tr>\n</table >\n\n\n\n### 3. Text-to-Long Audio (Natural sound)\n\n<table class=\"center\">\n  <td style=\"text-align:center;\" width=\"480\">Instruction: \"Develop a comprehensive audio that fully represents jake shimabukuro performs a complex ukulele piece in a studio, receives applause, and discusses his career in an interview. The total duration is 49.9 seconds.\"</td>\n  <td><video src=\"https://github.com/user-attachments/assets/461e8a34-4217-454e-87b3-e4285f36ec43\"></video></td>\n\t<tr>\n  <td style=\"text-align:center;\" width=\"480\">Instruction: \"Develop a comprehensive audio that fully represents a fire truck leaves the station with sirens blaring, signaling an emergency response, and drives away. The total duration is 35.1 seconds.\"</td>\n  <td><video src=\"https://github.com/user-attachments/assets/aac0243f-5d12-480e-9850-a7f6720e4f9c\"></video></td>",
    "ContentSha": "yRhZGCetsdPAFtaXJWFJd3W84q9IEM8trADc5+qPgP8=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## ⭐ Demos\n\n### 1. Video Dubbing (Tom & Jerry style)\n> Dubbing is achieved using AudioStory (trained on Tom & Jerry) with visual captions extracted from videos.\n\n<table class=\"center\">\n  <td><video src=\"https://github.com/user-attachments/assets/f06b5999-6649-44d3-af38-63fdcecd833c\"></video></td>\n  <td><video src=\"https://github.com/user-attachments/assets/17727c2a-bfea-4252-9aa8-48fc9ac33500\"></video></td>\n  <td><video src=\"https://github.com/user-attachments/assets/09589d82-62c9-47a6-838a-5a62319f35e2\"></video></td>\n  <tr>\n</table >\n\n\n### 2. Cross-domain Video Dubbing (Tom & Jerry style)\n\n<table class=\"center\">\n    <td><video src=\"https://github.com/user-attachments/assets/e62d0c09-cdf0-4e51-b550-0a2c23f8d68d\"></video></td>\n    <td><video src=\"https://github.com/user-attachments/assets/736d22ca-6636-4ef0-99f3-768e4dfb112a\"></video></td>\n    <td><video src=\"https://github.com/user-attachments/assets/f2f7c94c-7f72-4cc0-8edc-290910980b04\"></video></td>\n  <tr>\n  <td><video src=\"https://github.com/user-attachments/assets/d3e58dd4-31ae-4e32-aef1-03f1e649cb0c\"></video></td>\n  <td><video src=\"https://github.com/user-attachments/assets/4f68199f-e48a-4be7-b6dc-1acb8d377a6e\"></video></td>\n  <td><video src=\"https://github.com/user-attachments/assets/062236c3-1d26-4622-b843-cc0cd0c58053\"></video></td>\n\t<tr>\n  <td><video src=\"https://github.com/user-attachments/assets/8931f428-dd4d-430f-9927-068f2912dd36\"></video></td>\n  <td><video src=\"https://github.com/user-attachments/assets/ab7e46d5-f42c-472e-b66e-df786b658210\"></video></td>      \n  <td><video src=\"https://github.com/user-attachments/assets/9a0998ad-b5a4-42ac-bdaf-ceaf796fc586\"></video></td>\n  <tr>\n</table >\n\n\n\n### 3. Text-to-Long Audio (Natural sound)\n\n<table class=\"center\">\n  <td style=\"text-align:center;\" width=\"480\">Instruction: \"Develop a comprehensive audio that fully represents jake shimabukuro performs a complex ukulele piece in a studio, receives applause, and discusses his career in an interview. The total duration is 49.9 seconds.\"</td>\n  <td><video src=\"https://github.com/user-attachments/assets/461e8a34-4217-454e-87b3-e4285f36ec43\"></video></td>\n\t<tr>\n  <td style=\"text-align:center;\" width=\"480\">Instruction: \"Develop a comprehensive audio that fully represents a fire truck leaves the station with sirens blaring, signaling an emergency response, and drives away. The total duration is 35.1 seconds.\"</td>\n  <td><video src=\"https://github.com/user-attachments/assets/aac0243f-5d12-480e-9850-a7f6720e4f9c\"></video></td>",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "F2o4YxK5/okZTc8xAnW0ki0KPjAob5+4+EOhbNq8W4Y=",
        "originContent": "## ⭐ Demos",
        "translatedContent": "## ⭐ Demos"
      },
      {
        "row": 2,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 3,
        "rowsha": "d1aRTIXZKc38vEEa0aTxU1AK2CuZu0MPPd8ANvLBksY=",
        "originContent": "### 1. Video Dubbing (Tom & Jerry style)",
        "translatedContent": "### 1. Video Dubbing (Tom & Jerry style)"
      },
      {
        "row": 4,
        "rowsha": "xsuruvjWyp4/2tQLFG0owyXKXwcXVZRcDmPey5E3z+w=",
        "originContent": "> Dubbing is achieved using AudioStory (trained on Tom & Jerry) with visual captions extracted from videos.",
        "translatedContent": "> Dubbing is achieved using AudioStory (trained on Tom & Jerry) with visual captions extracted from videos."
      },
      {
        "row": 5,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 6,
        "rowsha": "7FG0iwt8PZSz598R2HJFKtpFGDfFezhtVCdnqadcLCQ=",
        "originContent": "<table class=\"center\">",
        "translatedContent": "<table class=\"center\">"
      },
      {
        "row": 7,
        "rowsha": "Be1u8gQovOwpLxd4PoQZgOSz0nWWDBPJ/j3FrcTMmos=",
        "originContent": "  <td><video src=\"https://github.com/user-attachments/assets/f06b5999-6649-44d3-af38-63fdcecd833c\"></video></td>",
        "translatedContent": "  <td><video src=\"https://github.com/user-attachments/assets/f06b5999-6649-44d3-af38-63fdcecd833c\"></video></td>"
      },
      {
        "row": 8,
        "rowsha": "UQ1j0QNf3cuxPGkERqHp/8a1bn3v/sG6lzui/xPdnFE=",
        "originContent": "  <td><video src=\"https://github.com/user-attachments/assets/17727c2a-bfea-4252-9aa8-48fc9ac33500\"></video></td>",
        "translatedContent": "  <td><video src=\"https://github.com/user-attachments/assets/17727c2a-bfea-4252-9aa8-48fc9ac33500\"></video></td>"
      },
      {
        "row": 9,
        "rowsha": "bY1aa91+o6Lj8GiuHFVga5VyhYxYNu5seIoXPIQgfDQ=",
        "originContent": "  <td><video src=\"https://github.com/user-attachments/assets/09589d82-62c9-47a6-838a-5a62319f35e2\"></video></td>",
        "translatedContent": "  <td><video src=\"https://github.com/user-attachments/assets/09589d82-62c9-47a6-838a-5a62319f35e2\"></video></td>"
      },
      {
        "row": 10,
        "rowsha": "6o+qeLcGxX0dvrGBva8HKZWaO7Np0FI8WMiMEuNtFfo=",
        "originContent": "  <tr>",
        "translatedContent": "  <tr>"
      },
      {
        "row": 11,
        "rowsha": "+hgoFVIJCNdOMAGtb2zTTnLrSdPnvT/pIoTy2M2EOaI=",
        "originContent": "</table >",
        "translatedContent": "</table >"
      },
      {
        "row": 12,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 13,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 14,
        "rowsha": "XiXvIW8MSuKVMG/Ubq2sxIrsI04P8LAAWR1F9+rcL8w=",
        "originContent": "### 2. Cross-domain Video Dubbing (Tom & Jerry style)",
        "translatedContent": "### 2. Cross-domain Video Dubbing (Tom & Jerry style)"
      },
      {
        "row": 15,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 16,
        "rowsha": "7FG0iwt8PZSz598R2HJFKtpFGDfFezhtVCdnqadcLCQ=",
        "originContent": "<table class=\"center\">",
        "translatedContent": "<table class=\"center\">"
      },
      {
        "row": 17,
        "rowsha": "rC3Gg9ZB9dkeQADRH8X0y7BJ9NRFWRPUKfXUq4jK5A8=",
        "originContent": "    <td><video src=\"https://github.com/user-attachments/assets/e62d0c09-cdf0-4e51-b550-0a2c23f8d68d\"></video></td>",
        "translatedContent": "    <td><video src=\"https://github.com/user-attachments/assets/e62d0c09-cdf0-4e51-b550-0a2c23f8d68d\"></video></td>"
      },
      {
        "row": 18,
        "rowsha": "q7k6xpgdTW2XaA00mXwPguhjbz62JBzFx2mgQi3MF58=",
        "originContent": "    <td><video src=\"https://github.com/user-attachments/assets/736d22ca-6636-4ef0-99f3-768e4dfb112a\"></video></td>",
        "translatedContent": "    <td><video src=\"https://github.com/user-attachments/assets/736d22ca-6636-4ef0-99f3-768e4dfb112a\"></video></td>"
      },
      {
        "row": 19,
        "rowsha": "f/NV8lJG3/wKBuvpWIe44S3uqSsyCu6QnhQNdP+m+x4=",
        "originContent": "    <td><video src=\"https://github.com/user-attachments/assets/f2f7c94c-7f72-4cc0-8edc-290910980b04\"></video></td>",
        "translatedContent": "    <td><video src=\"https://github.com/user-attachments/assets/f2f7c94c-7f72-4cc0-8edc-290910980b04\"></video></td>"
      },
      {
        "row": 20,
        "rowsha": "6o+qeLcGxX0dvrGBva8HKZWaO7Np0FI8WMiMEuNtFfo=",
        "originContent": "  <tr>",
        "translatedContent": "  <tr>"
      },
      {
        "row": 21,
        "rowsha": "4gr1GgVFlU3L3QjwTtUsfD17IHjNN34F/9NKaoIf0hY=",
        "originContent": "  <td><video src=\"https://github.com/user-attachments/assets/d3e58dd4-31ae-4e32-aef1-03f1e649cb0c\"></video></td>",
        "translatedContent": "  <td><video src=\"https://github.com/user-attachments/assets/d3e58dd4-31ae-4e32-aef1-03f1e649cb0c\"></video></td>"
      },
      {
        "row": 22,
        "rowsha": "40t9Y3yyaNH5W/E377F5s1ZeMWoyl8tA14tFCJgnNwc=",
        "originContent": "  <td><video src=\"https://github.com/user-attachments/assets/4f68199f-e48a-4be7-b6dc-1acb8d377a6e\"></video></td>",
        "translatedContent": "  <td><video src=\"https://github.com/user-attachments/assets/4f68199f-e48a-4be7-b6dc-1acb8d377a6e\"></video></td>"
      },
      {
        "row": 23,
        "rowsha": "E65VGlVfX8z4s1OMSSsebyozZw1yRIhcXnI2xq/cle4=",
        "originContent": "  <td><video src=\"https://github.com/user-attachments/assets/062236c3-1d26-4622-b843-cc0cd0c58053\"></video></td>",
        "translatedContent": "  <td><video src=\"https://github.com/user-attachments/assets/062236c3-1d26-4622-b843-cc0cd0c58053\"></video></td>"
      },
      {
        "row": 24,
        "rowsha": "OiNQYEf9lQ4VY4ev1P8fYEqhldXvPCT4oCAzyfkyWEs=",
        "originContent": "\t<tr>",
        "translatedContent": "\t<tr>"
      },
      {
        "row": 25,
        "rowsha": "qAjHrxg1MaGiBckyj/fm3Bg0IJFroNsOGgqNgjWD1rw=",
        "originContent": "  <td><video src=\"https://github.com/user-attachments/assets/8931f428-dd4d-430f-9927-068f2912dd36\"></video></td>",
        "translatedContent": "  <td><video src=\"https://github.com/user-attachments/assets/8931f428-dd4d-430f-9927-068f2912dd36\"></video></td>"
      },
      {
        "row": 26,
        "rowsha": "EDqm9uTGhUaiwbJOXO504J5Q7fhCeBn3W/+HloAmKZs=",
        "originContent": "  <td><video src=\"https://github.com/user-attachments/assets/ab7e46d5-f42c-472e-b66e-df786b658210\"></video></td>      ",
        "translatedContent": "  <td><video src=\"https://github.com/user-attachments/assets/ab7e46d5-f42c-472e-b66e-df786b658210\"></video></td>      "
      },
      {
        "row": 27,
        "rowsha": "xf9Jp12r2D1odgr6MJ+40q5xuyv277conuiNnh6Fpt0=",
        "originContent": "  <td><video src=\"https://github.com/user-attachments/assets/9a0998ad-b5a4-42ac-bdaf-ceaf796fc586\"></video></td>",
        "translatedContent": "  <td><video src=\"https://github.com/user-attachments/assets/9a0998ad-b5a4-42ac-bdaf-ceaf796fc586\"></video></td>"
      },
      {
        "row": 28,
        "rowsha": "6o+qeLcGxX0dvrGBva8HKZWaO7Np0FI8WMiMEuNtFfo=",
        "originContent": "  <tr>",
        "translatedContent": "  <tr>"
      },
      {
        "row": 29,
        "rowsha": "+hgoFVIJCNdOMAGtb2zTTnLrSdPnvT/pIoTy2M2EOaI=",
        "originContent": "</table >",
        "translatedContent": "</table >"
      },
      {
        "row": 30,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 31,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 32,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 33,
        "rowsha": "bqRMr9Ypst45FZhaZtIL960+4R8vcRhCw4XVLHj851M=",
        "originContent": "### 3. Text-to-Long Audio (Natural sound)",
        "translatedContent": "### 3. Text-to-Long Audio (Natural sound)"
      },
      {
        "row": 34,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 35,
        "rowsha": "7FG0iwt8PZSz598R2HJFKtpFGDfFezhtVCdnqadcLCQ=",
        "originContent": "<table class=\"center\">",
        "translatedContent": "<table class=\"center\">"
      },
      {
        "row": 36,
        "rowsha": "kMl1wJJ0HS3Ld38pT9ZOCTUq1CtHLTO6iKBL9jjBh00=",
        "originContent": "  <td style=\"text-align:center;\" width=\"480\">Instruction: \"Develop a comprehensive audio that fully represents jake shimabukuro performs a complex ukulele piece in a studio, receives applause, and discusses his career in an interview. The total duration is 49.9 seconds.\"</td>",
        "translatedContent": "  <td style=\"text-align:center;\" width=\"480\">Instruction: \"Develop a comprehensive audio that fully represents jake shimabukuro performs a complex ukulele piece in a studio, receives applause, and discusses his career in an interview. The total duration is 49.9 seconds.\"</td>"
      },
      {
        "row": 37,
        "rowsha": "bVjjQDN5t3tZPc6WndJCodkwijyURdfLT+irZpx6NMQ=",
        "originContent": "  <td><video src=\"https://github.com/user-attachments/assets/461e8a34-4217-454e-87b3-e4285f36ec43\"></video></td>",
        "translatedContent": "  <td><video src=\"https://github.com/user-attachments/assets/461e8a34-4217-454e-87b3-e4285f36ec43\"></video></td>"
      },
      {
        "row": 38,
        "rowsha": "OiNQYEf9lQ4VY4ev1P8fYEqhldXvPCT4oCAzyfkyWEs=",
        "originContent": "\t<tr>",
        "translatedContent": "\t<tr>"
      },
      {
        "row": 39,
        "rowsha": "WbDkggqFZprYFRmHly74ThpjAst9amNC4V84yMr1nd0=",
        "originContent": "  <td style=\"text-align:center;\" width=\"480\">Instruction: \"Develop a comprehensive audio that fully represents a fire truck leaves the station with sirens blaring, signaling an emergency response, and drives away. The total duration is 35.1 seconds.\"</td>",
        "translatedContent": "  <td style=\"text-align:center;\" width=\"480\">Instruction: \"Develop a comprehensive audio that fully represents a fire truck leaves the station with sirens blaring, signaling an emergency response, and drives away. The total duration is 35.1 seconds.\"</td>"
      },
      {
        "row": 40,
        "rowsha": "9lkZnoSiQ/+N8CvPzh6Ssopa76N50YfMqCRiFMNU26U=",
        "originContent": "  <td><video src=\"https://github.com/user-attachments/assets/aac0243f-5d12-480e-9850-a7f6720e4f9c\"></video></td>",
        "translatedContent": "  <td><video src=\"https://github.com/user-attachments/assets/aac0243f-5d12-480e-9850-a7f6720e4f9c\"></video></td>"
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 3,
    "Content": "\t<tr>\n     <td style=\"text-align:center;\" width=\"480\">Instruction: \"Understand the input audio, infer the subsequent events, and generate the continued audio of the coach giving basketball lessons to the players. The total duration is 36.6 seconds.\"</td>    \n    <td><video src=\"https://github.com/user-attachments/assets/c4ed306a-651e-43d6-aeea-ee159542418a\"></video></td>\n\t<tr>\n</table >\n\n\n\n\n## 🔎 Methods\n\n![audiostory_framework](https://raw.githubusercontent.com/TencentARC/AudioStory/main/audiostory_framework.png)\n\nTo achieve effective instruction-following audio generation, the ability to understand the input instruction or audio stream and reason about relevant audio sub-events is essential. To this end,  AudioStory adopts a unified understanding-generation framework (Fig.). Specifically, given textual instruction or audio input, the LLM analyzes and decomposes it into structured audio sub-events with context. Based on the inferred sub-events, the LLM performs **interleaved reasoning generation**, sequentially producing captions, semantic tokens, and residual tokens for each audio clip. These two types of tokens are fused and passed to the DiT, effectively bridging the LLM with the audio generator. Through progressive training, AudioStory ultimately achieves both strong instruction comprehension and high-quality audio generation.\n\n\n\n## 🔩 Installation\n\n### Dependencies\n\n* Python >= 3.10 (Recommend to use [Anaconda](https://www.anaconda.com/download/#linux))\n* [PyTorch >=2.1.0](https://pytorch.org/)\n* NVIDIA GPU + [CUDA](https://developer.nvidia.com/cuda-downloads)\n\n### Installation\n",
    "ContentSha": "/GyPwsJGVpnaZEh9wkUBW13gnt3S8xfye/LjEg7KI5Q=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "\t<tr>\n     <td style=\"text-align:center;\" width=\"480\">Instruction: \"Understand the input audio, infer the subsequent events, and generate the continued audio of the coach giving basketball lessons to the players. The total duration is 36.6 seconds.\"</td>    \n    <td><video src=\"https://github.com/user-attachments/assets/c4ed306a-651e-43d6-aeea-ee159542418a\"></video></td>\n\t<tr>\n</table >\n\n\n\n\n## 🔎 Methods\n\n![audiostory_framework](https://raw.githubusercontent.com/TencentARC/AudioStory/main/audiostory_framework.png)\n\nTo achieve effective instruction-following audio generation, the ability to understand the input instruction or audio stream and reason about relevant audio sub-events is essential. To this end,  AudioStory adopts a unified understanding-generation framework (Fig.). Specifically, given textual instruction or audio input, the LLM analyzes and decomposes it into structured audio sub-events with context. Based on the inferred sub-events, the LLM performs **interleaved reasoning generation**, sequentially producing captions, semantic tokens, and residual tokens for each audio clip. These two types of tokens are fused and passed to the DiT, effectively bridging the LLM with the audio generator. Through progressive training, AudioStory ultimately achieves both strong instruction comprehension and high-quality audio generation.\n\n\n\n## 🔩 Installation\n\n### Dependencies\n\n* Python >= 3.10 (Recommend to use [Anaconda](https://www.anaconda.com/download/#linux))\n* [PyTorch >=2.1.0](https://pytorch.org/)\n* NVIDIA GPU + [CUDA](https://developer.nvidia.com/cuda-downloads)\n\n### Installation\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "OiNQYEf9lQ4VY4ev1P8fYEqhldXvPCT4oCAzyfkyWEs=",
        "originContent": "\t<tr>",
        "translatedContent": "\t<tr>"
      },
      {
        "row": 2,
        "rowsha": "Ubj/ESZSw8PuG2wDYpXEupbQmCK4+A6G3TJPe8f9dj0=",
        "originContent": "     <td style=\"text-align:center;\" width=\"480\">Instruction: \"Understand the input audio, infer the subsequent events, and generate the continued audio of the coach giving basketball lessons to the players. The total duration is 36.6 seconds.\"</td>    ",
        "translatedContent": "     <td style=\"text-align:center;\" width=\"480\">Instruction: \"Understand the input audio, infer the subsequent events, and generate the continued audio of the coach giving basketball lessons to the players. The total duration is 36.6 seconds.\"</td>    "
      },
      {
        "row": 3,
        "rowsha": "kSoQNgOfaTU7IxCn8zhbXySiTHuy7pubDSaVGFNhc0U=",
        "originContent": "    <td><video src=\"https://github.com/user-attachments/assets/c4ed306a-651e-43d6-aeea-ee159542418a\"></video></td>",
        "translatedContent": "    <td><video src=\"https://github.com/user-attachments/assets/c4ed306a-651e-43d6-aeea-ee159542418a\"></video></td>"
      },
      {
        "row": 4,
        "rowsha": "OiNQYEf9lQ4VY4ev1P8fYEqhldXvPCT4oCAzyfkyWEs=",
        "originContent": "\t<tr>",
        "translatedContent": "\t<tr>"
      },
      {
        "row": 5,
        "rowsha": "+hgoFVIJCNdOMAGtb2zTTnLrSdPnvT/pIoTy2M2EOaI=",
        "originContent": "</table >",
        "translatedContent": "</table >"
      },
      {
        "row": 6,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 7,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 8,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 9,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 10,
        "rowsha": "DSR9DEZFtYlp7rFmJdaJn543CfSMiyJtVeJd41Shuqk=",
        "originContent": "## 🔎 Methods",
        "translatedContent": "## 🔎 Methods"
      },
      {
        "row": 11,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 12,
        "rowsha": "9BH0xGdcfBS+VhBWoWwKAVZ3mgHTm/j1BgvwybER7o8=",
        "originContent": "![audiostory_framework](https://raw.githubusercontent.com/TencentARC/AudioStory/main/audiostory_framework.png)",
        "translatedContent": "![audiostory_framework](https://raw.githubusercontent.com/TencentARC/AudioStory/main/audiostory_framework.png)"
      },
      {
        "row": 13,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 14,
        "rowsha": "GPV5B4bE7PBjL8Nf8gfo6IGZc0RTyX/CfLJH6fpFaAs=",
        "originContent": "To achieve effective instruction-following audio generation, the ability to understand the input instruction or audio stream and reason about relevant audio sub-events is essential. To this end,  AudioStory adopts a unified understanding-generation framework (Fig.). Specifically, given textual instruction or audio input, the LLM analyzes and decomposes it into structured audio sub-events with context. Based on the inferred sub-events, the LLM performs **interleaved reasoning generation**, sequentially producing captions, semantic tokens, and residual tokens for each audio clip. These two types of tokens are fused and passed to the DiT, effectively bridging the LLM with the audio generator. Through progressive training, AudioStory ultimately achieves both strong instruction comprehension and high-quality audio generation.",
        "translatedContent": "To achieve effective instruction-following audio generation, the ability to understand the input instruction or audio stream and reason about relevant audio sub-events is essential. To this end,  AudioStory adopts a unified understanding-generation framework (Fig.). Specifically, given textual instruction or audio input, the LLM analyzes and decomposes it into structured audio sub-events with context. Based on the inferred sub-events, the LLM performs **interleaved reasoning generation**, sequentially producing captions, semantic tokens, and residual tokens for each audio clip. These two types of tokens are fused and passed to the DiT, effectively bridging the LLM with the audio generator. Through progressive training, AudioStory ultimately achieves both strong instruction comprehension and high-quality audio generation."
      },
      {
        "row": 15,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 16,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 17,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 18,
        "rowsha": "K5BKhOEr4dNsKz5aQYjGDFsb5cAXzh2rRww3aq1fToY=",
        "originContent": "## 🔩 Installation",
        "translatedContent": "## 🔩 Installation"
      },
      {
        "row": 19,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 20,
        "rowsha": "cA9I4m4MLDGazdniXFF/VJ7jgnkzPSvBbISFVX7s/5s=",
        "originContent": "### Dependencies",
        "translatedContent": "### Dependencies"
      },
      {
        "row": 21,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 22,
        "rowsha": "ybFy651pkGBiUaSzaUrVuJTCbVsItiZJqXTekxenDOs=",
        "originContent": "* Python >= 3.10 (Recommend to use [Anaconda](https://www.anaconda.com/download/#linux))",
        "translatedContent": "* Python >= 3.10 (Recommend to use [Anaconda](https://www.anaconda.com/download/#linux))"
      },
      {
        "row": 23,
        "rowsha": "2clsJHIWgOmQ5F5S50Xg0VdGp29U34jP7Md27T2D6hQ=",
        "originContent": "* [PyTorch >=2.1.0](https://pytorch.org/)",
        "translatedContent": "* [PyTorch >=2.1.0](https://pytorch.org/)"
      },
      {
        "row": 24,
        "rowsha": "jayRuOQfJklvG5CCRlsnsLExVb7ZiAnQNvgN4dziUNE=",
        "originContent": "* NVIDIA GPU + [CUDA](https://developer.nvidia.com/cuda-downloads)",
        "translatedContent": "* NVIDIA GPU + [CUDA](https://developer.nvidia.com/cuda-downloads)"
      },
      {
        "row": 25,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 26,
        "rowsha": "JuAC4s82hMbNkRqX17s0ltqjVmeI/HhsmWljgf+i7Kg=",
        "originContent": "### Installation",
        "translatedContent": "### Installation"
      },
      {
        "row": 27,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 4,
    "Content": "```\ngit clone https://github.com/TencentARC/AudioStory.git\ncd AudioStory\nconda create -n audiostory python=3.10 -y\nconda activate audiostory\nbash install_audiostory.sh\n```",
    "ContentSha": "torQeDrtkkKZDXrpn1NU9BCi1oPIYyqAjGXcQv3AB1A=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```\ngit clone https://github.com/TencentARC/AudioStory.git\ncd AudioStory\nconda create -n audiostory python=3.10 -y\nconda activate audiostory\nbash install_audiostory.sh\n```",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "8bkBhHOQsO1+N058HkZOwXtGpCfEh6WtbL0pBkBQg9U=",
        "originContent": "```",
        "translatedContent": "```"
      },
      {
        "row": 2,
        "rowsha": "EPmucZkseZe9CkGYkev1GLT10FBhL1aNrGP/yj11YCk=",
        "originContent": "git clone https://github.com/TencentARC/AudioStory.git",
        "translatedContent": "git clone https://github.com/TencentARC/AudioStory.git"
      },
      {
        "row": 3,
        "rowsha": "0NZ9wSgVtafsjHQpQHI6UrYESexC7Zt6QfFMDCs6no4=",
        "originContent": "cd AudioStory",
        "translatedContent": "cd AudioStory"
      },
      {
        "row": 4,
        "rowsha": "b6InjqOMdN4MWLHgzMqBe0kIp8aZoXOify/79OYxSoQ=",
        "originContent": "conda create -n audiostory python=3.10 -y",
        "translatedContent": "conda create -n audiostory python=3.10 -y"
      },
      {
        "row": 5,
        "rowsha": "6EvnIjWjc45YgSD65Q5ld8GdStTLPBPMF2Y7UsLJrxg=",
        "originContent": "conda activate audiostory",
        "translatedContent": "conda activate audiostory"
      },
      {
        "row": 6,
        "rowsha": "lz0yrVadp6kN4Y0yuBpmqXWQj5H72CMu8CZollmEaKA=",
        "originContent": "bash install_audiostory.sh",
        "translatedContent": "bash install_audiostory.sh"
      },
      {
        "row": 7,
        "rowsha": "8bkBhHOQsO1+N058HkZOwXtGpCfEh6WtbL0pBkBQg9U=",
        "originContent": "```",
        "translatedContent": "```"
      }
    ],
    "IsCodeBlock": true
  },
  {
    "Id": 5,
    "Content": "\n\n\n## 📊 Evaluation\n\n### inference\n",
    "ContentSha": "VXPWsF6XWv8o+fu5ShnIm806PI4eUZhbnTgIUOFTXow=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "\n\n\n## 📊 Evaluation\n\n### inference\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 2,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 3,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 4,
        "rowsha": "RV/lkFyd4r4TQLY1qeDzx60mM+znUBMQr8fvaK78ARc=",
        "originContent": "## 📊 Evaluation",
        "translatedContent": "## 📊 Evaluation"
      },
      {
        "row": 5,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 6,
        "rowsha": "nPTFwGS68FhDKEEpm6ZdfcFiiCl0YPFcoEoP4JW/kgc=",
        "originContent": "### inference",
        "translatedContent": "### inference"
      },
      {
        "row": 7,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 6,
    "Content": "```\npython evaluate/inference.py --model_path /path/to/ckpt --guidance 4.0 --save_folder_name audiostory --total_duration 50\n```",
    "ContentSha": "KaFRY3J/KQfqoMUzdWhseF4Ie11Qsa5GX7kzvSuPb/8=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```\npython evaluate/inference.py --model_path /path/to/ckpt --guidance 4.0 --save_folder_name audiostory --total_duration 50\n```",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "8bkBhHOQsO1+N058HkZOwXtGpCfEh6WtbL0pBkBQg9U=",
        "originContent": "```",
        "translatedContent": "```"
      },
      {
        "row": 2,
        "rowsha": "ZnsFCbLX8ufyjJRVjZqrJuJpwkDqjg17DUZ4Jls4H38=",
        "originContent": "python evaluate/inference.py --model_path /path/to/ckpt --guidance 4.0 --save_folder_name audiostory --total_duration 50",
        "translatedContent": "python evaluate/inference.py --model_path /path/to/ckpt --guidance 4.0 --save_folder_name audiostory --total_duration 50"
      },
      {
        "row": 3,
        "rowsha": "8bkBhHOQsO1+N058HkZOwXtGpCfEh6WtbL0pBkBQg9U=",
        "originContent": "```",
        "translatedContent": "```"
      }
    ],
    "IsCodeBlock": true
  },
  {
    "Id": 7,
    "Content": "\n\n\n## 🔋 Acknowledgement\n\nWhen building the codebase of continuous denosiers, we refer to [SEED-X](https://github.com/AILab-CVC/SEED-X) and [TangoFlux](https://github.com/declare-lab/TangoFlux). Thanks for their wonderful projects.\n\n\n\n## 📆 TO DO\n\n- [ ] Release our gradio demo.\n- [ ] Release checkpoints of AudioStory.\n- [ ] Release training codes of all three stages.\n\n\n\n## 📜 License\n\nThis repository is under the [Apache 2 License](https://github.com/mashijie1028/Gen4Rep/blob/main/LICENSE).\n\n\n\n## 📚 BibTeX\n",
    "ContentSha": "KqevFHQFhagGOJl+ZPG92WY3KPbdVOYnC0wRC6I3Syo=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "\n\n## 🔋 Acknowledgement\n\nWhen building the codebase of continuous denoisers, we refer to [SEED-X](https://github.com/AILab-CVC/SEED-X) and [TangoFlux](https://github.com/declare-lab/TangoFlux). Thanks for their wonderful projects.\n\n\n\n## 📆 TO DO\n\n- [ ] Release our gradio demo.\n- [ ] Release checkpoints of AudioStory.\n- [ ] Release training codes of all three stages.\n\n\n\n## 📜 License\n\nThis repository is under the [Apache 2 License](https://github.com/mashijie1028/Gen4Rep/blob/main/LICENSE).\n\n\n\n## 📚 BibTeX\n\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 2,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 3,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## 🔋 Acknowledgement"
      },
      {
        "row": 4,
        "rowsha": "2wITwUlzAo4l/8qItt2XUr+0BnCsPysASmH5gx69T7w=",
        "originContent": "## 🔋 Acknowledgement",
        "translatedContent": ""
      },
      {
        "row": 5,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "When building the codebase of continuous denoisers, we refer to [SEED-X](https://github.com/AILab-CVC/SEED-X) and [TangoFlux](https://github.com/declare-lab/TangoFlux). Thanks for their wonderful projects."
      },
      {
        "row": 6,
        "rowsha": "8lTT7v2VuQ0QtdsydJaX8vAjYN5nizcwZ/fhChW28pA=",
        "originContent": "When building the codebase of continuous denosiers, we refer to [SEED-X](https://github.com/AILab-CVC/SEED-X) and [TangoFlux](https://github.com/declare-lab/TangoFlux). Thanks for their wonderful projects.",
        "translatedContent": ""
      },
      {
        "row": 7,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 8,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 9,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## 📆 TO DO"
      },
      {
        "row": 10,
        "rowsha": "GC23nj3oE9/vfv7oaTalUvYrK5FOFDTk7jfTM6fZ59w=",
        "originContent": "## 📆 TO DO",
        "translatedContent": ""
      },
      {
        "row": 11,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "- [ ] Release our gradio demo."
      },
      {
        "row": 12,
        "rowsha": "Ev8F7RO049LbUw2wYkPRhR9N/P+kHtNfq+/JqoYCKTQ=",
        "originContent": "- [ ] Release our gradio demo.",
        "translatedContent": "- [ ] Release checkpoints of AudioStory."
      },
      {
        "row": 13,
        "rowsha": "NdhvCxdI7tcZ2BADUj+Lf1faPs4omgHrlmDe9OLatKw=",
        "originContent": "- [ ] Release checkpoints of AudioStory.",
        "translatedContent": "- [ ] Release training codes of all three stages."
      },
      {
        "row": 14,
        "rowsha": "cU6Kqe85RhIzRzZZSmmW6go4ItSFAE13SJGOKdBAlk8=",
        "originContent": "- [ ] Release training codes of all three stages.",
        "translatedContent": ""
      },
      {
        "row": 15,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 16,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 17,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## 📜 License"
      },
      {
        "row": 18,
        "rowsha": "Xg7z9THMMv8mm3IS7LJHSaXGCrZhD8D4NGbb/+/ra/Y=",
        "originContent": "## 📜 License",
        "translatedContent": ""
      },
      {
        "row": 19,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "This repository is under the [Apache 2 License](https://github.com/mashijie1028/Gen4Rep/blob/main/LICENSE)."
      },
      {
        "row": 20,
        "rowsha": "EZFCjf6XUzozZCx6zuDMh0DwwcxgS9mIiOHr2h1fT2k=",
        "originContent": "This repository is under the [Apache 2 License](https://github.com/mashijie1028/Gen4Rep/blob/main/LICENSE).",
        "translatedContent": ""
      },
      {
        "row": 21,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 22,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 23,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## 📚 BibTeX"
      },
      {
        "row": 24,
        "rowsha": "Ol70b6U8dEWdUG46wK7jYP1taws7iJl4nBgER5ckRWk=",
        "originContent": "## 📚 BibTeX",
        "translatedContent": ""
      },
      {
        "row": 25,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 8,
    "Content": "```\n@misc{guo2025audiostory,\n      title={AudioStory: Generating Long-Form Narrative Audio with Large Language Models}, \n      author={Yuxin Guo and Teng Wang and Yuying Ge and Shijie Ma and Yixiao Ge and Wei Zou and Ying Shan},\n      year={2025},\n      eprint={2508.20088},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2508.20088}, \n}\n```",
    "ContentSha": "tDvow+Q6zrM97CRU0hSl4LYmQnB/4O/k83/ptCYWq3I=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```\n@misc{guo2025audiostory,\n      title={AudioStory: Generating Long-Form Narrative Audio with Large Language Models}, \n      author={Yuxin Guo and Teng Wang and Yuying Ge and Shijie Ma and Yixiao Ge and Wei Zou and Ying Shan},\n      year={2025},\n      eprint={2508.20088},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2508.20088}, \n}\n```",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "8bkBhHOQsO1+N058HkZOwXtGpCfEh6WtbL0pBkBQg9U=",
        "originContent": "```",
        "translatedContent": "```"
      },
      {
        "row": 2,
        "rowsha": "zG+SHLM1YaUdKpVrhSHgd+F3ENF1Tu0sdECnTtfj2T0=",
        "originContent": "@misc{guo2025audiostory,",
        "translatedContent": "@misc{guo2025audiostory,"
      },
      {
        "row": 3,
        "rowsha": "wRos7vm1oMESsZP/y3oCTU8pDdBwTNkutSMbCenULPQ=",
        "originContent": "      title={AudioStory: Generating Long-Form Narrative Audio with Large Language Models}, ",
        "translatedContent": "      title={AudioStory: Generating Long-Form Narrative Audio with Large Language Models}, "
      },
      {
        "row": 4,
        "rowsha": "J3AFWUtKeIdEng2gdsNNS/aaTvmB5y4qXjaM19l1ros=",
        "originContent": "      author={Yuxin Guo and Teng Wang and Yuying Ge and Shijie Ma and Yixiao Ge and Wei Zou and Ying Shan},",
        "translatedContent": "      author={Yuxin Guo and Teng Wang and Yuying Ge and Shijie Ma and Yixiao Ge and Wei Zou and Ying Shan},"
      },
      {
        "row": 5,
        "rowsha": "1cuvfM9h03loQfZOlvsx9juVCvU41kevaYb2CnD9Gak=",
        "originContent": "      year={2025},",
        "translatedContent": "      year={2025},"
      },
      {
        "row": 6,
        "rowsha": "aVyrdaXiDdq72O2IEEra144lvU5mp8vtcvui+Yc67Y8=",
        "originContent": "      eprint={2508.20088},",
        "translatedContent": "      eprint={2508.20088},"
      },
      {
        "row": 7,
        "rowsha": "Fr73/KLqU4TaDaJVUDLO211nM029JE4YRpN5hXSZZqk=",
        "originContent": "      archivePrefix={arXiv},",
        "translatedContent": "      archivePrefix={arXiv},"
      },
      {
        "row": 8,
        "rowsha": "RPNBhgHdrY2A+XYLnuhpAr/aqag2LU2pAjasgtM0tg4=",
        "originContent": "      primaryClass={cs.CV},",
        "translatedContent": "      primaryClass={cs.CV},"
      },
      {
        "row": 9,
        "rowsha": "hP+VcwdqOvUO3cyxMORLXst2UhI7aWcsza6RkNrQcLU=",
        "originContent": "      url={https://arxiv.org/abs/2508.20088}, ",
        "translatedContent": "      url={https://arxiv.org/abs/2508.20088}, "
      },
      {
        "row": 10,
        "rowsha": "0Qs2qnSlm89KiBhYN/ZYr682Ru/yuxbDko0OkzXpRdI=",
        "originContent": "}",
        "translatedContent": "}"
      },
      {
        "row": 11,
        "rowsha": "8bkBhHOQsO1+N058HkZOwXtGpCfEh6WtbL0pBkBQg9U=",
        "originContent": "```",
        "translatedContent": "```"
      }
    ],
    "IsCodeBlock": true
  },
  {
    "Id": 9,
    "Content": "\n\n\n## 📧 Contact\n\nIf you have further questions, feel free to contact me: guoyuxin2021@ia.ac.cn\n\nDiscussions and potential collaborations are also welcome.\n",
    "ContentSha": "ep0eO4/5mD+WOJt3GO3K6E+5j9UwGmybkmcqBkIPIEU=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## 📧 Contact\n\nIf you have further questions, feel free to contact me: guoyuxin2021@ia.ac.cn\n\nDiscussions and potential collaborations are also welcome.\n\n\n\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## 📧 Contact"
      },
      {
        "row": 2,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 3,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "If you have further questions, feel free to contact me: guoyuxin2021@ia.ac.cn"
      },
      {
        "row": 4,
        "rowsha": "9XM9zLdHzI7rLQpxWwSAIpta/O8ZZ3JUA/iWlKatt1U=",
        "originContent": "## 📧 Contact",
        "translatedContent": ""
      },
      {
        "row": 5,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "Discussions and potential collaborations are also welcome."
      },
      {
        "row": 6,
        "rowsha": "CVhFCz7Qsh+ZlaXPyiCOrHyQDF3GIS9j8RaIjP6cP44=",
        "originContent": "If you have further questions, feel free to contact me: guoyuxin2021@ia.ac.cn",
        "translatedContent": ""
      },
      {
        "row": 7,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 8,
        "rowsha": "G3RStoG3klhI6y7b7G1lU2zdGS23bL1DBPh+G2x4V2I=",
        "originContent": "Discussions and potential collaborations are also welcome.",
        "translatedContent": ""
      },
      {
        "row": 9,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  }
]