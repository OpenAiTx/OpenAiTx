# DeepSeek-R1
<!-- markdownlint-disable first-line-h1 -->
<!-- markdownlint-disable html -->
<!-- markdownlint-disable no-duplicate-header -->

<div align="center">
  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-R1" />
</div>
<hr>
<div align="center" style="line-height: 1;">
  <a href="https://www.deepseek.com/" target="_blank"><img alt="Homepage"
    src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true"/></a>
  <a href="https://chat.deepseek.com/" target="_blank"><img alt="Chat"
    src="https://img.shields.io/badge/ğŸ¤–%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white"/></a>
  <a href="https://huggingface.co/deepseek-ai" target="_blank"><img alt="Hugging Face"
    src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white"/></a>
  <br>
  <a href="https://discord.gg/Tc7c45Zzu5" target="_blank"><img alt="Discord"
    src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da"/></a>
  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true" target="_blank"><img alt="WeChat"
    src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white"/></a>
  <a href="https://twitter.com/deepseek_ai" target="_blank"><img alt="Twitter Follow"
    src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white"/></a>
  <br>
  <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE"><img alt="License"
    src="https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53"/></a>
  <br>
  <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf"><b>Lien vers l'article</b>ğŸ‘ï¸</a>
</div>

## 1. Introduction

Nous prÃ©sentons nos modÃ¨les de raisonnement de premiÃ¨re gÃ©nÃ©ration, DeepSeek-R1-Zero et DeepSeek-R1.  
DeepSeek-R1-Zero, un modÃ¨le entraÃ®nÃ© via un apprentissage par renforcement (RL) Ã  grande Ã©chelle sans ajustement supervisÃ© (SFT) comme Ã©tape prÃ©liminaire, a dÃ©montrÃ© des performances remarquables en raisonnement.  
GrÃ¢ce au RL, DeepSeek-R1-Zero a naturellement dÃ©veloppÃ© de nombreux comportements de raisonnement puissants et intÃ©ressants.  
Cependant, DeepSeek-R1-Zero rencontre des problÃ¨mes tels que des rÃ©pÃ©titions infinies, une faible lisibilitÃ© et le mÃ©lange des langues. Pour rÃ©soudre ces problÃ¨mes et amÃ©liorer davantage les performances de raisonnement,  
nous introduisons DeepSeek-R1, qui intÃ¨gre des donnÃ©es de dÃ©marrage Ã  froid avant le RL.  
DeepSeek-R1 atteint des performances comparables Ã  OpenAI-o1 sur les tÃ¢ches de mathÃ©matiques, de code et de raisonnement.  
Pour soutenir la communautÃ© de recherche, nous avons ouvert DeepSeek-R1-Zero, DeepSeek-R1 et six modÃ¨les denses distillÃ©s Ã  partir de DeepSeek-R1 basÃ©s sur Llama et Qwen. DeepSeek-R1-Distill-Qwen-32B surpasse OpenAI-o1-mini sur divers benchmarks, Ã©tablissant de nouveaux rÃ©sultats SOTA pour les modÃ¨les denses.

**REMARQUE : Avant d'exÃ©cuter localement les modÃ¨les de la sÃ©rie DeepSeek-R1, nous vous recommandons vivement de consulter la section [Recommandations d'utilisation](#usage-recommendations).**

<p align="center">
  <img width="80%" src="figures/benchmark.jpg">
</p>

## 2. RÃ©sumÃ© du modÃ¨le

---

**Post-entraÃ®nement : Apprentissage par renforcement Ã  grande Ã©chelle sur le modÃ¨le de base**

-  Nous appliquons directement l'apprentissage par renforcement (RL) au modÃ¨le de base sans recourir Ã  l'ajustement supervisÃ© (SFT) comme Ã©tape prÃ©liminaire. Cette approche permet au modÃ¨le d'explorer le raisonnement en chaÃ®ne (CoT) pour rÃ©soudre des problÃ¨mes complexes, ce qui aboutit au dÃ©veloppement de DeepSeek-R1-Zero. DeepSeek-R1-Zero dÃ©montre des capacitÃ©s telles que l'auto-vÃ©rification, la rÃ©flexion et la gÃ©nÃ©ration de longues chaÃ®nes de raisonnement, marquant une Ã©tape importante pour la communautÃ© de recherche. Notamment, il s'agit de la premiÃ¨re recherche ouverte Ã  valider que les capacitÃ©s de raisonnement des LLMs peuvent Ãªtre stimulÃ©es uniquement via le RL, sans besoin de SFT. Cette avancÃ©e ouvre la voie Ã  de futurs progrÃ¨s dans ce domaine.

-   Nous prÃ©sentons notre pipeline pour dÃ©velopper DeepSeek-R1. Le pipeline intÃ¨gre deux Ã©tapes de RL visant Ã  dÃ©couvrir de meilleurs schÃ©mas de raisonnement et Ã  s'aligner sur les prÃ©fÃ©rences humaines, ainsi que deux Ã©tapes de SFT qui servent de graine pour les capacitÃ©s de raisonnement et non-raisonnement du modÃ¨le.  
    Nous pensons que ce pipeline bÃ©nÃ©ficiera Ã  l'industrie en crÃ©ant de meilleurs modÃ¨les.

---

**Distillation : Les petits modÃ¨les peuvent aussi Ãªtre puissants**

-  Nous dÃ©montrons que les schÃ©mas de raisonnement des modÃ¨les plus grands peuvent Ãªtre distillÃ©s dans des modÃ¨les plus petits, donnant de meilleures performances que les schÃ©mas de raisonnement dÃ©couverts par RL sur des petits modÃ¨les. Le DeepSeek-R1 open source, ainsi que son API, permettront Ã  la communautÃ© de recherche de distiller de meilleurs petits modÃ¨les Ã  l'avenir.
-  En utilisant les donnÃ©es de raisonnement gÃ©nÃ©rÃ©es par DeepSeek-R1, nous avons affinÃ© plusieurs modÃ¨les denses largement utilisÃ©s dans la communautÃ© de recherche. Les rÃ©sultats d'Ã©valuation dÃ©montrent que les petits modÃ¨les denses distillÃ©s performent exceptionnellement bien sur les benchmarks. Nous publions en open source les checkpoints distillÃ©s 1.5B, 7B, 8B, 14B, 32B et 70B basÃ©s sur Qwen2.5 et la sÃ©rie Llama3 Ã  la communautÃ©.

## 3. TÃ©lÃ©chargement des modÃ¨les

### ModÃ¨les DeepSeek-R1

<div align="center">

| **ModÃ¨le**         | **#ParamÃ¨tres totaux** | **#ParamÃ¨tres activÃ©s** | **Longueur du contexte** | **TÃ©lÃ©charger** |
| :----------------: | :-------------------: | :---------------------: | :----------------------: | :-------------: |
| DeepSeek-R1-Zero   | 671B                  | 37B                     | 128K                     | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)   |
| DeepSeek-R1        | 671B                  | 37B                     | 128K                     | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   |

</div>

DeepSeek-R1-Zero & DeepSeek-R1 sont entraÃ®nÃ©s Ã  partir de DeepSeek-V3-Base.  
Pour plus de dÃ©tails concernant l'architecture du modÃ¨le, veuillez vous rÃ©fÃ©rer au dÃ©pÃ´t [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3).

### ModÃ¨les DeepSeek-R1-Distill

<div align="center">

| **ModÃ¨le** | **ModÃ¨le de base** | **TÃ©lÃ©charger** |
| :----------------------: | :---------------------: | :-----------------: |
| DeepSeek-R1-Distill-Qwen-1.5B  | [Qwen2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B) | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)   |
| DeepSeek-R1-Distill-Qwen-7B  | [Qwen2.5-Math-7B](https://huggingface.co/Qwen/Qwen2.5-Math-7B) | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)   |
| DeepSeek-R1-Distill-Llama-8B  | [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)   |
| DeepSeek-R1-Distill-Qwen-14B   | [Qwen2.5-14B](https://huggingface.co/Qwen/Qwen2.5-14B) | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)   |
|DeepSeek-R1-Distill-Qwen-32B  | [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B) | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)   |
| DeepSeek-R1-Distill-Llama-70B  | [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)   |

</div>

Les modÃ¨les DeepSeek-R1-Distill sont affinÃ©s Ã  partir de modÃ¨les open source, en utilisant des exemples gÃ©nÃ©rÃ©s par DeepSeek-R1.  
Nous avons lÃ©gÃ¨rement modifiÃ© leurs configurations et leurs tokenizers. Veuillez utiliser notre configuration pour exÃ©cuter ces modÃ¨les.

## 4. RÃ©sultats d'Ã©valuation

### DeepSeek-R1-Ã‰valuation
Pour tous nos modÃ¨les, la longueur maximale de gÃ©nÃ©ration est fixÃ©e Ã  32 768 tokens. Pour les benchmarks nÃ©cessitant un Ã©chantillonnage, nous utilisons une tempÃ©rature de $0,6$, une valeur top-p de $0,95$ et gÃ©nÃ©rons 64 rÃ©ponses par requÃªte pour estimer pass@1.
<div align="center">


| CatÃ©gorie | Benchmark (MÃ©trique) | Claude-3.5-Sonnet-1022 | GPT-4o 0513 | DeepSeek V3 | OpenAI o1-mini | OpenAI o1-1217 | DeepSeek R1 |
|----------|-------------------|----------------------|------------|--------------|----------------|------------|--------------|
| | Architecture | - | - | MoE | - | - | MoE |
| | # ParamÃ¨tres activÃ©s | - | - | 37B | - | - | 37B |
| | # ParamÃ¨tres totaux | - | - | 671B | - | - | 671B |
| Anglais | MMLU (Pass@1) | 88.3 | 87.2 | 88.5 | 85.2 | **91.8** | 90.8 |
| | MMLU-Redux (EM) | 88.9 | 88.0 | 89.1 | 86.7 | - | **92.9** |
| | MMLU-Pro (EM) | 78.0 | 72.6 | 75.9 | 80.3 | - | **84.0** |
| | DROP (3-shot F1) | 88.3 | 83.7 | 91.6 | 83.9 | 90.2 | **92.2** |
| | IF-Eval (Prompt Strict) | **86.5** | 84.3 | 86.1 | 84.8 | - | 83.3 |
| | GPQA-Diamond (Pass@1) | 65.0 | 49.9 | 59.1 | 60.0 | **75.7** | 71.5 |
| | SimpleQA (Correct) | 28.4 | 38.2 | 24.9 | 7.0 | **47.0** | 30.1 |
| | FRAMES (Acc.) | 72.5 | 80.5 | 73.3 | 76.9 | - | **82.5** |
| | AlpacaEval2.0 (LC-winrate) | 52.0 | 51.1 | 70.0 | 57.8 | - | **87.6** |
| | ArenaHard (GPT-4-1106) | 85.2 | 80.4 | 85.5 | 92.0 | - | **92.3** |
| Code | LiveCodeBench (Pass@1-COT) | 33.8 | 34.2 | - | 53.8 | 63.4 | **65.9** |
| | Codeforces (Percentile) | 20.3 | 23.6 | 58.7 | 93.4 | **96.6** | 96.3 |
| | Codeforces (Rating) | 717 | 759 | 1134 | 1820 | **2061** | 2029 |
| | SWE Verified (RÃ©solu) | **50.8** | 38.8 | 42.0 | 41.6 | 48.9 | 49.2 |
| | Aider-Polyglot (Acc.) | 45.3 | 16.0 | 49.6 | 32.9 | **61.7** | 53.3 |
| Math | AIME 2024 (Pass@1) | 16.0 | 9.3 | 39.2 | 63.6 | 79.2 | **79.8** |
| | MATH-500 (Pass@1) | 78.3 | 74.6 | 90.2 | 90.0 | 96.4 | **97.3** |
| | CNMO 2024 (Pass@1) | 13.1 | 10.8 | 43.2 | 67.6 | - | **78.8** |
| Chinois | CLUEWSC (EM) | 85.4 | 87.9 | 90.9 | 89.9 | - | **92.8** |
| | C-Eval (EM) | 76.7 | 76.0 | 86.5 | 68.9 | - | **91.8** |
| | C-SimpleQA (Correct) | 55.4 | 58.7 | **68.0** | 40.3 | - | 63.7 |

</div>


### Ã‰valuation des modÃ¨les distillÃ©s


<div align="center">

| ModÃ¨le                                   | AIME 2024 pass@1 | AIME 2024 cons@64 | MATH-500 pass@1 | GPQA Diamond pass@1 | LiveCodeBench pass@1 | CodeForces rating |
|------------------------------------------|------------------|-------------------|-----------------|----------------------|----------------------|-------------------|
| GPT-4o-0513                          | 9.3              | 13.4              | 74.6            | 49.9                 | 32.9                 | 759               |
| Claude-3.5-Sonnet-1022             | 16.0             | 26.7                 | 78.3            | 65.0                 | 38.9                 | 717               |
| o1-mini                              | 63.6             | 80.0              | 90.0            | 60.0                 | 53.8                 | **1820**          |
| QwQ-32B-Preview                              | 44.0             | 60.0                 | 90.6            | 54.5               | 41.9                 | 1316              |
| DeepSeek-R1-Distill-Qwen-1.5B       | 28.9             | 52.7              | 83.9            | 33.8                 | 16.9                 | 954               |
| DeepSeek-R1-Distill-Qwen-7B          | 55.5             | 83.3              | 92.8            | 49.1                 | 37.6                 | 1189              |
| DeepSeek-R1-Distill-Qwen-14B         | 69.7             | 80.0              | 93.9            | 59.1                 | 53.1                 | 1481              |
| DeepSeek-R1-Distill-Qwen-32B        | **72.6**         | 83.3              | 94.3            | 62.1                 | 57.2                 | 1691              |
| DeepSeek-R1-Distill-Llama-8B         | 50.4             | 80.0              | 89.1            | 49.0                 | 39.6                 | 1205              |
| DeepSeek-R1-Distill-Llama-70B        | 70.0             | **86.7**          | **94.5**        | **65.2**             | **57.5**             | 1633              |

</div>


## 5. Site de chat & Plateforme API
Vous pouvez discuter avec DeepSeek-R1 sur le site officiel de DeepSeek : [chat.deepseek.com](https://chat.deepseek.com), et activer le bouton "DeepThink".

Nous proposons Ã©galement une API compatible OpenAI sur la plateforme DeepSeek : [platform.deepseek.com](https://platform.deepseek.com/)

## 6. Comment exÃ©cuter en local

### ModÃ¨les DeepSeek-R1

Veuillez consulter le dÃ©pÃ´t [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) pour plus d'informations sur l'exÃ©cution locale de DeepSeek-R1.

**REMARQUE : Transformers de Hugging Face n'est pas encore directement supportÃ©.**

### ModÃ¨les DeepSeek-R1-Distill

Les modÃ¨les DeepSeek-R1-Distill peuvent Ãªtre utilisÃ©s de la mÃªme maniÃ¨re que les modÃ¨les Qwen ou Llama.

Par exemple, vous pouvez facilement dÃ©marrer un service avec [vLLM](https://github.com/vllm-project/vllm) :

```shell
vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager
```

Vous pouvez aussi facilement dÃ©marrer un service avec [SGLang](https://github.com/sgl-project/sglang)

```bash
python3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2
```

### Recommandations d'utilisation

**Nous recommandons de respecter les configurations suivantes lors de l'utilisation des modÃ¨les de la sÃ©rie DeepSeek-R1, y compris pour les benchmarks, afin d'obtenir les performances attendues :**

1. RÃ©glez la tempÃ©rature entre 0,5 et 0,7 (0,6 recommandÃ©) pour Ã©viter les rÃ©pÃ©titions infinies ou les sorties incohÃ©rentes.
2. **Ã‰vitez d'ajouter un prompt systÃ¨me ; toutes les instructions doivent Ãªtre contenues dans le prompt utilisateur.**
3. Pour les problÃ¨mes mathÃ©matiques, il est conseillÃ© d'inclure une directive dans votre prompt telle que : "Veuillez raisonner Ã©tape par Ã©tape et placer votre rÃ©ponse finale dans \boxed{}."
4. Lors de l'Ã©valuation des performances du modÃ¨le, il est recommandÃ© d'effectuer plusieurs tests et de faire la moyenne des rÃ©sultats.

De plus, nous avons observÃ© que les modÃ¨les de la sÃ©rie DeepSeek-R1 ont tendance Ã  ignorer le schÃ©ma de rÃ©flexion (c'est-Ã -dire Ã  sortir "\<think\>\n\n\</think\>") lors de certaines requÃªtes, ce qui peut affecter nÃ©gativement les performances du modÃ¨le.  
**Pour garantir un raisonnement approfondi, nous recommandons de forcer le modÃ¨le Ã  commencer sa rÃ©ponse par "\<think\>\n" au dÃ©but de chaque sortie.**

### Prompts officiels
Dans l'application web/app officielle DeepSeek, nous n'utilisons pas de prompts systÃ¨me mais concevons deux prompts spÃ©cifiques pour l'envoi de fichiers et la recherche web afin d'amÃ©liorer l'expÃ©rience utilisateur. De plus, la tempÃ©rature dans le web/app est de 0,6.

Pour l'envoi de fichier, veuillez suivre le modÃ¨le pour crÃ©er les prompts, oÃ¹ {file_name}, {file_content} et {question} sont des arguments.
```
file_template = \
"""[file name]: {file_name}
[file content begin]
{file_content}
[file content end]
{question}"""
```

Pour la recherche web, {search_results}, {cur_date}, et {question} sont des arguments.

Pour une requÃªte en chinois, nous utilisons le prompt :
```
search_answer_zh_template = \
'''# ä»¥ä¸‹å†…å®¹æ˜¯åŸºäºç”¨æˆ·å‘é€çš„æ¶ˆæ¯çš„æœç´¢ç»“æœ:
{search_results}
åœ¨æˆ‘ç»™ä½ çš„æœç´¢ç»“æœä¸­ï¼Œæ¯ä¸ªç»“æœéƒ½æ˜¯[webpage X begin]...[webpage X end]æ ¼å¼çš„ï¼ŒXä»£è¡¨æ¯ç¯‡æ–‡ç« çš„æ•°å­—ç´¢å¼•ã€‚è¯·åœ¨é€‚å½“çš„æƒ…å†µä¸‹åœ¨å¥å­æœ«å°¾å¼•ç”¨ä¸Šä¸‹æ–‡ã€‚è¯·æŒ‰ç…§å¼•ç”¨ç¼–å·[citation:X]çš„æ ¼å¼åœ¨ç­”æ¡ˆä¸­å¯¹åº”éƒ¨åˆ†å¼•ç”¨ä¸Šä¸‹æ–‡ã€‚å¦‚æœä¸€å¥è¯æºè‡ªå¤šä¸ªä¸Šä¸‹æ–‡ï¼Œè¯·åˆ—å‡ºæ‰€æœ‰ç›¸å…³çš„å¼•ç”¨ç¼–å·ï¼Œä¾‹å¦‚[citation:3][citation:5]ï¼Œåˆ‡è®°ä¸è¦å°†å¼•ç”¨é›†ä¸­åœ¨æœ€åè¿”å›å¼•ç”¨ç¼–å·ï¼Œè€Œæ˜¯åœ¨ç­”æ¡ˆå¯¹åº”éƒ¨åˆ†åˆ—å‡ºã€‚
åœ¨å›ç­”æ—¶ï¼Œè¯·æ³¨æ„ä»¥ä¸‹å‡ ç‚¹ï¼š
- ä»Šå¤©æ˜¯{cur_date}ã€‚
- å¹¶éæœç´¢ç»“æœçš„æ‰€æœ‰å†…å®¹éƒ½ä¸ç”¨æˆ·çš„é—®é¢˜å¯†åˆ‡ç›¸å…³ï¼Œä½ éœ€è¦ç»“åˆé—®é¢˜ï¼Œå¯¹æœç´¢ç»“æœè¿›è¡Œç”„åˆ«ã€ç­›é€‰ã€‚
- å¯¹äºåˆ—ä¸¾ç±»çš„é—®é¢˜ï¼ˆå¦‚åˆ—ä¸¾æ‰€æœ‰èˆªç­ä¿¡æ¯ï¼‰ï¼Œå°½é‡å°†ç­”æ¡ˆæ§åˆ¶åœ¨10ä¸ªè¦ç‚¹ä»¥å†…ï¼Œå¹¶å‘Šè¯‰ç”¨æˆ·å¯ä»¥æŸ¥çœ‹æœç´¢æ¥æºã€è·å¾—å®Œæ•´ä¿¡æ¯ã€‚ä¼˜å…ˆæä¾›ä¿¡æ¯å®Œæ•´ã€æœ€ç›¸å…³çš„åˆ—ä¸¾é¡¹ï¼›å¦‚éå¿…è¦ï¼Œä¸è¦ä¸»åŠ¨å‘Šè¯‰ç”¨æˆ·æœç´¢ç»“æœæœªæä¾›çš„å†…å®¹ã€‚
- å¯¹äºåˆ›ä½œç±»çš„é—®é¢˜ï¼ˆå¦‚å†™è®ºæ–‡ï¼‰ï¼Œè¯·åŠ¡å¿…åœ¨æ­£æ–‡çš„æ®µè½ä¸­å¼•ç”¨å¯¹åº”çš„å‚è€ƒç¼–å·ï¼Œä¾‹å¦‚[citation:3][citation:5]ï¼Œä¸èƒ½åªåœ¨æ–‡ç« æœ«å°¾å¼•ç”¨ã€‚ä½ éœ€è¦è§£è¯»å¹¶æ¦‚æ‹¬ç”¨æˆ·çš„é¢˜ç›®è¦æ±‚ï¼Œé€‰æ‹©åˆé€‚çš„æ ¼å¼ï¼Œå……åˆ†åˆ©ç”¨æœç´¢ç»“æœå¹¶æŠ½å–é‡è¦ä¿¡æ¯ï¼Œç”Ÿæˆç¬¦åˆç”¨æˆ·è¦æ±‚ã€æå…·æ€æƒ³æ·±åº¦ã€å¯Œæœ‰åˆ›é€ åŠ›ä¸ä¸“ä¸šæ€§çš„ç­”æ¡ˆã€‚ä½ çš„åˆ›ä½œç¯‡å¹…éœ€è¦å°½å¯èƒ½å»¶é•¿ï¼Œå¯¹äºæ¯ä¸€ä¸ªè¦ç‚¹çš„è®ºè¿°è¦æ¨æµ‹ç”¨æˆ·çš„æ„å›¾ï¼Œç»™å‡ºå°½å¯èƒ½å¤šè§’åº¦çš„å›ç­”è¦ç‚¹ï¼Œä¸”åŠ¡å¿…ä¿¡æ¯é‡å¤§ã€è®ºè¿°è¯¦å°½ã€‚
- å¦‚æœå›ç­”å¾ˆé•¿ï¼Œè¯·å°½é‡ç»“æ„åŒ–ã€åˆ†æ®µè½æ€»ç»“ã€‚å¦‚æœéœ€è¦åˆ†ç‚¹ä½œç­”ï¼Œå°½é‡æ§åˆ¶åœ¨5ä¸ªç‚¹ä»¥å†…ï¼Œå¹¶åˆå¹¶ç›¸å…³çš„å†…å®¹ã€‚
- å¯¹äºå®¢è§‚ç±»çš„é—®ç­”ï¼Œå¦‚æœé—®é¢˜çš„ç­”æ¡ˆéå¸¸ç®€çŸ­ï¼Œå¯ä»¥é€‚å½“è¡¥å……ä¸€åˆ°ä¸¤å¥ç›¸å…³ä¿¡æ¯ï¼Œä»¥ä¸°å¯Œå†…å®¹ã€‚
- ä½ éœ€è¦æ ¹æ®ç”¨æˆ·è¦æ±‚å’Œå›ç­”å†…å®¹é€‰æ‹©åˆé€‚ã€ç¾è§‚çš„å›ç­”æ ¼å¼ï¼Œç¡®ä¿å¯è¯»æ€§å¼ºã€‚
- ä½ çš„å›ç­”åº”è¯¥ç»¼åˆå¤šä¸ªç›¸å…³ç½‘é¡µæ¥å›ç­”ï¼Œä¸èƒ½é‡å¤å¼•ç”¨ä¸€ä¸ªç½‘é¡µã€‚
- é™¤éç”¨æˆ·è¦æ±‚ï¼Œå¦åˆ™ä½ å›ç­”çš„è¯­è¨€éœ€è¦å’Œç”¨æˆ·æé—®çš„è¯­è¨€ä¿æŒä¸€è‡´ã€‚

# ç”¨æˆ·æ¶ˆæ¯ä¸ºï¼š
{question}'''
```


Pour une requÃªte en anglais, nous utilisons le prompt :
```
search_answer_en_template = \
'''# The following contents are the search results related to the user's message:
{search_results}
In the search results I provide to you, each result is formatted as [webpage X begin]...[webpage X end], where X represents the numerical index of each article. Please cite the context at the end of the relevant sentence when appropriate. Use the citation format [citation:X] in the corresponding part of your answer. If a sentence is derived from multiple contexts, list all relevant citation numbers, such as [citation:3][citation:5]. Be sure not to cluster all citations at the end; instead, include them in the corresponding parts of the answer.
When responding, please keep the following points in mind:
- Today is {cur_date}.
- Not all content in the search results is closely related to the user's question. You need to evaluate and filter the search results based on the question.
- For listing-type questions (e.g., listing all flight information), try to limit the answer to 10 key points and inform the user that they can refer to the search sources for complete information. Prioritize providing the most complete and relevant items in the list. Avoid mentioning content not provided in the search results unless necessary.
- For creative tasks (e.g., writing an essay), ensure that references are cited within the body of the text, such as [citation:3][citation:5], rather than only at the end of the text. You need to interpret and summarize the user's requirements, choose an appropriate format, fully utilize the search results, extract key information, and generate an answer that is insightful, creative, and professional. Extend the length of your response as much as possible, addressing each point in detail and from multiple perspectives, ensuring the content is rich and thorough.
- If the response is lengthy, structure it well and summarize it in paragraphs. If a point-by-point format is needed, try to limit it to 5 points and merge related content.
- For objective Q&A, if the answer is very brief, you may add one or two related sentences to enrich the content.
- Choose an appropriate and visually appealing format for your response based on the user's requirements and the content of the answer, ensuring strong readability.
- Your answer should synthesize information from multiple relevant webpages and avoid repeatedly citing the same webpage.
- Unless the user requests otherwise, your response should be in the same language as the user's question.

# The user's message is:
{question}'''
```

## 7. Licence
Ce dÃ©pÃ´t de code et les poids du modÃ¨le sont licenciÃ©s sous la [Licence MIT](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE).  
La sÃ©rie DeepSeek-R1 prend en charge l'utilisation commerciale, permet toute modification et travaux dÃ©rivÃ©s, y compris, mais sans s'y limiter, la distillation pour entraÃ®ner d'autres LLMs. Veuillez noter que :
- DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B et DeepSeek-R1-Distill-Qwen-32B sont dÃ©rivÃ©s de la [sÃ©rie Qwen-2.5](https://github.com/QwenLM/Qwen2.5), qui sont initialement sous licence [Apache 2.0 License](https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE), et maintenant affinÃ©s avec 800k exemples sÃ©lectionnÃ©s avec DeepSeek-R1.
- DeepSeek-R1-Distill-Llama-8B est dÃ©rivÃ© de Llama3.1-8B-Base et est initialement sous licence [Llama3.1 license](https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE).
- DeepSeek-R1-Distill-Llama-70B est dÃ©rivÃ© de Llama3.3-70B-Instruct et est initialement sous licence [Llama3.3 license](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE).

## 8. Citation
```bibtex
@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.12948}, 
}
```

## 9. Contact
Si vous avez des questions, veuillez ouvrir une issue ou nous contacter Ã  [service@deepseek.com](mailto:service@deepseek.com).


---

[Powered By OpenAiTx](https://github.com/OpenAiTx/OpenAiTx)

---