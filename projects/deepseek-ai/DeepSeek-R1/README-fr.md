# DeepSeek-R1
<!-- markdownlint-disable first-line-h1 -->
<!-- markdownlint-disable html -->
<!-- markdownlint-disable no-duplicate-header -->

<div align="center">
  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-R1" />
</div>
<hr>
<div align="center" style="line-height: 1;">
  <a href="https://www.deepseek.com/" target="_blank"><img alt="Homepage"
    src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true"/></a>
  <a href="https://chat.deepseek.com/" target="_blank"><img alt="Chat"
    src="https://img.shields.io/badge/ü§ñ%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white"/></a>
  <a href="https://huggingface.co/deepseek-ai" target="_blank"><img alt="Hugging Face"
    src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white"/></a>
  <br>
  <a href="https://discord.gg/Tc7c45Zzu5" target="_blank"><img alt="Discord"
    src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da"/></a>
  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true" target="_blank"><img alt="WeChat"
    src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white"/></a>
  <a href="https://twitter.com/deepseek_ai" target="_blank"><img alt="Twitter Follow"
    src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white"/></a>
  <br>
  <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE"><img alt="License"
    src="https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53"/></a>
  <br>
  <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf"><b>Lien vers l'article</b>üëÅÔ∏è</a>
</div>

## 1. Introduction

Nous pr√©sentons nos mod√®les de raisonnement de premi√®re g√©n√©ration, DeepSeek-R1-Zero et DeepSeek-R1.  
DeepSeek-R1-Zero, un mod√®le entra√Æn√© via un apprentissage par renforcement (RL) √† grande √©chelle sans ajustement supervis√© (SFT) comme √©tape pr√©liminaire, a d√©montr√© des performances remarquables en raisonnement.  
Gr√¢ce au RL, DeepSeek-R1-Zero a naturellement d√©velopp√© de nombreux comportements de raisonnement puissants et int√©ressants.  
Cependant, DeepSeek-R1-Zero rencontre des probl√®mes tels que des r√©p√©titions infinies, une faible lisibilit√© et le m√©lange des langues. Pour r√©soudre ces probl√®mes et am√©liorer davantage les performances de raisonnement,  
nous introduisons DeepSeek-R1, qui int√®gre des donn√©es de d√©marrage √† froid avant le RL.  
DeepSeek-R1 atteint des performances comparables √† OpenAI-o1 sur les t√¢ches de math√©matiques, de code et de raisonnement.  
Pour soutenir la communaut√© de recherche, nous avons ouvert DeepSeek-R1-Zero, DeepSeek-R1 et six mod√®les denses distill√©s √† partir de DeepSeek-R1 bas√©s sur Llama et Qwen. DeepSeek-R1-Distill-Qwen-32B surpasse OpenAI-o1-mini sur divers benchmarks, √©tablissant de nouveaux r√©sultats SOTA pour les mod√®les denses.

**REMARQUE : Avant d'ex√©cuter localement les mod√®les de la s√©rie DeepSeek-R1, nous vous recommandons vivement de consulter la section [Recommandations d'utilisation](#usage-recommendations).**

<p align="center">
  <img width="80%" src="figures/benchmark.jpg">
</p>

## 2. R√©sum√© du mod√®le

---

**Post-entra√Ænement : Apprentissage par renforcement √† grande √©chelle sur le mod√®le de base**

-  Nous appliquons directement l'apprentissage par renforcement (RL) au mod√®le de base sans recourir √† l'ajustement supervis√© (SFT) comme √©tape pr√©liminaire. Cette approche permet au mod√®le d'explorer le raisonnement en cha√Æne (CoT) pour r√©soudre des probl√®mes complexes, ce qui aboutit au d√©veloppement de DeepSeek-R1-Zero. DeepSeek-R1-Zero d√©montre des capacit√©s telles que l'auto-v√©rification, la r√©flexion et la g√©n√©ration de longues cha√Ænes de raisonnement, marquant une √©tape importante pour la communaut√© de recherche. Notamment, il s'agit de la premi√®re recherche ouverte √† valider que les capacit√©s de raisonnement des LLMs peuvent √™tre stimul√©es uniquement via le RL, sans besoin de SFT. Cette avanc√©e ouvre la voie √† de futurs progr√®s dans ce domaine.

-   Nous pr√©sentons notre pipeline pour d√©velopper DeepSeek-R1. Le pipeline int√®gre deux √©tapes de RL visant √† d√©couvrir de meilleurs sch√©mas de raisonnement et √† s'aligner sur les pr√©f√©rences humaines, ainsi que deux √©tapes de SFT qui servent de graine pour les capacit√©s de raisonnement et non-raisonnement du mod√®le.  
    Nous pensons que ce pipeline b√©n√©ficiera √† l'industrie en cr√©ant de meilleurs mod√®les.

---

**Distillation : Les petits mod√®les peuvent aussi √™tre puissants**

-  Nous d√©montrons que les sch√©mas de raisonnement des mod√®les plus grands peuvent √™tre distill√©s dans des mod√®les plus petits, donnant de meilleures performances que les sch√©mas de raisonnement d√©couverts par RL sur des petits mod√®les. Le DeepSeek-R1 open source, ainsi que son API, permettront √† la communaut√© de recherche de distiller de meilleurs petits mod√®les √† l'avenir.
-  En utilisant les donn√©es de raisonnement g√©n√©r√©es par DeepSeek-R1, nous avons affin√© plusieurs mod√®les denses largement utilis√©s dans la communaut√© de recherche. Les r√©sultats d'√©valuation d√©montrent que les petits mod√®les denses distill√©s performent exceptionnellement bien sur les benchmarks. Nous publions en open source les checkpoints distill√©s 1.5B, 7B, 8B, 14B, 32B et 70B bas√©s sur Qwen2.5 et la s√©rie Llama3 √† la communaut√©.

## 3. T√©l√©chargement des mod√®les

### Mod√®les DeepSeek-R1

<div align="center">

| **Mod√®le**         | **#Param√®tres totaux** | **#Param√®tres activ√©s** | **Longueur du contexte** | **T√©l√©charger** |
| :----------------: | :-------------------: | :---------------------: | :----------------------: | :-------------: |
| DeepSeek-R1-Zero   | 671B                  | 37B                     | 128K                     | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)   |
| DeepSeek-R1        | 671B                  | 37B                     | 128K                     | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   |

</div>

DeepSeek-R1-Zero & DeepSeek-R1 sont entra√Æn√©s √† partir de DeepSeek-V3-Base.  
Pour plus de d√©tails concernant l'architecture du mod√®le, veuillez vous r√©f√©rer au d√©p√¥t [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3).

### Mod√®les DeepSeek-R1-Distill

<div align="center">

| **Mod√®le** | **Mod√®le de base** | **T√©l√©charger** |
| :----------------------: | :---------------------: | :-----------------: |
| DeepSeek-R1-Distill-Qwen-1.5B  | [Qwen2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)   |
| DeepSeek-R1-Distill-Qwen-7B  | [Qwen2.5-Math-7B](https://huggingface.co/Qwen/Qwen2.5-Math-7B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)   |
| DeepSeek-R1-Distill-Llama-8B  | [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)   |
| DeepSeek-R1-Distill-Qwen-14B   | [Qwen2.5-14B](https://huggingface.co/Qwen/Qwen2.5-14B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)   |
|DeepSeek-R1-Distill-Qwen-32B  | [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)   |
| DeepSeek-R1-Distill-Llama-70B  | [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)   |

</div>

Les mod√®les DeepSeek-R1-Distill sont affin√©s √† partir de mod√®les open source, en utilisant des exemples g√©n√©r√©s par DeepSeek-R1.  
Nous avons l√©g√®rement modifi√© leurs configurations et leurs tokenizers. Veuillez utiliser notre configuration pour ex√©cuter ces mod√®les.

## 4. R√©sultats d'√©valuation

### DeepSeek-R1-√âvaluation
Pour tous nos mod√®les, la longueur maximale de g√©n√©ration est fix√©e √† 32 768 tokens. Pour les benchmarks n√©cessitant un √©chantillonnage, nous utilisons une temp√©rature de $0,6$, une valeur top-p de $0,95$ et g√©n√©rons 64 r√©ponses par requ√™te pour estimer pass@1.
<div align="center">


| Cat√©gorie | Benchmark (M√©trique) | Claude-3.5-Sonnet-1022 | GPT-4o 0513 | DeepSeek V3 | OpenAI o1-mini | OpenAI o1-1217 | DeepSeek R1 |
|----------|-------------------|----------------------|------------|--------------|----------------|------------|--------------|
| | Architecture | - | - | MoE | - | - | MoE |
| | # Param√®tres activ√©s | - | - | 37B | - | - | 37B |
| | # Param√®tres totaux | - | - | 671B | - | - | 671B |
| Anglais | MMLU (Pass@1) | 88.3 | 87.2 | 88.5 | 85.2 | **91.8** | 90.8 |
| | MMLU-Redux (EM) | 88.9 | 88.0 | 89.1 | 86.7 | - | **92.9** |
| | MMLU-Pro (EM) | 78.0 | 72.6 | 75.9 | 80.3 | - | **84.0** |
| | DROP (3-shot F1) | 88.3 | 83.7 | 91.6 | 83.9 | 90.2 | **92.2** |
| | IF-Eval (Prompt Strict) | **86.5** | 84.3 | 86.1 | 84.8 | - | 83.3 |
| | GPQA-Diamond (Pass@1) | 65.0 | 49.9 | 59.1 | 60.0 | **75.7** | 71.5 |
| | SimpleQA (Correct) | 28.4 | 38.2 | 24.9 | 7.0 | **47.0** | 30.1 |
| | FRAMES (Acc.) | 72.5 | 80.5 | 73.3 | 76.9 | - | **82.5** |
| | AlpacaEval2.0 (LC-winrate) | 52.0 | 51.1 | 70.0 | 57.8 | - | **87.6** |
| | ArenaHard (GPT-4-1106) | 85.2 | 80.4 | 85.5 | 92.0 | - | **92.3** |
| Code | LiveCodeBench (Pass@1-COT) | 33.8 | 34.2 | - | 53.8 | 63.4 | **65.9** |
| | Codeforces (Percentile) | 20.3 | 23.6 | 58.7 | 93.4 | **96.6** | 96.3 |
| | Codeforces (Rating) | 717 | 759 | 1134 | 1820 | **2061** | 2029 |
| | SWE Verified (R√©solu) | **50.8** | 38.8 | 42.0 | 41.6 | 48.9 | 49.2 |
| | Aider-Polyglot (Acc.) | 45.3 | 16.0 | 49.6 | 32.9 | **61.7** | 53.3 |
| Math | AIME 2024 (Pass@1) | 16.0 | 9.3 | 39.2 | 63.6 | 79.2 | **79.8** |
| | MATH-500 (Pass@1) | 78.3 | 74.6 | 90.2 | 90.0 | 96.4 | **97.3** |
| | CNMO 2024 (Pass@1) | 13.1 | 10.8 | 43.2 | 67.6 | - | **78.8** |
| Chinois | CLUEWSC (EM) | 85.4 | 87.9 | 90.9 | 89.9 | - | **92.8** |
| | C-Eval (EM) | 76.7 | 76.0 | 86.5 | 68.9 | - | **91.8** |
| | C-SimpleQA (Correct) | 55.4 | 58.7 | **68.0** | 40.3 | - | 63.7 |

</div>


### √âvaluation des mod√®les distill√©s


<div align="center">

| Mod√®le                                   | AIME 2024 pass@1 | AIME 2024 cons@64 | MATH-500 pass@1 | GPQA Diamond pass@1 | LiveCodeBench pass@1 | CodeForces rating |
|------------------------------------------|------------------|-------------------|-----------------|----------------------|----------------------|-------------------|
| GPT-4o-0513                          | 9.3              | 13.4              | 74.6            | 49.9                 | 32.9                 | 759               |
| Claude-3.5-Sonnet-1022             | 16.0             | 26.7                 | 78.3            | 65.0                 | 38.9                 | 717               |
| o1-mini                              | 63.6             | 80.0              | 90.0            | 60.0                 | 53.8                 | **1820**          |
| QwQ-32B-Preview                              | 44.0             | 60.0                 | 90.6            | 54.5               | 41.9                 | 1316              |
| DeepSeek-R1-Distill-Qwen-1.5B       | 28.9             | 52.7              | 83.9            | 33.8                 | 16.9                 | 954               |
| DeepSeek-R1-Distill-Qwen-7B          | 55.5             | 83.3              | 92.8            | 49.1                 | 37.6                 | 1189              |
| DeepSeek-R1-Distill-Qwen-14B         | 69.7             | 80.0              | 93.9            | 59.1                 | 53.1                 | 1481              |
| DeepSeek-R1-Distill-Qwen-32B        | **72.6**         | 83.3              | 94.3            | 62.1                 | 57.2                 | 1691              |
| DeepSeek-R1-Distill-Llama-8B         | 50.4             | 80.0              | 89.1            | 49.0                 | 39.6                 | 1205              |
| DeepSeek-R1-Distill-Llama-70B        | 70.0             | **86.7**          | **94.5**        | **65.2**             | **57.5**             | 1633              |

</div>


## 5. Site de chat & Plateforme API
Vous pouvez discuter avec DeepSeek-R1 sur le site officiel de DeepSeek : [chat.deepseek.com](https://chat.deepseek.com), et activer le bouton "DeepThink".

Nous proposons √©galement une API compatible OpenAI sur la plateforme DeepSeek : [platform.deepseek.com](https://platform.deepseek.com/)

## 6. Comment ex√©cuter en local

### Mod√®les DeepSeek-R1

Veuillez consulter le d√©p√¥t [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) pour plus d'informations sur l'ex√©cution locale de DeepSeek-R1.

**REMARQUE : Transformers de Hugging Face n'est pas encore directement support√©.**

### Mod√®les DeepSeek-R1-Distill

Les mod√®les DeepSeek-R1-Distill peuvent √™tre utilis√©s de la m√™me mani√®re que les mod√®les Qwen ou Llama.

Par exemple, vous pouvez facilement d√©marrer un service avec [vLLM](https://github.com/vllm-project/vllm) :

```shell
vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager
```

Vous pouvez aussi facilement d√©marrer un service avec [SGLang](https://github.com/sgl-project/sglang)

```bash
python3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2
```

### Recommandations d'utilisation

**Nous recommandons de respecter les configurations suivantes lors de l'utilisation des mod√®les de la s√©rie DeepSeek-R1, y compris pour les benchmarks, afin d'obtenir les performances attendues :**

1. R√©glez la temp√©rature entre 0,5 et 0,7 (0,6 recommand√©) pour √©viter les r√©p√©titions infinies ou les sorties incoh√©rentes.
2. **√âvitez d'ajouter un prompt syst√®me ; toutes les instructions doivent √™tre contenues dans le prompt utilisateur.**
3. Pour les probl√®mes math√©matiques, il est conseill√© d'inclure une directive dans votre prompt telle que : "Veuillez raisonner √©tape par √©tape et placer votre r√©ponse finale dans \boxed{}."
4. Lors de l'√©valuation des performances du mod√®le, il est recommand√© d'effectuer plusieurs tests et de faire la moyenne des r√©sultats.

De plus, nous avons observ√© que les mod√®les de la s√©rie DeepSeek-R1 ont tendance √† ignorer le sch√©ma de r√©flexion (c'est-√†-dire √† sortir "\<think\>\n\n\</think\>") lors de certaines requ√™tes, ce qui peut affecter n√©gativement les performances du mod√®le.  
**Pour garantir un raisonnement approfondi, nous recommandons de forcer le mod√®le √† commencer sa r√©ponse par "\<think\>\n" au d√©but de chaque sortie.**

### Prompts officiels
Dans l'application web/app officielle DeepSeek, nous n'utilisons pas de prompts syst√®me mais concevons deux prompts sp√©cifiques pour l'envoi de fichiers et la recherche web afin d'am√©liorer l'exp√©rience utilisateur. De plus, la temp√©rature dans le web/app est de 0,6.

Pour l'envoi de fichier, veuillez suivre le mod√®le pour cr√©er les prompts, o√π {file_name}, {file_content} et {question} sont des arguments.
```
file_template = \
"""[file name]: {file_name}
[file content begin]
{file_content}
[file content end]
{question}"""
```

Pour la recherche web, {search_results}, {cur_date}, et {question} sont des arguments.

Pour une requ√™te en chinois, nous utilisons le prompt :
```
search_answer_zh_template = \
'''# ‰ª•‰∏ãÂÜÖÂÆπÊòØÂü∫‰∫éÁî®Êà∑ÂèëÈÄÅÁöÑÊ∂àÊÅØÁöÑÊêúÁ¥¢ÁªìÊûú:
{search_results}
Âú®ÊàëÁªô‰Ω†ÁöÑÊêúÁ¥¢ÁªìÊûú‰∏≠ÔºåÊØè‰∏™ÁªìÊûúÈÉΩÊòØ[webpage X begin]...[webpage X end]Ê†ºÂºèÁöÑÔºåX‰ª£Ë°®ÊØèÁØáÊñáÁ´†ÁöÑÊï∞Â≠óÁ¥¢Âºï„ÄÇËØ∑Âú®ÈÄÇÂΩìÁöÑÊÉÖÂÜµ‰∏ãÂú®Âè•Â≠êÊú´Â∞æÂºïÁî®‰∏ä‰∏ãÊñá„ÄÇËØ∑ÊåâÁÖßÂºïÁî®ÁºñÂè∑[citation:X]ÁöÑÊ†ºÂºèÂú®Á≠îÊ°à‰∏≠ÂØπÂ∫îÈÉ®ÂàÜÂºïÁî®‰∏ä‰∏ãÊñá„ÄÇÂ¶ÇÊûú‰∏ÄÂè•ËØùÊ∫êËá™Â§ö‰∏™‰∏ä‰∏ãÊñáÔºåËØ∑ÂàóÂá∫ÊâÄÊúâÁõ∏ÂÖ≥ÁöÑÂºïÁî®ÁºñÂè∑Ôºå‰æãÂ¶Ç[citation:3][citation:5]ÔºåÂàáËÆ∞‰∏çË¶ÅÂ∞ÜÂºïÁî®ÈõÜ‰∏≠Âú®ÊúÄÂêéËøîÂõûÂºïÁî®ÁºñÂè∑ÔºåËÄåÊòØÂú®Á≠îÊ°àÂØπÂ∫îÈÉ®ÂàÜÂàóÂá∫„ÄÇ
Âú®ÂõûÁ≠îÊó∂ÔºåËØ∑Ê≥®ÊÑè‰ª•‰∏ãÂá†ÁÇπÔºö
- ‰ªäÂ§©ÊòØ{cur_date}„ÄÇ
- Âπ∂ÈùûÊêúÁ¥¢ÁªìÊûúÁöÑÊâÄÊúâÂÜÖÂÆπÈÉΩ‰∏éÁî®Êà∑ÁöÑÈóÆÈ¢òÂØÜÂàáÁõ∏ÂÖ≥Ôºå‰Ω†ÈúÄË¶ÅÁªìÂêàÈóÆÈ¢òÔºåÂØπÊêúÁ¥¢ÁªìÊûúËøõË°åÁîÑÂà´„ÄÅÁ≠õÈÄâ„ÄÇ
- ÂØπ‰∫éÂàó‰∏æÁ±ªÁöÑÈóÆÈ¢òÔºàÂ¶ÇÂàó‰∏æÊâÄÊúâËà™Áè≠‰ø°ÊÅØÔºâÔºåÂ∞ΩÈáèÂ∞ÜÁ≠îÊ°àÊéßÂà∂Âú®10‰∏™Ë¶ÅÁÇπ‰ª•ÂÜÖÔºåÂπ∂ÂëäËØâÁî®Êà∑ÂèØ‰ª•Êü•ÁúãÊêúÁ¥¢Êù•Ê∫ê„ÄÅËé∑ÂæóÂÆåÊï¥‰ø°ÊÅØ„ÄÇ‰ºòÂÖàÊèê‰æõ‰ø°ÊÅØÂÆåÊï¥„ÄÅÊúÄÁõ∏ÂÖ≥ÁöÑÂàó‰∏æÈ°πÔºõÂ¶ÇÈùûÂøÖË¶ÅÔºå‰∏çË¶Å‰∏ªÂä®ÂëäËØâÁî®Êà∑ÊêúÁ¥¢ÁªìÊûúÊú™Êèê‰æõÁöÑÂÜÖÂÆπ„ÄÇ
- ÂØπ‰∫éÂàõ‰ΩúÁ±ªÁöÑÈóÆÈ¢òÔºàÂ¶ÇÂÜôËÆ∫ÊñáÔºâÔºåËØ∑Âä°ÂøÖÂú®Ê≠£ÊñáÁöÑÊÆµËêΩ‰∏≠ÂºïÁî®ÂØπÂ∫îÁöÑÂèÇËÄÉÁºñÂè∑Ôºå‰æãÂ¶Ç[citation:3][citation:5]Ôºå‰∏çËÉΩÂè™Âú®ÊñáÁ´†Êú´Â∞æÂºïÁî®„ÄÇ‰Ω†ÈúÄË¶ÅËß£ËØªÂπ∂Ê¶ÇÊã¨Áî®Êà∑ÁöÑÈ¢òÁõÆË¶ÅÊ±ÇÔºåÈÄâÊã©ÂêàÈÄÇÁöÑÊ†ºÂºèÔºåÂÖÖÂàÜÂà©Áî®ÊêúÁ¥¢ÁªìÊûúÂπ∂ÊäΩÂèñÈáçË¶Å‰ø°ÊÅØÔºåÁîüÊàêÁ¨¶ÂêàÁî®Êà∑Ë¶ÅÊ±Ç„ÄÅÊûÅÂÖ∑ÊÄùÊÉ≥Ê∑±Â∫¶„ÄÅÂØåÊúâÂàõÈÄ†Âäõ‰∏é‰∏ì‰∏öÊÄßÁöÑÁ≠îÊ°à„ÄÇ‰Ω†ÁöÑÂàõ‰ΩúÁØáÂπÖÈúÄË¶ÅÂ∞ΩÂèØËÉΩÂª∂ÈïøÔºåÂØπ‰∫éÊØè‰∏Ä‰∏™Ë¶ÅÁÇπÁöÑËÆ∫Ëø∞Ë¶ÅÊé®ÊµãÁî®Êà∑ÁöÑÊÑèÂõæÔºåÁªôÂá∫Â∞ΩÂèØËÉΩÂ§öËßíÂ∫¶ÁöÑÂõûÁ≠îË¶ÅÁÇπÔºå‰∏îÂä°ÂøÖ‰ø°ÊÅØÈáèÂ§ß„ÄÅËÆ∫Ëø∞ËØ¶Â∞Ω„ÄÇ
- Â¶ÇÊûúÂõûÁ≠îÂæàÈïøÔºåËØ∑Â∞ΩÈáèÁªìÊûÑÂåñ„ÄÅÂàÜÊÆµËêΩÊÄªÁªì„ÄÇÂ¶ÇÊûúÈúÄË¶ÅÂàÜÁÇπ‰ΩúÁ≠îÔºåÂ∞ΩÈáèÊéßÂà∂Âú®5‰∏™ÁÇπ‰ª•ÂÜÖÔºåÂπ∂ÂêàÂπ∂Áõ∏ÂÖ≥ÁöÑÂÜÖÂÆπ„ÄÇ
- ÂØπ‰∫éÂÆ¢ËßÇÁ±ªÁöÑÈóÆÁ≠îÔºåÂ¶ÇÊûúÈóÆÈ¢òÁöÑÁ≠îÊ°àÈùûÂ∏∏ÁÆÄÁü≠ÔºåÂèØ‰ª•ÈÄÇÂΩìË°•ÂÖÖ‰∏ÄÂà∞‰∏§Âè•Áõ∏ÂÖ≥‰ø°ÊÅØÔºå‰ª•‰∏∞ÂØåÂÜÖÂÆπ„ÄÇ
- ‰Ω†ÈúÄË¶ÅÊ†πÊçÆÁî®Êà∑Ë¶ÅÊ±ÇÂíåÂõûÁ≠îÂÜÖÂÆπÈÄâÊã©ÂêàÈÄÇ„ÄÅÁæéËßÇÁöÑÂõûÁ≠îÊ†ºÂºèÔºåÁ°Æ‰øùÂèØËØªÊÄßÂº∫„ÄÇ
- ‰Ω†ÁöÑÂõûÁ≠îÂ∫îËØ•ÁªºÂêàÂ§ö‰∏™Áõ∏ÂÖ≥ÁΩëÈ°µÊù•ÂõûÁ≠îÔºå‰∏çËÉΩÈáçÂ§çÂºïÁî®‰∏Ä‰∏™ÁΩëÈ°µ„ÄÇ
- Èô§ÈùûÁî®Êà∑Ë¶ÅÊ±ÇÔºåÂê¶Âàô‰Ω†ÂõûÁ≠îÁöÑËØ≠Ë®ÄÈúÄË¶ÅÂíåÁî®Êà∑ÊèêÈóÆÁöÑËØ≠Ë®Ä‰øùÊåÅ‰∏ÄËá¥„ÄÇ

# Áî®Êà∑Ê∂àÊÅØ‰∏∫Ôºö
{question}'''
```


Pour une requ√™te en anglais, nous utilisons le prompt :
```
search_answer_en_template = \
'''# The following contents are the search results related to the user's message:
{search_results}
In the search results I provide to you, each result is formatted as [webpage X begin]...[webpage X end], where X represents the numerical index of each article. Please cite the context at the end of the relevant sentence when appropriate. Use the citation format [citation:X] in the corresponding part of your answer. If a sentence is derived from multiple contexts, list all relevant citation numbers, such as [citation:3][citation:5]. Be sure not to cluster all citations at the end; instead, include them in the corresponding parts of the answer.
When responding, please keep the following points in mind:
- Today is {cur_date}.
- Not all content in the search results is closely related to the user's question. You need to evaluate and filter the search results based on the question.
- For listing-type questions (e.g., listing all flight information), try to limit the answer to 10 key points and inform the user that they can refer to the search sources for complete information. Prioritize providing the most complete and relevant items in the list. Avoid mentioning content not provided in the search results unless necessary.
- For creative tasks (e.g., writing an essay), ensure that references are cited within the body of the text, such as [citation:3][citation:5], rather than only at the end of the text. You need to interpret and summarize the user's requirements, choose an appropriate format, fully utilize the search results, extract key information, and generate an answer that is insightful, creative, and professional. Extend the length of your response as much as possible, addressing each point in detail and from multiple perspectives, ensuring the content is rich and thorough.
- If the response is lengthy, structure it well and summarize it in paragraphs. If a point-by-point format is needed, try to limit it to 5 points and merge related content.
- For objective Q&A, if the answer is very brief, you may add one or two related sentences to enrich the content.
- Choose an appropriate and visually appealing format for your response based on the user's requirements and the content of the answer, ensuring strong readability.
- Your answer should synthesize information from multiple relevant webpages and avoid repeatedly citing the same webpage.
- Unless the user requests otherwise, your response should be in the same language as the user's question.

# The user's message is:
{question}'''
```

## 7. Licence
Ce d√©p√¥t de code et les poids du mod√®le sont licenci√©s sous la [Licence MIT](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE).  
La s√©rie DeepSeek-R1 prend en charge l'utilisation commerciale, permet toute modification et travaux d√©riv√©s, y compris, mais sans s'y limiter, la distillation pour entra√Æner d'autres LLMs. Veuillez noter que :
- DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B et DeepSeek-R1-Distill-Qwen-32B sont d√©riv√©s de la [s√©rie Qwen-2.5](https://github.com/QwenLM/Qwen2.5), qui sont initialement sous licence [Apache 2.0 License](https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE), et maintenant affin√©s avec 800k exemples s√©lectionn√©s avec DeepSeek-R1.
- DeepSeek-R1-Distill-Llama-8B est d√©riv√© de Llama3.1-8B-Base et est initialement sous licence [Llama3.1 license](https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE).
- DeepSeek-R1-Distill-Llama-70B est d√©riv√© de Llama3.3-70B-Instruct et est initialement sous licence [Llama3.3 license](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE).

## 8. Citation
```bibtex
@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.12948}, 
}
```

## 9. Contact
Si vous avez des questions, veuillez ouvrir une issue ou nous contacter √† [service@deepseek.com](mailto:service@deepseek.com).


---

[Powered By OpenAiTx](https://github.com/OpenAiTx/OpenAiTx)

---