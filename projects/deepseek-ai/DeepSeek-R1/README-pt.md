# DeepSeek-R1
<!-- markdownlint-disable first-line-h1 -->
<!-- markdownlint-disable html -->
<!-- markdownlint-disable no-duplicate-header -->

<div align="center">
  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-R1" />
</div>
<hr>
<div align="center" style="line-height: 1;">
  <a href="https://www.deepseek.com/" target="_blank"><img alt="Homepage"
    src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true"/></a>
  <a href="https://chat.deepseek.com/" target="_blank"><img alt="Chat"
    src="https://img.shields.io/badge/ğŸ¤–%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white"/></a>
  <a href="https://huggingface.co/deepseek-ai" target="_blank"><img alt="Hugging Face"
    src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white"/></a>
  <br>
  <a href="https://discord.gg/Tc7c45Zzu5" target="_blank"><img alt="Discord"
    src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da"/></a>
  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true" target="_blank"><img alt="WeChat"
    src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white"/></a>
  <a href="https://twitter.com/deepseek_ai" target="_blank"><img alt="Twitter Follow"
    src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white"/></a>
  <br>
  <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE"><img alt="License"
    src="https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53"/></a>
  <br>
  <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf"><b>Link do Artigo</b>ğŸ‘ï¸</a>
</div>

## 1. IntroduÃ§Ã£o

Apresentamos nossos modelos de raciocÃ­nio de primeira geraÃ§Ã£o, DeepSeek-R1-Zero e DeepSeek-R1.  
O DeepSeek-R1-Zero, um modelo treinado via aprendizado por reforÃ§o em larga escala (RL) sem ajuste fino supervisionado (SFT) como etapa preliminar, demonstrou desempenho notÃ¡vel em raciocÃ­nio.  
Com RL, o DeepSeek-R1-Zero emergiu naturalmente com diversos comportamentos poderosos e interessantes de raciocÃ­nio.  
No entanto, o DeepSeek-R1-Zero enfrenta desafios como repetiÃ§Ã£o sem fim, baixa legibilidade e mistura de idiomas. Para abordar essas questÃµes e aprimorar ainda mais o desempenho de raciocÃ­nio,  
apresentamos o DeepSeek-R1, que incorpora dados de inÃ­cio a frio antes do RL.  
O DeepSeek-R1 alcanÃ§a desempenho comparÃ¡vel ao OpenAI-o1 em tarefas de matemÃ¡tica, cÃ³digo e raciocÃ­nio.  
Para apoiar a comunidade de pesquisa, abrimos o cÃ³digo-fonte do DeepSeek-R1-Zero, DeepSeek-R1 e seis modelos densos destilados do DeepSeek-R1 baseados em Llama e Qwen. O DeepSeek-R1-Distill-Qwen-32B supera o OpenAI-o1-mini em vÃ¡rios benchmarks, alcanÃ§ando novos resultados de estado da arte para modelos densos.

**NOTA: Antes de executar localmente os modelos da sÃ©rie DeepSeek-R1, recomendamos gentilmente revisar a seÃ§Ã£o [RecomendaÃ§Ãµes de Uso](#usage-recommendations).**

<p align="center">
  <img width="80%" src="figures/benchmark.jpg">
</p>

## 2. Resumo do Modelo

---

**PÃ³s-Treinamento: Aprendizado por ReforÃ§o em Larga Escala no Modelo Base**

- Aplicamos diretamente o aprendizado por reforÃ§o (RL) ao modelo base sem depender de ajuste fino supervisionado (SFT) como etapa preliminar. Essa abordagem permite que o modelo explore cadeias de pensamento (CoT) para resolver problemas complexos, resultando no desenvolvimento do DeepSeek-R1-Zero. O DeepSeek-R1-Zero demonstra capacidades como auto-verificaÃ§Ã£o, reflexÃ£o e geraÃ§Ã£o de longas cadeias de pensamento, marcando um marco importante para a comunidade de pesquisa. Notavelmente, Ã© a primeira pesquisa aberta a validar que as capacidades de raciocÃ­nio de LLMs podem ser incentivadas puramente por RL, sem a necessidade de SFT. Este avanÃ§o abre caminho para futuros desenvolvimentos nesta Ã¡rea.

- Apresentamos nosso pipeline para desenvolver o DeepSeek-R1. O pipeline incorpora dois estÃ¡gios de RL visando descobrir padrÃµes de raciocÃ­nio aprimorados e alinhar com preferÃªncias humanas, alÃ©m de dois estÃ¡gios de SFT que servem como semente para as capacidades de raciocÃ­nio e nÃ£o raciocÃ­nio do modelo.  
  Acreditamos que o pipeline beneficiarÃ¡ a indÃºstria ao criar melhores modelos.

---

**DestilaÃ§Ã£o: Modelos Menores TambÃ©m Podem Ser Poderosos**

- Demonstramos que os padrÃµes de raciocÃ­nio de modelos maiores podem ser destilados em modelos menores, resultando em desempenho superior aos padrÃµes de raciocÃ­nio descobertos por RL em modelos pequenos. O DeepSeek-R1 de cÃ³digo aberto, bem como sua API, beneficiarÃ£o a comunidade de pesquisa para destilar melhores modelos menores no futuro.  
- Usando os dados de raciocÃ­nio gerados pelo DeepSeek-R1, ajustamos vÃ¡rios modelos densos amplamente utilizados na comunidade de pesquisa. Os resultados da avaliaÃ§Ã£o demonstram que os modelos densos menores destilados apresentam desempenho excepcional em benchmarks. Abrimos os checkpoints destilados de 1.5B, 7B, 8B, 14B, 32B e 70B baseados nas sÃ©ries Qwen2.5 e Llama3 para a comunidade.

## 3. Downloads dos Modelos

### Modelos DeepSeek-R1

<div align="center">

| **Modelo** | **#Params Totais** | **#Params Ativados** | **Comprimento do Contexto** | **Download** |
| :------------: | :------------: | :------------: | :------------: | :------------: |
| DeepSeek-R1-Zero | 671B | 37B | 128K   | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)   |
| DeepSeek-R1   | 671B | 37B |  128K   | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   |

</div>

O DeepSeek-R1-Zero & DeepSeek-R1 sÃ£o treinados com base no DeepSeek-V3-Base.  
Para mais detalhes sobre a arquitetura do modelo, consulte o repositÃ³rio [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3).

### Modelos DeepSeek-R1-Distill

<div align="center">

| **Modelo** | **Modelo Base** | **Download** |
| :------------: | :------------: | :------------: |
| DeepSeek-R1-Distill-Qwen-1.5B  | [Qwen2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B) | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)   |
| DeepSeek-R1-Distill-Qwen-7B  | [Qwen2.5-Math-7B](https://huggingface.co/Qwen/Qwen2.5-Math-7B) | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)   |
| DeepSeek-R1-Distill-Llama-8B  | [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)   |
| DeepSeek-R1-Distill-Qwen-14B   | [Qwen2.5-14B](https://huggingface.co/Qwen/Qwen2.5-14B) | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)   |
|DeepSeek-R1-Distill-Qwen-32B  | [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B) | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)   |
| DeepSeek-R1-Distill-Llama-70B  | [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)   |

</div>

Os modelos DeepSeek-R1-Distill sÃ£o ajustados com base em modelos de cÃ³digo aberto, utilizando amostras geradas pelo DeepSeek-R1.  
Alteramos levemente suas configuraÃ§Ãµes e tokenizadores. Por favor, use nossa configuraÃ§Ã£o para executar esses modelos.

## 4. Resultados de AvaliaÃ§Ã£o

### AvaliaÃ§Ã£o DeepSeek-R1
Para todos os nossos modelos, o comprimento mÃ¡ximo de geraÃ§Ã£o Ã© definido como 32.768 tokens. Para benchmarks que requerem amostragem, usamos temperatura de $0.6$, valor top-p de $0.95$ e geramos 64 respostas por consulta para estimar pass@1.
<div align="center">


| Categoria | Benchmark (MÃ©trica) | Claude-3.5-Sonnet-1022 | GPT-4o 0513 | DeepSeek V3 | OpenAI o1-mini | OpenAI o1-1217 | DeepSeek R1 |
|----------|-------------------|----------------------|------------|--------------|----------------|------------|--------------|
| | Arquitetura | - | - | MoE | - | - | MoE |
| | # Params Ativados | - | - | 37B | - | - | 37B |
| | # Params Totais | - | - | 671B | - | - | 671B |
| InglÃªs | MMLU (Pass@1) | 88.3 | 87.2 | 88.5 | 85.2 | **91.8** | 90.8 |
| | MMLU-Redux (EM) | 88.9 | 88.0 | 89.1 | 86.7 | - | **92.9** |
| | MMLU-Pro (EM) | 78.0 | 72.6 | 75.9 | 80.3 | - | **84.0** |
| | DROP (3-shot F1) | 88.3 | 83.7 | 91.6 | 83.9 | 90.2 | **92.2** |
| | IF-Eval (Prompt Strict) | **86.5** | 84.3 | 86.1 | 84.8 | - | 83.3 |
| | GPQA-Diamond (Pass@1) | 65.0 | 49.9 | 59.1 | 60.0 | **75.7** | 71.5 |
| | SimpleQA (Correct) | 28.4 | 38.2 | 24.9 | 7.0 | **47.0** | 30.1 |
| | FRAMES (Acc.) | 72.5 | 80.5 | 73.3 | 76.9 | - | **82.5** |
| | AlpacaEval2.0 (LC-winrate) | 52.0 | 51.1 | 70.0 | 57.8 | - | **87.6** |
| | ArenaHard (GPT-4-1106) | 85.2 | 80.4 | 85.5 | 92.0 | - | **92.3** |
| CÃ³digo | LiveCodeBench (Pass@1-COT) | 33.8 | 34.2 | - | 53.8 | 63.4 | **65.9** |
| | Codeforces (Percentil) | 20.3 | 23.6 | 58.7 | 93.4 | **96.6** | 96.3 |
| | Codeforces (Nota) | 717 | 759 | 1134 | 1820 | **2061** | 2029 |
| | SWE Verified (Resolvido) | **50.8** | 38.8 | 42.0 | 41.6 | 48.9 | 49.2 |
| | Aider-Polyglot (Acc.) | 45.3 | 16.0 | 49.6 | 32.9 | **61.7** | 53.3 |
| MatemÃ¡tica | AIME 2024 (Pass@1) | 16.0 | 9.3 | 39.2 | 63.6 | 79.2 | **79.8** |
| | MATH-500 (Pass@1) | 78.3 | 74.6 | 90.2 | 90.0 | 96.4 | **97.3** |
| | CNMO 2024 (Pass@1) | 13.1 | 10.8 | 43.2 | 67.6 | - | **78.8** |
| ChinÃªs | CLUEWSC (EM) | 85.4 | 87.9 | 90.9 | 89.9 | - | **92.8** |
| | C-Eval (EM) | 76.7 | 76.0 | 86.5 | 68.9 | - | **91.8** |
| | C-SimpleQA (Correct) | 55.4 | 58.7 | **68.0** | 40.3 | - | 63.7 |

</div>


### AvaliaÃ§Ã£o dos Modelos Destilados

<div align="center">

| Modelo                                    | AIME 2024 pass@1 | AIME 2024 cons@64 | MATH-500 pass@1 | GPQA Diamond pass@1 | LiveCodeBench pass@1 | CodeForces nota |
|------------------------------------------|------------------|-------------------|-----------------|----------------------|----------------------|-------------------|
| GPT-4o-0513                          | 9.3              | 13.4              | 74.6            | 49.9                 | 32.9                 | 759               |
| Claude-3.5-Sonnet-1022             | 16.0             | 26.7                 | 78.3            | 65.0                 | 38.9                 | 717               |
| o1-mini                              | 63.6             | 80.0              | 90.0            | 60.0                 | 53.8                 | **1820**          |
| QwQ-32B-Preview                              | 44.0             | 60.0                 | 90.6            | 54.5               | 41.9                 | 1316              |
| DeepSeek-R1-Distill-Qwen-1.5B       | 28.9             | 52.7              | 83.9            | 33.8                 | 16.9                 | 954               |
| DeepSeek-R1-Distill-Qwen-7B          | 55.5             | 83.3              | 92.8            | 49.1                 | 37.6                 | 1189              |
| DeepSeek-R1-Distill-Qwen-14B         | 69.7             | 80.0              | 93.9            | 59.1                 | 53.1                 | 1481              |
| DeepSeek-R1-Distill-Qwen-32B        | **72.6**         | 83.3              | 94.3            | 62.1                 | 57.2                 | 1691              |
| DeepSeek-R1-Distill-Llama-8B         | 50.4             | 80.0              | 89.1            | 49.0                 | 39.6                 | 1205              |
| DeepSeek-R1-Distill-Llama-70B        | 70.0             | **86.7**          | **94.5**        | **65.2**             | **57.5**             | 1633              |

</div>

## 5. Site de Chat & Plataforma de API

VocÃª pode conversar com o DeepSeek-R1 no site oficial da DeepSeek: [chat.deepseek.com](https://chat.deepseek.com), e ativar o botÃ£o "DeepThink".

TambÃ©m fornecemos uma API compatÃ­vel com OpenAI na Plataforma DeepSeek: [platform.deepseek.com](https://platform.deepseek.com/)

## 6. Como Rodar Localmente

### Modelos DeepSeek-R1

Por favor, visite o repositÃ³rio [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) para mais informaÃ§Ãµes sobre como rodar o DeepSeek-R1 localmente.

**NOTA: O Transformers do Hugging Face ainda nÃ£o Ã© suportado diretamente.**

### Modelos DeepSeek-R1-Distill

Os modelos DeepSeek-R1-Distill podem ser utilizados da mesma forma que os modelos Qwen ou Llama.

Por exemplo, vocÃª pode facilmente iniciar um serviÃ§o usando o [vLLM](https://github.com/vllm-project/vllm):

```shell
vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager
```

VocÃª tambÃ©m pode iniciar facilmente um serviÃ§o usando o [SGLang](https://github.com/sgl-project/sglang)

```bash
python3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2
```

### RecomendaÃ§Ãµes de Uso

**Recomendamos aderir Ã s seguintes configuraÃ§Ãµes ao utilizar os modelos da sÃ©rie DeepSeek-R1, incluindo benchmarking, para obter o desempenho esperado:**

1. Defina a temperatura dentro da faixa de 0.5-0.7 (0.6 Ã© recomendado) para evitar repetiÃ§Ãµes interminÃ¡veis ou saÃ­das incoerentes.
2. **Evite adicionar um prompt de sistema; todas as instruÃ§Ãµes devem estar contidas no prompt do usuÃ¡rio.**
3. Para problemas matemÃ¡ticos, Ã© aconselhÃ¡vel incluir uma diretiva no seu prompt, como: "Por favor, raciocine passo a passo e coloque sua resposta final dentro de \boxed{}."
4. Ao avaliar o desempenho do modelo, recomenda-se realizar vÃ¡rios testes e calcular a mÃ©dia dos resultados.

AlÃ©m disso, observamos que os modelos da sÃ©rie DeepSeek-R1 tendem a ignorar o padrÃ£o de pensamento (ou seja, ao gerar "\<think\>\n\n\</think\>") ao responder a determinadas consultas, o que pode prejudicar o desempenho do modelo.  
**Para garantir que o modelo realize um raciocÃ­nio completo, recomendamos forÃ§ar o modelo a iniciar sua resposta com "\<think\>\n" no inÃ­cio de cada saÃ­da.**

### Prompts Oficiais
No site/app oficial da DeepSeek, nÃ£o usamos prompts de sistema, mas projetamos dois prompts especÃ­ficos para upload de arquivos e busca na web para melhor experiÃªncia do usuÃ¡rio. AlÃ©m disso, a temperatura no site/app Ã© 0.6.

Para upload de arquivos, siga o template para criar prompts, onde {file_name}, {file_content} e {question} sÃ£o argumentos. 
```
file_template = \
"""[file name]: {file_name}
[file content begin]
{file_content}
[file content end]
{question}"""
```

Para Busca na Web, {search_results}, {cur_date} e {question} sÃ£o argumentos. 

Para consulta em chinÃªs, usamos o prompt:
```
search_answer_zh_template = \
'''# ä»¥ä¸‹å†…å®¹æ˜¯åŸºäºç”¨æˆ·å‘é€çš„æ¶ˆæ¯çš„æœç´¢ç»“æœ:
{search_results}
åœ¨æˆ‘ç»™ä½ çš„æœç´¢ç»“æœä¸­ï¼Œæ¯ä¸ªç»“æœéƒ½æ˜¯[webpage X begin]...[webpage X end]æ ¼å¼çš„ï¼ŒXä»£è¡¨æ¯ç¯‡æ–‡ç« çš„æ•°å­—ç´¢å¼•ã€‚è¯·åœ¨é€‚å½“çš„æƒ…å†µä¸‹åœ¨å¥å­æœ«å°¾å¼•ç”¨ä¸Šä¸‹æ–‡ã€‚è¯·æŒ‰ç…§å¼•ç”¨ç¼–å·[citation:X]çš„æ ¼å¼åœ¨ç­”æ¡ˆä¸­å¯¹åº”éƒ¨åˆ†å¼•ç”¨ä¸Šä¸‹æ–‡ã€‚å¦‚æœä¸€å¥è¯æºè‡ªå¤šä¸ªä¸Šä¸‹æ–‡ï¼Œè¯·åˆ—å‡ºæ‰€æœ‰ç›¸å…³çš„å¼•ç”¨ç¼–å·ï¼Œä¾‹å¦‚[citation:3][citation:5]ï¼Œåˆ‡è®°ä¸è¦å°†å¼•ç”¨é›†ä¸­åœ¨æœ€åè¿”å›å¼•ç”¨ç¼–å·ï¼Œè€Œæ˜¯åœ¨ç­”æ¡ˆå¯¹åº”éƒ¨åˆ†åˆ—å‡ºã€‚
åœ¨å›ç­”æ—¶ï¼Œè¯·æ³¨æ„ä»¥ä¸‹å‡ ç‚¹ï¼š
- ä»Šå¤©æ˜¯{cur_date}ã€‚
- å¹¶éæœç´¢ç»“æœçš„æ‰€æœ‰å†…å®¹éƒ½ä¸ç”¨æˆ·çš„é—®é¢˜å¯†åˆ‡ç›¸å…³ï¼Œä½ éœ€è¦ç»“åˆé—®é¢˜ï¼Œå¯¹æœç´¢ç»“æœè¿›è¡Œç”„åˆ«ã€ç­›é€‰ã€‚
- å¯¹äºåˆ—ä¸¾ç±»çš„é—®é¢˜ï¼ˆå¦‚åˆ—ä¸¾æ‰€æœ‰èˆªç­ä¿¡æ¯ï¼‰ï¼Œå°½é‡å°†ç­”æ¡ˆæ§åˆ¶åœ¨10ä¸ªè¦ç‚¹ä»¥å†…ï¼Œå¹¶å‘Šè¯‰ç”¨æˆ·å¯ä»¥æŸ¥çœ‹æœç´¢æ¥æºã€è·å¾—å®Œæ•´ä¿¡æ¯ã€‚ä¼˜å…ˆæä¾›ä¿¡æ¯å®Œæ•´ã€æœ€ç›¸å…³çš„åˆ—ä¸¾é¡¹ï¼›å¦‚éå¿…è¦ï¼Œä¸è¦ä¸»åŠ¨å‘Šè¯‰ç”¨æˆ·æœç´¢ç»“æœæœªæä¾›çš„å†…å®¹ã€‚
- å¯¹äºåˆ›ä½œç±»çš„é—®é¢˜ï¼ˆå¦‚å†™è®ºæ–‡ï¼‰ï¼Œè¯·åŠ¡å¿…åœ¨æ­£æ–‡çš„æ®µè½ä¸­å¼•ç”¨å¯¹åº”çš„å‚è€ƒç¼–å·ï¼Œä¾‹å¦‚[citation:3][citation:5]ï¼Œä¸èƒ½åªåœ¨æ–‡ç« æœ«å°¾å¼•ç”¨ã€‚ä½ éœ€è¦è§£è¯»å¹¶æ¦‚æ‹¬ç”¨æˆ·çš„é¢˜ç›®è¦æ±‚ï¼Œé€‰æ‹©åˆé€‚çš„æ ¼å¼ï¼Œå……åˆ†åˆ©ç”¨æœç´¢ç»“æœå¹¶æŠ½å–é‡è¦ä¿¡æ¯ï¼Œç”Ÿæˆç¬¦åˆç”¨æˆ·è¦æ±‚ã€æå…·æ€æƒ³æ·±åº¦ã€å¯Œæœ‰åˆ›é€ åŠ›ä¸ä¸“ä¸šæ€§çš„ç­”æ¡ˆã€‚ä½ çš„åˆ›ä½œç¯‡å¹…éœ€è¦å°½å¯èƒ½å»¶é•¿ï¼Œå¯¹äºæ¯ä¸€ä¸ªè¦ç‚¹çš„è®ºè¿°è¦æ¨æµ‹ç”¨æˆ·çš„æ„å›¾ï¼Œç»™å‡ºå°½å¯èƒ½å¤šè§’åº¦çš„å›ç­”è¦ç‚¹ï¼Œä¸”åŠ¡å¿…ä¿¡æ¯é‡å¤§ã€è®ºè¿°è¯¦å°½ã€‚
- å¦‚æœå›ç­”å¾ˆé•¿ï¼Œè¯·å°½é‡ç»“æ„åŒ–ã€åˆ†æ®µè½æ€»ç»“ã€‚å¦‚æœéœ€è¦åˆ†ç‚¹ä½œç­”ï¼Œå°½é‡æ§åˆ¶åœ¨5ä¸ªç‚¹ä»¥å†…ï¼Œå¹¶åˆå¹¶ç›¸å…³çš„å†…å®¹ã€‚
- å¯¹äºå®¢è§‚ç±»çš„é—®ç­”ï¼Œå¦‚æœé—®é¢˜çš„ç­”æ¡ˆéå¸¸ç®€çŸ­ï¼Œå¯ä»¥é€‚å½“è¡¥å……ä¸€åˆ°ä¸¤å¥ç›¸å…³ä¿¡æ¯ï¼Œä»¥ä¸°å¯Œå†…å®¹ã€‚
- ä½ éœ€è¦æ ¹æ®ç”¨æˆ·è¦æ±‚å’Œå›ç­”å†…å®¹é€‰æ‹©åˆé€‚ã€ç¾è§‚çš„å›ç­”æ ¼å¼ï¼Œç¡®ä¿å¯è¯»æ€§å¼ºã€‚
- ä½ çš„å›ç­”åº”è¯¥ç»¼åˆå¤šä¸ªç›¸å…³ç½‘é¡µæ¥å›ç­”ï¼Œä¸èƒ½é‡å¤å¼•ç”¨ä¸€ä¸ªç½‘é¡µã€‚
- é™¤éç”¨æˆ·è¦æ±‚ï¼Œå¦åˆ™ä½ å›ç­”çš„è¯­è¨€éœ€è¦å’Œç”¨æˆ·æé—®çš„è¯­è¨€ä¿æŒä¸€è‡´ã€‚

# ç”¨æˆ·æ¶ˆæ¯ä¸ºï¼š
{question}'''
```

Para consulta em inglÃªs, usamos o prompt:
```
search_answer_en_template = \
'''# The following contents are the search results related to the user's message:
{search_results}
In the search results I provide to you, each result is formatted as [webpage X begin]...[webpage X end], where X represents the numerical index of each article. Please cite the context at the end of the relevant sentence when appropriate. Use the citation format [citation:X] in the corresponding part of your answer. If a sentence is derived from multiple contexts, list all relevant citation numbers, such as [citation:3][citation:5]. Be sure not to cluster all citations at the end; instead, include them in the corresponding parts of the answer.
When responding, please keep the following points in mind:
- Today is {cur_date}.
- Not all content in the search results is closely related to the user's question. You need to evaluate and filter the search results based on the question.
- For listing-type questions (e.g., listing all flight information), try to limit the answer to 10 key points and inform the user that they can refer to the search sources for complete information. Prioritize providing the most complete and relevant items in the list. Avoid mentioning content not provided in the search results unless necessary.
- For creative tasks (e.g., writing an essay), ensure that references are cited within the body of the text, such as [citation:3][citation:5], rather than only at the end of the text. You need to interpret and summarize the user's requirements, choose an appropriate format, fully utilize the search results, extract key information, and generate an answer that is insightful, creative, and professional. Extend the length of your response as much as possible, addressing each point in detail and from multiple perspectives, ensuring the content is rich and thorough.
- If the response is lengthy, structure it well and summarize it in paragraphs. If a point-by-point format is needed, try to limit it to 5 points and merge related content.
- For objective Q&A, if the answer is very brief, you may add one or two related sentences to enrich the content.
- Choose an appropriate and visually appealing format for your response based on the user's requirements and the content of the answer, ensuring strong readability.
- Your answer should synthesize information from multiple relevant webpages and avoid repeatedly citing the same webpage.
- Unless the user requests otherwise, your response should be in the same language as the user's question.

# The user's message is:
{question}'''
```

## 7. LicenÃ§a

Este repositÃ³rio de cÃ³digo e os pesos do modelo estÃ£o licenciados sob a [LicenÃ§a MIT](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE).  
A sÃ©rie DeepSeek-R1 suporta uso comercial, permite quaisquer modificaÃ§Ãµes e trabalhos derivados, incluindo, mas nÃ£o se limitando, Ã  destilaÃ§Ã£o para treinamento de outros LLMs. Observe que:
- DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B e DeepSeek-R1-Distill-Qwen-32B sÃ£o derivados da [sÃ©rie Qwen-2.5](https://github.com/QwenLM/Qwen2.5), originalmente licenciados sob a [LicenÃ§a Apache 2.0](https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE), e agora ajustados com 800k amostras curadas com DeepSeek-R1.
- DeepSeek-R1-Distill-Llama-8B Ã© derivado de Llama3.1-8B-Base e originalmente licenciado sob a [licenÃ§a Llama3.1](https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE).
- DeepSeek-R1-Distill-Llama-70B Ã© derivado de Llama3.3-70B-Instruct e originalmente licenciado sob a [licenÃ§a Llama3.3](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE).

## 8. CitaÃ§Ã£o
```bibtex
@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.12948}, 
}
```

## 9. Contato
Se vocÃª tiver alguma dÃºvida, por favor, abra uma issue ou entre em contato pelo [service@deepseek.com](mailto:service@deepseek.com).


---

[Powered By OpenAiTx](https://github.com/OpenAiTx/OpenAiTx)

---