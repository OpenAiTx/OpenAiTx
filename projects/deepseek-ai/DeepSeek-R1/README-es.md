# DeepSeek-R1
<!-- markdownlint-disable first-line-h1 -->
<!-- markdownlint-disable html -->
<!-- markdownlint-disable no-duplicate-header -->

<div align="center">
  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-R1" />
</div>
<hr>
<div align="center" style="line-height: 1;">
  <a href="https://www.deepseek.com/" target="_blank"><img alt="Homepage"
    src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true"/></a>
  <a href="https://chat.deepseek.com/" target="_blank"><img alt="Chat"
    src="https://img.shields.io/badge/ğŸ¤–%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white"/></a>
  <a href="https://huggingface.co/deepseek-ai" target="_blank"><img alt="Hugging Face"
    src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white"/></a>
  <br>
  <a href="https://discord.gg/Tc7c45Zzu5" target="_blank"><img alt="Discord"
    src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da"/></a>
  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true" target="_blank"><img alt="WeChat"
    src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white"/></a>
  <a href="https://twitter.com/deepseek_ai" target="_blank"><img alt="Twitter Follow"
    src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white"/></a>
  <br>
  <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE"><img alt="License"
    src="https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53"/></a>
  <br>
  <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf"><b>Enlace al ArtÃ­culo</b>ğŸ‘ï¸</a>
</div>

## 1. IntroducciÃ³n

Presentamos nuestros modelos de razonamiento de primera generaciÃ³n, DeepSeek-R1-Zero y DeepSeek-R1.  
DeepSeek-R1-Zero, un modelo entrenado mediante aprendizaje por refuerzo (RL) a gran escala sin ajuste fino supervisado (SFT) como paso preliminar, demostrÃ³ un rendimiento notable en razonamiento.  
Con RL, DeepSeek-R1-Zero desarrollÃ³ de manera natural numerosos comportamientos de razonamiento potentes e interesantes.  
Sin embargo, DeepSeek-R1-Zero enfrenta desafÃ­os como repeticiÃ³n infinita, baja legibilidad y mezcla de idiomas. Para abordar estos problemas y mejorar aÃºn mÃ¡s el rendimiento en razonamiento, presentamos DeepSeek-R1, que incorpora datos de arranque en frÃ­o antes del RL.  
DeepSeek-R1 logra un rendimiento comparable a OpenAI-o1 en tareas de matemÃ¡ticas, cÃ³digo y razonamiento.  
Para apoyar a la comunidad de investigaciÃ³n, hemos liberado el cÃ³digo fuente de DeepSeek-R1-Zero, DeepSeek-R1 y seis modelos densos destilados de DeepSeek-R1 basados en Llama y Qwen. DeepSeek-R1-Distill-Qwen-32B supera a OpenAI-o1-mini en varios benchmarks, logrando nuevos resultados de vanguardia para modelos densos.

**NOTA: Antes de ejecutar localmente los modelos de la serie DeepSeek-R1, recomendamos revisar la secciÃ³n [Recomendaciones de uso](#usage-recommendations).**

<p align="center">
  <img width="80%" src="figures/benchmark.jpg">
</p>

## 2. Resumen del Modelo

---

**Post-Entrenamiento: Aprendizaje por Refuerzo a Gran Escala en el Modelo Base**

-  Aplicamos directamente aprendizaje por refuerzo (RL) al modelo base sin depender del ajuste fino supervisado (SFT) como paso preliminar. Este enfoque permite que el modelo explore cadenas de pensamiento (CoT) para resolver problemas complejos, dando como resultado el desarrollo de DeepSeek-R1-Zero. DeepSeek-R1-Zero demuestra capacidades como auto-verificaciÃ³n, reflexiÃ³n y generaciÃ³n de largas cadenas de pensamiento, marcando un hito significativo para la comunidad investigadora. Cabe destacar que es la primera investigaciÃ³n abierta que valida que las capacidades de razonamiento de los LLM pueden incentivarse Ãºnicamente mediante RL, sin necesidad de SFT. Este avance allana el camino para futuros desarrollos en esta Ã¡rea.

-  Presentamos nuestra pipeline para desarrollar DeepSeek-R1. La pipeline incorpora dos etapas de RL dirigidas a descubrir mejores patrones de razonamiento y alinearse con las preferencias humanas, asÃ­ como dos etapas de SFT que sirven como semilla para las capacidades de razonamiento y no razonamiento del modelo.  
Creemos que la pipeline beneficiarÃ¡ a la industria al crear mejores modelos.

---

**DestilaciÃ³n: Los Modelos MÃ¡s PequeÃ±os TambiÃ©n Pueden Ser Potentes**

-  Demostramos que los patrones de razonamiento de modelos mÃ¡s grandes pueden destilarse en modelos mÃ¡s pequeÃ±os, resultando en un mejor rendimiento comparado con los patrones de razonamiento descubiertos mediante RL en modelos pequeÃ±os. El DeepSeek-R1 open source, asÃ­ como su API, beneficiarÃ¡ a la comunidad de investigaciÃ³n para destilar mejores modelos pequeÃ±os en el futuro.
-  Utilizando los datos de razonamiento generados por DeepSeek-R1, afinamos varios modelos densos que son ampliamente utilizados en la comunidad investigadora. Los resultados de la evaluaciÃ³n demuestran que los modelos densos destilados pequeÃ±os tienen un rendimiento excepcional en los benchmarks. Liberamos los checkpoints destilados de 1.5B, 7B, 8B, 14B, 32B y 70B basados en Qwen2.5 y la serie Llama3 para la comunidad.

## 3. Descarga de Modelos

### Modelos DeepSeek-R1

<div align="center">

| **Modelo** | **#Total de ParÃ¡metros** | **#ParÃ¡metros Activados** | **Longitud de Contexto** | **Descargar** |
| :------------: | :------------: | :------------: | :------------: | :------------: |
| DeepSeek-R1-Zero | 671B | 37B | 128K   | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)   |
| DeepSeek-R1   | 671B | 37B |  128K   | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   |

</div>

DeepSeek-R1-Zero y DeepSeek-R1 se entrenan a partir de DeepSeek-V3-Base.  
Para mÃ¡s detalles respecto a la arquitectura del modelo, por favor consulte el repositorio de [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3).

### Modelos DeepSeek-R1-Distill

<div align="center">

| **Modelo** | **Modelo Base** | **Descargar** |
| :------------: | :------------: | :------------: |
| DeepSeek-R1-Distill-Qwen-1.5B  | [Qwen2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B) | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)   |
| DeepSeek-R1-Distill-Qwen-7B  | [Qwen2.5-Math-7B](https://huggingface.co/Qwen/Qwen2.5-Math-7B) | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)   |
| DeepSeek-R1-Distill-Llama-8B  | [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)   |
| DeepSeek-R1-Distill-Qwen-14B   | [Qwen2.5-14B](https://huggingface.co/Qwen/Qwen2.5-14B) | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)   |
|DeepSeek-R1-Distill-Qwen-32B  | [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B) | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)   |
| DeepSeek-R1-Distill-Llama-70B  | [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)   |

</div>

Los modelos DeepSeek-R1-Distill estÃ¡n afinados a partir de modelos open source, utilizando muestras generadas por DeepSeek-R1.  
Hemos realizado pequeÃ±os cambios en sus configuraciones y tokenizadores. Por favor, utilice nuestra configuraciÃ³n para ejecutar estos modelos.

## 4. Resultados de EvaluaciÃ³n

### DeepSeek-R1-EvaluaciÃ³n
Para todos nuestros modelos, la longitud mÃ¡xima de generaciÃ³n se establece en 32,768 tokens. Para benchmarks que requieren muestreo, utilizamos una temperatura de $0.6$, un valor top-p de $0.95$ y generamos 64 respuestas por consulta para estimar pass@1.
<div align="center">


| CategorÃ­a | Benchmark (MÃ©trica) | Claude-3.5-Sonnet-1022 | GPT-4o 0513 | DeepSeek V3 | OpenAI o1-mini | OpenAI o1-1217 | DeepSeek R1 |
|----------|---------------------|------------------------|-------------|--------------|----------------|---------------|-------------|
| | Arquitectura | - | - | MoE | - | - | MoE |
| | # ParÃ¡metros Activados | - | - | 37B | - | - | 37B |
| | # Total de ParÃ¡metros | - | - | 671B | - | - | 671B |
| InglÃ©s | MMLU (Pass@1) | 88.3 | 87.2 | 88.5 | 85.2 | **91.8** | 90.8 |
| | MMLU-Redux (EM) | 88.9 | 88.0 | 89.1 | 86.7 | - | **92.9** |
| | MMLU-Pro (EM) | 78.0 | 72.6 | 75.9 | 80.3 | - | **84.0** |
| | DROP (3-shot F1) | 88.3 | 83.7 | 91.6 | 83.9 | 90.2 | **92.2** |
| | IF-Eval (Prompt Strict) | **86.5** | 84.3 | 86.1 | 84.8 | - | 83.3 |
| | GPQA-Diamond (Pass@1) | 65.0 | 49.9 | 59.1 | 60.0 | **75.7** | 71.5 |
| | SimpleQA (Correcto) | 28.4 | 38.2 | 24.9 | 7.0 | **47.0** | 30.1 |
| | FRAMES (Prec.) | 72.5 | 80.5 | 73.3 | 76.9 | - | **82.5** |
| | AlpacaEval2.0 (LC-winrate) | 52.0 | 51.1 | 70.0 | 57.8 | - | **87.6** |
| | ArenaHard (GPT-4-1106) | 85.2 | 80.4 | 85.5 | 92.0 | - | **92.3** |
| CÃ³digo | LiveCodeBench (Pass@1-COT) | 33.8 | 34.2 | - | 53.8 | 63.4 | **65.9** |
| | Codeforces (Percentil) | 20.3 | 23.6 | 58.7 | 93.4 | **96.6** | 96.3 |
| | Codeforces (Rating) | 717 | 759 | 1134 | 1820 | **2061** | 2029 |
| | SWE Verified (Resueltos) | **50.8** | 38.8 | 42.0 | 41.6 | 48.9 | 49.2 |
| | Aider-Polyglot (Prec.) | 45.3 | 16.0 | 49.6 | 32.9 | **61.7** | 53.3 |
| MatemÃ¡ticas | AIME 2024 (Pass@1) | 16.0 | 9.3 | 39.2 | 63.6 | 79.2 | **79.8** |
| | MATH-500 (Pass@1) | 78.3 | 74.6 | 90.2 | 90.0 | 96.4 | **97.3** |
| | CNMO 2024 (Pass@1) | 13.1 | 10.8 | 43.2 | 67.6 | - | **78.8** |
| Chino | CLUEWSC (EM) | 85.4 | 87.9 | 90.9 | 89.9 | - | **92.8** |
| | C-Eval (EM) | 76.7 | 76.0 | 86.5 | 68.9 | - | **91.8** |
| | C-SimpleQA (Correcto) | 55.4 | 58.7 | **68.0** | 40.3 | - | 63.7 |

</div>


### EvaluaciÃ³n de Modelos Destilados

<div align="center">

| Modelo                                   | AIME 2024 pass@1 | AIME 2024 cons@64 | MATH-500 pass@1 | GPQA Diamond pass@1 | LiveCodeBench pass@1 | CodeForces rating |
|------------------------------------------|------------------|-------------------|-----------------|----------------------|----------------------|-------------------|
| GPT-4o-0513                              | 9.3              | 13.4              | 74.6            | 49.9                 | 32.9                 | 759               |
| Claude-3.5-Sonnet-1022                   | 16.0             | 26.7              | 78.3            | 65.0                 | 38.9                 | 717               |
| o1-mini                                  | 63.6             | 80.0              | 90.0            | 60.0                 | 53.8                 | **1820**          |
| QwQ-32B-Preview                          | 44.0             | 60.0              | 90.6            | 54.5                 | 41.9                 | 1316              |
| DeepSeek-R1-Distill-Qwen-1.5B            | 28.9             | 52.7              | 83.9            | 33.8                 | 16.9                 | 954               |
| DeepSeek-R1-Distill-Qwen-7B              | 55.5             | 83.3              | 92.8            | 49.1                 | 37.6                 | 1189              |
| DeepSeek-R1-Distill-Qwen-14B             | 69.7             | 80.0              | 93.9            | 59.1                 | 53.1                 | 1481              |
| DeepSeek-R1-Distill-Qwen-32B             | **72.6**         | 83.3              | 94.3            | 62.1                 | 57.2                 | 1691              |
| DeepSeek-R1-Distill-Llama-8B             | 50.4             | 80.0              | 89.1            | 49.0                 | 39.6                 | 1205              |
| DeepSeek-R1-Distill-Llama-70B            | 70.0             | **86.7**          | **94.5**        | **65.2**             | **57.5**             | 1633              |

</div>


## 5. Sitio Web de Chat y Plataforma API
Puede conversar con DeepSeek-R1 en el sitio web oficial de DeepSeek: [chat.deepseek.com](https://chat.deepseek.com) y activar el botÃ³n "DeepThink".

TambiÃ©n proveemos API compatible con OpenAI en la Plataforma DeepSeek: [platform.deepseek.com](https://platform.deepseek.com/)

## 6. CÃ³mo Ejecutar Localmente

### Modelos DeepSeek-R1

Por favor, visite el repositorio [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) para obtener mÃ¡s informaciÃ³n sobre cÃ³mo ejecutar DeepSeek-R1 localmente.

**NOTA: AÃºn no se ha soportado directamente Transformers de Hugging Face.**

### Modelos DeepSeek-R1-Distill

Los modelos DeepSeek-R1-Distill pueden ser utilizados de la misma manera que los modelos Qwen o Llama.

Por ejemplo, puede iniciar fÃ¡cilmente un servicio usando [vLLM](https://github.com/vllm-project/vllm):

```shell
vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager
```

TambiÃ©n puede iniciar un servicio fÃ¡cilmente usando [SGLang](https://github.com/sgl-project/sglang):

```bash
python3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2
```

### Recomendaciones de Uso

**Recomendamos seguir las siguientes configuraciones al utilizar los modelos de la serie DeepSeek-R1, incluyendo la evaluaciÃ³n de benchmarks, para lograr el rendimiento esperado:**

1. Establezca la temperatura dentro del rango de 0.5-0.7 (0.6 recomendado) para evitar repeticiones infinitas o salidas incoherentes.
2. **Evite agregar un prompt de sistema; todas las instrucciones deben estar contenidas en el prompt del usuario.**
3. Para problemas matemÃ¡ticos, es recomendable incluir una directiva en su prompt tal como: "Por favor razona paso a paso, y pon tu respuesta final dentro de \boxed{}."
4. Al evaluar el rendimiento del modelo, se recomienda realizar mÃºltiples pruebas y promediar los resultados.

Adicionalmente, hemos observado que los modelos de la serie DeepSeek-R1 tienden a omitir el patrÃ³n de pensamiento (es decir, a emitir "\<think\>\n\n\</think\>") al responder ciertas consultas, lo que puede afectar negativamente el desempeÃ±o del modelo.  
**Para asegurar que el modelo realice un razonamiento exhaustivo, recomendamos forzar que el modelo inicie su respuesta con "\<think\>\n" al principio de cada salida.**

### Prompts Oficiales
En la web/app oficial de DeepSeek, no usamos prompts de sistema, pero diseÃ±amos dos prompts especÃ­ficos para la carga de archivos y la bÃºsqueda web para una mejor experiencia de usuario. AdemÃ¡s, la temperatura en la web/app es 0.6.

Para la carga de archivos, siga la plantilla para crear prompts, donde {file_name}, {file_content} y {question} son argumentos.
```
file_template = \
"""[file name]: {file_name}
[file content begin]
{file_content}
[file content end]
{question}"""
```

Para la bÃºsqueda web, {search_results}, {cur_date}, y {question} son argumentos.

Para la consulta en chino, usamos el prompt:
```
search_answer_zh_template = \
'''# ä»¥ä¸‹å†…å®¹æ˜¯åŸºäºç”¨æˆ·å‘é€çš„æ¶ˆæ¯çš„æœç´¢ç»“æœ:
{search_results}
åœ¨æˆ‘ç»™ä½ çš„æœç´¢ç»“æœä¸­ï¼Œæ¯ä¸ªç»“æœéƒ½æ˜¯[webpage X begin]...[webpage X end]æ ¼å¼çš„ï¼ŒXä»£è¡¨æ¯ç¯‡æ–‡ç« çš„æ•°å­—ç´¢å¼•ã€‚è¯·åœ¨é€‚å½“çš„æƒ…å†µä¸‹åœ¨å¥å­æœ«å°¾å¼•ç”¨ä¸Šä¸‹æ–‡ã€‚è¯·æŒ‰ç…§å¼•ç”¨ç¼–å·[citation:X]çš„æ ¼å¼åœ¨ç­”æ¡ˆä¸­å¯¹åº”éƒ¨åˆ†å¼•ç”¨ä¸Šä¸‹æ–‡ã€‚å¦‚æœä¸€å¥è¯æºè‡ªå¤šä¸ªä¸Šä¸‹æ–‡ï¼Œè¯·åˆ—å‡ºæ‰€æœ‰ç›¸å…³çš„å¼•ç”¨ç¼–å·ï¼Œä¾‹å¦‚[citation:3][citation:5]ï¼Œåˆ‡è®°ä¸è¦å°†å¼•ç”¨é›†ä¸­åœ¨æœ€åè¿”å›å¼•ç”¨ç¼–å·ï¼Œè€Œæ˜¯åœ¨ç­”æ¡ˆå¯¹åº”éƒ¨åˆ†åˆ—å‡ºã€‚
åœ¨å›ç­”æ—¶ï¼Œè¯·æ³¨æ„ä»¥ä¸‹å‡ ç‚¹ï¼š
- ä»Šå¤©æ˜¯{cur_date}ã€‚
- å¹¶éæœç´¢ç»“æœçš„æ‰€æœ‰å†…å®¹éƒ½ä¸ç”¨æˆ·çš„é—®é¢˜å¯†åˆ‡ç›¸å…³ï¼Œä½ éœ€è¦ç»“åˆé—®é¢˜ï¼Œå¯¹æœç´¢ç»“æœè¿›è¡Œç”„åˆ«ã€ç­›é€‰ã€‚
- å¯¹äºåˆ—ä¸¾ç±»çš„é—®é¢˜ï¼ˆå¦‚åˆ—ä¸¾æ‰€æœ‰èˆªç­ä¿¡æ¯ï¼‰ï¼Œå°½é‡å°†ç­”æ¡ˆæ§åˆ¶åœ¨10ä¸ªè¦ç‚¹ä»¥å†…ï¼Œå¹¶å‘Šè¯‰ç”¨æˆ·å¯ä»¥æŸ¥çœ‹æœç´¢æ¥æºã€è·å¾—å®Œæ•´ä¿¡æ¯ã€‚ä¼˜å…ˆæä¾›ä¿¡æ¯å®Œæ•´ã€æœ€ç›¸å…³çš„åˆ—ä¸¾é¡¹ï¼›å¦‚éå¿…è¦ï¼Œä¸è¦ä¸»åŠ¨å‘Šè¯‰ç”¨æˆ·æœç´¢ç»“æœæœªæä¾›çš„å†…å®¹ã€‚
- å¯¹äºåˆ›ä½œç±»çš„é—®é¢˜ï¼ˆå¦‚å†™è®ºæ–‡ï¼‰ï¼Œè¯·åŠ¡å¿…åœ¨æ­£æ–‡çš„æ®µè½ä¸­å¼•ç”¨å¯¹åº”çš„å‚è€ƒç¼–å·ï¼Œä¾‹å¦‚[citation:3][citation:5]ï¼Œä¸èƒ½åªåœ¨æ–‡ç« æœ«å°¾å¼•ç”¨ã€‚ä½ éœ€è¦è§£è¯»å¹¶æ¦‚æ‹¬ç”¨æˆ·çš„é¢˜ç›®è¦æ±‚ï¼Œé€‰æ‹©åˆé€‚çš„æ ¼å¼ï¼Œå……åˆ†åˆ©ç”¨æœç´¢ç»“æœå¹¶æŠ½å–é‡è¦ä¿¡æ¯ï¼Œç”Ÿæˆç¬¦åˆç”¨æˆ·è¦æ±‚ã€æå…·æ€æƒ³æ·±åº¦ã€å¯Œæœ‰åˆ›é€ åŠ›ä¸ä¸“ä¸šæ€§çš„ç­”æ¡ˆã€‚ä½ çš„åˆ›ä½œç¯‡å¹…éœ€è¦å°½å¯èƒ½å»¶é•¿ï¼Œå¯¹äºæ¯ä¸€ä¸ªè¦ç‚¹çš„è®ºè¿°è¦æ¨æµ‹ç”¨æˆ·çš„æ„å›¾ï¼Œç»™å‡ºå°½å¯èƒ½å¤šè§’åº¦çš„å›ç­”è¦ç‚¹ï¼Œä¸”åŠ¡å¿…ä¿¡æ¯é‡å¤§ã€è®ºè¿°è¯¦å°½ã€‚
- å¦‚æœå›ç­”å¾ˆé•¿ï¼Œè¯·å°½é‡ç»“æ„åŒ–ã€åˆ†æ®µè½æ€»ç»“ã€‚å¦‚æœéœ€è¦åˆ†ç‚¹ä½œç­”ï¼Œå°½é‡æ§åˆ¶åœ¨5ä¸ªç‚¹ä»¥å†…ï¼Œå¹¶åˆå¹¶ç›¸å…³çš„å†…å®¹ã€‚
- å¯¹äºå®¢è§‚ç±»çš„é—®ç­”ï¼Œå¦‚æœé—®é¢˜çš„ç­”æ¡ˆéå¸¸ç®€çŸ­ï¼Œå¯ä»¥é€‚å½“è¡¥å……ä¸€åˆ°ä¸¤å¥ç›¸å…³ä¿¡æ¯ï¼Œä»¥ä¸°å¯Œå†…å®¹ã€‚
- ä½ éœ€è¦æ ¹æ®ç”¨æˆ·è¦æ±‚å’Œå›ç­”å†…å®¹é€‰æ‹©åˆé€‚ã€ç¾è§‚çš„å›ç­”æ ¼å¼ï¼Œç¡®ä¿å¯è¯»æ€§å¼ºã€‚
- ä½ çš„å›ç­”åº”è¯¥ç»¼åˆå¤šä¸ªç›¸å…³ç½‘é¡µæ¥å›ç­”ï¼Œä¸èƒ½é‡å¤å¼•ç”¨ä¸€ä¸ªç½‘é¡µã€‚
- é™¤éç”¨æˆ·è¦æ±‚ï¼Œå¦åˆ™ä½ å›ç­”çš„è¯­è¨€éœ€è¦å’Œç”¨æˆ·æé—®çš„è¯­è¨€ä¿æŒä¸€è‡´ã€‚

# ç”¨æˆ·æ¶ˆæ¯ä¸ºï¼š
{question}'''
```

Para consultas en inglÃ©s, usamos el prompt:
```
search_answer_en_template = \
'''# The following contents are the search results related to the user's message:
{search_results}
In the search results I provide to you, each result is formatted as [webpage X begin]...[webpage X end], where X represents the numerical index of each article. Please cite the context at the end of the relevant sentence when appropriate. Use the citation format [citation:X] in the corresponding part of your answer. If a sentence is derived from multiple contexts, list all relevant citation numbers, such as [citation:3][citation:5]. Be sure not to cluster all citations at the end; instead, include them in the corresponding parts of the answer.
When responding, please keep the following points in mind:
- Today is {cur_date}.
- Not all content in the search results is closely related to the user's question. You need to evaluate and filter the search results based on the question.
- For listing-type questions (e.g., listing all flight information), try to limit the answer to 10 key points and inform the user that they can refer to the search sources for complete information. Prioritize providing the most complete and relevant items in the list. Avoid mentioning content not provided in the search results unless necessary.
- For creative tasks (e.g., writing an essay), ensure that references are cited within the body of the text, such as [citation:3][citation:5], rather than only at the end of the text. You need to interpret and summarize the user's requirements, choose an appropriate format, fully utilize the search results, extract key information, and generate an answer that is insightful, creative, and professional. Extend the length of your response as much as possible, addressing each point in detail and from multiple perspectives, ensuring the content is rich and thorough.
- If the response is lengthy, structure it well and summarize it in paragraphs. If a point-by-point format is needed, try to limit it to 5 points and merge related content.
- For objective Q&A, if the answer is very brief, you may add one or two related sentences to enrich the content.
- Choose an appropriate and visually appealing format for your response based on the user's requirements and the content of the answer, ensuring strong readability.
- Your answer should synthesize information from multiple relevant webpages and avoid repeatedly citing the same webpage.
- Unless the user requests otherwise, your response should be in the same language as the user's question.

# The user's message is:
{question}'''
```

## 7. Licencia
Este repositorio de cÃ³digo y los pesos del modelo estÃ¡n licenciados bajo la [Licencia MIT](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE).
La serie DeepSeek-R1 soporta uso comercial, permite cualquier modificaciÃ³n y trabajos derivados, incluyendo, pero no limitado a, destilaciÃ³n para entrenar otros LLMs. Tenga en cuenta que:
- DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B y DeepSeek-R1-Distill-Qwen-32B derivan de la [serie Qwen-2.5](https://github.com/QwenLM/Qwen2.5), que estÃ¡ originalmente licenciada bajo la [Licencia Apache 2.0](https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE), y ahora han sido afinados con 800k muestras curadas con DeepSeek-R1.
- DeepSeek-R1-Distill-Llama-8B deriva de Llama3.1-8B-Base y estÃ¡ originalmente licenciada bajo la [licencia Llama3.1](https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE).
- DeepSeek-R1-Distill-Llama-70B deriva de Llama3.3-70B-Instruct y estÃ¡ originalmente licenciada bajo la [licencia Llama3.3](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE).

## 8. CitaciÃ³n
```bibtex
@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.12948}, 
}
```

## 9. Contacto
Si tiene alguna pregunta, por favor abra un issue o contÃ¡ctenos en [service@deepseek.com](mailto:service@deepseek.com).

---

[Powered By OpenAiTx](https://github.com/OpenAiTx/OpenAiTx)

---