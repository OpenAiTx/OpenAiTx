[
  {
    "Id": 1,
    "Content": "# AlphaEarth Foundations\n\nA PyTorch implementation of the AlphaEarth geospatial foundation model from Google DeepMind, which generates Earth embeddings for global environmental monitoring and analysis.\nAccompanying the paper is a global dataset of embeddings from 2017 to 2024, available through Earth Engine. The goal of these embeddings is to serve as a highly general geospatial representation for a huge amount of downstream applications, without the need for retraining. \n\n> [!NOTE]\n> This model is a work in progress and was not actually trained on the full dataset, it is just a framework that provides a general base for the paper's architecture. The code is simplified compared to the DeepMind's actual implementation (in JAX). \n\n### Key parts of the methodology\n\n- **Continuous Time Support**: First EO featurization approach to support continuous time, allowing for temporal interpolation and extrapolation.\n- **Space Time Precision (STP) Architecture**: Multi-resolution encoder with spatial (1/16L), temporal (1/8L), and precision (1/2L) operators - designed to maintain localized representations while also modeling long-distance relationships across time and space. \n- **von Mises-Fisher Embeddings**: 64-byte embeddings distributed on unit sphere S^63, very compact representation. \n\n\n## Architecture\n\n### Space Time Precision (STP) Encoder\n\nThe STP encoder processes multi-temporal, multi-source data through three simultaneous operators:\n- **Space Operator**: ViT-like spatial self-attention (1/16L resolution)\n- **Time Operator**: Time-axial self-attention (1/8L resolution) \n- **Precision Operator**: 3x3 convolutions (1/2L resolution)\n\n### Teacher-Student-Text Framework\n\n1. **Teacher Video Embedding Model**: Main model with implicit decoders\n2. **Student Video Embedding Model**: Shares parameters with teacher for contrastive learning\n3. **Text Alignment Model**: Enables text-image contrastive learning\n\n\n## Data Sources\n\nThe model is trained on many data sources including:\n- **Optical**: Sentinel-2, Landsat 8/9. *Note: for simplicty, my implementation only supports Sentinel-2, but it should be relatively straightforward to add new datasets to the training*\n- **Radar**: Sentinel-1, PALSAR2\n- **LiDAR**: GEDI\n- **Environmental**: GLO-30, ERA5-Land, GRACE\n- **Annotated/Text**: NLCD, Wikipedia\n",
    "ContentSha": "EkbWGtQIhcz3fHn4pd/Jfx3lMLrzgQIes+q9yMy8LxM=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "# AlphaEarth Foundations\n\nGoogle DeepMindのAlphaEarth地理空間基盤モデルのPyTorch実装であり、地球の埋め込みを生成して世界的な環境モニタリングと解析に利用します。  \n論文に付随して、2017年から2024年までのグローバル埋め込みデータセットがEarth Engineを通じて利用可能です。これらの埋め込みの目的は、再学習なしで多くの下流アプリケーションに対応可能な非常に汎用的な地理空間表現を提供することです。\n\n> [!NOTE]\n> このモデルは進行中の作業であり、実際には完全なデータセットで学習されていません。論文のアーキテクチャの一般的な基盤を提供するフレームワークにすぎません。コードはDeepMindの実際の実装（JAX）に比べて簡略化されています。\n\n### 方法論の主な部分\n\n- **連続時間サポート**：連続時間をサポートする最初のEO特徴化アプローチであり、時間的内挿および外挿が可能です。  \n- **Space Time Precision (STP) アーキテクチャ**：空間（1/16L）、時間（1/8L）、精度（1/2L）のオペレーターを備えた多解像度エンコーダで、局所的表現を維持しつつ時間と空間を跨る長距離関係をモデル化します。  \n- **フォン・ミーゼス・フィッシャー埋め込み**：単位球面S^63上に分布する64バイトの埋め込みで、非常にコンパクトな表現です。  \n\n\n## アーキテクチャ\n\n### Space Time Precision (STP) エンコーダ\n\nSTPエンコーダは、3つの同時オペレーターを通じて多時点・多ソースデータを処理します：  \n- **空間オペレーター**：ViTに類似した空間的自己注意（1/16L解像度）  \n- **時間オペレーター**：時間軸自己注意（1/8L解像度）  \n- **精度オペレーター**：3x3畳み込み（1/2L解像度）  \n\n### 教師-生徒-テキストフレームワーク\n\n1. **教師ビデオ埋め込みモデル**：暗黙的デコーダを持つメインモデル  \n2. **生徒ビデオ埋め込みモデル**：教師とパラメータを共有し、対比学習を行う  \n3. **テキスト整合モデル**：テキストと画像の対比学習を可能にする  \n\n\n## データソース\n\nモデルは以下を含む多くのデータソースで学習されています：  \n- **光学**：Sentinel-2、Landsat 8/9。*注：簡略化のため、私の実装はSentinel-2のみ対応していますが、新しいデータセットを学習に追加するのは比較的簡単です*  \n- **レーダー**：Sentinel-1、PALSAR2  \n- **LiDAR**：GEDI  \n- **環境**：GLO-30、ERA5-Land、GRACE  \n- **注釈/テキスト**：NLCD、Wikipedia  \n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "KPMbQCLZii3Ghs1iTjKm1o6gVOZMnEr8FQBsh6nWoUk=",
        "originContent": "# AlphaEarth Foundations",
        "translatedContent": "# AlphaEarth Foundations"
      },
      {
        "row": 2,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 3,
        "rowsha": "Fb1MgEZINPEbsUTaP1c7v2Wuu7RxldhwZifx3Y2aJWo=",
        "originContent": "A PyTorch implementation of the AlphaEarth geospatial foundation model from Google DeepMind, which generates Earth embeddings for global environmental monitoring and analysis.",
        "translatedContent": "Google DeepMindのAlphaEarth地理空間基盤モデルのPyTorch実装であり、地球の埋め込みを生成して世界的な環境モニタリングと解析に利用します。  "
      },
      {
        "row": 4,
        "rowsha": "hRdX0nqtLvoc5/wqoseo7WgX+RcOLF9HGtoynqHq3Bw=",
        "originContent": "Accompanying the paper is a global dataset of embeddings from 2017 to 2024, available through Earth Engine. The goal of these embeddings is to serve as a highly general geospatial representation for a huge amount of downstream applications, without the need for retraining. ",
        "translatedContent": "論文に付随して、2017年から2024年までのグローバル埋め込みデータセットがEarth Engineを通じて利用可能です。これらの埋め込みの目的は、再学習なしで多くの下流アプリケーションに対応可能な非常に汎用的な地理空間表現を提供することです。"
      },
      {
        "row": 5,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 6,
        "rowsha": "f7t9zCxxpXM+gn2wD3Ca/9QN2c+xK2M4aKM0xiXBqeU=",
        "originContent": "> [!NOTE]",
        "translatedContent": "> [!NOTE]"
      },
      {
        "row": 7,
        "rowsha": "Id13cQ/S7E2taE2RU9gavgkFrHMsc+zQOgb27u/Ulew=",
        "originContent": "> This model is a work in progress and was not actually trained on the full dataset, it is just a framework that provides a general base for the paper's architecture. The code is simplified compared to the DeepMind's actual implementation (in JAX). ",
        "translatedContent": "> このモデルは進行中の作業であり、実際には完全なデータセットで学習されていません。論文のアーキテクチャの一般的な基盤を提供するフレームワークにすぎません。コードはDeepMindの実際の実装（JAX）に比べて簡略化されています。"
      },
      {
        "row": 8,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 9,
        "rowsha": "SR3dtB4YvVI0rFMdcWHOQf1i7TnafCh7wYaRTy1680k=",
        "originContent": "### Key parts of the methodology",
        "translatedContent": "### 方法論の主な部分"
      },
      {
        "row": 10,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 11,
        "rowsha": "Gd8kMTxP3TzVhLqYeunmfA4sA91HDtZ9k4LEpHv4Pg4=",
        "originContent": "- **Continuous Time Support**: First EO featurization approach to support continuous time, allowing for temporal interpolation and extrapolation.",
        "translatedContent": "- **連続時間サポート**：連続時間をサポートする最初のEO特徴化アプローチであり、時間的内挿および外挿が可能です。  "
      },
      {
        "row": 12,
        "rowsha": "GF/q3JYX9UVdYLx/ysUICEeztnB9VTstLDdvnX5rZ3A=",
        "originContent": "- **Space Time Precision (STP) Architecture**: Multi-resolution encoder with spatial (1/16L), temporal (1/8L), and precision (1/2L) operators - designed to maintain localized representations while also modeling long-distance relationships across time and space. ",
        "translatedContent": "- **Space Time Precision (STP) アーキテクチャ**：空間（1/16L）、時間（1/8L）、精度（1/2L）のオペレーターを備えた多解像度エンコーダで、局所的表現を維持しつつ時間と空間を跨る長距離関係をモデル化します。  "
      },
      {
        "row": 13,
        "rowsha": "IfhPAy063rMlaMeNTU/igYtNZQGqQmcl9JFeTgYK5z0=",
        "originContent": "- **von Mises-Fisher Embeddings**: 64-byte embeddings distributed on unit sphere S^63, very compact representation. ",
        "translatedContent": "- **フォン・ミーゼス・フィッシャー埋め込み**：単位球面S^63上に分布する64バイトの埋め込みで、非常にコンパクトな表現です。  "
      },
      {
        "row": 14,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 15,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 16,
        "rowsha": "p+VkcrEb08g4vIGZYB9aVScRKgA8afv5WwErwTVzoZM=",
        "originContent": "## Architecture",
        "translatedContent": "## アーキテクチャ"
      },
      {
        "row": 17,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 18,
        "rowsha": "o5VWZ+R3BQjZmk6lmESbF9FwplPc26ucF1aQmeFL5k8=",
        "originContent": "### Space Time Precision (STP) Encoder",
        "translatedContent": "### Space Time Precision (STP) エンコーダ"
      },
      {
        "row": 19,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 20,
        "rowsha": "otS2oKNNny5cMXxwTSLNpeBIx8C1nADwikQTU1iIXog=",
        "originContent": "The STP encoder processes multi-temporal, multi-source data through three simultaneous operators:",
        "translatedContent": "STPエンコーダは、3つの同時オペレーターを通じて多時点・多ソースデータを処理します：  "
      },
      {
        "row": 21,
        "rowsha": "nvQSTCykiai5/4D0S2LkGAjXwUWbQw7dXzIAhMwIL28=",
        "originContent": "- **Space Operator**: ViT-like spatial self-attention (1/16L resolution)",
        "translatedContent": "- **空間オペレーター**：ViTに類似した空間的自己注意（1/16L解像度）  "
      },
      {
        "row": 22,
        "rowsha": "P1Q2bkqWQJnM4a5ZSb2eFO0UcgDhh41olQPBHqMoST4=",
        "originContent": "- **Time Operator**: Time-axial self-attention (1/8L resolution) ",
        "translatedContent": "- **時間オペレーター**：時間軸自己注意（1/8L解像度）  "
      },
      {
        "row": 23,
        "rowsha": "/xUxNq6udkEOoUu+apjMPS+lIAlv0na/dzBGy800QOY=",
        "originContent": "- **Precision Operator**: 3x3 convolutions (1/2L resolution)",
        "translatedContent": "- **精度オペレーター**：3x3畳み込み（1/2L解像度）  "
      },
      {
        "row": 24,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 25,
        "rowsha": "G1/AXTf3DGrN9fp1a2HFaMseIN9O9LI5oMd2JvH+OKs=",
        "originContent": "### Teacher-Student-Text Framework",
        "translatedContent": "### 教師-生徒-テキストフレームワーク"
      },
      {
        "row": 26,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 27,
        "rowsha": "gluY3K9WZKj6TUrEeNo0UD3NnG8LnWhDn4hunKFY63Q=",
        "originContent": "1. **Teacher Video Embedding Model**: Main model with implicit decoders",
        "translatedContent": "1. **教師ビデオ埋め込みモデル**：暗黙的デコーダを持つメインモデル  "
      },
      {
        "row": 28,
        "rowsha": "Iyh6jMPaush2jH6+57EvvvGU2f+xnm1DLW9tAKWkofU=",
        "originContent": "2. **Student Video Embedding Model**: Shares parameters with teacher for contrastive learning",
        "translatedContent": "2. **生徒ビデオ埋め込みモデル**：教師とパラメータを共有し、対比学習を行う  "
      },
      {
        "row": 29,
        "rowsha": "O4wvoVGU/2CNVFhPfuHLhlk8Z9uS4eqW6I3r9F+kjR8=",
        "originContent": "3. **Text Alignment Model**: Enables text-image contrastive learning",
        "translatedContent": "3. **テキスト整合モデル**：テキストと画像の対比学習を可能にする  "
      },
      {
        "row": 30,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 31,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 32,
        "rowsha": "kWIv99NNWtJF2DTAQAGx/mkbCAJzZq3CoKk1l+THitk=",
        "originContent": "## Data Sources",
        "translatedContent": "## データソース"
      },
      {
        "row": 33,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 34,
        "rowsha": "RtbQDS4/HisTRO9N8bqYxv2BX/rkfTCCUjvx67SG4qc=",
        "originContent": "The model is trained on many data sources including:",
        "translatedContent": "モデルは以下を含む多くのデータソースで学習されています：  "
      },
      {
        "row": 35,
        "rowsha": "wLvnsbVnblSd2XjJ+hZ2lxkH41HRATzY384Kqxw2B18=",
        "originContent": "- **Optical**: Sentinel-2, Landsat 8/9. *Note: for simplicty, my implementation only supports Sentinel-2, but it should be relatively straightforward to add new datasets to the training*",
        "translatedContent": "- **光学**：Sentinel-2、Landsat 8/9。*注：簡略化のため、私の実装はSentinel-2のみ対応していますが、新しいデータセットを学習に追加するのは比較的簡単です*  "
      },
      {
        "row": 36,
        "rowsha": "Fc2Ch6A1JZeW8za/C8y5wwpkGylPconYjOcak8Ilisk=",
        "originContent": "- **Radar**: Sentinel-1, PALSAR2",
        "translatedContent": "- **レーダー**：Sentinel-1、PALSAR2  "
      },
      {
        "row": 37,
        "rowsha": "4GKRxVHnVv7yTzddJ2LaY5trxBWhcP7sXJCurm8Mj64=",
        "originContent": "- **LiDAR**: GEDI",
        "translatedContent": "- **LiDAR**：GEDI  "
      },
      {
        "row": 38,
        "rowsha": "RkAIFL9UNDHx/Eejx3AxTRq9flEu6Dq6nBNb/HVlhRw=",
        "originContent": "- **Environmental**: GLO-30, ERA5-Land, GRACE",
        "translatedContent": "- **環境**：GLO-30、ERA5-Land、GRACE  "
      },
      {
        "row": 39,
        "rowsha": "h9j054ypMbNVCbO5JR/y3YsmHHirDrKEgyfDZd33H6s=",
        "originContent": "- **Annotated/Text**: NLCD, Wikipedia",
        "translatedContent": "- **注釈/テキスト**：NLCD、Wikipedia  "
      },
      {
        "row": 40,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 2,
    "Content": "## Installation\n",
    "ContentSha": "wy/dzyG91ulFkJwMfpQbJ59mMV5SMRImMA8Bz2YVauA=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## インストール\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "oV0SUDvwD2VN8Gi9nlr2JZ2xcDrASmE2W5kc5SVX5eo=",
        "originContent": "## Installation",
        "translatedContent": "## インストール"
      },
      {
        "row": 2,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 3,
    "Content": "```bash\n# Clone the repository\ngit clone https://github.com/brayden-zhang/alphaearth-foundations.git\ncd alphaearth-foundations\n\n# Install dependencies\nuv pip install -r requirements.txt\n\n# Install the package \nuv pip install -e .\n```",
    "ContentSha": "ckzqGGHhMxwrVkBEoyff+/+4tTZCPVSBot47ZMyxYYI=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bash\n# Clone the repository\ngit clone https://github.com/brayden-zhang/alphaearth-foundations.git\ncd alphaearth-foundations\n\n# Install dependencies\nuv pip install -r requirements.txt\n\n# Install the package \nuv pip install -e .\n```",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "rTMjNc+qNIxb1xXDR5hEYiJw1fc4muBwuBsTXVIaIho=",
        "originContent": "```bash",
        "translatedContent": "```bash"
      },
      {
        "row": 2,
        "rowsha": "EJ1kGcOyM89S1RwOEHSj/NB/bjY/T7n0DRgAblC5zSQ=",
        "originContent": "# Clone the repository",
        "translatedContent": "# Clone the repository"
      },
      {
        "row": 3,
        "rowsha": "CahoAB9eGNjdPdi98TuAbfzbBk8tnOsIHC8rYRoHI9c=",
        "originContent": "git clone https://github.com/brayden-zhang/alphaearth-foundations.git",
        "translatedContent": "git clone https://github.com/brayden-zhang/alphaearth-foundations.git"
      },
      {
        "row": 4,
        "rowsha": "8pln9HFPgs0no9RxlXlH/H9kSqGzsxM8nbLICVaKXJ8=",
        "originContent": "cd alphaearth-foundations",
        "translatedContent": "cd alphaearth-foundations"
      },
      {
        "row": 5,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 6,
        "rowsha": "eZl+L3vwMSfnhJPApyjgp5MAN3lox2CikfYeIVqDi7M=",
        "originContent": "# Install dependencies",
        "translatedContent": "# Install dependencies"
      },
      {
        "row": 7,
        "rowsha": "8MGsvthhq9AAjXH5hZI63az8axUF8OKzH/5SwOoaQHU=",
        "originContent": "uv pip install -r requirements.txt",
        "translatedContent": "uv pip install -r requirements.txt"
      },
      {
        "row": 8,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 9,
        "rowsha": "FdRhPs14tAb3jHwyDXL6n0EZp71wa2nwYQe0YpcCyzg=",
        "originContent": "# Install the package ",
        "translatedContent": "# Install the package "
      },
      {
        "row": 10,
        "rowsha": "hGoeSLGWxuZSrbcBtdd4ULq19+HXZLyGH7wUYAZRF1E=",
        "originContent": "uv pip install -e .",
        "translatedContent": "uv pip install -e ."
      },
      {
        "row": 11,
        "rowsha": "8bkBhHOQsO1+N058HkZOwXtGpCfEh6WtbL0pBkBQg9U=",
        "originContent": "```",
        "translatedContent": "```"
      }
    ],
    "IsCodeBlock": true
  },
  {
    "Id": 4,
    "Content": "\nHow to run a training step:",
    "ContentSha": "8vbq343UDykHnc/WPG4Ry1PXcHyeC5RCpiu/W79mks4=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "トレーニングステップの実行方法：\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "トレーニングステップの実行方法："
      },
      {
        "row": 2,
        "rowsha": "Ne4MiMMzfyQHhqI8nBxdCvg+uAY/pRtc296mKkLrp+8=",
        "originContent": "How to run a training step:",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 5,
    "Content": "```\npython -m alphaearth.run_train\n```",
    "ContentSha": "p8CPGZ1Y9ZbviBDV0OQ7tdcgkAqr1qLMoMFhEUF0Fgk=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```\npython -m alphaearth.run_train\n```",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "8bkBhHOQsO1+N058HkZOwXtGpCfEh6WtbL0pBkBQg9U=",
        "originContent": "```",
        "translatedContent": "```"
      },
      {
        "row": 2,
        "rowsha": "8auTjWW6qZYKU1J6PZ8tYgJJ1DEWQ4FkSzGSap6doPM=",
        "originContent": "python -m alphaearth.run_train",
        "translatedContent": "python -m alphaearth.run_train"
      },
      {
        "row": 3,
        "rowsha": "8bkBhHOQsO1+N058HkZOwXtGpCfEh6WtbL0pBkBQg9U=",
        "originContent": "```",
        "translatedContent": "```"
      }
    ],
    "IsCodeBlock": true
  },
  {
    "Id": 6,
    "Content": "\n## Paper Citation\n",
    "ContentSha": "ZsmfhHbjEY25+AHcVm8s87QBG64PIduApyTlm13Wm8U=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "\n## 論文引用\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 2,
        "rowsha": "ZDFAelIjO2eZVUnkkAg5vsep95mR8iCcvKsQ+2eiR2U=",
        "originContent": "## Paper Citation",
        "translatedContent": "## 論文引用"
      },
      {
        "row": 3,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 7,
    "Content": "```bibtex\n@misc{brown2025alphaearthfoundationsembeddingfield,\n      title={AlphaEarth Foundations: An embedding field model for accurate and efficient global mapping from sparse label data}, \n      author={Christopher F. Brown and Michal R. Kazmierski and Valerie J. Pasquarella and William J. Rucklidge and Masha Samsikova and Chenhui Zhang and Evan Shelhamer and Estefania Lahera and Olivia Wiles and Simon Ilyushchenko and Noel Gorelick and Lihui Lydia Zhang and Sophia Alj and Emily Schechter and Sean Askay and Oliver Guinan and Rebecca Moore and Alexis Boukouvalas and Pushmeet Kohli},\n      year={2025},\n      eprint={2507.22291},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2507.22291}, \n}\n```",
    "ContentSha": "RaoycGMPMvAByHEgeOhisvR92B3rwtTay5pqEbxnuCQ=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bibtex\n@misc{brown2025alphaearthfoundationsembeddingfield,\n      title={AlphaEarth Foundations: An embedding field model for accurate and efficient global mapping from sparse label data}, \n      author={Christopher F. Brown and Michal R. Kazmierski and Valerie J. Pasquarella and William J. Rucklidge and Masha Samsikova and Chenhui Zhang and Evan Shelhamer and Estefania Lahera and Olivia Wiles and Simon Ilyushchenko and Noel Gorelick and Lihui Lydia Zhang and Sophia Alj and Emily Schechter and Sean Askay and Oliver Guinan and Rebecca Moore and Alexis Boukouvalas and Pushmeet Kohli},\n      year={2025},\n      eprint={2507.22291},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2507.22291}, \n}\n```",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "o+TmyQ6wneV6/FQB6aUlRSjIGr2/YLJtnz5uxBgsScQ=",
        "originContent": "```bibtex",
        "translatedContent": "```bibtex"
      },
      {
        "row": 2,
        "rowsha": "5xbl3YW1vVTQGHUcBwrmseRmT+Qx+LlXEl1v+RyhjBY=",
        "originContent": "@misc{brown2025alphaearthfoundationsembeddingfield,",
        "translatedContent": "@misc{brown2025alphaearthfoundationsembeddingfield,"
      },
      {
        "row": 3,
        "rowsha": "doBCfH+U8DPHJajlrqVi7g3HSrlZTebK0kH/TRRxMkk=",
        "originContent": "      title={AlphaEarth Foundations: An embedding field model for accurate and efficient global mapping from sparse label data}, ",
        "translatedContent": "      title={AlphaEarth Foundations: An embedding field model for accurate and efficient global mapping from sparse label data}, "
      },
      {
        "row": 4,
        "rowsha": "o2Dj4Jefh1YyHSQquBjHaLo8wZ6PvkaE+DfRTtUYqPg=",
        "originContent": "      author={Christopher F. Brown and Michal R. Kazmierski and Valerie J. Pasquarella and William J. Rucklidge and Masha Samsikova and Chenhui Zhang and Evan Shelhamer and Estefania Lahera and Olivia Wiles and Simon Ilyushchenko and Noel Gorelick and Lihui Lydia Zhang and Sophia Alj and Emily Schechter and Sean Askay and Oliver Guinan and Rebecca Moore and Alexis Boukouvalas and Pushmeet Kohli},",
        "translatedContent": "      author={Christopher F. Brown and Michal R. Kazmierski and Valerie J. Pasquarella and William J. Rucklidge and Masha Samsikova and Chenhui Zhang and Evan Shelhamer and Estefania Lahera and Olivia Wiles and Simon Ilyushchenko and Noel Gorelick and Lihui Lydia Zhang and Sophia Alj and Emily Schechter and Sean Askay and Oliver Guinan and Rebecca Moore and Alexis Boukouvalas and Pushmeet Kohli},"
      },
      {
        "row": 5,
        "rowsha": "1cuvfM9h03loQfZOlvsx9juVCvU41kevaYb2CnD9Gak=",
        "originContent": "      year={2025},",
        "translatedContent": "      year={2025},"
      },
      {
        "row": 6,
        "rowsha": "l+imRuLQbUKJADtaHyu9k3pQV1eOFkKmQEMgFVpWZBc=",
        "originContent": "      eprint={2507.22291},",
        "translatedContent": "      eprint={2507.22291},"
      },
      {
        "row": 7,
        "rowsha": "Fr73/KLqU4TaDaJVUDLO211nM029JE4YRpN5hXSZZqk=",
        "originContent": "      archivePrefix={arXiv},",
        "translatedContent": "      archivePrefix={arXiv},"
      },
      {
        "row": 8,
        "rowsha": "RPNBhgHdrY2A+XYLnuhpAr/aqag2LU2pAjasgtM0tg4=",
        "originContent": "      primaryClass={cs.CV},",
        "translatedContent": "      primaryClass={cs.CV},"
      },
      {
        "row": 9,
        "rowsha": "YQ4ptlMTWrflsq6+/CjZ1n9XVcmAwhkfK9L96ueBcHo=",
        "originContent": "      url={https://arxiv.org/abs/2507.22291}, ",
        "translatedContent": "      url={https://arxiv.org/abs/2507.22291}, "
      },
      {
        "row": 10,
        "rowsha": "0Qs2qnSlm89KiBhYN/ZYr682Ru/yuxbDko0OkzXpRdI=",
        "originContent": "}",
        "translatedContent": "}"
      },
      {
        "row": 11,
        "rowsha": "8bkBhHOQsO1+N058HkZOwXtGpCfEh6WtbL0pBkBQg9U=",
        "originContent": "```",
        "translatedContent": "```"
      }
    ],
    "IsCodeBlock": true
  },
  {
    "Id": 8,
    "Content": "",
    "ContentSha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  }
]