
<div align="right">
  <details>
    <summary >üåê Bahasa</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-TW">ÁπÅÈ´î‰∏≠Êñá</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=hi">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=th">‡πÑ‡∏ó‡∏¢</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fr">Fran√ßais</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=es">Espa√±ol</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=it">Italiano</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ru">–†—É—Å—Å–∫–∏–π</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pt">Portugu√™s</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ar">ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fa">ŸÅÿßÿ±ÿ≥€å</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=tr">T√ºrk√ße</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=vi">Ti·∫øng Vi·ªát</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=id">Bahasa Indonesia</a>
      </div>
    </div>
  </details>
</div>

<div align="center">

# üöÄ Tidak Ada Waktu untuk Melatih!  
### Segmentasi Instance Berbasis Referensi Tanpa Pelatihan  
[![GitHub](https://img.shields.io/badge/%E2%80%8B-No%20Time%20To%20Train-black?logo=github)](https://github.com/miquel-espinosa/no-time-to-train)
[![Website](https://img.shields.io/badge/üåê-Project%20Page-grey)](https://miquel-espinosa.github.io/no-time-to-train/)
[![arXiv](https://img.shields.io/badge/arXiv-2507.02798-b31b1b)](https://arxiv.org/abs/2507.02798)

**State-of-the-art (Papers with Code)**

[**_SOTA 1-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/badge/State%20of%20the%20Art-Few--Shot%20Object%20Detection%20on%20MS--COCO%20(1--shot)-21CBCE?style=flat&logo=paperswithcode)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference)

[**_SOTA 10-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/badge/State%20of%20the%20Art-Few--Shot%20Object%20Detection%20on%20MS--COCO%20(10--shot)-21CBCE?style=flat&logo=paperswithcode)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference)

[**_SOTA 30-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/badge/State%20of%20the%20Art-Few--Shot%20Object%20Detection%20on%20MS--COCO%20(30--shot)-21CBCE?style=flat&logo=paperswithcode)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference)

<!-- [**_SOTA 1-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-1-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference)

[**_SOTA 10-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-10-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference)

[**_SOTA 30-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-30-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference) -->

</div>

---

> üö® **Update (22 Juli 2025):** Instruksi untuk dataset kustom telah ditambahkan!
> 
> üîî **Update (16 Juli 2025):** Kode telah diperbarui beserta instruksinya!

---

## üìã Daftar Isi

- [üéØ Sorotan](#-sorotan)
- [üìú Abstrak](#-abstrak)
- [üß† Arsitektur](#-arsitektur)
- [üõ†Ô∏è Instruksi Instalasi](#Ô∏è-instruksi-instalasi)
  - [1. Clone repositori](#1-clone-repositori)
  - [2. Buat environment conda](#2-buat-environment-conda)
  - [3. Instal SAM2 dan DinoV2](#3-instal-sam2-dan-dinov2)
  - [4. Unduh dataset](#4-unduh-dataset)
  - [5. Unduh checkpoint SAM2 dan DinoV2](#5-unduh-checkpoint-sam2-dan-dinov2)
- [üìä Kode inferensi: Reproduksi hasil SOTA 30-shot pada Few-shot COCO](#-kode-inferensi)
  - [0. Buat set referensi](#0-buat-set-referensi)
  - [1. Isi memori dengan referensi](#1-isi-memori-dengan-referensi)
  - [2. Pascaproses memory bank](#2-pascaproses-memory-bank)
  - [3. Inferensi pada gambar target](#3-inferensi-pada-gambar-target)
  - [Hasil](#hasil)
- [üîç Dataset kustom](#-dataset-kustom)
  - [0. Siapkan dataset kustom ‚õµüê¶](#0-siapkan-dataset-kustom)
  - [0.1 Jika hanya tersedia anotasi bbox](#01-jika-hanya-tersedia-anotasi-bbox)
  - [0.2 Konversi anotasi coco ke file pickle](#02-konversi-anotasi-coco-ke-file-pickle)
  - [1. Isi memori dengan referensi](#1-isi-memori-dengan-referensi)
  - [2. Pascaproses memory bank](#2-pascaproses-memory-bank)
- [üìö Sitasi](#-sitasi)


## üéØ Sorotan
- üí° **Tanpa Pelatihan**: Tanpa fine-tuning, tanpa rekayasa prompt‚Äîhanya butuh gambar referensi.  
- üñºÔ∏è **Berbasis Referensi**: Segmentasikan objek baru hanya dengan beberapa contoh.  
- üî• **Performa SOTA**: Mengungguli pendekatan tanpa pelatihan sebelumnya pada COCO, PASCAL VOC, dan Cross-Domain FSOD.

**Tautan:**
- üßæ [**Makalah arXiv**](https://arxiv.org/abs/2507.02798)  
- üåê [**Website Proyek**](https://miquel-espinosa.github.io/no-time-to-train/)  
- üìà [**Papers with Code**](https://paperswithcode.com/paper/no-time-to-train-training-free-reference)

## üìú Abstrak

> The performance of image segmentation models has historically been constrained by the high cost of collecting large-scale annotated data. The Segment Anything Model (SAM) alleviates this original problem through a promptable, semantics-agnostic, segmentation paradigm and yet still requires manual visual-prompts or complex domain-dependent prompt-generation rules to process a new image. Towards reducing this new burden, our work investigates the task of object segmentation when provided with, alternatively, only a small set of reference images. Our key insight is to leverage strong semantic priors, as learned by foundation models, to identify corresponding regions between a reference and a target image. We find that correspondences enable automatic generation of instance-level segmentation masks for downstream tasks and instantiate our ideas via a multi-stage, training-free method incorporating (1) memory bank construction; (2) representation aggregation and (3) semantic-aware feature matching. Our experiments show significant improvements on segmentation metrics, leading to state-of-the-art performance on COCO FSOD (36.8% nAP), PASCAL VOC Few-Shot (71.2% nAP50) and outperforming existing training-free approaches on the Cross-Domain FSOD benchmark (22.4% nAP).

![cdfsod-results-final-comic-sans-min](https://github.com/user-attachments/assets/ab302c02-c080-4042-99fc-0e181ba8abb9)


## üß† Architecture

![training-free-architecture-comic-sans-min](https://github.com/user-attachments/assets/d84dd83a-505e-45a0-8ce3-98e1838017f9)


## üõ†Ô∏è Installation instructions

### 1. Clone the repository

```bash
git clone https://github.com/miquel-espinosa/no-time-to-train.git
cd no-time-to-train
```
### 2. Membuat lingkungan conda

Kita akan membuat lingkungan conda dengan paket-paket yang diperlukan.

```bash
conda env create -f environment.yml
conda activate no-time-to-train
```
### 3. Instal SAM2 dan DinoV2

Kita akan menginstal SAM2 dan DinoV2 dari sumbernya.

```bash
pip install -e .
cd dinov2
pip install -e .
cd ..
```
### 4. Unduh dataset

Silakan unduh dataset COCO dan letakkan di `data/coco`

### 5. Unduh checkpoint SAM2 dan DinoV2

Kami akan mengunduh checkpoint SAM2 yang persis digunakan dalam makalah.
(Namun, perlu dicatat bahwa checkpoint SAM2.1 sudah tersedia dan mungkin memberikan performa yang lebih baik.)


```bash
mkdir -p checkpoints/dinov2
cd checkpoints
wget https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt
cd dinov2
wget https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth
cd ../..
```
## üìä Kode Inferensi

‚ö†Ô∏è Penafian: Ini adalah kode penelitian ‚Äî harap maklum jika ada sedikit kekacauan!

### Mereproduksi hasil SOTA 30-shot pada Few-shot COCO

Definisikan variabel yang diperlukan dan buat folder untuk hasil:



```bash
CONFIG=./no_time_to_train/new_exps/coco_fewshot_10shot_Sam2L.yaml
CLASS_SPLIT="few_shot_classes"
RESULTS_DIR=work_dirs/few_shot_results
SHOTS=30
SEED=33
GPUS=4

mkdir -p $RESULTS_DIR
FILENAME=few_shot_${SHOTS}shot_seed${SEED}.pkl
```
#### 0. Buat set referensi


```bash
python no_time_to_train/dataset/few_shot_sampling.py \
        --n-shot $SHOTS \
        --out-path ${RESULTS_DIR}/${FILENAME} \
        --seed $SEED \
        --dataset $CLASS_SPLIT
```
#### 1. Isi memori dengan referensi


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode fill_memory \
                              --out_path ${RESULTS_DIR}/memory.ckpt \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.memory_pkl ${RESULTS_DIR}/${FILENAME} \
                              --model.init_args.dataset_cfgs.fill_memory.memory_length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
#### 2. Memori bank pasca-proses


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode postprocess_memory \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --ckpt_path ${RESULTS_DIR}/memory.ckpt \
                              --out_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --trainer.devices 1
```
#### 3. Inferensi pada gambar target


```bash
python run_lightening.py test --config $CONFIG  \
                              --ckpt_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --model.init_args.test_mode test \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.model_cfg.dataset_name $CLASS_SPLIT \
                              --model.init_args.dataset_cfgs.test.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
Jika Anda ingin melihat hasil inferensi secara online (saat sedang dihitung), hapus komentar pada baris 1746-1749 di `no_time_to_train/models/Sam2MatchingBaseline_noAMG.py` [di sini](https://github.com/miquel-espinosa/no-time-to-train/blob/main/no_time_to_train/models/Sam2MatchingBaseline_noAMG.py#L1746).
Sesuaikan parameter ambang skor `score_thr` sesuai kebutuhan untuk melihat lebih banyak atau lebih sedikit instance yang disegmentasi.
Gambar sekarang akan disimpan di `results_analysis/few_shot_classes/`. Gambar di sebelah kiri menunjukkan ground truth, gambar di sebelah kanan menunjukkan instance yang disegmentasi yang ditemukan oleh metode tanpa pelatihan kami.

Perhatikan bahwa pada contoh ini kita menggunakan split `few_shot_classes`, sehingga, kita hanya mengharapkan melihat instance yang disegmentasi dari kelas-kelas dalam split ini (bukan semua kelas di COCO).

#### Hasil

Setelah menjalankan semua gambar dalam set validasi, Anda seharusnya mendapatkan:


```
BBOX RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.368

SEGM RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.342
```
---

## üîç Dataset kustom

Kami menyediakan instruksi untuk menjalankan pipeline kami pada dataset kustom. Format anotasi selalu menggunakan format COCO.

> **TLDR;** Untuk langsung melihat cara menjalankan pipeline lengkap pada *dataset kustom*, temukan `scripts/matching_cdfsod_pipeline.sh` bersama dengan contoh skrip dataset CD-FSOD (misal `scripts/dior_fish.sh`)

### 0. Siapkan dataset kustom ‚õµüê¶

Bayangkan kita ingin mendeteksi **perahu**‚õµ dan **burung**üê¶ dalam dataset kustom. Untuk menggunakan metode kami, kita akan membutuhkan:
- Minimal 1 gambar referensi *yang sudah dianotasi* untuk setiap kelas (yaitu 1 gambar referensi untuk perahu dan 1 gambar referensi untuk burung)
- Beberapa gambar target untuk menemukan instance dari kelas yang diinginkan.

Kami telah menyiapkan skrip sederhana untuk membuat dataset kustom dengan gambar coco, untuk skenario **1-shot**.
```bash
python scripts/make_custom_dataset.py
```
Ini akan membuat dataset khusus dengan struktur folder sebagai berikut:
```
data/my_custom_dataset/
    ‚îú‚îÄ‚îÄ annotations/
    ‚îÇ   ‚îú‚îÄ‚îÄ custom_references.json
    ‚îÇ   ‚îú‚îÄ‚îÄ custom_targets.json
    ‚îÇ   ‚îî‚îÄ‚îÄ references_visualisations/
    ‚îÇ       ‚îú‚îÄ‚îÄ bird_1.jpg
    ‚îÇ       ‚îî‚îÄ‚îÄ boat_1.jpg
    ‚îî‚îÄ‚îÄ images/
        ‚îú‚îÄ‚îÄ 429819.jpg
        ‚îú‚îÄ‚îÄ 101435.jpg
        ‚îî‚îÄ‚îÄ (all target and reference images)
```
**Visualisasi gambar referensi (1-shot):**

| Gambar Referensi 1-shot untuk BURUNG üê¶ | Gambar Referensi 1-shot untuk PERAHU ‚õµ |
|:---------------------------------------:|:--------------------------------------:|
| <img src="https://github.com/user-attachments/assets/e59e580d-a7db-42ac-b386-892af211fc85" alt="bird_1" width="500"/> | <img src="https://github.com/user-attachments/assets/f94ee025-ae37-4a45-9c3e-0cfe8f8cd2bc" alt="boat_1" width="500"/> |


### 0.1 Jika hanya anotasi bbox yang tersedia

Kami juga menyediakan skrip untuk menghasilkan mask segmentasi tingkat instansi dengan menggunakan SAM2. Ini berguna jika Anda hanya memiliki anotasi bounding box untuk gambar referensi.


```bash
# Download sam_h checkpoint. Feel free to use more recent checkpoints (note: code might need to be adapted)
wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth -O checkpoints/sam_vit_h_4b8939.pth
# Run automatic instance segmentation from ground truth bounding boxes.
python no_time_to_train/dataset/sam_bbox_to_segm_batch.py \
    --input_json data/my_custom_dataset/annotations/custom_references.json \
    --image_dir data/my_custom_dataset/images \
    --sam_checkpoint checkpoints/sam_vit_h_4b8939.pth \
    --model_type vit_h \
    --device cuda \
    --batch_size 8 \
    --visualize
```
**Gambar referensi dengan masker segmentasi tingkat instansi (dihasilkan oleh SAM2 dari kotak pembatas gt, 1-shot):**

Visualisasi dari masker segmentasi yang dihasilkan disimpan di `data/my_custom_dataset/annotations/custom_references_with_SAM_segm/references_visualisations/`.


| Gambar Referensi 1-shot untuk BURUNG üê¶ (disegmentasi otomatis dengan SAM) | Gambar Referensi 1-shot untuk PERAHU ‚õµ (disegmentasi otomatis dengan SAM) |
|:---------------------------------:|:----------------------------------:|
| <img src="https://github.com/user-attachments/assets/65d38dc4-1454-43cd-9600-e8efc67b3a82" alt="bird_1_with_SAM_segm" width="500"/> | <img src="https://github.com/user-attachments/assets/43a558ad-50ca-4715-8285-9aa3268843c6" alt="boat_1_with_SAM_segm" width="500"/> |


### 0.2 Konversi anotasi coco ke file pickle


```bash
python no_time_to_train/dataset/coco_to_pkl.py \
    data/my_custom_dataset/annotations/custom_references_with_segm.json \
    data/my_custom_dataset/annotations/custom_references_with_segm.pkl \
    1
```
### 1. Mengisi memori dengan referensi

Pertama, definisikan variabel yang diperlukan dan buat folder untuk hasil. Untuk visualisasi label yang benar, nama kelas harus diurutkan berdasarkan id kategori seperti yang muncul di file json. Misalnya, `bird` memiliki id kategori `16`, `boat` memiliki id kategori `9`. Jadi, `CAT_NAMES=boat,bird`.


```bash
DATASET_NAME=my_custom_dataset
DATASET_PATH=data/my_custom_dataset
CAT_NAMES=boat,bird
CATEGORY_NUM=2
SHOT=1
YAML_PATH=no_time_to_train/pl_configs/matching_cdfsod_template.yaml
PATH_TO_SAVE_CKPTS=./tmp_ckpts/my_custom_dataset
mkdir -p $PATH_TO_SAVE_CKPTS
```
Jalankan langkah 1:

```bash
python run_lightening.py test --config $YAML_PATH \
    --model.test_mode fill_memory \
    --out_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory.pth \
    --model.init_args.dataset_cfgs.fill_memory.root $DATASET_PATH/images \
    --model.init_args.dataset_cfgs.fill_memory.json_file $DATASET_PATH/annotations/custom_references_with_segm.json \
    --model.init_args.dataset_cfgs.fill_memory.memory_pkl $DATASET_PATH/annotations/custom_references_with_segm.pkl \
    --model.init_args.dataset_cfgs.fill_memory.memory_length $SHOT \
    --model.init_args.dataset_cfgs.fill_memory.cat_names $CAT_NAMES \
    --model.init_args.model_cfg.dataset_name $DATASET_NAME \
    --model.init_args.model_cfg.memory_bank_cfg.length $SHOT \
    --model.init_args.model_cfg.memory_bank_cfg.category_num $CATEGORY_NUM \
    --trainer.devices 1
```
### 2. Bank memori pasca-proses


```bash
python run_lightening.py test --config $YAML_PATH \
    --model.test_mode postprocess_memory \
    --ckpt_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory.pth \
    --out_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory_postprocessed.pth \
    --model.init_args.model_cfg.dataset_name $DATASET_NAME \
    --model.init_args.model_cfg.memory_bank_cfg.length $SHOT \
    --model.init_args.model_cfg.memory_bank_cfg.category_num $CATEGORY_NUM \
    --trainer.devices 1
```
### 3. Inferensi pada gambar target

Jika `ONLINE_VIS` disetel ke True, hasil prediksi akan disimpan di `results_analysis/my_custom_dataset/` dan ditampilkan saat diproses. PERHATIKAN bahwa menjalankan dengan visualisasi online jauh lebih lambat.

Silakan ubah ambang skor `VIS_THR` untuk melihat lebih banyak atau lebih sedikit instance yang tersegmentasi.

```bash
ONLINE_VIS=True
VIS_THR=0.4
python run_lightening.py test --config $YAML_PATH \
    --model.test_mode test \
    --ckpt_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory_postprocessed.pth \
    --model.init_args.model_cfg.dataset_name $DATASET_NAME \
    --model.init_args.model_cfg.memory_bank_cfg.length $SHOT \
    --model.init_args.model_cfg.memory_bank_cfg.category_num $CATEGORY_NUM \
    --model.init_args.model_cfg.test.imgs_path $DATASET_PATH/images \
    --model.init_args.model_cfg.test.online_vis $ONLINE_VIS \
    --model.init_args.model_cfg.test.vis_thr $VIS_THR \
    --model.init_args.dataset_cfgs.test.root $DATASET_PATH/images \
    --model.init_args.dataset_cfgs.test.json_file $DATASET_PATH/annotations/custom_targets.json \
    --model.init_args.dataset_cfgs.test.cat_names $CAT_NAMES \
    --trainer.devices 1
```
### Hasil

Metode pengukuran kinerja (dengan parameter yang sama persis seperti perintah di atas) seharusnya:


```
BBOX RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.478

SEGM RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.458
```
Hasil visual disimpan di `results_analysis/my_custom_dataset/`. Perlu dicatat bahwa metode kami bekerja untuk false negative, yaitu gambar yang tidak mengandung instance dari kelas yang diinginkan.

*Klik gambar untuk memperbesar ‚¨áÔ∏è*

| Gambar target dengan perahu ‚õµ (kiri GT, kanan prediksi) | Gambar target dengan burung üê¶ (kiri GT, kanan prediksi) |
|:----------------------:|:----------------------:|
| ![000000459673](https://github.com/user-attachments/assets/678dc15a-dd3b-49d5-9287-6290da16aa6b) | ![000000407180](https://github.com/user-attachments/assets/fe306e48-af49-4d83-ac82-76fac6c456d1) |

| Gambar target dengan perahu dan burung ‚õµüê¶ (kiri GT, kanan prediksi) | Gambar target tanpa perahu atau burung üö´ (kiri GT, kanan prediksi) |
|:---------------------------------:|:----------------------------------:|
| ![000000517410](https://github.com/user-attachments/assets/9849b227-7f43-43d7-81ea-58010a623ad5) | ![000000460598](https://github.com/user-attachments/assets/7587700c-e09d-4cf6-8590-3df129c2568e) |


## üìö Sitasi

Jika Anda menggunakan karya ini, mohon kutip kami:


```bibtex
@article{espinosa2025notimetotrain,
  title={No time to train! Training-Free Reference-Based Instance Segmentation},
  author={Miguel Espinosa and Chenhongyi Yang and Linus Ericsson and Steven McDonagh and Elliot J. Crowley},
  journal={arXiv preprint arXiv:2507.02798},
  year={2025},
  primaryclass={cs.CV}
}
```


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-24

---