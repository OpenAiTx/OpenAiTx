<div align="right">
  <details>
    <summary >🌐 Bahasa</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-CN">简体中文</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-TW">繁體中文</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ko">한국어</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=hi">हिन्दी</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=th">ไทย</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fr">Français</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=es">Español</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=it">Italiano</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ru">Русский</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pt">Português</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ar">العربية</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fa">فارسی</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=tr">Türkçe</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=vi">Tiếng Việt</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=id">Bahasa Indonesia</a>
      </div>
    </div>
  </details>
</div>

<div align="center">

# 🚀 Tidak Ada Waktu untuk Melatih!  
### Segmentasi Instance Berbasis Referensi Tanpa Pelatihan  
[![GitHub](https://img.shields.io/badge/%E2%80%8B-No%20Time%20To%20Train-black?logo=github)](https://github.com/miquel-espinosa/no-time-to-train)
[![Website](https://img.shields.io/badge/🌐-Project%20Page-grey)](https://miquel-espinosa.github.io/no-time-to-train/)
[![arXiv](https://img.shields.io/badge/arXiv-2507.02798-b31b1b)](https://arxiv.org/abs/2507.02798)

**State-of-the-art (Papers with Code)**

[**_SOTA 1-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-1-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference)

[**_SOTA 10-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-10-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference)

[**_SOTA 30-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-30-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference)

</div>

---

> 🚨 **Pembaruan (22 Juli 2025):** Instruksi untuk dataset kustom telah ditambahkan!
> 
> 🔔 **Pembaruan (16 Juli 2025):** Kode telah diperbarui beserta instruksinya!

---

## 📋 Daftar Isi

- [🎯 Sorotan](#-highlights)
- [📜 Abstrak](#-abstract)
- [🧠 Arsitektur](#-architecture)
- [🛠️ Instruksi instalasi](#️-installation-instructions)
  - [1. Kloning repositori](#1-clone-the-repository)
  - [2. Buat environment conda](#2-create-conda-environment)
  - [3. Instal SAM2 dan DinoV2](#3-install-sam2-and-dinov2)
  - [4. Unduh dataset](#4-download-datasets)
  - [5. Unduh checkpoint SAM2 dan DinoV2](#5-download-sam2-and-dinov2-checkpoints)
- [📊 Kode inferensi: Reproduksi hasil SOTA 30-shot di Few-shot COCO](#-inference-code)
  - [0. Buat set referensi](#0-create-reference-set)
  - [1. Isi memori dengan referensi](#1-fill-memory-with-references)
  - [2. Pasca-proses memory bank](#2-post-process-memory-bank)
  - [3. Inferensi pada gambar target](#3-inference-on-target-images)
  - [Hasil](#results)
- [🔍 Dataset kustom](#-custom-dataset)
  - [0. Siapkan dataset kustom ⛵🐦](#0-prepare-a-custom-dataset)
  - [0.1 Jika hanya anotasi bbox yang tersedia](#01-if-only-bbox-annotations-are-available)
  - [0.2 Konversi anotasi coco ke file pickle](#02-convert-coco-annotations-to-pickle-file)
  - [1. Isi memori dengan referensi](#1-fill-memory-with-references)
  - [2. Pasca-proses memory bank](#2-post-process-memory-bank)
- [📚 Sitasi](#-citation)


## 🎯 Sorotan
- 💡 **Tanpa Pelatihan**: Tanpa fine-tuning, tanpa rekayasa prompt—cukup gambar referensi.  
- 🖼️ **Berbasis Referensi**: Segmentasikan objek baru hanya dengan beberapa contoh.  
- 🔥 **Performa SOTA**: Mengungguli pendekatan tanpa pelatihan sebelumnya pada COCO, PASCAL VOC, dan Cross-Domain FSOD.

**Tautan:**
- 🧾 [**Makalah arXiv**](https://arxiv.org/abs/2507.02798)  
- 🌐 [**Situs Proyek**](https://miquel-espinosa.github.io/no-time-to-train/)  
- 📈 [**Papers with Code**](https://paperswithcode.com/paper/no-time-to-train-training-free-reference)

## 📜 Abstrak

> Performa model segmentasi gambar secara historis dibatasi oleh tingginya biaya pengumpulan data beranotasi skala besar. Segment Anything Model (SAM) mengatasi masalah awal ini melalui paradigma segmentasi yang dapat diprompt dan agnostik terhadap semantik, namun tetap memerlukan prompt visual manual atau aturan pembuatan prompt yang kompleks dan tergantung domain untuk memproses gambar baru. Untuk mengurangi beban baru ini, karya kami meneliti tugas segmentasi objek dengan hanya menyediakan sejumlah kecil gambar referensi. Inti dari pendekatan kami adalah memanfaatkan prior semantik yang kuat, sebagaimana dipelajari oleh model fondasi, untuk mengidentifikasi wilayah yang sesuai antara gambar referensi dan gambar target. Kami menemukan bahwa korespondensi ini memungkinkan pembuatan otomatis mask segmentasi tingkat instance untuk tugas lanjutan dan mengimplementasikan ide kami melalui metode multi-tahap tanpa pelatihan yang mencakup (1) konstruksi memory bank; (2) agregasi representasi dan (3) pencocokan fitur berbasis semantik. Eksperimen kami menunjukkan peningkatan signifikan pada metrik segmentasi, menghasilkan performa state-of-the-art pada COCO FSOD (36.8% nAP), PASCAL VOC Few-Shot (71.2% nAP50) dan melampaui pendekatan tanpa pelatihan yang ada pada benchmark Cross-Domain FSOD (22.4% nAP).

![cdfsod-results-final-comic-sans-min](https://github.com/user-attachments/assets/ab302c02-c080-4042-99fc-0e181ba8abb9)




## 🧠 Arsitektur

![training-free-architecture-comic-sans-min](https://github.com/user-attachments/assets/d84dd83a-505e-45a0-8ce3-98e1838017f9)


## 🛠️ Instruksi instalasi

### 1. Kloning repositori

```bash
git clone https://github.com/miquel-espinosa/no-time-to-train.git
cd no-time-to-train
```
### 2. Membuat lingkungan conda

Kita akan membuat lingkungan conda dengan paket-paket yang diperlukan.

```bash
conda env create -f environment.yml
conda activate no-time-to-train
```
### 3. Instal SAM2 dan DinoV2

Kita akan menginstal SAM2 dan DinoV2 dari sumbernya.

```bash
pip install -e .
cd dinov2
pip install -e .
cd ..
```
### 4. Unduh dataset

Silakan unduh dataset COCO dan letakkan di `data/coco`

### 5. Unduh checkpoint SAM2 dan DinoV2

Kami akan mengunduh checkpoint SAM2 yang persis digunakan dalam makalah.
(Namun, perlu dicatat bahwa checkpoint SAM2.1 sudah tersedia dan mungkin memberikan performa yang lebih baik.)


```bash
mkdir -p checkpoints/dinov2
cd checkpoints
wget https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt
cd dinov2
wget https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth
cd ../..
```
## 📊 Kode Inferensi

⚠️ Penafian: Ini adalah kode penelitian — harap maklum jika ada sedikit kekacauan!

### Mereproduksi hasil SOTA 30-shot pada Few-shot COCO

Definisikan variabel yang diperlukan dan buat folder untuk hasil:



```bash
CONFIG=./no_time_to_train/new_exps/coco_fewshot_10shot_Sam2L.yaml
CLASS_SPLIT="few_shot_classes"
RESULTS_DIR=work_dirs/few_shot_results
SHOTS=30
SEED=33
GPUS=4

mkdir -p $RESULTS_DIR
FILENAME=few_shot_${SHOTS}shot_seed${SEED}.pkl
```
#### 0. Buat set referensi


```bash
python no_time_to_train/dataset/few_shot_sampling.py \
        --n-shot $SHOTS \
        --out-path ${RESULTS_DIR}/${FILENAME} \
        --seed $SEED \
        --dataset $CLASS_SPLIT
```
#### 1. Isi memori dengan referensi


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode fill_memory \
                              --out_path ${RESULTS_DIR}/memory.ckpt \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.memory_pkl ${RESULTS_DIR}/${FILENAME} \
                              --model.init_args.dataset_cfgs.fill_memory.memory_length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
#### 2. Memori bank pasca-proses


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode postprocess_memory \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --ckpt_path ${RESULTS_DIR}/memory.ckpt \
                              --out_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --trainer.devices 1
```
#### 3. Inferensi pada gambar target


```bash
python run_lightening.py test --config $CONFIG  \
                              --ckpt_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --model.init_args.test_mode test \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.model_cfg.dataset_name $CLASS_SPLIT \
                              --model.init_args.dataset_cfgs.test.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
Jika Anda ingin melihat hasil inferensi secara online (saat sedang dihitung), hapus komentar pada baris 1746-1749 di `no_time_to_train/models/Sam2MatchingBaseline_noAMG.py` [di sini](https://github.com/miquel-espinosa/no-time-to-train/blob/main/no_time_to_train/models/Sam2MatchingBaseline_noAMG.py#L1746).
Sesuaikan parameter ambang skor `score_thr` sesuai kebutuhan untuk melihat lebih banyak atau lebih sedikit instance yang disegmentasi.
Gambar sekarang akan disimpan di `results_analysis/few_shot_classes/`. Gambar di sebelah kiri menunjukkan ground truth, gambar di sebelah kanan menunjukkan instance yang disegmentasi yang ditemukan oleh metode tanpa pelatihan kami.

Perhatikan bahwa pada contoh ini kita menggunakan split `few_shot_classes`, sehingga, kita hanya mengharapkan melihat instance yang disegmentasi dari kelas-kelas dalam split ini (bukan semua kelas di COCO).

#### Hasil

Setelah menjalankan semua gambar dalam set validasi, Anda seharusnya mendapatkan:


```
BBOX RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.368

SEGM RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.342
```
---

## 🔍 Dataset kustom

Kami menyediakan instruksi untuk menjalankan pipeline kami pada dataset kustom. Format anotasi selalu menggunakan format COCO.

> **TLDR;** Untuk langsung melihat cara menjalankan pipeline lengkap pada *dataset kustom*, temukan `scripts/matching_cdfsod_pipeline.sh` bersama dengan contoh skrip dataset CD-FSOD (misal `scripts/dior_fish.sh`)

### 0. Siapkan dataset kustom ⛵🐦

Bayangkan kita ingin mendeteksi **perahu**⛵ dan **burung**🐦 dalam dataset kustom. Untuk menggunakan metode kami, kita akan membutuhkan:
- Minimal 1 gambar referensi *yang sudah dianotasi* untuk setiap kelas (yaitu 1 gambar referensi untuk perahu dan 1 gambar referensi untuk burung)
- Beberapa gambar target untuk menemukan instance dari kelas yang diinginkan.

Kami telah menyiapkan skrip sederhana untuk membuat dataset kustom dengan gambar coco, untuk skenario **1-shot**.
```bash
python scripts/make_custom_dataset.py
```
Ini akan membuat dataset khusus dengan struktur folder sebagai berikut:
```
data/my_custom_dataset/
    ├── annotations/
    │   ├── custom_references.json
    │   ├── custom_targets.json
    │   └── references_visualisations/
    │       ├── bird_1.jpg
    │       └── boat_1.jpg
    └── images/
        ├── 429819.jpg
        ├── 101435.jpg
        └── (all target and reference images)
```
**Visualisasi gambar referensi (1-shot):**

| Gambar Referensi 1-shot untuk BURUNG 🐦 | Gambar Referensi 1-shot untuk PERAHU ⛵ |
|:---------------------------------------:|:--------------------------------------:|
| <img src="https://github.com/user-attachments/assets/e59e580d-a7db-42ac-b386-892af211fc85" alt="bird_1" width="500"/> | <img src="https://github.com/user-attachments/assets/f94ee025-ae37-4a45-9c3e-0cfe8f8cd2bc" alt="boat_1" width="500"/> |


### 0.1 Jika hanya anotasi bbox yang tersedia

Kami juga menyediakan skrip untuk menghasilkan mask segmentasi tingkat instansi dengan menggunakan SAM2. Ini berguna jika Anda hanya memiliki anotasi bounding box untuk gambar referensi.


```bash
# Download sam_h checkpoint. Feel free to use more recent checkpoints (note: code might need to be adapted)
wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth -O checkpoints/sam_vit_h_4b8939.pth
# Run automatic instance segmentation from ground truth bounding boxes.
python no_time_to_train/dataset/sam_bbox_to_segm_batch.py \
    --input_json data/my_custom_dataset/annotations/custom_references.json \
    --image_dir data/my_custom_dataset/images \
    --sam_checkpoint checkpoints/sam_vit_h_4b8939.pth \
    --model_type vit_h \
    --device cuda \
    --batch_size 8 \
    --visualize
```
**Gambar referensi dengan masker segmentasi tingkat instansi (dihasilkan oleh SAM2 dari kotak pembatas gt, 1-shot):**

Visualisasi dari masker segmentasi yang dihasilkan disimpan di `data/my_custom_dataset/annotations/custom_references_with_SAM_segm/references_visualisations/`.


| Gambar Referensi 1-shot untuk BURUNG 🐦 (disegmentasi otomatis dengan SAM) | Gambar Referensi 1-shot untuk PERAHU ⛵ (disegmentasi otomatis dengan SAM) |
|:---------------------------------:|:----------------------------------:|
| <img src="https://github.com/user-attachments/assets/65d38dc4-1454-43cd-9600-e8efc67b3a82" alt="bird_1_with_SAM_segm" width="500"/> | <img src="https://github.com/user-attachments/assets/43a558ad-50ca-4715-8285-9aa3268843c6" alt="boat_1_with_SAM_segm" width="500"/> |


### 0.2 Konversi anotasi coco ke file pickle


```bash
python no_time_to_train/dataset/coco_to_pkl.py \
    data/my_custom_dataset/annotations/custom_references_with_segm.json \
    data/my_custom_dataset/annotations/custom_references_with_segm.pkl \
    1
```
### 1. Mengisi memori dengan referensi

Pertama, definisikan variabel yang diperlukan dan buat folder untuk hasil. Untuk visualisasi label yang benar, nama kelas harus diurutkan berdasarkan id kategori seperti yang muncul di file json. Misalnya, `bird` memiliki id kategori `16`, `boat` memiliki id kategori `9`. Jadi, `CAT_NAMES=boat,bird`.


```bash
DATASET_NAME=my_custom_dataset
DATASET_PATH=data/my_custom_dataset
CAT_NAMES=boat,bird
CATEGORY_NUM=2
SHOT=1
YAML_PATH=no_time_to_train/pl_configs/matching_cdfsod_template.yaml
PATH_TO_SAVE_CKPTS=./tmp_ckpts/my_custom_dataset
mkdir -p $PATH_TO_SAVE_CKPTS
```
Jalankan langkah 1:

```bash
python run_lightening.py test --config $YAML_PATH \
    --model.test_mode fill_memory \
    --out_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory.pth \
    --model.init_args.dataset_cfgs.fill_memory.root $DATASET_PATH/images \
    --model.init_args.dataset_cfgs.fill_memory.json_file $DATASET_PATH/annotations/custom_references_with_segm.json \
    --model.init_args.dataset_cfgs.fill_memory.memory_pkl $DATASET_PATH/annotations/custom_references_with_segm.pkl \
    --model.init_args.dataset_cfgs.fill_memory.memory_length $SHOT \
    --model.init_args.dataset_cfgs.fill_memory.cat_names $CAT_NAMES \
    --model.init_args.model_cfg.dataset_name $DATASET_NAME \
    --model.init_args.model_cfg.memory_bank_cfg.length $SHOT \
    --model.init_args.model_cfg.memory_bank_cfg.category_num $CATEGORY_NUM \
    --trainer.devices 1
```
### 2. Bank memori pasca-proses


```bash
python run_lightening.py test --config $YAML_PATH \
    --model.test_mode postprocess_memory \
    --ckpt_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory.pth \
    --out_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory_postprocessed.pth \
    --model.init_args.model_cfg.dataset_name $DATASET_NAME \
    --model.init_args.model_cfg.memory_bank_cfg.length $SHOT \
    --model.init_args.model_cfg.memory_bank_cfg.category_num $CATEGORY_NUM \
    --trainer.devices 1
```
### 3. Inferensi pada gambar target

Jika `ONLINE_VIS` disetel ke True, hasil prediksi akan disimpan di `results_analysis/my_custom_dataset/` dan ditampilkan saat diproses. PERHATIKAN bahwa menjalankan dengan visualisasi online jauh lebih lambat.

Silakan ubah ambang skor `VIS_THR` untuk melihat lebih banyak atau lebih sedikit instance yang tersegmentasi.

```bash
ONLINE_VIS=True
VIS_THR=0.4
python run_lightening.py test --config $YAML_PATH \
    --model.test_mode test \
    --ckpt_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory_postprocessed.pth \
    --model.init_args.model_cfg.dataset_name $DATASET_NAME \
    --model.init_args.model_cfg.memory_bank_cfg.length $SHOT \
    --model.init_args.model_cfg.memory_bank_cfg.category_num $CATEGORY_NUM \
    --model.init_args.model_cfg.test.imgs_path $DATASET_PATH/images \
    --model.init_args.model_cfg.test.online_vis $ONLINE_VIS \
    --model.init_args.model_cfg.test.vis_thr $VIS_THR \
    --model.init_args.dataset_cfgs.test.root $DATASET_PATH/images \
    --model.init_args.dataset_cfgs.test.json_file $DATASET_PATH/annotations/custom_targets.json \
    --model.init_args.dataset_cfgs.test.cat_names $CAT_NAMES \
    --trainer.devices 1
```
### Hasil

Metode pengukuran kinerja (dengan parameter yang sama persis seperti perintah di atas) seharusnya:


```
BBOX RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.478

SEGM RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.458
```
Hasil visual disimpan di `results_analysis/my_custom_dataset/`. Perlu dicatat bahwa metode kami bekerja untuk false negative, yaitu gambar yang tidak mengandung instance dari kelas yang diinginkan.

*Klik gambar untuk memperbesar ⬇️*

| Gambar target dengan perahu ⛵ (kiri GT, kanan prediksi) | Gambar target dengan burung 🐦 (kiri GT, kanan prediksi) |
|:----------------------:|:----------------------:|
| ![000000459673](https://github.com/user-attachments/assets/678dc15a-dd3b-49d5-9287-6290da16aa6b) | ![000000407180](https://github.com/user-attachments/assets/fe306e48-af49-4d83-ac82-76fac6c456d1) |

| Gambar target dengan perahu dan burung ⛵🐦 (kiri GT, kanan prediksi) | Gambar target tanpa perahu atau burung 🚫 (kiri GT, kanan prediksi) |
|:---------------------------------:|:----------------------------------:|
| ![000000517410](https://github.com/user-attachments/assets/9849b227-7f43-43d7-81ea-58010a623ad5) | ![000000460598](https://github.com/user-attachments/assets/7587700c-e09d-4cf6-8590-3df129c2568e) |


## 📚 Sitasi

Jika Anda menggunakan karya ini, mohon kutip kami:


```bibtex
@article{espinosa2025notimetotrain,
  title={No time to train! Training-Free Reference-Based Instance Segmentation},
  author={Miguel Espinosa and Chenhongyi Yang and Linus Ericsson and Steven McDonagh and Elliot J. Crowley},
  journal={arXiv preprint arXiv:2507.02798},
  year={2025},
  primaryclass={cs.CV}
}
```


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-23

---