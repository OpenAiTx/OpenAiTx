<div align="right">
  <details>
    <summary >üåê Bahasa</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-TW">ÁπÅÈ´î‰∏≠Êñá</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=hi">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=th">‡πÑ‡∏ó‡∏¢</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fr">Fran√ßais</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=es">Espa√±ol</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=it">Italiano</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ru">–†—É—Å—Å–∫–∏–π</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pt">Portugu√™s</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ar">ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fa">ŸÅÿßÿ±ÿ≥€å</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=tr">T√ºrk√ße</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=vi">Ti·∫øng Vi·ªát</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=id">Bahasa Indonesia</a>
      </div>
    </div>
  </details>
</div>

<div align="center">

# üöÄ Tidak Ada Waktu untuk Melatih!  
### Segmentasi Objek Referensi Tanpa Pelatihan  
[![GitHub](https://img.shields.io/badge/%E2%80%8B-No%20Time%20To%20Train-black?logo=github)](https://github.com/miquel-espinosa/no-time-to-train)
[![Website](https://img.shields.io/badge/üåê-Project%20Page-grey)](https://miquel-espinosa.github.io/no-time-to-train/)
[![arXiv](https://img.shields.io/badge/arXiv-2507.02798-b31b1b)](https://arxiv.org/abs/2507.02798)

**Terbaik di Bidangnya (Papers with Code)**

[**_1-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-1-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference)

[**_10-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-10-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference)

[**_30-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-30-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference)

</div>

---

> üîî **Update (Juli 2025):** Kode telah diperbarui dengan instruksi!

---

## üìã Daftar Isi

- [üéØ Sorotan](#-sorotan)
- [üìú Abstrak](#-abstrak)
- [üß† Arsitektur](#-arsitektur)
- [üõ†Ô∏è Instruksi Instalasi](#Ô∏è-instruksi-instalasi)
  - [1. Klon repositori](#1-klon-repositori)
  - [2. Buat lingkungan conda](#2-buat-lingkungan-conda)
  - [3. Instal SAM2 dan DinoV2](#3-instal-sam2-dan-dinov2)
  - [4. Unduh dataset](#4-unduh-dataset)
  - [5. Unduh checkpoint SAM2 dan DinoV2](#5-unduh-checkpoint-sam2-dan-dinov2)
- [üìä Kode Inferensi: Reproduksi hasil SOTA 30-shot di Few-shot COCO](#-kode-inferensi)
  - [0. Buat set referensi](#0-buat-set-referensi)
  - [1. Isi memori dengan referensi](#1-isi-memori-dengan-referensi)
  - [2. Proses ulang bank memori](#2-proses-ulang-bank-memori)
  - [3. Inferensi pada gambar target](#3-inferensi-pada-gambar-target)
  - [Hasil](#hasil)
- [üîç Sitasi](#-sitasi)


## üéØ Sorotan
- üí° **Tanpa Pelatihan**: Tidak perlu fine-tuning, tidak ada rekayasa prompt‚Äîcukup gambar referensi.  
- üñºÔ∏è **Berbasis Referensi**: Segmentasi objek baru hanya dengan beberapa contoh.  
- üî• **Performa SOTA**: Melampaui pendekatan tanpa pelatihan sebelumnya di COCO, PASCAL VOC, dan Cross-Domain FSOD.

**Tautan:**
- üßæ [**Makalah arXiv**](https://arxiv.org/abs/2507.02798)  
- üåê [**Situs Proyek**](https://miquel-espinosa.github.io/no-time-to-train/)  
- üìà [**Papers with Code**](https://paperswithcode.com/paper/no-time-to-train-training-free-reference)

## üìú Abstrak

> Kinerja model segmentasi gambar secara historis dibatasi oleh tingginya biaya pengumpulan data beranotasi skala besar. Segment Anything Model (SAM) mengatasi masalah awal ini melalui paradigma segmentasi yang dapat diprompt, tidak bergantung pada semantik, namun tetap memerlukan prompt visual manual atau aturan pembuatan prompt yang kompleks dan bergantung domain untuk memproses gambar baru. Untuk mengurangi beban baru ini, karya kami meneliti tugas segmentasi objek ketika hanya diberikan sejumlah kecil gambar referensi. Inti dari temuan kami adalah memanfaatkan prior semantik yang kuat, seperti yang dipelajari oleh model fondasi, untuk mengidentifikasi wilayah yang sesuai antara gambar referensi dan gambar target. Kami menemukan bahwa korespondensi ini memungkinkan generasi otomatis mask segmentasi tingkat instance untuk tugas lanjutan dan mengimplementasikan ide kami melalui metode multi-tahap tanpa pelatihan yang mencakup (1) konstruksi bank memori; (2) agregasi representasi dan (3) pencocokan fitur berbasis semantik. Eksperimen kami menunjukkan peningkatan signifikan pada metrik segmentasi, menghasilkan performa terbaik pada COCO FSOD (36.8% nAP), PASCAL VOC Few-Shot (71.2% nAP50) dan melampaui pendekatan tanpa pelatihan yang ada di benchmark Cross-Domain FSOD (22.4% nAP).

![cdfsod-results-final-comic-sans-min](https://github.com/user-attachments/assets/ab302c02-c080-4042-99fc-0e181ba8abb9)


## üß† Arsitektur

![training-free-architecture-comic-sans-min](https://github.com/user-attachments/assets/d84dd83a-505e-45a0-8ce3-98e1838017f9)


## üõ†Ô∏è Instruksi Instalasi

### 1. Klon repositori


```bash
git clone https://github.com/miquel-espinosa/no-time-to-train.git
cd no-time-to-train
```
### 2. Membuat lingkungan conda

Kita akan membuat lingkungan conda dengan paket-paket yang diperlukan.

```bash
conda env create -f environment.yml
conda activate no-time-to-train
```
### 3. Instal SAM2 dan DinoV2

Kita akan menginstal SAM2 dan DinoV2 dari sumbernya.

```bash
pip install -e .
cd dinov2
pip install -e .
cd ..
```
### 4. Unduh dataset

Silakan unduh dataset COCO dan letakkan di `data/coco`

### 5. Unduh checkpoint SAM2 dan DinoV2

Kami akan mengunduh checkpoint SAM2 yang persis digunakan dalam makalah.
(Namun, perlu dicatat bahwa checkpoint SAM2.1 sudah tersedia dan mungkin memberikan performa yang lebih baik.)


```bash
mkdir -p checkpoints/dinov2
cd checkpoints
wget https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt
cd dinov2
wget https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth
cd ../..
```
## üìä Kode Inferensi

‚ö†Ô∏è Penafian: Ini adalah kode penelitian ‚Äî harap maklum jika ada sedikit kekacauan!

### Mereproduksi hasil SOTA 30-shot pada Few-shot COCO

Definisikan variabel yang diperlukan dan buat folder untuk hasil:



```bash
CONFIG=./dev_hongyi/new_exps/coco_fewshot_10shot_Sam2L.yaml
CLASS_SPLIT="few_shot_classes"
RESULTS_DIR=work_dirs/few_shot_results
SHOTS=30
SEED=33
GPUS=4

mkdir -p $RESULTS_DIR
FILENAME=few_shot_${SHOTS}shot_seed${SEED}.pkl
```
#### 0. Buat set referensi


```bash
python dev_hongyi/dataset/few_shot_sampling.py \
        --n-shot $SHOTS \
        --out-path ${RESULTS_DIR}/${FILENAME} \
        --seed $SEED \
        --dataset $CLASS_SPLIT
```
#### 1. Isi memori dengan referensi


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode fill_memory \
                              --out_path ${RESULTS_DIR}/memory.ckpt \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.memory_pkl ${RESULTS_DIR}/${FILENAME} \
                              --model.init_args.dataset_cfgs.fill_memory.memory_length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
#### 2. Memori bank pasca-proses


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode postprocess_memory \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --ckpt_path ${RESULTS_DIR}/memory.ckpt \
                              --out_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --trainer.devices 1
```
#### 3. Inferensi pada gambar target


```bash
python run_lightening.py test --config $CONFIG  \
                              --ckpt_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --model.init_args.test_mode test \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.model_cfg.dataset_name $CLASS_SPLIT \
                              --model.init_args.dataset_cfgs.test.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
Jika Anda ingin melihat hasil inferensi secara online (seiring mereka dihitung), hapus komentar pada baris 1746-1749 di `dev_hongyi/models/Sam2MatchingBaseline_noAMG.py` [di sini](https://github.com/miquel-espinosa/no-time-to-train/blob/main/dev_hongyi/models/Sam2MatchingBaseline_noAMG.py#L1746).
Sesuaikan parameter ambang skor `score_thr` sesuai kebutuhan untuk melihat lebih banyak atau lebih sedikit instance yang tersegmentasi.
Gambar sekarang akan disimpan di `results_analysis/few_shot_classes/`. Gambar di sebelah kiri menunjukkan ground truth, gambar di sebelah kanan menunjukkan instance yang tersegmentasi yang ditemukan oleh metode kami tanpa pelatihan.

Perlu dicatat bahwa dalam contoh ini kita menggunakan split `few_shot_classes`, sehingga, kita hanya boleh mengharapkan untuk melihat instance tersegmentasi dari kelas-kelas dalam split ini (bukan semua kelas di COCO).

#### Hasil

Setelah menjalankan semua gambar pada validation set, Anda seharusnya memperoleh:


```
BBOX RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.368

SEGM RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.342
```
---


## üîç Citation

If you use this work, please cite us:

```bibtex
@article{espinosa2025notimetotrain,
  title={No time to train! Training-Free Reference-Based Instance Segmentation},
  author={Miguel Espinosa and Chenhongyi Yang and Linus Ericsson and Steven McDonagh and Elliot J. Crowley},
  journal={arXiv preprint arXiv:2507.02798},
  year={2025},
  primaryclass={cs.CV}
}
```

---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-22

---