<div align="right">
  <details>
    <summary >🌐 اللغة</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-CN">简体中文</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-TW">繁體中文</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ko">한국어</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=hi">हिन्दी</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=th">ไทย</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fr">Français</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=es">Español</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=it">Italiano</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ru">Русский</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pt">Português</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ar">العربية</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fa">فارسی</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=tr">Türkçe</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=vi">Tiếng Việt</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=id">Bahasa Indonesia</a>
      </div>
    </div>
  </details>
</div>

<div align="center">

# 🚀 لا وقت للتدريب!  
### التقسيم الفوري للعينات بالاعتماد على المرجع وبدون تدريب  
[![GitHub](https://img.shields.io/badge/%E2%80%8B-No%20Time%20To%20Train-black?logo=github)](https://github.com/miquel-espinosa/no-time-to-train)
[![Website](https://img.shields.io/badge/🌐-Project%20Page-grey)](https://miquel-espinosa.github.io/no-time-to-train/)
[![arXiv](https://img.shields.io/badge/arXiv-2507.02798-b31b1b)](https://arxiv.org/abs/2507.02798)

**الأحدث عالميًا (Papers with Code)**

[**_1-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-1-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference)

[**_10-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-10-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference)

[**_30-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-30-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference)

</div>

---

> 🔔 **تحديث (يوليو 2025):** تم تحديث الكود مع التعليمات!

---

## 📋 جدول المحتويات

- [🎯 النقاط البارزة](#-highlights)
- [📜 الملخص](#-abstract)
- [🧠 المعمارية](#-architecture)
- [🛠️ تعليمات التثبيت](#️-installation-instructions)
  - [1. استنساخ المستودع](#1-clone-the-repository)
  - [2. إنشاء بيئة كوندا](#2-create-conda-environment)
  - [3. تثبيت SAM2 و DinoV2](#3-install-sam2-and-dinov2)
  - [4. تحميل مجموعات البيانات](#4-download-datasets)
  - [5. تحميل نقاط تحقق SAM2 و DinoV2](#5-download-sam2-and-dinov2-checkpoints)
- [📊 كود الاستدلال: إعادة إنتاج نتائج SOTA بـ 30 عينة في Few-shot COCO](#-inference-code)
  - [0. إنشاء مجموعة المراجع](#0-create-reference-set)
  - [1. تعبئة الذاكرة بالمراجع](#1-fill-memory-with-references)
  - [2. معالجة بنك الذاكرة بعد التخزين](#2-post-process-memory-bank)
  - [3. الاستدلال على الصور الهدف](#3-inference-on-target-images)
  - [النتائج](#results)
- [🔍 الاقتباس](#-citation)


## 🎯 النقاط البارزة
- 💡 **بدون تدريب**: لا حاجة لإعادة الضبط أو هندسة المطالبات—فقط صورة مرجعية واحدة.  
- 🖼️ **يعتمد على المرجع**: قسّم الكائنات الجديدة باستخدام أمثلة قليلة فقط.  
- 🔥 **أداء SOTA**: يتفوق على الأساليب السابقة بدون تدريب على COCO و PASCAL VOC و Cross-Domain FSOD.

**روابط:**
- 🧾 [**ورقة arXiv**](https://arxiv.org/abs/2507.02798)  
- 🌐 [**موقع المشروع**](https://miquel-espinosa.github.io/no-time-to-train/)  
- 📈 [**Papers with Code**](https://paperswithcode.com/paper/no-time-to-train-training-free-reference)

## 📜 الملخص

> لطالما كان أداء نماذج تقسيم الصور مقيدًا بتكلفة جمع بيانات موسومة على نطاق واسع. يخفف نموذج Segment Anything (SAM) من هذه المشكلة الأصلية من خلال نموذج تقسيم قابل للمطالبة وغير معتمد على الدلالات، لكنه لا يزال يتطلب مطالبات بصرية يدوية أو قواعد معقدة لتوليد المطالبات حسب المجال لمعالجة صورة جديدة. في سبيل تقليل هذا العبء الجديد، تبحث دراستنا في مهمة تقسيم الكائنات عند توفر مجموعة صغيرة فقط من الصور المرجعية. رؤيتنا الأساسية هي الاستفادة من مؤشرات دلالية قوية، تتعلمها نماذج الأساس، لتحديد المناطق المقابلة بين صورة مرجعية وصورة هدف. وجدنا أن هذه المراسلات تمكن من توليد تلقائي لأقنعة تقسيم على مستوى العينات لمهام لاحقة، ونطبق أفكارنا من خلال طريقة متعددة المراحل وبدون تدريب، تتضمن (1) بناء بنك ذاكرة؛ (2) تجميع التمثيلات و(3) مطابقة الميزات المدركة دلاليًا. تظهر تجاربنا تحسنًا ملحوظًا في مقاييس التقسيم، مما يؤدي لتحقيق أداء متقدم على COCO FSOD (بنسبة 36.8% nAP)، وPASCAL VOC Few-Shot (بنسبة 71.2% nAP50) ويتفوق على الأساليب الحالية الخالية من التدريب على معيار Cross-Domain FSOD (بنسبة 22.4% nAP).

![cdfsod-results-final-comic-sans-min](https://github.com/user-attachments/assets/ab302c02-c080-4042-99fc-0e181ba8abb9)


## 🧠 المعمارية

![training-free-architecture-comic-sans-min](https://github.com/user-attachments/assets/d84dd83a-505e-45a0-8ce3-98e1838017f9)


## 🛠️ تعليمات التثبيت

### 1. استنساخ المستودع


```bash
git clone https://github.com/miquel-espinosa/no-time-to-train.git
cd no-time-to-train
```
### ٢. إنشاء بيئة كوندا

سنقوم بإنشاء بيئة كوندا مع الحزم المطلوبة.

```bash
conda env create -f environment.yml
conda activate no-time-to-train
```
### 3. تثبيت SAM2 و DinoV2

سوف نقوم بتثبيت SAM2 و DinoV2 من المصدر.

```bash
pip install -e .
cd dinov2
pip install -e .
cd ..
```
### 4. تنزيل مجموعات البيانات

يرجى تنزيل مجموعة بيانات COCO ووضعها في `data/coco`

### 5. تنزيل نقاط التحقق لـ SAM2 و DinoV2

سنقوم بتنزيل نقاط التحقق الخاصة بـ SAM2 المستخدمة في الورقة البحثية بالضبط.
(يرجى ملاحظة أن نقاط التحقق SAM2.1 متوفرة بالفعل وقد تؤدي بشكل أفضل.)


```bash
mkdir -p checkpoints/dinov2
cd checkpoints
wget https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt
cd dinov2
wget https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth
cd ../..
```
## 📊 كود الاستدلال

⚠️ إخلاء مسؤولية: هذا كود بحثي — توقع بعض الفوضى!

### إعادة إنتاج نتائج SOTA بعدد 30 لقطة في Few-shot COCO

قم بتعريف المتغيرات المفيدة وأنشئ مجلدًا للنتائج:



```bash
CONFIG=./dev_hongyi/new_exps/coco_fewshot_10shot_Sam2L.yaml
CLASS_SPLIT="few_shot_classes"
RESULTS_DIR=work_dirs/few_shot_results
SHOTS=30
SEED=33
GPUS=4

mkdir -p $RESULTS_DIR
FILENAME=few_shot_${SHOTS}shot_seed${SEED}.pkl
```
#### 0. إنشاء مجموعة مرجعية


```bash
python dev_hongyi/dataset/few_shot_sampling.py \
        --n-shot $SHOTS \
        --out-path ${RESULTS_DIR}/${FILENAME} \
        --seed $SEED \
        --dataset $CLASS_SPLIT
```
#### 1. املأ الذاكرة بالمراجع


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode fill_memory \
                              --out_path ${RESULTS_DIR}/memory.ckpt \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.memory_pkl ${RESULTS_DIR}/${FILENAME} \
                              --model.init_args.dataset_cfgs.fill_memory.memory_length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
#### 2. معالجة لاحقة لبنك الذاكرة


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode postprocess_memory \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --ckpt_path ${RESULTS_DIR}/memory.ckpt \
                              --out_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --trainer.devices 1
```
#### 3. الاستدلال على الصور الهدف


```bash
python run_lightening.py test --config $CONFIG  \
                              --ckpt_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --model.init_args.test_mode test \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.model_cfg.dataset_name $CLASS_SPLIT \
                              --model.init_args.dataset_cfgs.test.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
إذا كنت ترغب في مشاهدة نتائج الاستدلال مباشرة عبر الإنترنت (أثناء حسابها)، قم بإلغاء تعليق الأسطر 1746-1749 في الملف `dev_hongyi/models/Sam2MatchingBaseline_noAMG.py` [هنا](https://github.com/miquel-espinosa/no-time-to-train/blob/main/dev_hongyi/models/Sam2MatchingBaseline_noAMG.py#L1746).
قم بتعديل معامل عتبة النتيجة `score_thr` حسب الحاجة لرؤية المزيد أو القليل من الكائنات المجزأة.
سيتم الآن حفظ الصور في `results_analysis/few_shot_classes/`. تعرض الصورة الموجودة على اليسار الحقيقة الأرضية، بينما تعرض الصورة على اليمين الكائنات المجزأة التي تم اكتشافها بواسطة طريقتنا الخالية من التدريب.

يرجى ملاحظة أنه في هذا المثال نستخدم تقسيم `few_shot_classes`، وبالتالي يجب أن نتوقع فقط رؤية الكائنات المجزأة للفئات الموجودة في هذا التقسيم (وليس جميع الفئات في COCO).

#### النتائج

بعد معالجة جميع الصور في مجموعة التحقق، يجب أن تحصل على:


```
BBOX RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.368

SEGM RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.342
```
---


## 🔍 Citation

If you use this work, please cite us:

```bibtex
@article{espinosa2025notimetotrain,
  title={No time to train! Training-Free Reference-Based Instance Segmentation},
  author={Miguel Espinosa and Chenhongyi Yang and Linus Ericsson and Steven McDonagh and Elliot J. Crowley},
  journal={arXiv preprint arXiv:2507.02798},
  year={2025},
  primaryclass={cs.CV}
}
```

---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-22

---