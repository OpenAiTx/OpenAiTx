<div align="right">
  <details>
    <summary >🌐 اللغة</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-CN">简体中文</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-TW">繁體中文</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ko">한국어</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=hi">हिन्दी</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=th">ไทย</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fr">Français</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=es">Español</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=it">Italiano</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ru">Русский</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pt">Português</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ar">العربية</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fa">فارسی</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=tr">Türkçe</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=vi">Tiếng Việt</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=id">Bahasa Indonesia</a>
      </div>
    </div>
  </details>
</div>

<div align="center">

# 🚀 لا وقت للتدريب!  
### التقسيم الفوري للعينات المستند إلى المرجع بدون تدريب  
[![GitHub](https://img.shields.io/badge/%E2%80%8B-No%20Time%20To%20Train-black?logo=github)](https://github.com/miquel-espinosa/no-time-to-train)
[![Website](https://img.shields.io/badge/🌐-Project%20Page-grey)](https://miquel-espinosa.github.io/no-time-to-train/)
[![arXiv](https://img.shields.io/badge/arXiv-2507.02798-b31b1b)](https://arxiv.org/abs/2507.02798)

**أحدث النتائج (Papers with Code)**

[**_الأفضل في فئة 1-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-1-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference)

[**_الأفضل في فئة 10-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-10-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference)

[**_الأفضل في فئة 30-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-30-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference)

</div>

---

> 🚨 **تحديث (22 يوليو 2025):** تمت إضافة تعليمات لمجموعات البيانات المخصصة!
> 
> 🔔 **تحديث (16 يوليو 2025):** تم تحديث الكود مع الإرشادات!

---

## 📋 جدول المحتويات

- [🎯 أبرز النقاط](#-highlights)
- [📜 الملخص](#-abstract)
- [🧠 المعمارية](#-architecture)
- [🛠️ تعليمات التثبيت](#️-installation-instructions)
  - [1. استنساخ المستودع](#1-clone-the-repository)
  - [2. إنشاء بيئة conda](#2-create-conda-environment)
  - [3. تثبيت SAM2 وDinoV2](#3-install-sam2-and-dinov2)
  - [4. تحميل مجموعات البيانات](#4-download-datasets)
  - [5. تحميل نقاط تحقق SAM2 وDinoV2](#5-download-sam2-and-dinov2-checkpoints)
- [📊 كود الاستدلال: إعادة إنتاج نتائج 30-shot الأفضل على COCO](#-inference-code)
  - [0. إنشاء مجموعة مرجعية](#0-create-reference-set)
  - [1. ملء الذاكرة بالمرجعيات](#1-fill-memory-with-references)
  - [2. معالجة بنك الذاكرة](#2-post-process-memory-bank)
  - [3. الاستدلال على الصور المستهدفة](#3-inference-on-target-images)
  - [النتائج](#results)
- [🔍 مجموعة بيانات مخصصة](#-custom-dataset)
  - [0. إعداد مجموعة بيانات مخصصة ⛵🐦](#0-prepare-a-custom-dataset)
  - [0.1 إذا كانت التعليقات المتوفرة فقط مربعات الإحاطة](#01-if-only-bbox-annotations-are-available)
  - [0.2 تحويل تعليقات coco إلى ملف pickle](#02-convert-coco-annotations-to-pickle-file)
  - [1. ملء الذاكرة بالمرجعيات](#1-fill-memory-with-references)
  - [2. معالجة بنك الذاكرة](#2-post-process-memory-bank)
- [📚 الاقتباس](#-citation)


## 🎯 أبرز النقاط
- 💡 **بدون تدريب**: لا حاجة لضبط دقيق، ولا هندسة مطالبات—فقط صورة مرجعية.  
- 🖼️ **استنادًا إلى المرجع**: تقسيم كائنات جديدة باستخدام عدد قليل فقط من الأمثلة.  
- 🔥 **أداء متفوق**: يتفوق على النهج السابقة التي لا تعتمد على التدريب في COCO وPASCAL VOC وFSOD عبر المجالات.

**روابط:**
- 🧾 [**ورقة arXiv**](https://arxiv.org/abs/2507.02798)  
- 🌐 [**موقع المشروع**](https://miquel-espinosa.github.io/no-time-to-train/)  
- 📈 [**Papers with Code**](https://paperswithcode.com/paper/no-time-to-train-training-free-reference)

## 📜 الملخص

> كانت أداء نماذج تقسيم الصور تاريخيًا محدودًا بسبب التكلفة العالية لجمع بيانات موسومة على نطاق واسع. يقوم نموذج Segment Anything (SAM) بتخفيف هذه المشكلة الأصلية من خلال نموذج تقسيم يمكن تحفيزه، غير معتمد على الدلالات، إلا أنه لا يزال يحتاج إلى مطالبات بصرية يدوية أو قواعد توليد مطالبات معقدة تعتمد على المجال لمعالجة صورة جديدة. من أجل تقليل هذا العبء الجديد، يبحث عملنا في مهمة تقسيم الكائنات عند توفر مجموعة صغيرة فقط من الصور المرجعية. رؤيتنا الأساسية هي الاستفادة من المعرفة الدلالية القوية التي تعلمتها النماذج الأساسية لتحديد المناطق المطابقة بين صورة مرجعية وصورة هدف. وجدنا أن العلاقات التوافقية تتيح توليد تلقائي لأقنعة تقسيم على مستوى العينات لمهام تالية وننفذ أفكارنا عبر طريقة متعددة المراحل وبدون تدريب تتضمن (1) بناء بنك ذاكرة؛ (2) تجميع التمثيلات و(3) مطابقة الميزات بناءً على الدلالة. تظهر تجاربنا تحسينات ملحوظة في مقاييس التقسيم، مما يؤدي إلى أداء رائد في COCO FSOD (36.8% nAP)، وPASCAL VOC Few-Shot (71.2% nAP50) ويتفوق على النهج السابقة التي لا تعتمد على التدريب في معيار Cross-Domain FSOD (22.4% nAP).

![cdfsod-results-final-comic-sans-min](https://github.com/user-attachments/assets/ab302c02-c080-4042-99fc-0e181ba8abb9)

</translate-content>

## 🧠 الهيكلية

![training-free-architecture-comic-sans-min](https://github.com/user-attachments/assets/d84dd83a-505e-45a0-8ce3-98e1838017f9)


## 🛠️ تعليمات التثبيت

### 1. استنساخ المستودع

```bash
git clone https://github.com/miquel-espinosa/no-time-to-train.git
cd no-time-to-train
```
### ٢. إنشاء بيئة كوندا

سنقوم بإنشاء بيئة كوندا مع الحزم المطلوبة.

```bash
conda env create -f environment.yml
conda activate no-time-to-train
```
### 3. تثبيت SAM2 و DinoV2

سوف نقوم بتثبيت SAM2 و DinoV2 من المصدر.

```bash
pip install -e .
cd dinov2
pip install -e .
cd ..
```
### 4. تنزيل مجموعات البيانات

يرجى تنزيل مجموعة بيانات COCO ووضعها في `data/coco`

### 5. تنزيل نقاط التحقق لـ SAM2 و DinoV2

سنقوم بتنزيل نقاط التحقق الخاصة بـ SAM2 المستخدمة في الورقة البحثية بالضبط.
(يرجى ملاحظة أن نقاط التحقق SAM2.1 متوفرة بالفعل وقد تؤدي بشكل أفضل.)


```bash
mkdir -p checkpoints/dinov2
cd checkpoints
wget https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt
cd dinov2
wget https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth
cd ../..
```
## 📊 كود الاستدلال

⚠️ إخلاء مسؤولية: هذا كود بحثي — توقع بعض الفوضى!

### إعادة إنتاج نتائج SOTA بعدد 30 لقطة في Few-shot COCO

قم بتعريف المتغيرات المفيدة وأنشئ مجلدًا للنتائج:



```bash
CONFIG=./no_time_to_train/new_exps/coco_fewshot_10shot_Sam2L.yaml
CLASS_SPLIT="few_shot_classes"
RESULTS_DIR=work_dirs/few_shot_results
SHOTS=30
SEED=33
GPUS=4

mkdir -p $RESULTS_DIR
FILENAME=few_shot_${SHOTS}shot_seed${SEED}.pkl
```
#### 0. إنشاء مجموعة مرجعية


```bash
python no_time_to_train/dataset/few_shot_sampling.py \
        --n-shot $SHOTS \
        --out-path ${RESULTS_DIR}/${FILENAME} \
        --seed $SEED \
        --dataset $CLASS_SPLIT
```
#### 1. املأ الذاكرة بالمراجع


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode fill_memory \
                              --out_path ${RESULTS_DIR}/memory.ckpt \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.memory_pkl ${RESULTS_DIR}/${FILENAME} \
                              --model.init_args.dataset_cfgs.fill_memory.memory_length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
#### 2. معالجة لاحقة لبنك الذاكرة


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode postprocess_memory \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --ckpt_path ${RESULTS_DIR}/memory.ckpt \
                              --out_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --trainer.devices 1
```
#### 3. الاستدلال على الصور الهدف


```bash
python run_lightening.py test --config $CONFIG  \
                              --ckpt_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --model.init_args.test_mode test \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.model_cfg.dataset_name $CLASS_SPLIT \
                              --model.init_args.dataset_cfgs.test.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
إذا كنت ترغب في مشاهدة نتائج الاستدلال مباشرة عبر الإنترنت (أثناء حسابها)، قم بإلغاء تعليق الأسطر 1746-1749 في الملف `no_time_to_train/models/Sam2MatchingBaseline_noAMG.py` [هنا](https://github.com/miquel-espinosa/no-time-to-train/blob/main/no_time_to_train/models/Sam2MatchingBaseline_noAMG.py#L1746).
قم بضبط معلمة حد الدرجة `score_thr` حسب الحاجة لرؤية المزيد أو القليل من الكائنات المُجزأة.
سيتم الآن حفظ الصور في المجلد `results_analysis/few_shot_classes/`. الصورة على اليسار تعرض الحقيقة الأرضية، والصورة على اليمين تعرض الكائنات المُجزأة التي تم العثور عليها بواسطة طريقتنا الخالية من التدريب.

لاحظ أنه في هذا المثال نستخدم تقسيمة `few_shot_classes`، لذا يجب أن نتوقع فقط رؤية كائنات مُجزأة من الفئات الموجودة في هذه التقسيمة (وليس جميع الفئات في COCO).

#### النتائج

بعد تشغيل جميع الصور في مجموعة التحقق، يجب أن تحصل على:


```
BBOX RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.368

SEGM RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.342
```
---

## 🔍 مجموعة بيانات مخصصة

نحن نوفر التعليمات لتشغيل خط الأنابيب الخاص بنا على مجموعة بيانات مخصصة. تنسيقات التعليقات التوضيحية دائمًا بتنسيق COCO.

> **باختصار؛** لرؤية كيفية تشغيل خط الأنابيب الكامل على *مجموعات بيانات مخصصة* مباشرةً، ابحث عن `scripts/matching_cdfsod_pipeline.sh` مع أمثلة لبرامج مجموعات بيانات CD-FSOD (مثل `scripts/dior_fish.sh`)

### 0. تجهيز مجموعة بيانات مخصصة ⛵🐦

لنتخيل أننا نريد اكتشاف **القوارب**⛵ و**الطيور**🐦 في مجموعة بيانات مخصصة. لاستخدام طريقتنا سنحتاج إلى:
- صورة مرجعية *مشروحة* واحدة على الأقل لكل فئة (أي صورة مرجعية واحدة للقارب وصورة مرجعية واحدة للطائر)
- عدة صور هدف للعثور على أمثلة للفئات المطلوبة.

لقد أعددنا برنامجًا مبسطًا لإنشاء مجموعة بيانات مخصصة باستخدام صور coco، لإعداد **لقطة واحدة**.
```bash
python scripts/make_custom_dataset.py
```
سيؤدي هذا إلى إنشاء مجموعة بيانات مخصصة بالهيكلية التالية للمجلدات:
```
data/my_custom_dataset/
    ├── annotations/
    │   ├── custom_references.json
    │   ├── custom_targets.json
    │   └── references_visualisations/
    │       ├── bird_1.jpg
    │       └── boat_1.jpg
    └── images/
        ├── 429819.jpg
        ├── 101435.jpg
        └── (all target and reference images)
```
**تصوير صور المرجع (لقطة واحدة):**

| صورة مرجعية واحدة للطائر 🐦 | صورة مرجعية واحدة للقارب ⛵ |
|:-----------------------------:|:-------------------------------:|
| <img src="https://github.com/user-attachments/assets/e59e580d-a7db-42ac-b386-892af211fc85" alt="bird_1" width="500"/> | <img src="https://github.com/user-attachments/assets/f94ee025-ae37-4a45-9c3e-0cfe8f8cd2bc" alt="boat_1" width="500"/> |


### 0.1 إذا كانت التعليقات التوضيحية للإطار المحيط فقط متوفرة

نحن نوفر أيضًا نصًا برمجيًا لتوليد أقنعة تقسيم على مستوى الكائن باستخدام SAM2. هذا مفيد إذا كان لديك فقط تعليقات توضيحية للإطار المحيط للصور المرجعية.


```bash
# Download sam_h checkpoint. Feel free to use more recent checkpoints (note: code might need to be adapted)
wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth -O checkpoints/sam_vit_h_4b8939.pth
# Run automatic instance segmentation from ground truth bounding boxes.
python no_time_to_train/dataset/sam_bbox_to_segm_batch.py \
    --input_json data/my_custom_dataset/annotations/custom_references.json \
    --image_dir data/my_custom_dataset/images \
    --sam_checkpoint checkpoints/sam_vit_h_4b8939.pth \
    --model_type vit_h \
    --device cuda \
    --batch_size 8 \
    --visualize
```
**صور مرجعية مع أقنعة التقسيم على مستوى الكائن (تم إنشاؤها بواسطة SAM2 من مربعات الإحاطة الحقيقية، لقطة واحدة):**

تم حفظ تصوّر أقنعة التقسيم المُولدة في `data/my_custom_dataset/annotations/custom_references_with_SAM_segm/references_visualisations/`.

| صورة مرجعية بلقطة واحدة لعصفور 🐦 (تم تقسيمها تلقائياً بواسطة SAM) | صورة مرجعية بلقطة واحدة لقارب ⛵ (تم تقسيمها تلقائياً بواسطة SAM) |
|:---------------------------------:|:----------------------------------:|
| <img src="https://github.com/user-attachments/assets/65d38dc4-1454-43cd-9600-e8efc67b3a82" alt="bird_1_with_SAM_segm" width="500"/> | <img src="https://github.com/user-attachments/assets/43a558ad-50ca-4715-8285-9aa3268843c6" alt="boat_1_with_SAM_segm" width="500"/> |


### 0.2 تحويل تعليقات coco إلى ملف pickle



```bash
python no_time_to_train/dataset/coco_to_pkl.py \
    data/my_custom_dataset/annotations/custom_references_with_segm.json \
    data/my_custom_dataset/annotations/custom_references_with_segm.pkl \
    1
```
### 1. ملء الذاكرة بالمراجع

أولاً، قم بتعريف المتغيرات المفيدة وأنشئ مجلدًا للنتائج. من أجل العرض الصحيح للتسميات، يجب ترتيب أسماء الفئات حسب معرف الفئة كما يظهر في ملف json. على سبيل المثال، الفئة `bird` لها معرف فئة `16`، والفئة `boat` لها معرف فئة `9`. لذا، يجب أن تكون `CAT_NAMES=boat,bird`.


```bash
DATASET_NAME=my_custom_dataset
DATASET_PATH=data/my_custom_dataset
CAT_NAMES=boat,bird
CATEGORY_NUM=2
SHOT=1
YAML_PATH=no_time_to_train/pl_configs/matching_cdfsod_template.yaml
PATH_TO_SAVE_CKPTS=./tmp_ckpts/my_custom_dataset
mkdir -p $PATH_TO_SAVE_CKPTS
```
قم بتنفيذ الخطوة 1:

```bash
python run_lightening.py test --config $YAML_PATH \
    --model.test_mode fill_memory \
    --out_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory.pth \
    --model.init_args.dataset_cfgs.fill_memory.root $DATASET_PATH/images \
    --model.init_args.dataset_cfgs.fill_memory.json_file $DATASET_PATH/annotations/custom_references_with_segm.json \
    --model.init_args.dataset_cfgs.fill_memory.memory_pkl $DATASET_PATH/annotations/custom_references_with_segm.pkl \
    --model.init_args.dataset_cfgs.fill_memory.memory_length $SHOT \
    --model.init_args.dataset_cfgs.fill_memory.cat_names $CAT_NAMES \
    --model.init_args.model_cfg.dataset_name $DATASET_NAME \
    --model.init_args.model_cfg.memory_bank_cfg.length $SHOT \
    --model.init_args.model_cfg.memory_bank_cfg.category_num $CATEGORY_NUM \
    --trainer.devices 1
```
### 2. بنك الذاكرة بعد المعالجة


```bash
python run_lightening.py test --config $YAML_PATH \
    --model.test_mode postprocess_memory \
    --ckpt_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory.pth \
    --out_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory_postprocessed.pth \
    --model.init_args.model_cfg.dataset_name $DATASET_NAME \
    --model.init_args.model_cfg.memory_bank_cfg.length $SHOT \
    --model.init_args.model_cfg.memory_bank_cfg.category_num $CATEGORY_NUM \
    --trainer.devices 1
```
### 3. الاستدلال على الصور الهدف

إذا تم تعيين `ONLINE_VIS` إلى True، سيتم حفظ نتائج التنبؤ في `results_analysis/my_custom_dataset/` وعرضها أثناء حسابها. ملاحظة: التشغيل مع التصور اللحظي أبطأ بكثير.

يمكنك تغيير عتبة الدرجات `VIS_THR` لرؤية المزيد أو القليل من الكائنات المقسمة.

```bash
ONLINE_VIS=True
VIS_THR=0.4
python run_lightening.py test --config $YAML_PATH \
    --model.test_mode test \
    --ckpt_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory_postprocessed.pth \
    --model.init_args.model_cfg.dataset_name $DATASET_NAME \
    --model.init_args.model_cfg.memory_bank_cfg.length $SHOT \
    --model.init_args.model_cfg.memory_bank_cfg.category_num $CATEGORY_NUM \
    --model.init_args.model_cfg.test.imgs_path $DATASET_PATH/images \
    --model.init_args.model_cfg.test.online_vis $ONLINE_VIS \
    --model.init_args.model_cfg.test.vis_thr $VIS_THR \
    --model.init_args.dataset_cfgs.test.root $DATASET_PATH/images \
    --model.init_args.dataset_cfgs.test.json_file $DATASET_PATH/annotations/custom_targets.json \
    --model.init_args.dataset_cfgs.test.cat_names $CAT_NAMES \
    --trainer.devices 1
```
### النتائج

يجب أن تكون مقاييس الأداء (بنفس المعلمات المستخدمة في الأوامر أعلاه) كما يلي:


```
BBOX RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.478

SEGM RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.458
```
يتم حفظ النتائج المرئية في `results_analysis/my_custom_dataset/`. لاحظ أن طريقتنا تعمل مع النتائج السلبية الكاذبة، أي الصور التي لا تحتوي على أي أمثلة من الفئات المطلوبة.

*انقر على الصور للتكبير ⬇️*

| صورة الهدف مع القوارب ⛵ (يسار GT، يمين التوقعات) | صورة الهدف مع الطيور 🐦 (يسار GT، يمين التوقعات) |
|:----------------------:|:----------------------:|
| ![000000459673](https://github.com/user-attachments/assets/678dc15a-dd3b-49d5-9287-6290da16aa6b) | ![000000407180](https://github.com/user-attachments/assets/fe306e48-af49-4d83-ac82-76fac6c456d1) |

| صورة الهدف مع القوارب والطيور ⛵🐦 (يسار GT، يمين التوقعات) | صورة الهدف بدون قوارب أو طيور 🚫 (يسار GT، يمين التوقعات) |
|:---------------------------------:|:----------------------------------:|
| ![000000517410](https://github.com/user-attachments/assets/9849b227-7f43-43d7-81ea-58010a623ad5) | ![000000460598](https://github.com/user-attachments/assets/7587700c-e09d-4cf6-8590-3df129c2568e) |


## 📚 الاقتباس

إذا استخدمت هذا العمل، يرجى الاستشهاد بنا:


```bibtex
@article{espinosa2025notimetotrain,
  title={No time to train! Training-Free Reference-Based Instance Segmentation},
  author={Miguel Espinosa and Chenhongyi Yang and Linus Ericsson and Steven McDonagh and Elliot J. Crowley},
  journal={arXiv preprint arXiv:2507.02798},
  year={2025},
  primaryclass={cs.CV}
}
```


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-23

---