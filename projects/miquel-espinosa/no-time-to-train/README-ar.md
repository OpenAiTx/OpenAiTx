<div align="right">
  <details>
    <summary >🌐 اللغة</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-CN">简体中文</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-TW">繁體中文</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ko">한국어</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=hi">हिन्दी</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=th">ไทย</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fr">Français</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=es">Español</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=it">Italiano</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ru">Русский</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pt">Português</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ar">العربية</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fa">فارسی</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=tr">Türkçe</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=vi">Tiếng Việt</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=id">Bahasa Indonesia</a>
      </div>
    </div>
  </details>
</div>

<div align="center">

# 🚀 لا وقت للتدريب!
### تقسيم الكائنات بناءً على المرجع بدون تدريب
[![GitHub](https://img.shields.io/badge/%E2%80%8B-No%20Time%20To%20Train-black?logo=github)](https://github.com/miquel-espinosa/no-time-to-train)
[![Website](https://img.shields.io/badge/🌐-Project%20Page-grey)](https://miquel-espinosa.github.io/no-time-to-train/)
[![arXiv](https://img.shields.io/badge/arXiv-2507.02798-b31b1b)](https://arxiv.org/abs/2507.02798)

**أحدث ما توصلت إليه التقنية (Papers with Code)**

[**_الأفضل في لقطة واحدة (1-shot)_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/badge/State%20of%20the%20Art-Few--Shot%20Object%20Detection%20on%20MS--COCO%20(1--shot)-21CBCE?style=flat&logo=paperswithcode)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference)

[**_الأفضل في عشر لقطات (10-shot)_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/badge/State%20of%20the%20Art-Few--Shot%20Object%20Detection%20on%20MS--COCO%20(10--shot)-21CBCE?style=flat&logo=paperswithcode)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference)

[**_الأفضل في ثلاثين لقطة (30-shot)_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/badge/State%20of%20the%20Art-Few--Shot%20Object%20Detection%20on%20MS--COCO%20(30--shot)-21CBCE?style=flat&logo=paperswithcode)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference)

<!-- [**_SOTA 1-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-1-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference)

[**_SOTA 10-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-10-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference)

[**_SOTA 30-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-30-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference) -->

</div>

---

> 🚨 **تحديث (22 يوليو 2025):** تمت إضافة تعليمات لمجموعات البيانات المخصصة!
> 
> 🔔 **تحديث (16 يوليو 2025):** تم تحديث الكود مع التعليمات!

---

## 📋 جدول المحتويات

- [🎯 النقاط البارزة](#-highlights)
- [📜 الملخص](#-abstract)
- [🧠 الهيكلية](#-architecture)
- [🛠️ تعليمات التثبيت](#️-installation-instructions)
  - [1. استنساخ المستودع](#1-clone-the-repository)
  - [2. إنشاء بيئة conda](#2-create-conda-environment)
  - [3. تثبيت SAM2 و DinoV2](#3-install-sam2-and-dinov2)
  - [4. تحميل مجموعات البيانات](#4-download-datasets)
  - [5. تحميل نقاط تحقق SAM2 و DinoV2](#5-download-sam2-and-dinov2-checkpoints)
- [📊 كود الاستدلال: إعادة إنتاج نتائج SOTA في 30 لقطة على Few-shot COCO](#-inference-code)
  - [0. إنشاء مجموعة مرجعية](#0-create-reference-set)
  - [1. تعبئة الذاكرة بالمراجع](#1-fill-memory-with-references)
  - [2. معالجة بنك الذاكرة](#2-post-process-memory-bank)
  - [3. الاستدلال على الصور الهدف](#3-inference-on-target-images)
  - [النتائج](#results)
- [🔍 مجموعة بيانات مخصصة](#-custom-dataset)
  - [0. تحضير مجموعة بيانات مخصصة ⛵🐦](#0-prepare-a-custom-dataset)
  - [0.1 إذا كانت التعليقات التوضيحية للإطارات فقط متوفرة](#01-if-only-bbox-annotations-are-available)
  - [0.2 تحويل تعليقات coco إلى ملف pickle](#02-convert-coco-annotations-to-pickle-file)
  - [1. تعبئة الذاكرة بالمراجع](#1-fill-memory-with-references)
  - [2. معالجة بنك الذاكرة](#2-post-process-memory-bank)
- [📚 الاقتباس](#-citation)


## 🎯 النقاط البارزة
- 💡 **بدون تدريب**: لا حاجة لضبط دقيق ولا هندسة موجهات—فقط صورة مرجعية.
- 🖼️ **استنادًا إلى المرجع**: قسّم الكائنات الجديدة باستخدام أمثلة قليلة فقط.
- 🔥 **أداء متقدم (SOTA)**: يتفوق على الأساليب السابقة بدون تدريب على مجموعات COCO و PASCAL VOC و Cross-Domain FSOD.

**روابط:**
- 🧾 [**ورقة arXiv**](https://arxiv.org/abs/2507.02798)  
- 🌐 [**موقع المشروع**](https://miquel-espinosa.github.io/no-time-to-train/)  
- 📈 [**Papers with Code**](https://paperswithcode.com/paper/no-time-to-train-training-free-reference)

## 📜 الملخص


> The performance of image segmentation models has historically been constrained by the high cost of collecting large-scale annotated data. The Segment Anything Model (SAM) alleviates this original problem through a promptable, semantics-agnostic, segmentation paradigm and yet still requires manual visual-prompts or complex domain-dependent prompt-generation rules to process a new image. Towards reducing this new burden, our work investigates the task of object segmentation when provided with, alternatively, only a small set of reference images. Our key insight is to leverage strong semantic priors, as learned by foundation models, to identify corresponding regions between a reference and a target image. We find that correspondences enable automatic generation of instance-level segmentation masks for downstream tasks and instantiate our ideas via a multi-stage, training-free method incorporating (1) memory bank construction; (2) representation aggregation and (3) semantic-aware feature matching. Our experiments show significant improvements on segmentation metrics, leading to state-of-the-art performance on COCO FSOD (36.8% nAP), PASCAL VOC Few-Shot (71.2% nAP50) and outperforming existing training-free approaches on the Cross-Domain FSOD benchmark (22.4% nAP).

![cdfsod-results-final-comic-sans-min](https://github.com/user-attachments/assets/ab302c02-c080-4042-99fc-0e181ba8abb9)


## 🧠 Architecture

![training-free-architecture-comic-sans-min](https://github.com/user-attachments/assets/d84dd83a-505e-45a0-8ce3-98e1838017f9)


## 🛠️ Installation instructions

### 1. Clone the repository

```bash
git clone https://github.com/miquel-espinosa/no-time-to-train.git
cd no-time-to-train
```
### ٢. إنشاء بيئة كوندا

سنقوم بإنشاء بيئة كوندا مع الحزم المطلوبة.

```bash
conda env create -f environment.yml
conda activate no-time-to-train
```
### 3. تثبيت SAM2 و DinoV2

سوف نقوم بتثبيت SAM2 و DinoV2 من المصدر.

```bash
pip install -e .
cd dinov2
pip install -e .
cd ..
```
### 4. تنزيل مجموعات البيانات

يرجى تنزيل مجموعة بيانات COCO ووضعها في `data/coco`

### 5. تنزيل نقاط التحقق لـ SAM2 و DinoV2

سنقوم بتنزيل نقاط التحقق الخاصة بـ SAM2 المستخدمة في الورقة البحثية بالضبط.
(يرجى ملاحظة أن نقاط التحقق SAM2.1 متوفرة بالفعل وقد تؤدي بشكل أفضل.)


```bash
mkdir -p checkpoints/dinov2
cd checkpoints
wget https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt
cd dinov2
wget https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth
cd ../..
```
## 📊 كود الاستدلال

⚠️ إخلاء مسؤولية: هذا كود بحثي — توقع بعض الفوضى!

### إعادة إنتاج نتائج SOTA بعدد 30 لقطة في Few-shot COCO

قم بتعريف المتغيرات المفيدة وأنشئ مجلدًا للنتائج:



```bash
CONFIG=./no_time_to_train/new_exps/coco_fewshot_10shot_Sam2L.yaml
CLASS_SPLIT="few_shot_classes"
RESULTS_DIR=work_dirs/few_shot_results
SHOTS=30
SEED=33
GPUS=4

mkdir -p $RESULTS_DIR
FILENAME=few_shot_${SHOTS}shot_seed${SEED}.pkl
```
#### 0. إنشاء مجموعة مرجعية


```bash
python no_time_to_train/dataset/few_shot_sampling.py \
        --n-shot $SHOTS \
        --out-path ${RESULTS_DIR}/${FILENAME} \
        --seed $SEED \
        --dataset $CLASS_SPLIT
```
#### 1. املأ الذاكرة بالمراجع


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode fill_memory \
                              --out_path ${RESULTS_DIR}/memory.ckpt \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.memory_pkl ${RESULTS_DIR}/${FILENAME} \
                              --model.init_args.dataset_cfgs.fill_memory.memory_length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
#### 2. معالجة لاحقة لبنك الذاكرة


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode postprocess_memory \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --ckpt_path ${RESULTS_DIR}/memory.ckpt \
                              --out_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --trainer.devices 1
```
#### 3. الاستدلال على الصور الهدف


```bash
python run_lightening.py test --config $CONFIG  \
                              --ckpt_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --model.init_args.test_mode test \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.model_cfg.dataset_name $CLASS_SPLIT \
                              --model.init_args.dataset_cfgs.test.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
إذا كنت ترغب في مشاهدة نتائج الاستدلال مباشرة عبر الإنترنت (أثناء حسابها)، قم بإلغاء تعليق الأسطر 1746-1749 في الملف `no_time_to_train/models/Sam2MatchingBaseline_noAMG.py` [هنا](https://github.com/miquel-espinosa/no-time-to-train/blob/main/no_time_to_train/models/Sam2MatchingBaseline_noAMG.py#L1746).
قم بضبط معلمة حد الدرجة `score_thr` حسب الحاجة لرؤية المزيد أو القليل من الكائنات المُجزأة.
سيتم الآن حفظ الصور في المجلد `results_analysis/few_shot_classes/`. الصورة على اليسار تعرض الحقيقة الأرضية، والصورة على اليمين تعرض الكائنات المُجزأة التي تم العثور عليها بواسطة طريقتنا الخالية من التدريب.

لاحظ أنه في هذا المثال نستخدم تقسيمة `few_shot_classes`، لذا يجب أن نتوقع فقط رؤية كائنات مُجزأة من الفئات الموجودة في هذه التقسيمة (وليس جميع الفئات في COCO).

#### النتائج

بعد تشغيل جميع الصور في مجموعة التحقق، يجب أن تحصل على:


```
BBOX RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.368

SEGM RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.342
```
---

## 🔍 مجموعة بيانات مخصصة

نحن نوفر التعليمات لتشغيل خط الأنابيب الخاص بنا على مجموعة بيانات مخصصة. تنسيقات التعليقات التوضيحية دائمًا بتنسيق COCO.

> **باختصار؛** لرؤية كيفية تشغيل خط الأنابيب الكامل على *مجموعات بيانات مخصصة* مباشرةً، ابحث عن `scripts/matching_cdfsod_pipeline.sh` مع أمثلة لبرامج مجموعات بيانات CD-FSOD (مثل `scripts/dior_fish.sh`)

### 0. تجهيز مجموعة بيانات مخصصة ⛵🐦

لنتخيل أننا نريد اكتشاف **القوارب**⛵ و**الطيور**🐦 في مجموعة بيانات مخصصة. لاستخدام طريقتنا سنحتاج إلى:
- صورة مرجعية *مشروحة* واحدة على الأقل لكل فئة (أي صورة مرجعية واحدة للقارب وصورة مرجعية واحدة للطائر)
- عدة صور هدف للعثور على أمثلة للفئات المطلوبة.

لقد أعددنا برنامجًا مبسطًا لإنشاء مجموعة بيانات مخصصة باستخدام صور coco، لإعداد **لقطة واحدة**.
```bash
python scripts/make_custom_dataset.py
```
سيؤدي هذا إلى إنشاء مجموعة بيانات مخصصة بالهيكلية التالية للمجلدات:
```
data/my_custom_dataset/
    ├── annotations/
    │   ├── custom_references.json
    │   ├── custom_targets.json
    │   └── references_visualisations/
    │       ├── bird_1.jpg
    │       └── boat_1.jpg
    └── images/
        ├── 429819.jpg
        ├── 101435.jpg
        └── (all target and reference images)
```
**تصوير صور المرجع (لقطة واحدة):**

| صورة مرجعية واحدة للطائر 🐦 | صورة مرجعية واحدة للقارب ⛵ |
|:-----------------------------:|:-------------------------------:|
| <img src="https://github.com/user-attachments/assets/e59e580d-a7db-42ac-b386-892af211fc85" alt="bird_1" width="500"/> | <img src="https://github.com/user-attachments/assets/f94ee025-ae37-4a45-9c3e-0cfe8f8cd2bc" alt="boat_1" width="500"/> |


### 0.1 إذا كانت التعليقات التوضيحية للإطار المحيط فقط متوفرة

نحن نوفر أيضًا نصًا برمجيًا لتوليد أقنعة تقسيم على مستوى الكائن باستخدام SAM2. هذا مفيد إذا كان لديك فقط تعليقات توضيحية للإطار المحيط للصور المرجعية.


```bash
# Download sam_h checkpoint. Feel free to use more recent checkpoints (note: code might need to be adapted)
wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth -O checkpoints/sam_vit_h_4b8939.pth
# Run automatic instance segmentation from ground truth bounding boxes.
python no_time_to_train/dataset/sam_bbox_to_segm_batch.py \
    --input_json data/my_custom_dataset/annotations/custom_references.json \
    --image_dir data/my_custom_dataset/images \
    --sam_checkpoint checkpoints/sam_vit_h_4b8939.pth \
    --model_type vit_h \
    --device cuda \
    --batch_size 8 \
    --visualize
```
**صور مرجعية مع أقنعة التقسيم على مستوى الكائن (تم إنشاؤها بواسطة SAM2 من مربعات الإحاطة الحقيقية، لقطة واحدة):**

تم حفظ تصوّر أقنعة التقسيم المُولدة في `data/my_custom_dataset/annotations/custom_references_with_SAM_segm/references_visualisations/`.

| صورة مرجعية بلقطة واحدة لعصفور 🐦 (تم تقسيمها تلقائياً بواسطة SAM) | صورة مرجعية بلقطة واحدة لقارب ⛵ (تم تقسيمها تلقائياً بواسطة SAM) |
|:---------------------------------:|:----------------------------------:|
| <img src="https://github.com/user-attachments/assets/65d38dc4-1454-43cd-9600-e8efc67b3a82" alt="bird_1_with_SAM_segm" width="500"/> | <img src="https://github.com/user-attachments/assets/43a558ad-50ca-4715-8285-9aa3268843c6" alt="boat_1_with_SAM_segm" width="500"/> |


### 0.2 تحويل تعليقات coco إلى ملف pickle



```bash
python no_time_to_train/dataset/coco_to_pkl.py \
    data/my_custom_dataset/annotations/custom_references_with_segm.json \
    data/my_custom_dataset/annotations/custom_references_with_segm.pkl \
    1
```
### 1. ملء الذاكرة بالمراجع

أولاً، قم بتعريف المتغيرات المفيدة وأنشئ مجلدًا للنتائج. من أجل العرض الصحيح للتسميات، يجب ترتيب أسماء الفئات حسب معرف الفئة كما يظهر في ملف json. على سبيل المثال، الفئة `bird` لها معرف فئة `16`، والفئة `boat` لها معرف فئة `9`. لذا، يجب أن تكون `CAT_NAMES=boat,bird`.


```bash
DATASET_NAME=my_custom_dataset
DATASET_PATH=data/my_custom_dataset
CAT_NAMES=boat,bird
CATEGORY_NUM=2
SHOT=1
YAML_PATH=no_time_to_train/pl_configs/matching_cdfsod_template.yaml
PATH_TO_SAVE_CKPTS=./tmp_ckpts/my_custom_dataset
mkdir -p $PATH_TO_SAVE_CKPTS
```
قم بتنفيذ الخطوة 1:

```bash
python run_lightening.py test --config $YAML_PATH \
    --model.test_mode fill_memory \
    --out_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory.pth \
    --model.init_args.dataset_cfgs.fill_memory.root $DATASET_PATH/images \
    --model.init_args.dataset_cfgs.fill_memory.json_file $DATASET_PATH/annotations/custom_references_with_segm.json \
    --model.init_args.dataset_cfgs.fill_memory.memory_pkl $DATASET_PATH/annotations/custom_references_with_segm.pkl \
    --model.init_args.dataset_cfgs.fill_memory.memory_length $SHOT \
    --model.init_args.dataset_cfgs.fill_memory.cat_names $CAT_NAMES \
    --model.init_args.model_cfg.dataset_name $DATASET_NAME \
    --model.init_args.model_cfg.memory_bank_cfg.length $SHOT \
    --model.init_args.model_cfg.memory_bank_cfg.category_num $CATEGORY_NUM \
    --trainer.devices 1
```
### 2. بنك الذاكرة بعد المعالجة


```bash
python run_lightening.py test --config $YAML_PATH \
    --model.test_mode postprocess_memory \
    --ckpt_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory.pth \
    --out_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory_postprocessed.pth \
    --model.init_args.model_cfg.dataset_name $DATASET_NAME \
    --model.init_args.model_cfg.memory_bank_cfg.length $SHOT \
    --model.init_args.model_cfg.memory_bank_cfg.category_num $CATEGORY_NUM \
    --trainer.devices 1
```
### 3. الاستدلال على الصور الهدف

إذا تم تعيين `ONLINE_VIS` إلى True، سيتم حفظ نتائج التنبؤ في `results_analysis/my_custom_dataset/` وعرضها أثناء حسابها. ملاحظة: التشغيل مع التصور اللحظي أبطأ بكثير.

يمكنك تغيير عتبة الدرجات `VIS_THR` لرؤية المزيد أو القليل من الكائنات المقسمة.

```bash
ONLINE_VIS=True
VIS_THR=0.4
python run_lightening.py test --config $YAML_PATH \
    --model.test_mode test \
    --ckpt_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory_postprocessed.pth \
    --model.init_args.model_cfg.dataset_name $DATASET_NAME \
    --model.init_args.model_cfg.memory_bank_cfg.length $SHOT \
    --model.init_args.model_cfg.memory_bank_cfg.category_num $CATEGORY_NUM \
    --model.init_args.model_cfg.test.imgs_path $DATASET_PATH/images \
    --model.init_args.model_cfg.test.online_vis $ONLINE_VIS \
    --model.init_args.model_cfg.test.vis_thr $VIS_THR \
    --model.init_args.dataset_cfgs.test.root $DATASET_PATH/images \
    --model.init_args.dataset_cfgs.test.json_file $DATASET_PATH/annotations/custom_targets.json \
    --model.init_args.dataset_cfgs.test.cat_names $CAT_NAMES \
    --trainer.devices 1
```
### النتائج

يجب أن تكون مقاييس الأداء (بنفس المعلمات المستخدمة في الأوامر أعلاه) كما يلي:


```
BBOX RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.478

SEGM RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.458
```
يتم حفظ النتائج المرئية في `results_analysis/my_custom_dataset/`. لاحظ أن طريقتنا تعمل مع النتائج السلبية الكاذبة، أي الصور التي لا تحتوي على أي أمثلة من الفئات المطلوبة.

*انقر على الصور للتكبير ⬇️*

| صورة الهدف مع القوارب ⛵ (يسار GT، يمين التوقعات) | صورة الهدف مع الطيور 🐦 (يسار GT، يمين التوقعات) |
|:----------------------:|:----------------------:|
| ![000000459673](https://github.com/user-attachments/assets/678dc15a-dd3b-49d5-9287-6290da16aa6b) | ![000000407180](https://github.com/user-attachments/assets/fe306e48-af49-4d83-ac82-76fac6c456d1) |

| صورة الهدف مع القوارب والطيور ⛵🐦 (يسار GT، يمين التوقعات) | صورة الهدف بدون قوارب أو طيور 🚫 (يسار GT، يمين التوقعات) |
|:---------------------------------:|:----------------------------------:|
| ![000000517410](https://github.com/user-attachments/assets/9849b227-7f43-43d7-81ea-58010a623ad5) | ![000000460598](https://github.com/user-attachments/assets/7587700c-e09d-4cf6-8590-3df129c2568e) |


## 📚 الاقتباس

إذا استخدمت هذا العمل، يرجى الاستشهاد بنا:


```bibtex
@article{espinosa2025notimetotrain,
  title={No time to train! Training-Free Reference-Based Instance Segmentation},
  author={Miguel Espinosa and Chenhongyi Yang and Linus Ericsson and Steven McDonagh and Elliot J. Crowley},
  journal={arXiv preprint arXiv:2507.02798},
  year={2025},
  primaryclass={cs.CV}
}
```


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-24

---