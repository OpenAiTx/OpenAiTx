<div align="right">
  <details>
    <summary >🌐 Ngôn ngữ</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-CN">简体中文</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-TW">繁體中文</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ko">한국어</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=hi">हिन्दी</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=th">ไทย</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fr">Français</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=es">Español</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=it">Italiano</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ru">Русский</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pt">Português</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ar">العربية</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fa">فارسی</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=tr">Türkçe</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=vi">Tiếng Việt</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=id">Bahasa Indonesia</a>
      </div>
    </div>
  </details>
</div>

<div align="center">

# 🚀 Không Cần Thời Gian Để Huấn Luyện!  
### Phân Đoạn Đối Tượng Tham Chiếu Không Cần Huấn Luyện  
[![GitHub](https://img.shields.io/badge/%E2%80%8B-No%20Time%20To%20Train-black?logo=github)](https://github.com/miquel-espinosa/no-time-to-train)
[![Website](https://img.shields.io/badge/🌐-Project%20Page-grey)](https://miquel-espinosa.github.io/no-time-to-train/)
[![arXiv](https://img.shields.io/badge/arXiv-2507.02798-b31b1b)](https://arxiv.org/abs/2507.02798)

**Hiện đại nhất (Papers with Code)**

[**_1-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-1-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference)

[**_10-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-10-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference)

[**_30-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-30-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference)

</div>

---

> 🔔 **Cập nhật (Tháng 7/2025):** Mã nguồn đã được cập nhật kèm hướng dẫn!

---

## 📋 Mục Lục

- [🎯 Điểm nổi bật](#-highlights)
- [📜 Tóm tắt](#-abstract)
- [🧠 Kiến trúc](#-architecture)
- [🛠️ Hướng dẫn cài đặt](#️-installation-instructions)
  - [1. Sao chép kho lưu trữ](#1-clone-the-repository)
  - [2. Tạo môi trường conda](#2-create-conda-environment)
  - [3. Cài đặt SAM2 và DinoV2](#3-install-sam2-and-dinov2)
  - [4. Tải xuống bộ dữ liệu](#4-download-datasets)
  - [5. Tải xuống các checkpoint SAM2 và DinoV2](#5-download-sam2-and-dinov2-checkpoints)
- [📊 Mã suy luận: Tái tạo kết quả SOTA 30-shot trên Few-shot COCO](#-inference-code)
  - [0. Tạo bộ tham chiếu](#0-create-reference-set)
  - [1. Đổ bộ nhớ với các tham chiếu](#1-fill-memory-with-references)
  - [2. Xử lý hậu kỳ bộ nhớ](#2-post-process-memory-bank)
  - [3. Suy luận trên ảnh mục tiêu](#3-inference-on-target-images)
  - [Kết quả](#results)
- [🔍 Trích dẫn](#-citation)


## 🎯 Điểm nổi bật
- 💡 **Không cần huấn luyện**: Không tinh chỉnh, không cần thiết kế prompt—chỉ cần ảnh tham chiếu.  
- 🖼️ **Dựa trên tham chiếu**: Phân đoạn đối tượng mới chỉ với một vài ví dụ.  
- 🔥 **Hiệu năng SOTA**: Vượt trội các phương pháp không cần huấn luyện trước trên COCO, PASCAL VOC và Cross-Domain FSOD.

**Liên kết:**
- 🧾 [**Bài báo arXiv**](https://arxiv.org/abs/2507.02798)  
- 🌐 [**Trang dự án**](https://miquel-espinosa.github.io/no-time-to-train/)  
- 📈 [**Papers with Code**](https://paperswithcode.com/paper/no-time-to-train-training-free-reference)

## 📜 Tóm tắt

> Hiệu suất của các mô hình phân đoạn ảnh từ trước tới nay bị giới hạn bởi chi phí cao trong việc thu thập dữ liệu được gán nhãn quy mô lớn. Mô hình Segment Anything (SAM) đã giảm bớt vấn đề này nhờ mô hình phân đoạn không phụ thuộc ngữ nghĩa với khả năng nhận prompt, tuy nhiên vẫn cần prompt hình ảnh thủ công hoặc các quy tắc tạo prompt phức tạp phụ thuộc vào miền để xử lý ảnh mới. Nhằm giảm gánh nặng này, nghiên cứu của chúng tôi khảo sát bài toán phân đoạn đối tượng khi chỉ có một tập nhỏ ảnh tham chiếu. Quan sát chính là tận dụng các prior ngữ nghĩa mạnh mẽ được học bởi các mô hình nền tảng để xác định vùng tương ứng giữa ảnh tham chiếu và ảnh mục tiêu. Chúng tôi nhận thấy việc tìm tương ứng này cho phép tự động tạo mặt nạ phân đoạn cấp đối tượng phục vụ các tác vụ phía sau, và hiện thực hóa ý tưởng thông qua phương pháp nhiều giai đoạn, không cần huấn luyện bao gồm (1) xây dựng bộ nhớ; (2) tổng hợp biểu diễn và (3) ghép đặc trưng nhận biết ngữ nghĩa. Thí nghiệm cho thấy cải thiện đáng kể về chỉ số phân đoạn, đạt hiệu năng hàng đầu trên COCO FSOD (36.8% nAP), PASCAL VOC Few-Shot (71.2% nAP50) và vượt các phương pháp không cần huấn luyện trước trên benchmark Cross-Domain FSOD (22.4% nAP).

![cdfsod-results-final-comic-sans-min](https://github.com/user-attachments/assets/ab302c02-c080-4042-99fc-0e181ba8abb9)


## 🧠 Kiến trúc

![training-free-architecture-comic-sans-min](https://github.com/user-attachments/assets/d84dd83a-505e-45a0-8ce3-98e1838017f9)


## 🛠️ Hướng dẫn cài đặt

### 1. Sao chép kho lưu trữ


```bash
git clone https://github.com/miquel-espinosa/no-time-to-train.git
cd no-time-to-train
```
### 2. Tạo môi trường conda

Chúng ta sẽ tạo một môi trường conda với các gói cần thiết.

```bash
conda env create -f environment.yml
conda activate no-time-to-train
```
### 3. Cài đặt SAM2 và DinoV2

Chúng ta sẽ cài đặt SAM2 và DinoV2 từ mã nguồn.

```bash
pip install -e .
cd dinov2
pip install -e .
cd ..
```
### 4. Tải xuống các bộ dữ liệu

Vui lòng tải xuống bộ dữ liệu COCO và đặt nó vào `data/coco`

### 5. Tải xuống các checkpoint SAM2 và DinoV2

Chúng ta sẽ tải xuống các checkpoint SAM2 chính xác đã được sử dụng trong bài báo.
(Tuy nhiên, lưu ý rằng các checkpoint SAM2.1 đã có sẵn và có thể hoạt động tốt hơn.)


```bash
mkdir -p checkpoints/dinov2
cd checkpoints
wget https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt
cd dinov2
wget https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth
cd ../..
```
## 📊 Mã suy luận

⚠️ Lưu ý: Đây là mã nghiên cứu — có thể sẽ hơi lộn xộn!

### Tái tạo kết quả SOTA 30-shot trong Few-shot COCO

Định nghĩa các biến hữu ích và tạo một thư mục cho kết quả:



```bash
CONFIG=./dev_hongyi/new_exps/coco_fewshot_10shot_Sam2L.yaml
CLASS_SPLIT="few_shot_classes"
RESULTS_DIR=work_dirs/few_shot_results
SHOTS=30
SEED=33
GPUS=4

mkdir -p $RESULTS_DIR
FILENAME=few_shot_${SHOTS}shot_seed${SEED}.pkl
```
#### 0. Tạo bộ tham chiếu


```bash
python dev_hongyi/dataset/few_shot_sampling.py \
        --n-shot $SHOTS \
        --out-path ${RESULTS_DIR}/${FILENAME} \
        --seed $SEED \
        --dataset $CLASS_SPLIT
```
#### 1. Lấp đầy bộ nhớ bằng các tham chiếu


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode fill_memory \
                              --out_path ${RESULTS_DIR}/memory.ckpt \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.memory_pkl ${RESULTS_DIR}/${FILENAME} \
                              --model.init_args.dataset_cfgs.fill_memory.memory_length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
#### 2. Xử lý hậu kỳ bộ nhớ ngân hàng


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode postprocess_memory \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --ckpt_path ${RESULTS_DIR}/memory.ckpt \
                              --out_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --trainer.devices 1
```
#### 3. Suy luận trên các hình ảnh mục tiêu


```bash
python run_lightening.py test --config $CONFIG  \
                              --ckpt_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --model.init_args.test_mode test \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.model_cfg.dataset_name $CLASS_SPLIT \
                              --model.init_args.dataset_cfgs.test.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
Nếu bạn muốn xem kết quả suy luận trực tuyến (khi chúng được tính toán), hãy bỏ chú thích các dòng 1746-1749 trong `dev_hongyi/models/Sam2MatchingBaseline_noAMG.py` [tại đây](https://github.com/miquel-espinosa/no-time-to-train/blob/main/dev_hongyi/models/Sam2MatchingBaseline_noAMG.py#L1746).
Điều chỉnh tham số ngưỡng điểm số `score_thr` nếu cần để xem nhiều hoặc ít hơn các đối tượng được phân đoạn.
Hình ảnh bây giờ sẽ được lưu trong `results_analysis/few_shot_classes/`. Hình bên trái thể hiện nhãn thực tế, hình bên phải thể hiện các đối tượng được phân đoạn do phương pháp không cần huấn luyện của chúng tôi tìm thấy.

Lưu ý rằng trong ví dụ này chúng ta đang sử dụng tập chia `few_shot_classes`, do đó, chúng ta chỉ nên kỳ vọng sẽ thấy các đối tượng được phân đoạn thuộc các lớp trong tập này (không phải tất cả các lớp trong COCO).

#### Kết quả

Sau khi chạy tất cả hình ảnh trong tập kiểm tra, bạn sẽ nhận được:


```
BBOX RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.368

SEGM RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.342
```
---


## 🔍 Citation

If you use this work, please cite us:

```bibtex
@article{espinosa2025notimetotrain,
  title={No time to train! Training-Free Reference-Based Instance Segmentation},
  author={Miguel Espinosa and Chenhongyi Yang and Linus Ericsson and Steven McDonagh and Elliot J. Crowley},
  journal={arXiv preprint arXiv:2507.02798},
  year={2025},
  primaryclass={cs.CV}
}
```

---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-22

---