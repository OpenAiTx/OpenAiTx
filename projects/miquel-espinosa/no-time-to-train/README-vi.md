<div align="right">
  <details>
    <summary >🌐 Ngôn ngữ</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-CN">简体中文</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-TW">繁體中文</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ko">한국어</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=hi">हिन्दी</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=th">ไทย</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fr">Français</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=es">Español</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=it">Italiano</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ru">Русский</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pt">Português</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ar">العربية</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fa">فارسی</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=tr">Türkçe</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=vi">Tiếng Việt</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=id">Bahasa Indonesia</a>
      </div>
    </div>
  </details>
</div>

<div align="center">

# 🚀 Không Cần Huấn Luyện!  
### Phân Đoạn Thể Hiện Dựa Trên Tham Chiếu Không Cần Huấn Luyện  
[![GitHub](https://img.shields.io/badge/%E2%80%8B-No%20Time%20To%20Train-black?logo=github)](https://github.com/miquel-espinosa/no-time-to-train)
[![Website](https://img.shields.io/badge/🌐-Project%20Page-grey)](https://miquel-espinosa.github.io/no-time-to-train/)
[![arXiv](https://img.shields.io/badge/arXiv-2507.02798-b31b1b)](https://arxiv.org/abs/2507.02798)

**Công nghệ tiên tiến nhất (Papers with Code)**

[**_SOTA 1-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-1-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference)

[**_SOTA 10-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-10-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference)

[**_SOTA 30-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-30-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference)

</div>

---

> 🚨 **Cập nhật (22 tháng 7, 2025):** Đã thêm hướng dẫn cho bộ dữ liệu tuỳ chỉnh!
> 
> 🔔 **Cập nhật (16 tháng 7, 2025):** Đã cập nhật mã nguồn cùng hướng dẫn sử dụng!

---

## 📋 Mục Lục

- [🎯 Điểm nổi bật](#-highlights)
- [📜 Tóm tắt](#-abstract)
- [🧠 Kiến trúc](#-architecture)
- [🛠️ Hướng dẫn cài đặt](#️-installation-instructions)
  - [1. Sao chép kho lưu trữ](#1-clone-the-repository)
  - [2. Tạo môi trường conda](#2-create-conda-environment)
  - [3. Cài đặt SAM2 và DinoV2](#3-install-sam2-and-dinov2)
  - [4. Tải bộ dữ liệu](#4-download-datasets)
  - [5. Tải các checkpoint của SAM2 và DinoV2](#5-download-sam2-and-dinov2-checkpoints)
- [📊 Mã suy luận: Tái tạo kết quả SOTA 30-shot trên Few-shot COCO](#-inference-code)
  - [0. Tạo tập tham chiếu](#0-create-reference-set)
  - [1. Nạp bộ nhớ với các tham chiếu](#1-fill-memory-with-references)
  - [2. Hậu xử lý bộ nhớ](#2-post-process-memory-bank)
  - [3. Suy luận trên ảnh mục tiêu](#3-inference-on-target-images)
  - [Kết quả](#results)
- [🔍 Bộ dữ liệu tuỳ chỉnh](#-custom-dataset)
  - [0. Chuẩn bị bộ dữ liệu tuỳ chỉnh ⛵🐦](#0-prepare-a-custom-dataset)
  - [0.1 Nếu chỉ có annotation bbox](#01-if-only-bbox-annotations-are-available)
  - [0.2 Chuyển đổi annotation coco sang tệp pickle](#02-convert-coco-annotations-to-pickle-file)
  - [1. Nạp bộ nhớ với các tham chiếu](#1-fill-memory-with-references)
  - [2. Hậu xử lý bộ nhớ](#2-post-process-memory-bank)
- [📚 Trích dẫn](#-citation)


## 🎯 Điểm nổi bật
- 💡 **Không cần huấn luyện**: Không cần tinh chỉnh, không cần thiết kế prompt—chỉ cần một ảnh tham chiếu.  
- 🖼️ **Dựa trên tham chiếu**: Phân đoạn đối tượng mới chỉ với một vài ví dụ.  
- 🔥 **Hiệu năng SOTA**: Vượt trội các phương pháp không cần huấn luyện trước đó trên COCO, PASCAL VOC, và Cross-Domain FSOD.

**Liên kết:**
- 🧾 [**Bài báo arXiv**](https://arxiv.org/abs/2507.02798)  
- 🌐 [**Trang dự án**](https://miquel-espinosa.github.io/no-time-to-train/)  
- 📈 [**Papers with Code**](https://paperswithcode.com/paper/no-time-to-train-training-free-reference)

## 📜 Tóm tắt

> Hiệu năng của các mô hình phân đoạn ảnh từ trước đến nay bị giới hạn bởi chi phí cao khi thu thập dữ liệu có gán nhãn quy mô lớn. Segment Anything Model (SAM) đã giảm bớt vấn đề này nhờ vào phương pháp phân đoạn dựa trên prompt không phụ thuộc ngữ nghĩa và có thể điều chỉnh, tuy nhiên vẫn cần prompt trực quan thủ công hoặc quy tắc tạo prompt phức tạp phụ thuộc miền để xử lý ảnh mới. Nhằm giảm gánh nặng mới này, chúng tôi nghiên cứu bài toán phân đoạn đối tượng khi chỉ được cung cấp một tập nhỏ các ảnh tham chiếu. Quan sát then chốt của chúng tôi là tận dụng các tiên nghiệm ngữ nghĩa mạnh mẽ, đã được học bởi các mô hình nền tảng, để xác định các vùng tương ứng giữa ảnh tham chiếu và ảnh mục tiêu. Chúng tôi nhận thấy rằng phép tương ứng này cho phép tự động tạo mặt nạ phân đoạn cấp thể hiện cho các tác vụ phía sau và hiện thực hoá ý tưởng bằng phương pháp nhiều giai đoạn, không cần huấn luyện gồm (1) xây dựng bộ nhớ; (2) tổng hợp biểu diễn và (3) ghép đặc trưng nhận biết ngữ nghĩa. Thí nghiệm của chúng tôi cho thấy cải thiện rõ rệt trên các chỉ số phân đoạn, dẫn đầu SOTA trên COCO FSOD (36.8% nAP), PASCAL VOC Few-Shot (71.2% nAP50) và vượt các phương pháp không cần huấn luyện hiện có trên benchmark Cross-Domain FSOD (22.4% nAP).

![cdfsod-results-final-comic-sans-min](https://github.com/user-attachments/assets/ab302c02-c080-4042-99fc-0e181ba8abb9)




## 🧠 Kiến trúc

![training-free-architecture-comic-sans-min](https://github.com/user-attachments/assets/d84dd83a-505e-45a0-8ce3-98e1838017f9)


## 🛠️ Hướng dẫn cài đặt

### 1. Sao chép kho lưu trữ

```bash
git clone https://github.com/miquel-espinosa/no-time-to-train.git
cd no-time-to-train
```
### 2. Tạo môi trường conda

Chúng ta sẽ tạo một môi trường conda với các gói cần thiết.

```bash
conda env create -f environment.yml
conda activate no-time-to-train
```
### 3. Cài đặt SAM2 và DinoV2

Chúng ta sẽ cài đặt SAM2 và DinoV2 từ mã nguồn.

```bash
pip install -e .
cd dinov2
pip install -e .
cd ..
```
### 4. Tải xuống các bộ dữ liệu

Vui lòng tải xuống bộ dữ liệu COCO và đặt nó vào `data/coco`

### 5. Tải xuống các checkpoint SAM2 và DinoV2

Chúng ta sẽ tải xuống các checkpoint SAM2 chính xác đã được sử dụng trong bài báo.
(Tuy nhiên, lưu ý rằng các checkpoint SAM2.1 đã có sẵn và có thể hoạt động tốt hơn.)


```bash
mkdir -p checkpoints/dinov2
cd checkpoints
wget https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt
cd dinov2
wget https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth
cd ../..
```
## 📊 Mã suy luận

⚠️ Lưu ý: Đây là mã nghiên cứu — có thể sẽ hơi lộn xộn!

### Tái tạo kết quả SOTA 30-shot trong Few-shot COCO

Định nghĩa các biến hữu ích và tạo một thư mục cho kết quả:



```bash
CONFIG=./no_time_to_train/new_exps/coco_fewshot_10shot_Sam2L.yaml
CLASS_SPLIT="few_shot_classes"
RESULTS_DIR=work_dirs/few_shot_results
SHOTS=30
SEED=33
GPUS=4

mkdir -p $RESULTS_DIR
FILENAME=few_shot_${SHOTS}shot_seed${SEED}.pkl
```
#### 0. Tạo bộ tham chiếu


```bash
python no_time_to_train/dataset/few_shot_sampling.py \
        --n-shot $SHOTS \
        --out-path ${RESULTS_DIR}/${FILENAME} \
        --seed $SEED \
        --dataset $CLASS_SPLIT
```
#### 1. Lấp đầy bộ nhớ bằng các tham chiếu


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode fill_memory \
                              --out_path ${RESULTS_DIR}/memory.ckpt \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.memory_pkl ${RESULTS_DIR}/${FILENAME} \
                              --model.init_args.dataset_cfgs.fill_memory.memory_length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
#### 2. Xử lý hậu kỳ bộ nhớ ngân hàng


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode postprocess_memory \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --ckpt_path ${RESULTS_DIR}/memory.ckpt \
                              --out_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --trainer.devices 1
```
#### 3. Suy luận trên các hình ảnh mục tiêu


```bash
python run_lightening.py test --config $CONFIG  \
                              --ckpt_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --model.init_args.test_mode test \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.model_cfg.dataset_name $CLASS_SPLIT \
                              --model.init_args.dataset_cfgs.test.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
Nếu bạn muốn xem kết quả suy luận trực tuyến (khi chúng được tính toán), hãy bỏ chú thích các dòng 1746-1749 trong `no_time_to_train/models/Sam2MatchingBaseline_noAMG.py` [tại đây](https://github.com/miquel-espinosa/no-time-to-train/blob/main/no_time_to_train/models/Sam2MatchingBaseline_noAMG.py#L1746).
Điều chỉnh tham số ngưỡng điểm số `score_thr` nếu cần để xem nhiều hoặc ít các vùng phân đoạn hơn.
Các hình ảnh bây giờ sẽ được lưu trong `results_analysis/few_shot_classes/`. Hình bên trái là dữ liệu thực tế, hình bên phải là các vùng phân đoạn được tìm thấy bởi phương pháp không cần huấn luyện của chúng tôi.

Lưu ý rằng trong ví dụ này chúng ta đang sử dụng tập chia `few_shot_classes`, do đó, chỉ nên mong đợi thấy các vùng phân đoạn của những lớp trong tập này (không phải tất cả các lớp trong COCO).

#### Kết quả

Sau khi chạy tất cả các hình ảnh trong tập kiểm định, bạn sẽ thu được:


```
BBOX RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.368

SEGM RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.342
```
---

## 🔍 Bộ dữ liệu tùy chỉnh

Chúng tôi cung cấp hướng dẫn để chạy pipeline của mình trên một bộ dữ liệu tùy chỉnh. Định dạng chú thích luôn ở định dạng COCO.

> **TLDR;** Để xem trực tiếp cách chạy toàn bộ pipeline trên *bộ dữ liệu tùy chỉnh*, hãy xem `scripts/matching_cdfsod_pipeline.sh` cùng với các script ví dụ của bộ dữ liệu CD-FSOD (ví dụ: `scripts/dior_fish.sh`)

### 0. Chuẩn bị bộ dữ liệu tùy chỉnh ⛵🐦

Hãy tưởng tượng chúng ta muốn phát hiện **thuyền**⛵ và **chim**🐦 trong một bộ dữ liệu tùy chỉnh. Để sử dụng phương pháp của chúng tôi, bạn sẽ cần:
- Ít nhất 1 ảnh tham chiếu *được chú thích* cho mỗi lớp (ví dụ: 1 ảnh tham chiếu cho thuyền và 1 ảnh tham chiếu cho chim)
- Nhiều ảnh mục tiêu để tìm các đối tượng của lớp mong muốn.

Chúng tôi đã chuẩn bị một script ví dụ để tạo bộ dữ liệu tùy chỉnh với ảnh coco, cho trường hợp **1-shot**.
```bash
python scripts/make_custom_dataset.py
```
Điều này sẽ tạo ra một bộ dữ liệu tùy chỉnh với cấu trúc thư mục như sau:
```
data/my_custom_dataset/
    ├── annotations/
    │   ├── custom_references.json
    │   ├── custom_targets.json
    │   └── references_visualisations/
    │       ├── bird_1.jpg
    │       └── boat_1.jpg
    └── images/
        ├── 429819.jpg
        ├── 101435.jpg
        └── (all target and reference images)
```
**Trực quan hóa hình ảnh tham chiếu (1-shot):**

| Hình ảnh tham chiếu 1-shot cho CHIM 🐦 | Hình ảnh tham chiếu 1-shot cho THUYỀN ⛵ |
|:--------------------------------------:|:---------------------------------------:|
| <img src="https://github.com/user-attachments/assets/e59e580d-a7db-42ac-b386-892af211fc85" alt="bird_1" width="500"/> | <img src="https://github.com/user-attachments/assets/f94ee025-ae37-4a45-9c3e-0cfe8f8cd2bc" alt="boat_1" width="500"/> |


### 0.1 Nếu chỉ có chú thích bbox

Chúng tôi cũng cung cấp một script để tạo mặt nạ phân đoạn cấp đối tượng bằng cách sử dụng SAM2. Điều này hữu ích nếu bạn chỉ có các chú thích bounding box cho hình ảnh tham chiếu.


```bash
# Download sam_h checkpoint. Feel free to use more recent checkpoints (note: code might need to be adapted)
wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth -O checkpoints/sam_vit_h_4b8939.pth
# Run automatic instance segmentation from ground truth bounding boxes.
python no_time_to_train/dataset/sam_bbox_to_segm_batch.py \
    --input_json data/my_custom_dataset/annotations/custom_references.json \
    --image_dir data/my_custom_dataset/images \
    --sam_checkpoint checkpoints/sam_vit_h_4b8939.pth \
    --model_type vit_h \
    --device cuda \
    --batch_size 8 \
    --visualize
```
**Hình ảnh tham chiếu với mặt nạ phân đoạn cấp độ đối tượng (được tạo bởi SAM2 từ các hộp chứa gt, 1-shot):**

Hình ảnh trực quan hóa của các mặt nạ phân đoạn đã được lưu tại `data/my_custom_dataset/annotations/custom_references_with_SAM_segm/references_visualisations/`.


| Hình ảnh tham chiếu 1-shot cho CHIM 🐦 (tự động phân đoạn bằng SAM) | Hình ảnh tham chiếu 1-shot cho THUYỀN ⛵ (tự động phân đoạn bằng SAM) |
|:---------------------------------:|:----------------------------------:|
| <img src="https://github.com/user-attachments/assets/65d38dc4-1454-43cd-9600-e8efc67b3a82" alt="bird_1_with_SAM_segm" width="500"/> | <img src="https://github.com/user-attachments/assets/43a558ad-50ca-4715-8285-9aa3268843c6" alt="boat_1_with_SAM_segm" width="500"/> |


### 0.2 Chuyển đổi chú thích coco sang tập tin pickle


```bash
python no_time_to_train/dataset/coco_to_pkl.py \
    data/my_custom_dataset/annotations/custom_references_with_segm.json \
    data/my_custom_dataset/annotations/custom_references_with_segm.pkl \
    1
```
### 1. Đổ đầy bộ nhớ với các tham chiếu

Đầu tiên, định nghĩa các biến hữu ích và tạo một thư mục để lưu kết quả. Để hiển thị nhãn đúng cách, tên các lớp phải được sắp xếp theo id danh mục như trong tệp json. Ví dụ, `bird` có id danh mục là `16`, `boat` có id danh mục là `9`. Do đó, `CAT_NAMES=boat,bird`.


```bash
DATASET_NAME=my_custom_dataset
DATASET_PATH=data/my_custom_dataset
CAT_NAMES=boat,bird
CATEGORY_NUM=2
SHOT=1
YAML_PATH=no_time_to_train/pl_configs/matching_cdfsod_template.yaml
PATH_TO_SAVE_CKPTS=./tmp_ckpts/my_custom_dataset
mkdir -p $PATH_TO_SAVE_CKPTS
```
Chạy bước 1:

```bash
python run_lightening.py test --config $YAML_PATH \
    --model.test_mode fill_memory \
    --out_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory.pth \
    --model.init_args.dataset_cfgs.fill_memory.root $DATASET_PATH/images \
    --model.init_args.dataset_cfgs.fill_memory.json_file $DATASET_PATH/annotations/custom_references_with_segm.json \
    --model.init_args.dataset_cfgs.fill_memory.memory_pkl $DATASET_PATH/annotations/custom_references_with_segm.pkl \
    --model.init_args.dataset_cfgs.fill_memory.memory_length $SHOT \
    --model.init_args.dataset_cfgs.fill_memory.cat_names $CAT_NAMES \
    --model.init_args.model_cfg.dataset_name $DATASET_NAME \
    --model.init_args.model_cfg.memory_bank_cfg.length $SHOT \
    --model.init_args.model_cfg.memory_bank_cfg.category_num $CATEGORY_NUM \
    --trainer.devices 1
```
### 2. Xử lý hậu kỳ bộ nhớ ngân hàng


```bash
python run_lightening.py test --config $YAML_PATH \
    --model.test_mode postprocess_memory \
    --ckpt_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory.pth \
    --out_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory_postprocessed.pth \
    --model.init_args.model_cfg.dataset_name $DATASET_NAME \
    --model.init_args.model_cfg.memory_bank_cfg.length $SHOT \
    --model.init_args.model_cfg.memory_bank_cfg.category_num $CATEGORY_NUM \
    --trainer.devices 1
```
### 3. Suy luận trên các ảnh mục tiêu

Nếu `ONLINE_VIS` được đặt thành True, kết quả dự đoán sẽ được lưu trong `results_analysis/my_custom_dataset/` và hiển thị ngay khi được tính toán. LƯU Ý rằng chạy với chế độ trực quan hóa trực tuyến sẽ chậm hơn nhiều.

Bạn có thể thay đổi ngưỡng điểm số `VIS_THR` để xem nhiều hoặc ít đối tượng được phân đoạn hơn.

```bash
ONLINE_VIS=True
VIS_THR=0.4
python run_lightening.py test --config $YAML_PATH \
    --model.test_mode test \
    --ckpt_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory_postprocessed.pth \
    --model.init_args.model_cfg.dataset_name $DATASET_NAME \
    --model.init_args.model_cfg.memory_bank_cfg.length $SHOT \
    --model.init_args.model_cfg.memory_bank_cfg.category_num $CATEGORY_NUM \
    --model.init_args.model_cfg.test.imgs_path $DATASET_PATH/images \
    --model.init_args.model_cfg.test.online_vis $ONLINE_VIS \
    --model.init_args.model_cfg.test.vis_thr $VIS_THR \
    --model.init_args.dataset_cfgs.test.root $DATASET_PATH/images \
    --model.init_args.dataset_cfgs.test.json_file $DATASET_PATH/annotations/custom_targets.json \
    --model.init_args.dataset_cfgs.test.cat_names $CAT_NAMES \
    --trainer.devices 1
```
### Kết quả

Các chỉ số hiệu suất (với đúng các tham số như các lệnh trên) nên là:


```
BBOX RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.478

SEGM RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.458
```
Kết quả trực quan được lưu trong `results_analysis/my_custom_dataset/`. Lưu ý rằng phương pháp của chúng tôi hoạt động với các trường hợp âm tính giả, tức là các hình ảnh không chứa bất kỳ đối tượng nào thuộc các lớp mong muốn.

*Bấm vào hình để phóng to ⬇️*

| Ảnh mục tiêu với thuyền ⛵ (trái GT, phải dự đoán) | Ảnh mục tiêu với chim 🐦 (trái GT, phải dự đoán) |
|:----------------------:|:----------------------:|
| ![000000459673](https://github.com/user-attachments/assets/678dc15a-dd3b-49d5-9287-6290da16aa6b) | ![000000407180](https://github.com/user-attachments/assets/fe306e48-af49-4d83-ac82-76fac6c456d1) |

| Ảnh mục tiêu với thuyền và chim ⛵🐦 (trái GT, phải dự đoán) | Ảnh mục tiêu không có thuyền hoặc chim 🚫 (trái GT, phải dự đoán) |
|:---------------------------------:|:----------------------------------:|
| ![000000517410](https://github.com/user-attachments/assets/9849b227-7f43-43d7-81ea-58010a623ad5) | ![000000460598](https://github.com/user-attachments/assets/7587700c-e09d-4cf6-8590-3df129c2568e) |


## 📚 Trích dẫn

Nếu bạn sử dụng công trình này, vui lòng trích dẫn chúng tôi:


```bibtex
@article{espinosa2025notimetotrain,
  title={No time to train! Training-Free Reference-Based Instance Segmentation},
  author={Miguel Espinosa and Chenhongyi Yang and Linus Ericsson and Steven McDonagh and Elliot J. Crowley},
  journal={arXiv preprint arXiv:2507.02798},
  year={2025},
  primaryclass={cs.CV}
}
```


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-23

---