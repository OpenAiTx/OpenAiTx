
<div align="right">
  <details>
    <summary >🌐 언어</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-CN">简体中文</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-TW">繁體中文</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ko">한국어</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=hi">हिन्दी</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=th">ไทย</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fr">Français</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=es">Español</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=it">Italiano</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ru">Русский</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pt">Português</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ar">العربية</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fa">فارسی</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=tr">Türkçe</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=vi">Tiếng Việt</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=id">Bahasa Indonesia</a>
      </div>
    </div>
  </details>
</div>

<div align="center">

# 🚀 훈련할 시간이 없다!  
### 훈련 없이 참조 기반 인스턴스 분할  
[![GitHub](https://img.shields.io/badge/%E2%80%8B-No%20Time%20To%20Train-black?logo=github)](https://github.com/miquel-espinosa/no-time-to-train)
[![Website](https://img.shields.io/badge/🌐-Project%20Page-grey)](https://miquel-espinosa.github.io/no-time-to-train/)
[![arXiv](https://img.shields.io/badge/arXiv-2507.02798-b31b1b)](https://arxiv.org/abs/2507.02798)

**최신 연구 (Papers with Code)**
[**_SOTA 1-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/badge/State%20of%20the%20Art-Few--Shot%20Object%20Detection%20on%20MS--COCO%20(1--shot)-21CBCE?style=flat&logo=paperswithcode)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference)

[**_SOTA 10-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/badge/State%20of%20the%20Art-Few--Shot%20Object%20Detection%20on%20MS--COCO%20(10--shot)-21CBCE?style=flat&logo=paperswithcode)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference)

[**_SOTA 30-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/badge/State%20of%20the%20Art-Few--Shot%20Object%20Detection%20on%20MS--COCO%20(30--shot)-21CBCE?style=flat&logo=paperswithcode)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference)

<!-- [**_SOTA 1-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-1-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference)

[**_SOTA 10-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-10-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference)

[**_SOTA 30-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-30-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference) -->

</div>

---

> 🚨 **업데이트 (2025년 7월 22일):** 사용자 지정 데이터셋에 대한 지침이 추가되었습니다!
> 
> 🔔 **업데이트 (2025년 7월 16일):** 코드가 설명과 함께 업데이트되었습니다!

---

## 📋 목차

- [🎯 하이라이트](#-highlights)
- [📜 초록](#-abstract)
- [🧠 아키텍처](#-architecture)
- [🛠️ 설치 방법](#️-installation-instructions)
  - [1. 저장소 클론](#1-clone-the-repository)
  - [2. conda 환경 생성](#2-create-conda-environment)
  - [3. SAM2 및 DinoV2 설치](#3-install-sam2-and-dinov2)
  - [4. 데이터셋 다운로드](#4-download-datasets)
  - [5. SAM2 및 DinoV2 체크포인트 다운로드](#5-download-sam2-and-dinov2-checkpoints)
- [📊 추론 코드: Few-shot COCO에서 30-shot SOTA 결과 재현](#-inference-code)
  - [0. 참조 세트 생성](#0-create-reference-set)
  - [1. 참조로 메모리 채우기](#1-fill-memory-with-references)
  - [2. 메모리 뱅크 후처리](#2-post-process-memory-bank)
  - [3. 대상 이미지에서 추론](#3-inference-on-target-images)
  - [결과](#results)

- [🔍 커스텀 데이터셋](#-custom-dataset)
  - [0. 커스텀 데이터셋 준비 ⛵🐦](#0-prepare-a-custom-dataset)
  - [0.1 바운딩 박스 어노테이션만 있을 경우](#01-if-only-bbox-annotations-are-available)
  - [0.2 coco 어노테이션을 pickle 파일로 변환](#02-convert-coco-annotations-to-pickle-file)
  - [1. 참조로 메모리 채우기](#1-fill-memory-with-references)
  - [2. 메모리 뱅크 후처리](#2-post-process-memory-bank)
- [📚 인용](#-citation)


## 🎯 하이라이트
- 💡 **학습 불필요**: 파인튜닝도, 프롬프트 엔지니어링도 없음—참조 이미지만 사용합니다.  
- 🖼️ **참조 기반**: 소수의 예시만으로 새로운 객체를 분할합니다.  
- 🔥 **최첨단 성능**: COCO, PASCAL VOC, Cross-Domain FSOD에서 기존 학습 불필요 방법보다 뛰어난 성능을 보입니다.

**링크:**
- 🧾 [**arXiv 논문**](https://arxiv.org/abs/2507.02798)  
- 🌐 [**프로젝트 웹사이트**](https://miquel-espinosa.github.io/no-time-to-train/)  
- 📈 [**Papers with Code**](https://paperswithcode.com/paper/no-time-to-train-training-free-reference)

## 📜 초록

> The performance of image segmentation models has historically been constrained by the high cost of collecting large-scale annotated data. The Segment Anything Model (SAM) alleviates this original problem through a promptable, semantics-agnostic, segmentation paradigm and yet still requires manual visual-prompts or complex domain-dependent prompt-generation rules to process a new image. Towards reducing this new burden, our work investigates the task of object segmentation when provided with, alternatively, only a small set of reference images. Our key insight is to leverage strong semantic priors, as learned by foundation models, to identify corresponding regions between a reference and a target image. We find that correspondences enable automatic generation of instance-level segmentation masks for downstream tasks and instantiate our ideas via a multi-stage, training-free method incorporating (1) memory bank construction; (2) representation aggregation and (3) semantic-aware feature matching. Our experiments show significant improvements on segmentation metrics, leading to state-of-the-art performance on COCO FSOD (36.8% nAP), PASCAL VOC Few-Shot (71.2% nAP50) and outperforming existing training-free approaches on the Cross-Domain FSOD benchmark (22.4% nAP).

![cdfsod-results-final-comic-sans-min](https://github.com/user-attachments/assets/ab302c02-c080-4042-99fc-0e181ba8abb9)


## 🧠 Architecture

![training-free-architecture-comic-sans-min](https://github.com/user-attachments/assets/d84dd83a-505e-45a0-8ce3-98e1838017f9)


## 🛠️ Installation instructions

### 1. Clone the repository

```bash
git clone https://github.com/miquel-espinosa/no-time-to-train.git
cd no-time-to-train
```
### 2. conda 환경 생성

필요한 패키지를 포함한 conda 환경을 생성합니다.

```bash
conda env create -f environment.yml
conda activate no-time-to-train
```
### 3. SAM2 및 DinoV2 설치

우리는 SAM2와 DinoV2를 소스에서 설치할 것입니다.

```bash
pip install -e .
cd dinov2
pip install -e .
cd ..
```
### 4. 데이터셋 다운로드

COCO 데이터셋을 다운로드하여 `data/coco`에 저장하세요.

### 5. SAM2 및 DinoV2 체크포인트 다운로드

논문에서 사용된 정확한 SAM2 체크포인트를 다운로드할 것입니다.
(단, SAM2.1 체크포인트는 이미 제공되고 있으며 더 나은 성능을 보일 수 있습니다.)


```bash
mkdir -p checkpoints/dinov2
cd checkpoints
wget https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt
cd dinov2
wget https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth
cd ../..
```



## 📊 추론 코드

⚠️ 면책 조항: 이 코드는 연구용입니다 — 약간의 혼란이 있을 수 있습니다!

### Few-shot COCO에서 30-shot SOTA 결과 재현하기

유용한 변수를 정의하고 결과를 위한 폴더를 만듭니다:


```bash
CONFIG=./no_time_to_train/new_exps/coco_fewshot_10shot_Sam2L.yaml
CLASS_SPLIT="few_shot_classes"
RESULTS_DIR=work_dirs/few_shot_results
SHOTS=30
SEED=33
GPUS=4

mkdir -p $RESULTS_DIR
FILENAME=few_shot_${SHOTS}shot_seed${SEED}.pkl
```
#### 0. 참조 세트 생성


```bash
python no_time_to_train/dataset/few_shot_sampling.py \
        --n-shot $SHOTS \
        --out-path ${RESULTS_DIR}/${FILENAME} \
        --seed $SEED \
        --dataset $CLASS_SPLIT
```
#### 1. 참조로 메모리 채우기


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode fill_memory \
                              --out_path ${RESULTS_DIR}/memory.ckpt \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.memory_pkl ${RESULTS_DIR}/${FILENAME} \
                              --model.init_args.dataset_cfgs.fill_memory.memory_length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
#### 2. 후처리 메모리 뱅크


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode postprocess_memory \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --ckpt_path ${RESULTS_DIR}/memory.ckpt \
                              --out_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --trainer.devices 1
```
#### 3. 대상 이미지에 대한 추론


```bash
python run_lightening.py test --config $CONFIG  \
                              --ckpt_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --model.init_args.test_mode test \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.model_cfg.dataset_name $CLASS_SPLIT \
                              --model.init_args.dataset_cfgs.test.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
온라인에서 추론 결과를 실시간으로 확인하고 싶다면 다음 인자를 추가하세요:

```bash
    --model.init_args.model_cfg.test.online_vis True
```
`score_thr` 매개변수의 점수 임계값을 조정하려면 인자를 추가하세요 (예를 들어, 점수가 `0.4`보다 높은 모든 인스턴스를 시각화하려면):
```bash
    --model.init_args.model_cfg.test.vis_thr 0.4
```
이미지는 이제 `results_analysis/few_shot_classes/`에 저장됩니다. 왼쪽 이미지는 실제 정답(ground truth)을, 오른쪽 이미지는 학습 없이 우리 방법으로 찾아낸 분할 인스턴스를 보여줍니다.

이 예제에서는 `few_shot_classes` 분할을 사용하므로, 해당 분할에 포함된 클래스의 분할된 인스턴스만 볼 수 있습니다(COCO의 모든 클래스가 아님).

#### 결과

검증 세트의 모든 이미지를 실행한 후, 다음과 같은 결과를 얻게 됩니다:

```
BBOX RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.368

SEGM RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.342
```
---

## 🔍 커스텀 데이터셋

우리는 커스텀 데이터셋에서 파이프라인을 실행하는 방법에 대한 지침을 제공합니다. 어노테이션 포맷은 항상 COCO 포맷입니다.

> **요약;** *커스텀 데이터셋*에서 전체 파이프라인을 실행하는 방법을 바로 확인하려면, `scripts/matching_cdfsod_pipeline.sh`와 함께 CD-FSOD 데이터셋의 예시 스크립트(예: `scripts/dior_fish.sh`)를 참고하세요.

### 0. 커스텀 데이터셋 준비하기 ⛵🐦

예를 들어, 커스텀 데이터셋에서 **보트**⛵와 **새**🐦를 탐지하고 싶다고 가정해봅시다. 우리의 방법을 사용하려면 다음이 필요합니다:
- 각 클래스마다 적어도 1장의 *어노테이션된* 기준 이미지(예: 보트 1장, 새 1장)
- 원하는 클래스를 찾기 위한 여러 장의 타깃 이미지

우리는 coco 이미지를 사용하여 **1-shot** 설정에 맞는 커스텀 데이터셋을 만드는 토이 스크립트를 준비했습니다.
```bash
mkdir -p data/my_custom_dataset
python scripts/make_custom_dataset.py
```
이렇게 하면 다음과 같은 폴더 구조를 가진 사용자 지정 데이터셋이 생성됩니다:
```
data/my_custom_dataset/
    ├── annotations/
    │   ├── custom_references.json
    │   ├── custom_targets.json
    │   └── references_visualisations/
    │       ├── bird_1.jpg
    │       └── boat_1.jpg
    └── images/
        ├── 429819.jpg
        ├── 101435.jpg
        └── (all target and reference images)
```
**참조 이미지 시각화 (1-shot):**

| BIRD 🐦의 1-shot 참조 이미지 | BOAT ⛵의 1-shot 참조 이미지 |
|:-----------------------------:|:------------------------------:|
| <img src="https://github.com/user-attachments/assets/e59e580d-a7db-42ac-b386-892af211fc85" alt="bird_1" width="500"/> | <img src="https://github.com/user-attachments/assets/f94ee025-ae37-4a45-9c3e-0cfe8f8cd2bc" alt="boat_1" width="500"/> |


### 0.1 바운딩 박스 주석만 있는 경우

SAM2를 사용하여 인스턴스 수준의 분할 마스크를 생성하는 스크립트도 제공합니다. 이는 참조 이미지에 대해 바운딩 박스 주석만 있는 경우에 유용합니다.


```bash
# Download sam_h checkpoint. Feel free to use more recent checkpoints (note: code might need to be adapted)
wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth -O checkpoints/sam_vit_h_4b8939.pth
# Run automatic instance segmentation from ground truth bounding boxes.
python no_time_to_train/dataset/sam_bbox_to_segm_batch.py \
    --input_json data/my_custom_dataset/annotations/custom_references.json \
    --image_dir data/my_custom_dataset/images \
    --sam_checkpoint checkpoints/sam_vit_h_4b8939.pth \
    --model_type vit_h \
    --device cuda \
    --batch_size 8 \
    --visualize
```
**SAM2로 gt 바운딩 박스에서 생성된 인스턴스 수준 분할 마스크가 포함된 참조 이미지(1-shot):**

생성된 분할 마스크의 시각화 결과는 `data/my_custom_dataset/annotations/custom_references_with_SAM_segm/references_visualisations/`에 저장됩니다.

| BIRD 🐦의 1-shot 참조 이미지 (SAM으로 자동 분할) | BOAT ⛵의 1-shot 참조 이미지 (SAM으로 자동 분할) |
|:---------------------------------:|:----------------------------------:|
| <img src="https://github.com/user-attachments/assets/65d38dc4-1454-43cd-9600-e8efc67b3a82" alt="bird_1_with_SAM_segm" width="500"/> | <img src="https://github.com/user-attachments/assets/43a558ad-50ca-4715-8285-9aa3268843c6" alt="boat_1_with_SAM_segm" width="500"/> |

### 0.2 coco 어노테이션을 피클 파일로 변환




```bash
python no_time_to_train/dataset/coco_to_pkl.py \
    data/my_custom_dataset/annotations/custom_references_with_segm.json \
    data/my_custom_dataset/annotations/custom_references_with_segm.pkl \
    1
```
### 1. 참조로 메모리 채우기

먼저, 유용한 변수를 정의하고 결과를 위한 폴더를 만듭니다. 라벨의 올바른 시각화를 위해 클래스 이름은 json 파일에 나타나는 카테고리 id 순서대로 정렬되어야 합니다. 예를 들어, `bird`의 카테고리 id는 `16`, `boat`의 카테고리 id는 `9`입니다. 따라서 `CAT_NAMES=boat,bird`가 됩니다.


```bash
DATASET_NAME=my_custom_dataset
DATASET_PATH=data/my_custom_dataset
CAT_NAMES=boat,bird
CATEGORY_NUM=2
SHOT=1
YAML_PATH=no_time_to_train/pl_configs/matching_cdfsod_template.yaml
PATH_TO_SAVE_CKPTS=./tmp_ckpts/my_custom_dataset
mkdir -p $PATH_TO_SAVE_CKPTS
```
1단계 실행:

```bash
python run_lightening.py test --config $YAML_PATH \
    --model.test_mode fill_memory \
    --out_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory.pth \
    --model.init_args.dataset_cfgs.fill_memory.root $DATASET_PATH/images \
    --model.init_args.dataset_cfgs.fill_memory.json_file $DATASET_PATH/annotations/custom_references_with_segm.json \
    --model.init_args.dataset_cfgs.fill_memory.memory_pkl $DATASET_PATH/annotations/custom_references_with_segm.pkl \
    --model.init_args.dataset_cfgs.fill_memory.memory_length $SHOT \
    --model.init_args.dataset_cfgs.fill_memory.cat_names $CAT_NAMES \
    --model.init_args.model_cfg.dataset_name $DATASET_NAME \
    --model.init_args.model_cfg.memory_bank_cfg.length $SHOT \
    --model.init_args.model_cfg.memory_bank_cfg.category_num $CATEGORY_NUM \
    --trainer.devices 1
```
### 2. 후처리 메모리 뱅크


```bash
python run_lightening.py test --config $YAML_PATH \
    --model.test_mode postprocess_memory \
    --ckpt_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory.pth \
    --out_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory_postprocessed.pth \
    --model.init_args.model_cfg.dataset_name $DATASET_NAME \
    --model.init_args.model_cfg.memory_bank_cfg.length $SHOT \
    --model.init_args.model_cfg.memory_bank_cfg.category_num $CATEGORY_NUM \
    --trainer.devices 1
```
### 3. 타겟 이미지에 대한 추론

`ONLINE_VIS`가 True로 설정되어 있으면, 예측 결과가 `results_analysis/my_custom_dataset/`에 저장되고 계산되는 대로 표시됩니다. 온라인 시각화와 함께 실행하면 속도가 훨씬 느려진다는 점에 유의하세요.

분할된 인스턴스를 더 많이 또는 더 적게 보고 싶다면 점수 임계값 `VIS_THR`를 자유롭게 변경하세요.

```bash
ONLINE_VIS=True
VIS_THR=0.4
python run_lightening.py test --config $YAML_PATH \
    --model.test_mode test \
    --ckpt_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory_postprocessed.pth \
    --model.init_args.model_cfg.dataset_name $DATASET_NAME \
    --model.init_args.model_cfg.memory_bank_cfg.length $SHOT \
    --model.init_args.model_cfg.memory_bank_cfg.category_num $CATEGORY_NUM \
    --model.init_args.model_cfg.test.imgs_path $DATASET_PATH/images \
    --model.init_args.model_cfg.test.online_vis $ONLINE_VIS \
    --model.init_args.model_cfg.test.vis_thr $VIS_THR \
    --model.init_args.dataset_cfgs.test.root $DATASET_PATH/images \
    --model.init_args.dataset_cfgs.test.json_file $DATASET_PATH/annotations/custom_targets.json \
    --model.init_args.dataset_cfgs.test.cat_names $CAT_NAMES \
    --trainer.devices 1
```
### 결과

성능 지표(위 명령어와 정확히 동일한 매개변수 사용)는 다음과 같아야 합니다:


```
BBOX RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.478

SEGM RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.458
```
시각적 결과는 `results_analysis/my_custom_dataset/`에 저장됩니다. 본 방법은 원하는 클래스의 인스턴스가 없는 이미지(즉, false negative)에 대해서도 작동합니다.

*이미지를 클릭하면 확대됩니다 ⬇️*

| 보트가 있는 대상 이미지 ⛵ (왼쪽 GT, 오른쪽 예측) | 새가 있는 대상 이미지 🐦 (왼쪽 GT, 오른쪽 예측) |
|:----------------------:|:----------------------:|
| ![000000459673](https://github.com/user-attachments/assets/678dc15a-dd3b-49d5-9287-6290da16aa6b) | ![000000407180](https://github.com/user-attachments/assets/fe306e48-af49-4d83-ac82-76fac6c456d1) |

| 보트와 새가 모두 있는 대상 이미지 ⛵🐦 (왼쪽 GT, 오른쪽 예측) | 보트나 새가 없는 대상 이미지 🚫 (왼쪽 GT, 오른쪽 예측) |
|:---------------------------------:|:----------------------------------:|
| ![000000517410](https://github.com/user-attachments/assets/9849b227-7f43-43d7-81ea-58010a623ad5) | ![000000460598](https://github.com/user-attachments/assets/7587700c-e09d-4cf6-8590-3df129c2568e) |


## 📚 인용

이 연구를 사용하신다면, 아래와 같이 인용해 주세요:


```bibtex
@article{espinosa2025notimetotrain,
  title={No time to train! Training-Free Reference-Based Instance Segmentation},
  author={Miguel Espinosa and Chenhongyi Yang and Linus Ericsson and Steven McDonagh and Elliot J. Crowley},
  journal={arXiv preprint arXiv:2507.02798},
  year={2025},
  primaryclass={cs.CV}
}
```


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-09-06

---