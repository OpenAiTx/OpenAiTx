<div align="right">
  <details>
    <summary >🌐 언어</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-CN">简体中文</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-TW">繁體中文</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ko">한국어</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=hi">हिन्दी</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=th">ไทย</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fr">Français</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=es">Español</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=it">Italiano</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ru">Русский</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pt">Português</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ar">العربية</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fa">فارسی</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=tr">Türkçe</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=vi">Tiếng Việt</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=id">Bahasa Indonesia</a>
      </div>
    </div>
  </details>
</div>

<div align="center">

# 🚀 훈련할 시간 없어!  
### 훈련 없는 참조 기반 인스턴스 분할  
[![GitHub](https://img.shields.io/badge/%E2%80%8B-No%20Time%20To%20Train-black?logo=github)](https://github.com/miquel-espinosa/no-time-to-train)
[![Website](https://img.shields.io/badge/🌐-Project%20Page-grey)](https://miquel-espinosa.github.io/no-time-to-train/)
[![arXiv](https://img.shields.io/badge/arXiv-2507.02798-b31b1b)](https://arxiv.org/abs/2507.02798)

**최신 성능 (Papers with Code)**

[**_1-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-1-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference)

[**_10-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-10-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference)

[**_30-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-30-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference)

</div>

---

> 🔔 **업데이트 (2025년 7월):** 코드가 지침과 함께 업데이트되었습니다!

---

## 📋 목차

- [🎯 주요 특징](#-highlights)
- [📜 초록](#-abstract)
- [🧠 아키텍처](#-architecture)
- [🛠️ 설치 안내](#️-installation-instructions)
  - [1. 저장소 복제](#1-clone-the-repository)
  - [2. conda 환경 생성](#2-create-conda-environment)
  - [3. SAM2 및 DinoV2 설치](#3-install-sam2-and-dinov2)
  - [4. 데이터셋 다운로드](#4-download-datasets)
  - [5. SAM2 및 DinoV2 체크포인트 다운로드](#5-download-sam2-and-dinov2-checkpoints)
- [📊 추론 코드: Few-shot COCO에서 30-shot SOTA 결과 재현](#-inference-code)
  - [0. 참조 세트 생성](#0-create-reference-set)
  - [1. 참조로 메모리 채우기](#1-fill-memory-with-references)
  - [2. 메모리 뱅크 후처리](#2-post-process-memory-bank)
  - [3. 대상 이미지에서 추론](#3-inference-on-target-images)
  - [결과](#results)
- [🔍 인용](#-citation)


## 🎯 주요 특징
- 💡 **훈련 필요 없음**: 파인튜닝이나 프롬프트 엔지니어링 없이, 참조 이미지만 사용  
- 🖼️ **참조 기반**: 몇 개의 예시로 새로운 객체 분할  
- 🔥 **최신 성능**: COCO, PASCAL VOC, Cross-Domain FSOD에서 기존 훈련 없는 방법보다 우수한 성능

**링크:**
- 🧾 [**arXiv 논문**](https://arxiv.org/abs/2507.02798)  
- 🌐 [**프로젝트 웹사이트**](https://miquel-espinosa.github.io/no-time-to-train/)  
- 📈 [**Papers with Code**](https://paperswithcode.com/paper/no-time-to-train-training-free-reference)

## 📜 초록

> 이미지 분할 모델의 성능은 대규모 주석 데이터 수집의 높은 비용에 의해 역사적으로 제한되어 왔습니다. Segment Anything Model(SAM)은 프롬프트 기반, 의미 불가지론적 분할 패러다임을 통해 이 문제를 완화하지만 여전히 새로운 이미지를 처리하기 위해 수동 시각 프롬프트나 복잡한 도메인 종속 프롬프트 생성 규칙이 필요합니다. 이 새로운 부담을 줄이기 위해, 본 연구는 소수의 참조 이미지만 제공되는 객체 분할 작업을 탐구합니다. 우리의 핵심 통찰은 파운데이션 모델이 학습한 강력한 의미론적 사전 지식을 활용하여 참조 이미지와 대상 이미지 간의 대응 영역을 식별하는 것입니다. 이러한 대응은 다운스트림 작업을 위한 인스턴스 수준 분할 마스크의 자동 생성을 가능하게 하며, (1) 메모리 뱅크 구축, (2) 표현 집계, (3) 의미 인식 특성 매칭을 통합한 다단계 훈련 없는 방법으로 구현됩니다. 실험 결과, COCO FSOD(36.8% nAP), PASCAL VOC Few-Shot(71.2% nAP50), Cross-Domain FSOD 벤치마크(22.4% nAP)에서 최신 성능을 달성하며 분할 지표에서 상당한 개선을 보였습니다.

![cdfsod-results-final-comic-sans-min](https://github.com/user-attachments/assets/ab302c02-c080-4042-99fc-0e181ba8abb9)


## 🧠 아키텍처

![training-free-architecture-comic-sans-min](https://github.com/user-attachments/assets/d84dd83a-505e-45a0-8ce3-98e1838017f9)


## 🛠️ 설치 안내

### 1. 저장소 복제


```bash
git clone https://github.com/miquel-espinosa/no-time-to-train.git
cd no-time-to-train
```
### 2. conda 환경 생성

필요한 패키지를 포함한 conda 환경을 생성합니다.

```bash
conda env create -f environment.yml
conda activate no-time-to-train
```
### 3. SAM2 및 DinoV2 설치

우리는 SAM2와 DinoV2를 소스에서 설치할 것입니다.

```bash
pip install -e .
cd dinov2
pip install -e .
cd ..
```
### 4. 데이터셋 다운로드

COCO 데이터셋을 다운로드하여 `data/coco`에 저장하세요.

### 5. SAM2 및 DinoV2 체크포인트 다운로드

논문에서 사용된 정확한 SAM2 체크포인트를 다운로드할 것입니다.
(단, SAM2.1 체크포인트는 이미 제공되고 있으며 더 나은 성능을 보일 수 있습니다.)


```bash
mkdir -p checkpoints/dinov2
cd checkpoints
wget https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt
cd dinov2
wget https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth
cd ../..
```
<translate-content>

## 📊 추론 코드

⚠️ 면책 조항: 이 코드는 연구용입니다 — 약간의 혼란이 있을 수 있습니다!

### Few-shot COCO에서 30-shot SOTA 결과 재현하기

유용한 변수를 정의하고 결과를 위한 폴더를 만듭니다:
</translate-content>
```bash
CONFIG=./dev_hongyi/new_exps/coco_fewshot_10shot_Sam2L.yaml
CLASS_SPLIT="few_shot_classes"
RESULTS_DIR=work_dirs/few_shot_results
SHOTS=30
SEED=33
GPUS=4

mkdir -p $RESULTS_DIR
FILENAME=few_shot_${SHOTS}shot_seed${SEED}.pkl
```
#### 0. 참조 세트 생성


```bash
python dev_hongyi/dataset/few_shot_sampling.py \
        --n-shot $SHOTS \
        --out-path ${RESULTS_DIR}/${FILENAME} \
        --seed $SEED \
        --dataset $CLASS_SPLIT
```
#### 1. 참조로 메모리 채우기


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode fill_memory \
                              --out_path ${RESULTS_DIR}/memory.ckpt \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.memory_pkl ${RESULTS_DIR}/${FILENAME} \
                              --model.init_args.dataset_cfgs.fill_memory.memory_length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
#### 2. 후처리 메모리 뱅크


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode postprocess_memory \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --ckpt_path ${RESULTS_DIR}/memory.ckpt \
                              --out_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --trainer.devices 1
```
#### 3. 대상 이미지에 대한 추론


```bash
python run_lightening.py test --config $CONFIG  \
                              --ckpt_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --model.init_args.test_mode test \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.model_cfg.dataset_name $CLASS_SPLIT \
                              --model.init_args.dataset_cfgs.test.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
<translate-content>
온라인으로 추론 결과를 실시간으로 확인하고 싶다면 `dev_hongyi/models/Sam2MatchingBaseline_noAMG.py` 파일의 1746-1749행의 주석을 해제하세요 [여기](https://github.com/miquel-espinosa/no-time-to-train/blob/main/dev_hongyi/models/Sam2MatchingBaseline_noAMG.py#L1746).
더 많거나 적은 분할된 인스턴스를 보려면 `score_thr` 점수 임계값 파라미터를 필요에 따라 조정하세요.
이미지는 이제 `results_analysis/few_shot_classes/`에 저장됩니다. 왼쪽 이미지는 실제 정답을, 오른쪽 이미지는 학습 없는 방법으로 찾은 분할 인스턴스를 보여줍니다.

이 예제에서는 `few_shot_classes` 분할을 사용하므로, 이 분할에 포함된 클래스의 분할 인스턴스만 볼 수 있어야 합니다 (COCO의 모든 클래스는 아님).

#### 결과

검증 세트의 모든 이미지를 실행한 후에는 다음을 얻을 수 있습니다:
</translate-content>
```
BBOX RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.368

SEGM RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.342
```
---


## 🔍 Citation

If you use this work, please cite us:

```bibtex
@article{espinosa2025notimetotrain,
  title={No time to train! Training-Free Reference-Based Instance Segmentation},
  author={Miguel Espinosa and Chenhongyi Yang and Linus Ericsson and Steven McDonagh and Elliot J. Crowley},
  journal={arXiv preprint arXiv:2507.02798},
  year={2025},
  primaryclass={cs.CV}
}
```

---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-22

---