<div align="right">
  <details>
    <summary >🌐 言語</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=en">英語</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-CN">簡体中文</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-TW">繁體中文</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ko">한국어</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=hi">ヒンディー語</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=th">タイ語</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fr">フランス語</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=de">ドイツ語</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=es">スペイン語</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=it">イタリア語</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ru">ロシア語</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pt">ポルトガル語</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=nl">オランダ語</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pl">ポーランド語</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ar">アラビア語</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fa">ペルシャ語</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=tr">トルコ語</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=vi">ベトナム語</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=id">インドネシア語</a>
      </div>
    </div>
  </details>
</div>

<div align="center">

# 🚀 トレーニング不要！  
### トレーニング不要の参照ベースインスタンスセグメンテーション  
[![GitHub](https://img.shields.io/badge/%E2%80%8B-No%20Time%20To%20Train-black?logo=github)](https://github.com/miquel-espinosa/no-time-to-train)
[![Website](https://img.shields.io/badge/🌐-Project%20Page-grey)](https://miquel-espinosa.github.io/no-time-to-train/)
[![arXiv](https://img.shields.io/badge/arXiv-2507.02798-b31b1b)](https://arxiv.org/abs/2507.02798)

**最先端技術（Papers with Code）**

[**_SOTA 1-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-1-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference)

[**_SOTA 10-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-10-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference)

[**_SOTA 30-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-30-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference)

</div>

---

> 🚨 **更新（2025年7月22日）:** カスタムデータセット用の手順を追加しました！
> 
> 🔔 **更新（2025年7月16日）:** コードと手順を更新しました！

---

## 📋 目次

- [🎯 ハイライト](#-highlights)
- [📜 概要](#-abstract)
- [🧠 アーキテクチャ](#-architecture)
- [🛠️ インストール手順](#️-installation-instructions)
  - [1. リポジトリのクローン](#1-clone-the-repository)
  - [2. conda環境の作成](#2-create-conda-environment)
  - [3. SAM2とDinoV2のインストール](#3-install-sam2-and-dinov2)
  - [4. データセットのダウンロード](#4-download-datasets)
  - [5. SAM2とDinoV2のチェックポイントをダウンロード](#5-download-sam2-and-dinov2-checkpoints)
- [📊 推論コード: Few-shot COCOで30-shot SOTAを再現](#-inference-code)
  - [0. 参照セットの作成](#0-create-reference-set)
  - [1. メモリに参照を格納](#1-fill-memory-with-references)
  - [2. メモリバンクの後処理](#2-post-process-memory-bank)
  - [3. ターゲット画像で推論](#3-inference-on-target-images)
  - [結果](#results)
- [🔍 カスタムデータセット](#-custom-dataset)
  - [0. カスタムデータセットの準備 ⛵🐦](#0-prepare-a-custom-dataset)
  - [0.1 bboxアノテーションのみの場合](#01-if-only-bbox-annotations-are-available)
  - [0.2 cocoアノテーションをpickleファイルに変換](#02-convert-coco-annotations-to-pickle-file)
  - [1. メモリに参照を格納](#1-fill-memory-with-references)
  - [2. メモリバンクの後処理](#2-post-process-memory-bank)
- [📚 引用](#-citation)


## 🎯 ハイライト
- 💡 **トレーニング不要**: ファインチューニングもプロンプト設計も不要―参照画像だけでOK。  
- 🖼️ **参照ベース**: わずかな例のみで新しい物体をセグメント化。  
- 🔥 **SOTA性能**: COCO、PASCAL VOC、Cross-Domain FSODで従来のトレーニング不要手法を凌駕。

**リンク:**
- 🧾 [**arXiv 論文**](https://arxiv.org/abs/2507.02798)  
- 🌐 [**プロジェクトサイト**](https://miquel-espinosa.github.io/no-time-to-train/)  
- 📈 [**Papers with Code**](https://paperswithcode.com/paper/no-time-to-train-training-free-reference)

## 📜 概要

> 画像セグメンテーションモデルの性能は、大規模なアノテーションデータ収集の高コストにより、歴史的に制約されてきました。Segment Anything Model（SAM）は、プロンプト可能かつセマンティクス非依存なセグメンテーションパラダイムにより、この従来の課題を解決しますが、新しい画像の処理には手動での視覚プロンプトや複雑なドメイン依存プロンプト生成ルールが依然として必要です。この新たな負担の軽減を目指し、本研究では少数の参照画像のみが与えられた場合の物体セグメンテーションに着目します。我々の主な着想は、ファウンデーションモデルによって学習された強力なセマンティック事前知識を活用し、参照画像とターゲット画像間の対応領域を特定することです。対応を見つけることで、下流タスクのためのインスタンスレベルのセグメンテーションマスクを自動生成できることが分かり、我々はこのアイデアを、（1）メモリバンク構築、（2）表現の集約、（3）セマンティクス認識特徴マッチングを組み合わせたマルチステージのトレーニング不要手法として具現化しました。実験ではセグメンテーション指標が大きく向上し、COCO FSOD（36.8% nAP）、PASCAL VOC Few-Shot（71.2% nAP50）で最先端性能を達成し、Cross-Domain FSODベンチマーク（22.4% nAP）でも既存のトレーニング不要手法を上回りました。

![cdfsod-results-final-comic-sans-min](https://github.com/user-attachments/assets/ab302c02-c080-4042-99fc-0e181ba8abb9)




## 🧠 アーキテクチャ

![training-free-architecture-comic-sans-min](https://github.com/user-attachments/assets/d84dd83a-505e-45a0-8ce3-98e1838017f9)


## 🛠️ インストール手順

### 1. リポジトリをクローンする

```bash
git clone https://github.com/miquel-espinosa/no-time-to-train.git
cd no-time-to-train
```
### 2. conda環境の作成

必要なパッケージを含むconda環境を作成します。

```bash
conda env create -f environment.yml
conda activate no-time-to-train
```
### 3. SAM2 と DinoV2 のインストール

SAM2 と DinoV2 をソースからインストールします。

```bash
pip install -e .
cd dinov2
pip install -e .
cd ..
```
### 4. データセットのダウンロード

COCOデータセットをダウンロードし、`data/coco` に配置してください。

### 5. SAM2およびDinoV2チェックポイントのダウンロード

論文で使用された正確なSAM2チェックポイントをダウンロードします。
（ただし、SAM2.1のチェックポイントはすでに利用可能であり、より良い性能を示す可能性があります。）


```bash
mkdir -p checkpoints/dinov2
cd checkpoints
wget https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt
cd dinov2
wget https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth
cd ../..
```



## 📊 推論コード

⚠️ 免責事項：これは研究用コードです — 多少の混乱はご容赦ください！

### Few-shot COCOで30ショットSOTA結果の再現

有用な変数を定義し、結果用のフォルダを作成します：


```bash
CONFIG=./no_time_to_train/new_exps/coco_fewshot_10shot_Sam2L.yaml
CLASS_SPLIT="few_shot_classes"
RESULTS_DIR=work_dirs/few_shot_results
SHOTS=30
SEED=33
GPUS=4

mkdir -p $RESULTS_DIR
FILENAME=few_shot_${SHOTS}shot_seed${SEED}.pkl
```
#### 0. 参照セットの作成


```bash
python no_time_to_train/dataset/few_shot_sampling.py \
        --n-shot $SHOTS \
        --out-path ${RESULTS_DIR}/${FILENAME} \
        --seed $SEED \
        --dataset $CLASS_SPLIT
```
#### 1. 参照でメモリを埋める


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode fill_memory \
                              --out_path ${RESULTS_DIR}/memory.ckpt \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.memory_pkl ${RESULTS_DIR}/${FILENAME} \
                              --model.init_args.dataset_cfgs.fill_memory.memory_length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
#### 2. ポストプロセスメモリバンク


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode postprocess_memory \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --ckpt_path ${RESULTS_DIR}/memory.ckpt \
                              --out_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --trainer.devices 1
```
#### 3. 対象画像に対する推論


```bash
python run_lightening.py test --config $CONFIG  \
                              --ckpt_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --model.init_args.test_mode test \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.model_cfg.dataset_name $CLASS_SPLIT \
                              --model.init_args.dataset_cfgs.test.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
オンラインで推論結果（計算中のもの）を見たい場合は、`no_time_to_train/models/Sam2MatchingBaseline_noAMG.py` の 1746-1749 行をコメント解除してください。[こちら](https://github.com/miquel-espinosa/no-time-to-train/blob/main/no_time_to_train/models/Sam2MatchingBaseline_noAMG.py#L1746)です。
より多くまたは少ないセグメント化インスタンスを表示するには、スコア閾値 `score_thr` パラメータを必要に応じて調整してください。
画像は `results_analysis/few_shot_classes/` に保存されます。左側の画像はグラウンドトゥルース、右側の画像はトレーニング不要な手法によって検出されたセグメント化インスタンスを示しています。

この例では `few_shot_classes` 分割を使用していることに注意してください。そのため、この分割に含まれるクラスのセグメント化インスタンスのみが表示されることが期待されます（COCOのすべてのクラスではありません）。

#### 結果

検証セット内のすべての画像を処理した後、以下の結果が得られるはずです：


```
BBOX RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.368

SEGM RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.342
```
---

## 🔍 カスタムデータセット

本パイプラインをカスタムデータセットで実行する手順を提供します。アノテーション形式は常にCOCOフォーマットです。

> **TLDR;** *カスタムデータセット*でフルパイプラインを実行する方法を直接確認したい場合は、`scripts/matching_cdfsod_pipeline.sh`およびCD-FSODデータセットの例スクリプト（例：`scripts/dior_fish.sh`）をご覧ください。

### 0. カスタムデータセットの準備 ⛵🐦

カスタムデータセットで**ボート**⛵と**鳥**🐦を検出したいと仮定します。本手法を利用するには以下が必要です：
- 各クラスごとに少なくとも1枚の*アノテート済み*参照画像（例：ボート用1枚、鳥用1枚）
- 対象クラスのインスタンスを見つけるための複数のターゲット画像

COCO画像を用いてカスタムデータセットを作成するトイスクリプトを用意しており、**1-shot**設定に対応しています。
```bash
python scripts/make_custom_dataset.py
```
これにより、以下のフォルダ構造を持つカスタムデータセットが作成されます。
```
data/my_custom_dataset/
    ├── annotations/
    │   ├── custom_references.json
    │   ├── custom_targets.json
    │   └── references_visualisations/
    │       ├── bird_1.jpg
    │       └── boat_1.jpg
    └── images/
        ├── 429819.jpg
        ├── 101435.jpg
        └── (all target and reference images)
```
**参照画像の可視化（1ショット）：**

| BIRD 🐦 の1ショット参照画像 | BOAT ⛵ の1ショット参照画像 |
|:-----------------------------:|:------------------------------:|
| <img src="https://github.com/user-attachments/assets/e59e580d-a7db-42ac-b386-892af211fc85" alt="bird_1" width="500"/> | <img src="https://github.com/user-attachments/assets/f94ee025-ae37-4a45-9c3e-0cfe8f8cd2bc" alt="boat_1" width="500"/> |


### 0.1 バウンディングボックスアノテーションのみが利用可能な場合

参照画像にバウンディングボックスアノテーションしかない場合にも対応できるよう、SAM2を用いてインスタンスレベルのセグメンテーションマスクを生成するスクリプトも提供しています。


```bash
# Download sam_h checkpoint. Feel free to use more recent checkpoints (note: code might need to be adapted)
wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth -O checkpoints/sam_vit_h_4b8939.pth
# Run automatic instance segmentation from ground truth bounding boxes.
python no_time_to_train/dataset/sam_bbox_to_segm_batch.py \
    --input_json data/my_custom_dataset/annotations/custom_references.json \
    --image_dir data/my_custom_dataset/images \
    --sam_checkpoint checkpoints/sam_vit_h_4b8939.pth \
    --model_type vit_h \
    --device cuda \
    --batch_size 8 \
    --visualize
```
**インスタンスレベルのセグメンテーションマスク付き参照画像（gtバウンディングボックスからSAM2で生成、1-shot）：**

生成されたセグメンテーションマスクの可視化は、`data/my_custom_dataset/annotations/custom_references_with_SAM_segm/references_visualisations/` に保存されています。


| BIRD 🐦 の1-shot参照画像（SAMで自動セグメント化） | BOAT ⛵ の1-shot参照画像（SAMで自動セグメント化） |
|:---------------------------------:|:----------------------------------:|
| <img src="https://github.com/user-attachments/assets/65d38dc4-1454-43cd-9600-e8efc67b3a82" alt="bird_1_with_SAM_segm" width="500"/> | <img src="https://github.com/user-attachments/assets/43a558ad-50ca-4715-8285-9aa3268843c6" alt="boat_1_with_SAM_segm" width="500"/> |


### 0.2 cocoアノテーションをpickleファイルに変換


```bash
python no_time_to_train/dataset/coco_to_pkl.py \
    data/my_custom_dataset/annotations/custom_references_with_segm.json \
    data/my_custom_dataset/annotations/custom_references_with_segm.pkl \
    1
```
### 1. 参照でメモリを埋める

最初に、便利な変数を定義し、結果用のフォルダを作成します。ラベルを正しく可視化するためには、クラス名をjsonファイルに記載されているカテゴリID順に並べる必要があります。例：`bird` のカテゴリIDは `16`、`boat` のカテゴリIDは `9` です。したがって、`CAT_NAMES=boat,bird` となります。


```bash
DATASET_NAME=my_custom_dataset
DATASET_PATH=data/my_custom_dataset
CAT_NAMES=boat,bird
CATEGORY_NUM=2
SHOT=1
YAML_PATH=no_time_to_train/pl_configs/matching_cdfsod_template.yaml
PATH_TO_SAVE_CKPTS=./tmp_ckpts/my_custom_dataset
mkdir -p $PATH_TO_SAVE_CKPTS
```
ステップ1を実行します：

```bash
python run_lightening.py test --config $YAML_PATH \
    --model.test_mode fill_memory \
    --out_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory.pth \
    --model.init_args.dataset_cfgs.fill_memory.root $DATASET_PATH/images \
    --model.init_args.dataset_cfgs.fill_memory.json_file $DATASET_PATH/annotations/custom_references_with_segm.json \
    --model.init_args.dataset_cfgs.fill_memory.memory_pkl $DATASET_PATH/annotations/custom_references_with_segm.pkl \
    --model.init_args.dataset_cfgs.fill_memory.memory_length $SHOT \
    --model.init_args.dataset_cfgs.fill_memory.cat_names $CAT_NAMES \
    --model.init_args.model_cfg.dataset_name $DATASET_NAME \
    --model.init_args.model_cfg.memory_bank_cfg.length $SHOT \
    --model.init_args.model_cfg.memory_bank_cfg.category_num $CATEGORY_NUM \
    --trainer.devices 1
```
### 2. ポストプロセスメモリバンク


```bash
python run_lightening.py test --config $YAML_PATH \
    --model.test_mode postprocess_memory \
    --ckpt_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory.pth \
    --out_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory_postprocessed.pth \
    --model.init_args.model_cfg.dataset_name $DATASET_NAME \
    --model.init_args.model_cfg.memory_bank_cfg.length $SHOT \
    --model.init_args.model_cfg.memory_bank_cfg.category_num $CATEGORY_NUM \
    --trainer.devices 1
```
### 3. ターゲット画像での推論

`ONLINE_VIS` を True に設定すると、予測結果は `results_analysis/my_custom_dataset/` に保存され、計算されると同時に表示されます。オンライン可視化を有効にすると処理速度が大幅に遅くなるので注意してください。

スコア閾値 `VIS_THR` を変更して、より多くまたは少ないセグメント化インスタンスを表示することができます。

```bash
ONLINE_VIS=True
VIS_THR=0.4
python run_lightening.py test --config $YAML_PATH \
    --model.test_mode test \
    --ckpt_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory_postprocessed.pth \
    --model.init_args.model_cfg.dataset_name $DATASET_NAME \
    --model.init_args.model_cfg.memory_bank_cfg.length $SHOT \
    --model.init_args.model_cfg.memory_bank_cfg.category_num $CATEGORY_NUM \
    --model.init_args.model_cfg.test.imgs_path $DATASET_PATH/images \
    --model.init_args.model_cfg.test.online_vis $ONLINE_VIS \
    --model.init_args.model_cfg.test.vis_thr $VIS_THR \
    --model.init_args.dataset_cfgs.test.root $DATASET_PATH/images \
    --model.init_args.dataset_cfgs.test.json_file $DATASET_PATH/annotations/custom_targets.json \
    --model.init_args.dataset_cfgs.test.cat_names $CAT_NAMES \
    --trainer.devices 1
```
### 結果

パフォーマンス指標（上記のコマンドと全く同じパラメータを使用）は次のとおりです。


```
BBOX RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.478

SEGM RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.458
```
`results_analysis/my_custom_dataset/` に視覚的な結果が保存されます。なお、本手法は偽陰性、すなわち目的クラスのインスタンスが含まれていない画像にも対応しています。

*画像をクリックすると拡大表示されます ⬇️*

| ボートがあるターゲット画像 ⛵（左：GT、右：予測） | 鳥がいるターゲット画像 🐦（左：GT、右：予測） |
|:----------------------:|:----------------------:|
| ![000000459673](https://github.com/user-attachments/assets/678dc15a-dd3b-49d5-9287-6290da16aa6b) | ![000000407180](https://github.com/user-attachments/assets/fe306e48-af49-4d83-ac82-76fac6c456d1) |

| ボートと鳥があるターゲット画像 ⛵🐦（左：GT、右：予測） | ボートや鳥がいないターゲット画像 🚫（左：GT、右：予測） |
|:---------------------------------:|:----------------------------------:|
| ![000000517410](https://github.com/user-attachments/assets/9849b227-7f43-43d7-81ea-58010a623ad5) | ![000000460598](https://github.com/user-attachments/assets/7587700c-e09d-4cf6-8590-3df129c2568e) |


## 📚 引用

本研究を使用される場合は、以下のように引用してください:


```bibtex
@article{espinosa2025notimetotrain,
  title={No time to train! Training-Free Reference-Based Instance Segmentation},
  author={Miguel Espinosa and Chenhongyi Yang and Linus Ericsson and Steven McDonagh and Elliot J. Crowley},
  journal={arXiv preprint arXiv:2507.02798},
  year={2025},
  primaryclass={cs.CV}
}
```


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-23

---