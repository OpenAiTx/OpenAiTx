<div align="right">
  <details>
    <summary >üåê Sprache</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-TW">ÁπÅÈ´î‰∏≠Êñá</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=hi">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=th">‡πÑ‡∏ó‡∏¢</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fr">Fran√ßais</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=es">Espa√±ol</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=it">Italiano</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ru">–†—É—Å—Å–∫–∏–π</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pt">Portugu√™s</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ar">ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fa">ŸÅÿßÿ±ÿ≥€å</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=tr">T√ºrk√ße</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=vi">Ti·∫øng Vi·ªát</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=id">Bahasa Indonesia</a>
      </div>
    </div>
  </details>
</div>

<div align="center">

# üöÄ Keine Zeit zum Trainieren!  
### Trainingsfreie, referenzbasierte Instanzsegmentierung  
[![GitHub](https://img.shields.io/badge/%E2%80%8B-No%20Time%20To%20Train-black?logo=github)](https://github.com/miquel-espinosa/no-time-to-train)
[![Website](https://img.shields.io/badge/üåê-Project%20Page-grey)](https://miquel-espinosa.github.io/no-time-to-train/)
[![arXiv](https://img.shields.io/badge/arXiv-2507.02798-b31b1b)](https://arxiv.org/abs/2507.02798)

**State-of-the-art (Papers with Code)**

[**_1-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-1-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference)

[**_10-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-10-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference)

[**_30-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-30-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference)

</div>

---

> üîî **Update (Juli 2025):** Der Code wurde mit Anweisungen aktualisiert!

---

## üìã Inhaltsverzeichnis

- [üéØ Highlights](#-highlights)
- [üìú Zusammenfassung](#-abstract)
- [üß† Architektur](#-architecture)
- [üõ†Ô∏è Installationsanweisungen](#Ô∏è-installation-instructions)
  - [1. Repository klonen](#1-clone-the-repository)
  - [2. Conda-Umgebung erstellen](#2-create-conda-environment)
  - [3. SAM2 und DinoV2 installieren](#3-install-sam2-and-dinov2)
  - [4. Datens√§tze herunterladen](#4-download-datasets)
  - [5. SAM2- und DinoV2-Checkpoints herunterladen](#5-download-sam2-and-dinov2-checkpoints)
- [üìä Inferenz-Code: 30-shot SOTA-Ergebnisse auf Few-shot COCO reproduzieren](#-inference-code)
  - [0. Referenzsatz erstellen](#0-create-reference-set)
  - [1. Speicher mit Referenzen f√ºllen](#1-fill-memory-with-references)
  - [2. Speicherbank nachbearbeiten](#2-post-process-memory-bank)
  - [3. Inferenz auf Zielbildern](#3-inference-on-target-images)
  - [Ergebnisse](#results)
- [üîç Zitation](#-citation)


## üéØ Highlights
- üí° **Trainingsfrei**: Kein Fine-Tuning, kein Prompt-Engineering‚Äînur ein Referenzbild.  
- üñºÔ∏è **Referenzbasiert**: Segmentiere neue Objekte mit nur wenigen Beispielen.  
- üî• **SOTA-Leistung**: √úbertrifft bisherige trainingsfreie Ans√§tze auf COCO, PASCAL VOC und Cross-Domain FSOD.

**Links:**
- üßæ [**arXiv Paper**](https://arxiv.org/abs/2507.02798)  
- üåê [**Projektwebsite**](https://miquel-espinosa.github.io/no-time-to-train/)  
- üìà [**Papers with Code**](https://paperswithcode.com/paper/no-time-to-train-training-free-reference)

## üìú Zusammenfassung

> Die Leistung von Bildsegmentierungsmodellen war historisch durch die hohen Kosten f√ºr die Sammlung gro√ü angelegter, annotierter Daten begrenzt. Das Segment Anything Model (SAM) mindert dieses urspr√ºngliche Problem durch ein promptbasiertes, semantikagnostisches Segmentierungsparadigma, ben√∂tigt aber dennoch manuelle visuelle Prompts oder komplexe, dom√§nenspezifische Prompt-Generierungsregeln, um ein neues Bild zu verarbeiten. Um diese neue H√ºrde zu verringern, untersucht unsere Arbeit die Aufgabe der Objektsegmentierung, wenn stattdessen nur eine kleine Menge von Referenzbildern zur Verf√ºgung steht. Unsere zentrale Erkenntnis ist es, starke semantische Vorannahmen, wie sie von Foundation-Modellen gelernt werden, zu nutzen, um korrespondierende Regionen zwischen einem Referenz- und einem Zielbild zu identifizieren. Wir stellen fest, dass diese Korrespondenzen die automatische Generierung von Instanzsegmentierungsmasken f√ºr nachgelagerte Aufgaben erm√∂glichen und setzen unsere Ideen √ºber eine mehrstufige, trainingsfreie Methode um, die (1) den Aufbau einer Speicherbank, (2) Repr√§sentationsaggregation und (3) semantisch-bewusstes Feature-Matching umfasst. Unsere Experimente zeigen signifikante Verbesserungen bei Segmentierungsmetriken, die zu State-of-the-Art-Leistungen auf COCO FSOD (36,8 % nAP), PASCAL VOC Few-Shot (71,2 % nAP50) f√ºhren und bestehende trainingsfreie Ans√§tze auf dem Cross-Domain FSOD Benchmark (22,4 % nAP) √ºbertreffen.

![cdfsod-results-final-comic-sans-min](https://github.com/user-attachments/assets/ab302c02-c080-4042-99fc-0e181ba8abb9)


## üß† Architektur

![training-free-architecture-comic-sans-min](https://github.com/user-attachments/assets/d84dd83a-505e-45a0-8ce3-98e1838017f9)


## üõ†Ô∏è Installationsanweisungen

### 1. Repository klonen


```bash
git clone https://github.com/miquel-espinosa/no-time-to-train.git
cd no-time-to-train
```
### 2. Conda-Umgebung erstellen

Wir werden eine Conda-Umgebung mit den ben√∂tigten Paketen erstellen.

```bash
conda env create -f environment.yml
conda activate no-time-to-train
```
### 3. Installieren Sie SAM2 und DinoV2

Wir werden SAM2 und DinoV2 aus dem Quellcode installieren.

```bash
pip install -e .
cd dinov2
pip install -e .
cd ..
```
### 4. Datens√§tze herunterladen

Bitte laden Sie den COCO-Datensatz herunter und platzieren Sie ihn in `data/coco`

### 5. SAM2- und DinoV2-Checkpoints herunterladen

Wir werden die exakt im Paper verwendeten SAM2-Checkpoints herunterladen.
(Beachten Sie jedoch, dass SAM2.1-Checkpoints bereits verf√ºgbar sind und m√∂glicherweise bessere Ergebnisse liefern.)


```bash
mkdir -p checkpoints/dinov2
cd checkpoints
wget https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt
cd dinov2
wget https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth
cd ../..
```
## üìä Inferenzcode

‚ö†Ô∏è Haftungsausschluss: Dies ist Forschungscode ‚Äì erwarten Sie ein wenig Chaos!

### Reproduktion der 30-shot SOTA-Ergebnisse bei Few-shot COCO

Definieren Sie n√ºtzliche Variablen und erstellen Sie einen Ordner f√ºr die Ergebnisse:



```bash
CONFIG=./dev_hongyi/new_exps/coco_fewshot_10shot_Sam2L.yaml
CLASS_SPLIT="few_shot_classes"
RESULTS_DIR=work_dirs/few_shot_results
SHOTS=30
SEED=33
GPUS=4

mkdir -p $RESULTS_DIR
FILENAME=few_shot_${SHOTS}shot_seed${SEED}.pkl
```
#### 0. Referenzsatz erstellen


```bash
python dev_hongyi/dataset/few_shot_sampling.py \
        --n-shot $SHOTS \
        --out-path ${RESULTS_DIR}/${FILENAME} \
        --seed $SEED \
        --dataset $CLASS_SPLIT
```
#### 1. Speicher mit Referenzen f√ºllen


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode fill_memory \
                              --out_path ${RESULTS_DIR}/memory.ckpt \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.memory_pkl ${RESULTS_DIR}/${FILENAME} \
                              --model.init_args.dataset_cfgs.fill_memory.memory_length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
#### 2. Nachbearbeitung des Speicherbanks


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode postprocess_memory \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --ckpt_path ${RESULTS_DIR}/memory.ckpt \
                              --out_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --trainer.devices 1
```
#### 3. Inferenz auf Zielbildern


```bash
python run_lightening.py test --config $CONFIG  \
                              --ckpt_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --model.init_args.test_mode test \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.model_cfg.dataset_name $CLASS_SPLIT \
                              --model.init_args.dataset_cfgs.test.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
Wenn Sie die Inferenz-Ergebnisse online sehen m√∂chten (w√§hrend sie berechnet werden), kommentieren Sie die Zeilen 1746-1749 in `dev_hongyi/models/Sam2MatchingBaseline_noAMG.py` [hier](https://github.com/miquel-espinosa/no-time-to-train/blob/main/dev_hongyi/models/Sam2MatchingBaseline_noAMG.py#L1746) aus.
Passen Sie den Parameter f√ºr den Score-Schwellenwert `score_thr` nach Bedarf an, um mehr oder weniger segmentierte Instanzen zu sehen.
Bilder werden nun in `results_analysis/few_shot_classes/` gespeichert. Das Bild links zeigt die Ground Truth, das Bild rechts zeigt die durch unsere trainingsfreie Methode gefundenen segmentierten Instanzen.

Beachten Sie, dass wir in diesem Beispiel den `few_shot_classes`-Split verwenden und daher nur segmentierte Instanzen der Klassen in diesem Split erwarten sollten (nicht alle Klassen in COCO).

#### Ergebnisse

Nach dem Durchlauf aller Bilder im Validierungsdatensatz sollten Sie Folgendes erhalten:


```
BBOX RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.368

SEGM RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.342
```
---


## üîç Citation

If you use this work, please cite us:

```bibtex
@article{espinosa2025notimetotrain,
  title={No time to train! Training-Free Reference-Based Instance Segmentation},
  author={Miguel Espinosa and Chenhongyi Yang and Linus Ericsson and Steven McDonagh and Elliot J. Crowley},
  journal={arXiv preprint arXiv:2507.02798},
  year={2025},
  primaryclass={cs.CV}
}
```

---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-22

---