<div align="right">
  <details>
    <summary >üåê Idioma</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-TW">ÁπÅÈ´î‰∏≠Êñá</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=hi">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=th">‡πÑ‡∏ó‡∏¢</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fr">Fran√ßais</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=es">Espa√±ol</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=it">Italiano</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ru">–†—É—Å—Å–∫–∏–π</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pt">Portugu√™s</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ar">ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fa">ŸÅÿßÿ±ÿ≥€å</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=tr">T√ºrk√ße</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=vi">Ti·∫øng Vi·ªát</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=id">Bahasa Indonesia</a>
      </div>
    </div>
  </details>
</div>

<div align="center">

# üöÄ Sem Tempo para Treinar!  
### Segmenta√ß√£o de Inst√¢ncias Baseada em Refer√™ncia, Sem Treinamento  
[![GitHub](https://img.shields.io/badge/%E2%80%8B-No%20Time%20To%20Train-black?logo=github)](https://github.com/miquel-espinosa/no-time-to-train)
[![Website](https://img.shields.io/badge/üåê-Project%20Page-grey)](https://miquel-espinosa.github.io/no-time-to-train/)
[![arXiv](https://img.shields.io/badge/arXiv-2507.02798-b31b1b)](https://arxiv.org/abs/2507.02798)

**Estado da Arte (Papers with Code)**

[**_1-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-1-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference)

[**_10-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-10-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference)

[**_30-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-30-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference)

</div>

---

> üîî **Atualiza√ß√£o (julho de 2025):** O c√≥digo foi atualizado com instru√ß√µes!

---

## üìã √çndice

- [üéØ Destaques](#-destaques)
- [üìú Resumo](#-resumo)
- [üß† Arquitetura](#-arquitetura)
- [üõ†Ô∏è Instru√ß√µes de instala√ß√£o](#Ô∏è-instru√ß√µes-de-instala√ß√£o)
  - [1. Clone o reposit√≥rio](#1-clone-o-reposit√≥rio)
  - [2. Crie o ambiente conda](#2-crie-o-ambiente-conda)
  - [3. Instale SAM2 e DinoV2](#3-instale-sam2-e-dinov2)
  - [4. Baixe os conjuntos de dados](#4-baixe-os-conjuntos-de-dados)
  - [5. Baixe os checkpoints do SAM2 e DinoV2](#5-baixe-os-checkpoints-do-sam2-e-dinov2)
- [üìä C√≥digo de infer√™ncia: Reproduzindo os resultados SOTA 30-shot em Few-shot COCO](#-c√≥digo-de-infer√™ncia)
  - [0. Crie o conjunto de refer√™ncia](#0-crie-o-conjunto-de-refer√™ncia)
  - [1. Preencha a mem√≥ria com refer√™ncias](#1-preencha-a-mem√≥ria-com-refer√™ncias)
  - [2. P√≥s-processe o banco de mem√≥ria](#2-p√≥s-processe-o-banco-de-mem√≥ria)
  - [3. Infer√™ncia em imagens alvo](#3-infer√™ncia-em-imagens-alvo)
  - [Resultados](#resultados)
- [üîç Cita√ß√£o](#-cita√ß√£o)


## üéØ Destaques
- üí° **Sem Treinamento**: Sem fine-tuning, sem engenharia de prompts‚Äîapenas uma imagem de refer√™ncia.  
- üñºÔ∏è **Baseado em Refer√™ncia**: Segmente novos objetos usando apenas alguns exemplos.  
- üî• **Desempenho SOTA**: Supera abordagens anteriores sem treinamento no COCO, PASCAL VOC e FSOD entre dom√≠nios.

**Links:**
- üßæ [**Artigo no arXiv**](https://arxiv.org/abs/2507.02798)  
- üåê [**Site do Projeto**](https://miquel-espinosa.github.io/no-time-to-train/)  
- üìà [**Papers with Code**](https://paperswithcode.com/paper/no-time-to-train-training-free-reference)

## üìú Resumo

> O desempenho dos modelos de segmenta√ß√£o de imagens tem sido historicamente limitado pelo alto custo de coleta de grandes volumes de dados anotados. O Segment Anything Model (SAM) alivia esse problema original por meio de um paradigma de segmenta√ß√£o acionado por prompts, sem√¢ntica-agn√≥stico, mas ainda requer prompts visuais manuais ou regras complexas de gera√ß√£o de prompts dependentes do dom√≠nio para processar uma nova imagem. Visando reduzir esse novo fardo, nosso trabalho investiga a tarefa de segmenta√ß√£o de objetos quando fornecido, alternativamente, apenas um pequeno conjunto de imagens de refer√™ncia. Nosso principal insight √© aproveitar fortes pr√©-conhecimentos sem√¢nticos, aprendidos por modelos fundamentais, para identificar regi√µes correspondentes entre uma imagem de refer√™ncia e uma imagem alvo. Descobrimos que correspond√™ncias permitem a gera√ß√£o autom√°tica de m√°scaras de segmenta√ß√£o em n√≠vel de inst√¢ncia para tarefas subsequentes e implementamos nossas ideias via um m√©todo multiest√°gio, sem treinamento, incorporando (1) constru√ß√£o de banco de mem√≥ria; (2) agrega√ß√£o de representa√ß√µes e (3) correspond√™ncia de recursos sem√¢nticos. Nossos experimentos mostram melhorias significativas em m√©tricas de segmenta√ß√£o, levando a desempenho de estado da arte no COCO FSOD (36,8% nAP), PASCAL VOC Few-Shot (71,2% nAP50) e superando abordagens sem treinamento existentes no benchmark FSOD entre dom√≠nios (22,4% nAP).

![cdfsod-results-final-comic-sans-min](https://github.com/user-attachments/assets/ab302c02-c080-4042-99fc-0e181ba8abb9)


## üß† Arquitetura

![training-free-architecture-comic-sans-min](https://github.com/user-attachments/assets/d84dd83a-505e-45a0-8ce3-98e1838017f9)


## üõ†Ô∏è Instru√ß√µes de instala√ß√£o

### 1. Clone o reposit√≥rio


```bash
git clone https://github.com/miquel-espinosa/no-time-to-train.git
cd no-time-to-train
```
### 2. Criar ambiente conda

Vamos criar um ambiente conda com os pacotes necess√°rios.

```bash
conda env create -f environment.yml
conda activate no-time-to-train
```
### 3. Instale SAM2 e DinoV2

Vamos instalar o SAM2 e o DinoV2 a partir do c√≥digo-fonte.

```bash
pip install -e .
cd dinov2
pip install -e .
cd ..
```
### 4. Baixar conjuntos de dados

Por favor, baixe o conjunto de dados COCO e coloque-o em `data/coco`

### 5. Baixar checkpoints do SAM2 e DinoV2

Vamos baixar os mesmos checkpoints do SAM2 utilizados no artigo.
(Observe, no entanto, que os checkpoints do SAM2.1 j√° est√£o dispon√≠veis e podem apresentar melhor desempenho.)


```bash
mkdir -p checkpoints/dinov2
cd checkpoints
wget https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt
cd dinov2
wget https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth
cd ../..
```
## üìä C√≥digo de infer√™ncia

‚ö†Ô∏è Aviso: Este √© um c√≥digo de pesquisa ‚Äî espere um pouco de caos!

### Reproduzindo os resultados SOTA de 30 exemplos no Few-shot COCO

Defina vari√°veis √∫teis e crie uma pasta para os resultados:



```bash
CONFIG=./dev_hongyi/new_exps/coco_fewshot_10shot_Sam2L.yaml
CLASS_SPLIT="few_shot_classes"
RESULTS_DIR=work_dirs/few_shot_results
SHOTS=30
SEED=33
GPUS=4

mkdir -p $RESULTS_DIR
FILENAME=few_shot_${SHOTS}shot_seed${SEED}.pkl
```
#### 0. Criar conjunto de refer√™ncia


```bash
python dev_hongyi/dataset/few_shot_sampling.py \
        --n-shot $SHOTS \
        --out-path ${RESULTS_DIR}/${FILENAME} \
        --seed $SEED \
        --dataset $CLASS_SPLIT
```
#### 1. Preencha a mem√≥ria com refer√™ncias


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode fill_memory \
                              --out_path ${RESULTS_DIR}/memory.ckpt \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.memory_pkl ${RESULTS_DIR}/${FILENAME} \
                              --model.init_args.dataset_cfgs.fill_memory.memory_length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
#### 2. P√≥s-processar banco de mem√≥ria


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode postprocess_memory \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --ckpt_path ${RESULTS_DIR}/memory.ckpt \
                              --out_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --trainer.devices 1
```
#### 3. Infer√™ncia em imagens-alvo


```bash
python run_lightening.py test --config $CONFIG  \
                              --ckpt_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --model.init_args.test_mode test \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.model_cfg.dataset_name $CLASS_SPLIT \
                              --model.init_args.dataset_cfgs.test.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
Se voc√™ quiser ver os resultados de infer√™ncia online (√† medida que s√£o computados), descomente as linhas 1746-1749 em `dev_hongyi/models/Sam2MatchingBaseline_noAMG.py` [aqui](https://github.com/miquel-espinosa/no-time-to-train/blob/main/dev_hongyi/models/Sam2MatchingBaseline_noAMG.py#L1746).
Ajuste o par√¢metro de limiar de pontua√ß√£o `score_thr` conforme necess√°rio para ver mais ou menos inst√¢ncias segmentadas.
As imagens agora ser√£o salvas em `results_analysis/few_shot_classes/`. A imagem √† esquerda mostra o ground truth, a imagem √† direita mostra as inst√¢ncias segmentadas encontradas pelo nosso m√©todo sem treinamento.

Observe que neste exemplo estamos usando o conjunto `few_shot_classes`, portanto, devemos esperar ver apenas inst√¢ncias segmentadas das classes deste conjunto (n√£o de todas as classes do COCO).

#### Resultados

Ap√≥s rodar todas as imagens no conjunto de valida√ß√£o, voc√™ dever√° obter:


```
BBOX RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.368

SEGM RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.342
```
---


## üîç Citation

If you use this work, please cite us:

```bibtex
@article{espinosa2025notimetotrain,
  title={No time to train! Training-Free Reference-Based Instance Segmentation},
  author={Miguel Espinosa and Chenhongyi Yang and Linus Ericsson and Steven McDonagh and Elliot J. Crowley},
  journal={arXiv preprint arXiv:2507.02798},
  year={2025},
  primaryclass={cs.CV}
}
```

---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-22

---