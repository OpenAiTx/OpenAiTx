<div align="right">
  <details>
    <summary >ğŸŒ JÄ™zyk</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-CN">ç®€ä½“ä¸­æ–‡</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-TW">ç¹é«”ä¸­æ–‡</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ja">æ—¥æœ¬èª</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ko">í•œêµ­ì–´</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=hi">à¤¹à¤¿à¤¨à¥à¤¦à¥€</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=th">à¹„à¸—à¸¢</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fr">FranÃ§ais</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=es">EspaÃ±ol</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=it">Italiano</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ru">Ğ ÑƒÑÑĞºĞ¸Ğ¹</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pt">PortuguÃªs</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ar">Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fa">ÙØ§Ø±Ø³ÛŒ</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=tr">TÃ¼rkÃ§e</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=vi">Tiáº¿ng Viá»‡t</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=id">Bahasa Indonesia</a>
      </div>
    </div>
  </details>
</div>

<div align="center">

# ğŸš€ No Time to Train!  
### Segmentacja instancji bez treningu na podstawie referencji  
[![GitHub](https://img.shields.io/badge/%E2%80%8B-No%20Time%20To%20Train-black?logo=github)](https://github.com/miquel-espinosa/no-time-to-train)
[![Website](https://img.shields.io/badge/ğŸŒ-Project%20Page-grey)](https://miquel-espinosa.github.io/no-time-to-train/)
[![arXiv](https://img.shields.io/badge/arXiv-2507.02798-b31b1b)](https://arxiv.org/abs/2507.02798)

**Stan techniki (Papers with Code)**

[**_1-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-1-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference)

[**_10-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-10-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference)

[**_30-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-30-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference)

</div>

---

> ğŸ”” **Aktualizacja (lipiec 2025):** Kod zostaÅ‚ zaktualizowany wraz z instrukcjami!

---

## ğŸ“‹ Spis treÅ›ci

- [ğŸ¯ NajwaÅ¼niejsze cechy](#-najwazniejsze-cechy)
- [ğŸ“œ Abstrakt](#-abstrakt)
- [ğŸ§  Architektura](#-architektura)
- [ğŸ› ï¸ Instrukcje instalacji](#ï¸-instrukcje-instalacji)
  - [1. Sklonuj repozytorium](#1-sklonuj-repozytorium)
  - [2. UtwÃ³rz Å›rodowisko conda](#2-utworz-srodowisko-conda)
  - [3. Zainstaluj SAM2 i DinoV2](#3-zainstaluj-sam2-i-dinov2)
  - [4. Pobierz zbiory danych](#4-pobierz-zbiory-danych)
  - [5. Pobierz checkpointy SAM2 i DinoV2](#5-pobierz-checkpointy-sam2-i-dinov2)
- [ğŸ“Š Kod inferencji: Odwzoruj wyniki SOTA 30-shot na Few-shot COCO](#-kod-inferencji)
  - [0. UtwÃ³rz zbiÃ³r referencyjny](#0-utworz-zbior-referencyjny)
  - [1. WypeÅ‚nij pamiÄ™Ä‡ referencjami](#1-wypelnij-pamiec-referencjami)
  - [2. Przetwarzanie koÅ„cowe banku pamiÄ™ci](#2-przetwarzanie-koncowe-banku-pamieci)
  - [3. Inferencja na obrazach docelowych](#3-inferencja-na-obrazach-docelowych)
  - [Wyniki](#wyniki)
- [ğŸ” Cytowanie](#-cytowanie)


## ğŸ¯ NajwaÅ¼niejsze cechy
- ğŸ’¡ **Bez treningu**: Bez fine-tuningu, bez inÅ¼ynierii promptÃ³wâ€”tylko obraz referencyjny.  
- ğŸ–¼ï¸ **Na podstawie referencji**: Segmentuj nowe obiekty uÅ¼ywajÄ…c tylko kilku przykÅ‚adÃ³w.  
- ğŸ”¥ **WydajnoÅ›Ä‡ SOTA**: PrzewyÅ¼sza wczeÅ›niejsze podejÅ›cia beztreningowe na COCO, PASCAL VOC i Cross-Domain FSOD.

**Linki:**
- ğŸ§¾ [**ArtykuÅ‚ arXiv**](https://arxiv.org/abs/2507.02798)  
- ğŸŒ [**Strona projektu**](https://miquel-espinosa.github.io/no-time-to-train/)  
- ğŸ“ˆ [**Papers with Code**](https://paperswithcode.com/paper/no-time-to-train-training-free-reference)

## ğŸ“œ Abstrakt

> WydajnoÅ›Ä‡ modeli segmentacji obrazÃ³w byÅ‚a historycznie ograniczona przez wysokie koszty zbierania duÅ¼ych, oznakowanych zbiorÃ³w danych. Segment Anything Model (SAM) Å‚agodzi ten pierwotny problem poprzez promptowalny, niepowiÄ…zany z semantykÄ… paradygmat segmentacji, lecz nadal wymaga rÄ™cznych promptÃ³w wizualnych lub zÅ‚oÅ¼onych, zaleÅ¼nych od domeny reguÅ‚ generowania promptÃ³w do przetwarzania nowego obrazu. W celu ograniczenia tego nowego obciÄ…Å¼enia, nasza praca bada zadanie segmentacji obiektÃ³w, gdy zamiast tego dostÄ™pny jest jedynie maÅ‚y zbiÃ³r obrazÃ³w referencyjnych. Naszym kluczowym spostrzeÅ¼eniem jest wykorzystanie silnych semantycznych priors, wyuczonych przez modele bazowe, do identyfikacji odpowiadajÄ…cych sobie regionÃ³w pomiÄ™dzy obrazem referencyjnym a docelowym. Odkrywamy, Å¼e te korespondencje umoÅ¼liwiajÄ… automatyczne generowanie masek segmentacyjnych na poziomie instancji do zadaÅ„ downstream i realizujemy nasze pomysÅ‚y poprzez wieloetapowÄ…, beztreningowÄ… metodÄ™ obejmujÄ…cÄ… (1) budowÄ™ banku pamiÄ™ci; (2) agregacjÄ™ reprezentacji oraz (3) semantycznie Å›wiadome dopasowanie cech. Nasze eksperymenty pokazujÄ… znaczÄ…ce ulepszenia na miarach segmentacji, prowadzÄ…c do wynikÃ³w na poziomie SOTA na COCO FSOD (36.8% nAP), PASCAL VOC Few-Shot (71.2% nAP50) i przewyÅ¼szajÄ…c istniejÄ…ce podejÅ›cia beztreningowe na benchmarku Cross-Domain FSOD (22.4% nAP).

![cdfsod-results-final-comic-sans-min](https://github.com/user-attachments/assets/ab302c02-c080-4042-99fc-0e181ba8abb9)


## ğŸ§  Architektura

![training-free-architecture-comic-sans-min](https://github.com/user-attachments/assets/d84dd83a-505e-45a0-8ce3-98e1838017f9)


## ğŸ› ï¸ Instrukcje instalacji

### 1. Sklonuj repozytorium


```bash
git clone https://github.com/miquel-espinosa/no-time-to-train.git
cd no-time-to-train
```
### 2. UtwÃ³rz Å›rodowisko conda

Utworzymy Å›rodowisko conda z wymaganymi pakietami.

```bash
conda env create -f environment.yml
conda activate no-time-to-train
```
### 3. Zainstaluj SAM2 i DinoV2

Zainstalujemy SAM2 i DinoV2 ze ÅºrÃ³deÅ‚.

```bash
pip install -e .
cd dinov2
pip install -e .
cd ..
```
### 4. Pobierz zestawy danych

ProszÄ™ pobraÄ‡ zestaw danych COCO i umieÅ›ciÄ‡ go w `data/coco`

### 5. Pobierz checkpointy SAM2 i DinoV2

Pobierzemy dokÅ‚adnie te same checkpointy SAM2, ktÃ³re zostaÅ‚y uÅ¼yte w artykule.
(ZauwaÅ¼ jednak, Å¼e checkpointy SAM2.1 sÄ… juÅ¼ dostÄ™pne i mogÄ… dziaÅ‚aÄ‡ lepiej.)


```bash
mkdir -p checkpoints/dinov2
cd checkpoints
wget https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt
cd dinov2
wget https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth
cd ../..
```
## ğŸ“Š Kod inferencji

âš ï¸ ZastrzeÅ¼enie: To jest kod badawczy â€” spodziewaj siÄ™ trochÄ™ chaosu!

### Reprodukowanie wynikÃ³w SOTA z 30 prÃ³bkami w Few-shot COCO

Zdefiniuj przydatne zmienne i utwÃ³rz folder na wyniki:



```bash
CONFIG=./dev_hongyi/new_exps/coco_fewshot_10shot_Sam2L.yaml
CLASS_SPLIT="few_shot_classes"
RESULTS_DIR=work_dirs/few_shot_results
SHOTS=30
SEED=33
GPUS=4

mkdir -p $RESULTS_DIR
FILENAME=few_shot_${SHOTS}shot_seed${SEED}.pkl
```
#### 0. UtwÃ³rz zestaw referencyjny


```bash
python dev_hongyi/dataset/few_shot_sampling.py \
        --n-shot $SHOTS \
        --out-path ${RESULTS_DIR}/${FILENAME} \
        --seed $SEED \
        --dataset $CLASS_SPLIT
```
#### 1. WypeÅ‚nij pamiÄ™Ä‡ odniesieniami


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode fill_memory \
                              --out_path ${RESULTS_DIR}/memory.ckpt \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.memory_pkl ${RESULTS_DIR}/${FILENAME} \
                              --model.init_args.dataset_cfgs.fill_memory.memory_length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
#### 2. Post-process pamiÄ™ci podrÄ™cznej


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode postprocess_memory \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --ckpt_path ${RESULTS_DIR}/memory.ckpt \
                              --out_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --trainer.devices 1
```
#### 3. Wnioskowanie na obrazach docelowych


```bash
python run_lightening.py test --config $CONFIG  \
                              --ckpt_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --model.init_args.test_mode test \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.model_cfg.dataset_name $CLASS_SPLIT \
                              --model.init_args.dataset_cfgs.test.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
JeÅ›li chcesz zobaczyÄ‡ wyniki wnioskowania online (w miarÄ™ ich obliczania), odkomentuj linie 1746-1749 w pliku `dev_hongyi/models/Sam2MatchingBaseline_noAMG.py` [tutaj](https://github.com/miquel-espinosa/no-time-to-train/blob/main/dev_hongyi/models/Sam2MatchingBaseline_noAMG.py#L1746).
Dostosuj parametr progu punktacji `score_thr` w razie potrzeby, aby zobaczyÄ‡ wiÄ™cej lub mniej wysegmentowanych obiektÃ³w.
Obrazy zostanÄ… teraz zapisane w `results_analysis/few_shot_classes/`. Obraz po lewej pokazuje prawdziwe oznaczenia (ground truth), obraz po prawej pokazuje wysegmentowane obiekty wykryte przez naszÄ… metodÄ™ niewymagajÄ…cÄ… treningu.

ZwrÃ³Ä‡ uwagÄ™, Å¼e w tym przykÅ‚adzie uÅ¼ywamy podziaÅ‚u `few_shot_classes`, zatem powinniÅ›my spodziewaÄ‡ siÄ™ wyÅ‚Ä…cznie wysegmentowanych obiektÃ³w naleÅ¼Ä…cych do tych klas (nie wszystkich klas z COCO).

#### Wyniki

Po przetworzeniu wszystkich obrazÃ³w z zestawu walidacyjnego powinieneÅ› uzyskaÄ‡:


```
BBOX RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.368

SEGM RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.342
```
---


## ğŸ” Citation

If you use this work, please cite us:

```bibtex
@article{espinosa2025notimetotrain,
  title={No time to train! Training-Free Reference-Based Instance Segmentation},
  author={Miguel Espinosa and Chenhongyi Yang and Linus Ericsson and Steven McDonagh and Elliot J. Crowley},
  journal={arXiv preprint arXiv:2507.02798},
  year={2025},
  primaryclass={cs.CV}
}
```

---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-22

---