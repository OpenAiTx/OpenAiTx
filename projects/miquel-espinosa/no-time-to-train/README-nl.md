<div align="right">
  <details>
    <summary >üåê Taal</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=en">Engels</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-TW">ÁπÅÈ´î‰∏≠Êñá</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=hi">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=th">‡πÑ‡∏ó‡∏¢</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fr">Frans</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=de">Duits</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=es">Spaans</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=it">Italiaans</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ru">Russisch</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pt">Portugees</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pl">Pools</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ar">ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fa">ŸÅÿßÿ±ÿ≥€å</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=tr">Turks</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=vi">Vietnamees</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=id">Bahasa Indonesia</a>
      </div>
    </div>
  </details>
</div>

<div align="center">

# üöÄ No Time to Train!  
### Training-vrije referentie-gebaseerde instance segmentatie  
[![GitHub](https://img.shields.io/badge/%E2%80%8B-No%20Time%20To%20Train-black?logo=github)](https://github.com/miquel-espinosa/no-time-to-train)
[![Website](https://img.shields.io/badge/üåê-Project%20Page-grey)](https://miquel-espinosa.github.io/no-time-to-train/)
[![arXiv](https://img.shields.io/badge/arXiv-2507.02798-b31b1b)](https://arxiv.org/abs/2507.02798)

**State-of-the-art (Papers with Code)**

[**_1-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-1-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference)

[**_10-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-10-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference)

[**_30-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-30-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference)

</div>

---

> üîî **Update (juli 2025):** Code is bijgewerkt met instructies!

---

## üìã Inhoudsopgave

- [üéØ Hoogtepunten](#-hoogtepunten)
- [üìú Samenvatting](#-samenvatting)
- [üß† Architectuur](#-architectuur)
- [üõ†Ô∏è Installatie-instructies](#Ô∏è-installatie-instructies)
  - [1. Clone de repository](#1-clone-de-repository)
  - [2. Maak conda-omgeving aan](#2-maak-conda-omgeving-aan)
  - [3. Installeer SAM2 en DinoV2](#3-installeer-sam2-en-dinov2)
  - [4. Download datasets](#4-download-datasets)
  - [5. Download SAM2 en DinoV2 checkpoints](#5-download-sam2-en-dinov2-checkpoints)
- [üìä Inference-code: Reproduceer 30-shot SOTA-resultaten op Few-shot COCO](#-inference-code)
  - [0. Maak referentieset aan](#0-maak-referentieset-aan)
  - [1. Vul geheugen met referenties](#1-vul-geheugen-met-referenties)
  - [2. Post-processing geheugenbank](#2-post-processing-geheugenbank)
  - [3. Inferentie op doelafbeeldingen](#3-inferentie-op-doelafbeeldingen)
  - [Resultaten](#resultaten)
- [üîç Referentie](#-referentie)


## üéØ Hoogtepunten
- üí° **Training-vrij**: Geen fine-tuning, geen prompt engineering‚Äîalleen een referentieafbeelding.  
- üñºÔ∏è **Referentie-gebaseerd**: Segmenteer nieuwe objecten met slechts enkele voorbeelden.  
- üî• **SOTA-prestaties**: Overtreft eerdere training-vrije benaderingen op COCO, PASCAL VOC en Cross-Domain FSOD.

**Links:**
- üßæ [**arXiv Paper**](https://arxiv.org/abs/2507.02798)  
- üåê [**Project Website**](https://miquel-espinosa.github.io/no-time-to-train/)  
- üìà [**Papers with Code**](https://paperswithcode.com/paper/no-time-to-train-training-free-reference)

## üìú Samenvatting

> De prestaties van beeldsegmentatiemodellen zijn historisch beperkt door de hoge kosten van het verzamelen van grootschalige geannoteerde data. Het Segment Anything Model (SAM) verlicht dit oorspronkelijke probleem door middel van een promptable, semantiek-agnostisch segmentatieparadigma, maar vereist nog steeds handmatige visuele prompts of complexe domeinafhankelijke prompt-generatieregels om een nieuwe afbeelding te verwerken. Om deze nieuwe last te verminderen, onderzoekt ons werk de taak van objectsegmentatie wanneer er slechts een kleine set referentieafbeeldingen beschikbaar is. Ons belangrijkste inzicht is het benutten van sterke semantische priors, geleerd door foundation models, om corresponderende regio's tussen een referentie- en een doelafbeelding te identificeren. We constateren dat correspondenties automatische generatie van instance-level segmentatiemaskers voor downstream taken mogelijk maken en realiseren onze idee√´n via een multi-stage, training-vrije methode die bestaat uit (1) geheugenbankconstructie; (2) representatie-aggregatie en (3) semantisch-bewuste feature matching. Onze experimenten tonen significante verbeteringen op segmentatiemaatstaven, wat leidt tot state-of-the-art prestaties op COCO FSOD (36.8% nAP), PASCAL VOC Few-Shot (71.2% nAP50) en het overtreffen van bestaande training-vrije benaderingen op de Cross-Domain FSOD benchmark (22.4% nAP).

![cdfsod-results-final-comic-sans-min](https://github.com/user-attachments/assets/ab302c02-c080-4042-99fc-0e181ba8abb9)


## üß† Architectuur

![training-free-architecture-comic-sans-min](https://github.com/user-attachments/assets/d84dd83a-505e-45a0-8ce3-98e1838017f9)


## üõ†Ô∏è Installatie-instructies

### 1. Clone de repository


```bash
git clone https://github.com/miquel-espinosa/no-time-to-train.git
cd no-time-to-train
```
### 2. Conda-omgeving aanmaken

We zullen een conda-omgeving aanmaken met de vereiste pakketten.

```bash
conda env create -f environment.yml
conda activate no-time-to-train
```
### 3. Installeer SAM2 en DinoV2

We zullen SAM2 en DinoV2 vanaf de bron installeren.

```bash
pip install -e .
cd dinov2
pip install -e .
cd ..
```
### 4. Download datasets

Download alstublieft de COCO dataset en plaats deze in `data/coco`

### 5. Download SAM2 en DinoV2 checkpoints

We zullen exact dezelfde SAM2 checkpoints downloaden als gebruikt in het paper.
(Let op: SAM2.1 checkpoints zijn al beschikbaar en kunnen beter presteren.)


```bash
mkdir -p checkpoints/dinov2
cd checkpoints
wget https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt
cd dinov2
wget https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth
cd ../..
```
## üìä Inferentiecode

‚ö†Ô∏è Disclaimer: Dit is onderzoeks-code ‚Äî verwacht wat chaos!

### Reproduceren van 30-shot SOTA resultaten in Few-shot COCO

Definieer nuttige variabelen en maak een map aan voor de resultaten:



```bash
CONFIG=./dev_hongyi/new_exps/coco_fewshot_10shot_Sam2L.yaml
CLASS_SPLIT="few_shot_classes"
RESULTS_DIR=work_dirs/few_shot_results
SHOTS=30
SEED=33
GPUS=4

mkdir -p $RESULTS_DIR
FILENAME=few_shot_${SHOTS}shot_seed${SEED}.pkl
```
#### 0. Referentieset aanmaken


```bash
python dev_hongyi/dataset/few_shot_sampling.py \
        --n-shot $SHOTS \
        --out-path ${RESULTS_DIR}/${FILENAME} \
        --seed $SEED \
        --dataset $CLASS_SPLIT
```
#### 1. Vul het geheugen met referenties


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode fill_memory \
                              --out_path ${RESULTS_DIR}/memory.ckpt \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.memory_pkl ${RESULTS_DIR}/${FILENAME} \
                              --model.init_args.dataset_cfgs.fill_memory.memory_length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
#### 2. Post-processing geheugenbank


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode postprocess_memory \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --ckpt_path ${RESULTS_DIR}/memory.ckpt \
                              --out_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --trainer.devices 1
```
#### 3. Inferentie op doelfoto's


```bash
python run_lightening.py test --config $CONFIG  \
                              --ckpt_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --model.init_args.test_mode test \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.model_cfg.dataset_name $CLASS_SPLIT \
                              --model.init_args.dataset_cfgs.test.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
Als je de inferentieresultaten online wilt zien (terwijl ze worden berekend), haal dan de commentaartekens weg bij regels 1746-1749 in `dev_hongyi/models/Sam2MatchingBaseline_noAMG.py` [hier](https://github.com/miquel-espinosa/no-time-to-train/blob/main/dev_hongyi/models/Sam2MatchingBaseline_noAMG.py#L1746).
Pas de score-drempelparameter `score_thr` aan indien nodig om meer of minder gesegmenteerde instanties te zien.
Afbeeldingen worden nu opgeslagen in `results_analysis/few_shot_classes/`. De afbeelding links toont de ground truth, de afbeelding rechts toont de gesegmenteerde instanties gevonden door onze trainingsvrije methode.

Let op dat we in dit voorbeeld de `few_shot_classes`-split gebruiken, dus we verwachten alleen gesegmenteerde instanties van de klassen in deze split te zien (niet alle klassen in COCO).

#### Resultaten

Na het verwerken van alle afbeeldingen in de validatieset, zou je het volgende moeten verkrijgen:


```
BBOX RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.368

SEGM RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.342
```
---


## üîç Citation

If you use this work, please cite us:

```bibtex
@article{espinosa2025notimetotrain,
  title={No time to train! Training-Free Reference-Based Instance Segmentation},
  author={Miguel Espinosa and Chenhongyi Yang and Linus Ericsson and Steven McDonagh and Elliot J. Crowley},
  journal={arXiv preprint arXiv:2507.02798},
  year={2025},
  primaryclass={cs.CV}
}
```

---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-22

---