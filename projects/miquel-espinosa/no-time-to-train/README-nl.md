<div align="right">
  <details>
    <summary >🌐 Taal</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=en">Engels</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-CN">简体中文</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-TW">繁體中文</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ko">한국어</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=hi">हिन्दी</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=th">ไทย</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fr">Français</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=es">Español</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=it">Italiano</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ru">Русский</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pt">Português</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ar">العربية</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fa">فارسی</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=tr">Türkçe</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=vi">Tiếng Việt</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=id">Bahasa Indonesia</a>
      </div>
    </div>
  </details>
</div>

<div align="center">

# 🚀 No Time to Train!  
### Trainingsvrije Referentie-gebaseerde Instance Segmentatie  
[![GitHub](https://img.shields.io/badge/%E2%80%8B-No%20Time%20To%20Train-black?logo=github)](https://github.com/miquel-espinosa/no-time-to-train)
[![Website](https://img.shields.io/badge/🌐-Project%20Page-grey)](https://miquel-espinosa.github.io/no-time-to-train/)
[![arXiv](https://img.shields.io/badge/arXiv-2507.02798-b31b1b)](https://arxiv.org/abs/2507.02798)

**State-of-the-art (Papers with Code)**

[**_SOTA 1-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-1-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference)

[**_SOTA 10-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-10-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference)

[**_SOTA 30-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-30-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference)

</div>

---

> 🚨 **Update (22 juli 2025):** Instructies voor aangepaste datasets zijn toegevoegd!
> 
> 🔔 **Update (16 juli 2025):** Code is bijgewerkt met instructies!

---

## 📋 Inhoudsopgave

- [🎯 Highlights](#-highlights)
- [📜 Samenvatting](#-abstract)
- [🧠 Architectuur](#-architecture)
- [🛠️ Installatie-instructies](#️-installation-instructions)
  - [1. Clone de repository](#1-clone-the-repository)
  - [2. Maak conda-omgeving aan](#2-create-conda-environment)
  - [3. Installeer SAM2 en DinoV2](#3-install-sam2-and-dinov2)
  - [4. Download datasets](#4-download-datasets)
  - [5. Download SAM2 en DinoV2 checkpoints](#5-download-sam2-and-dinov2-checkpoints)
- [📊 Inferentiecode: Reproduceer 30-shot SOTA resultaten op Few-shot COCO](#-inference-code)
  - [0. Maak referentieset aan](#0-create-reference-set)
  - [1. Vul geheugen met referenties](#1-fill-memory-with-references)
  - [2. Post-process geheugenbank](#2-post-process-memory-bank)
  - [3. Inferentie op doelafbeeldingen](#3-inference-on-target-images)
  - [Resultaten](#results)
- [🔍 Aangepaste dataset](#-custom-dataset)
  - [0. Bereid een aangepaste dataset voor ⛵🐦](#0-prepare-a-custom-dataset)
  - [0.1 Indien alleen bbox-annotaties beschikbaar zijn](#01-if-only-bbox-annotations-are-available)
  - [0.2 Converteer coco-annotaties naar pickle-bestand](#02-convert-coco-annotations-to-pickle-file)
  - [1. Vul geheugen met referenties](#1-fill-memory-with-references)
  - [2. Post-process geheugenbank](#2-post-process-memory-bank)
- [📚 Referentie](#-citation)


## 🎯 Highlights
- 💡 **Trainingsvrij**: Geen fine-tuning, geen prompt engineering—alleen een referentieafbeelding.  
- 🖼️ **Referentie-gebaseerd**: Segmenteer nieuwe objecten met slechts een paar voorbeelden.  
- 🔥 **SOTA Prestaties**: Overtreft eerdere trainingsvrije methoden op COCO, PASCAL VOC en Cross-Domain FSOD.

**Links:**
- 🧾 [**arXiv Paper**](https://arxiv.org/abs/2507.02798)  
- 🌐 [**Projectwebsite**](https://miquel-espinosa.github.io/no-time-to-train/)  
- 📈 [**Papers with Code**](https://paperswithcode.com/paper/no-time-to-train-training-free-reference)

## 📜 Samenvatting

> De prestaties van beeldsegmentatiemodellen zijn historisch beperkt door de hoge kosten van het verzamelen van grootschalige geannoteerde data. Het Segment Anything Model (SAM) verlicht dit oorspronkelijke probleem via een promptable, semantiek-agnostisch segmentatieparadigma, maar vereist nog steeds handmatige visuele prompts of complexe domeinspecifieke promptgeneratieregels om een nieuwe afbeelding te verwerken. Om deze nieuwe last te verminderen, onderzoekt ons werk de taak van objectsegmentatie wanneer er slechts een kleine set referentieafbeeldingen wordt aangeboden. Ons belangrijkste inzicht is om sterke semantische priors, zoals geleerd door foundation models, te benutten om overeenkomstige regio's tussen een referentie- en doelafbeelding te identificeren. We constateren dat correspondenties automatische generatie van instance-level segmentatiemaskers mogelijk maken voor downstream taken en werken onze ideeën uit via een meerfasige, trainingsvrije methode met (1) geheugenbankconstructie; (2) representatie-aggregatie en (3) semantisch-bewuste feature-matching. Onze experimenten tonen significante verbeteringen op segmentatiestatistieken, wat leidt tot state-of-the-art prestaties op COCO FSOD (36,8% nAP), PASCAL VOC Few-Shot (71,2% nAP50) en betere resultaten dan bestaande trainingsvrije methoden op de Cross-Domain FSOD benchmark (22,4% nAP).

![cdfsod-results-final-comic-sans-min](https://github.com/user-attachments/assets/ab302c02-c080-4042-99fc-0e181ba8abb9)




## 🧠 Architectuur

![training-free-architecture-comic-sans-min](https://github.com/user-attachments/assets/d84dd83a-505e-45a0-8ce3-98e1838017f9)


## 🛠️ Installatie-instructies

### 1. Clone de repository

```bash
git clone https://github.com/miquel-espinosa/no-time-to-train.git
cd no-time-to-train
```
### 2. Conda-omgeving aanmaken

We zullen een conda-omgeving aanmaken met de vereiste pakketten.

```bash
conda env create -f environment.yml
conda activate no-time-to-train
```
### 3. Installeer SAM2 en DinoV2

We zullen SAM2 en DinoV2 vanaf de bron installeren.

```bash
pip install -e .
cd dinov2
pip install -e .
cd ..
```
### 4. Download datasets

Download alstublieft de COCO dataset en plaats deze in `data/coco`

### 5. Download SAM2 en DinoV2 checkpoints

We zullen exact dezelfde SAM2 checkpoints downloaden als gebruikt in het paper.
(Let op: SAM2.1 checkpoints zijn al beschikbaar en kunnen beter presteren.)


```bash
mkdir -p checkpoints/dinov2
cd checkpoints
wget https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt
cd dinov2
wget https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth
cd ../..
```
## 📊 Inferentiecode

⚠️ Disclaimer: Dit is onderzoeks-code — verwacht wat chaos!

### Reproduceren van 30-shot SOTA resultaten in Few-shot COCO

Definieer nuttige variabelen en maak een map aan voor de resultaten:



```bash
CONFIG=./no_time_to_train/new_exps/coco_fewshot_10shot_Sam2L.yaml
CLASS_SPLIT="few_shot_classes"
RESULTS_DIR=work_dirs/few_shot_results
SHOTS=30
SEED=33
GPUS=4

mkdir -p $RESULTS_DIR
FILENAME=few_shot_${SHOTS}shot_seed${SEED}.pkl
```
#### 0. Referentieset aanmaken


```bash
python no_time_to_train/dataset/few_shot_sampling.py \
        --n-shot $SHOTS \
        --out-path ${RESULTS_DIR}/${FILENAME} \
        --seed $SEED \
        --dataset $CLASS_SPLIT
```
#### 1. Vul het geheugen met referenties


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode fill_memory \
                              --out_path ${RESULTS_DIR}/memory.ckpt \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.memory_pkl ${RESULTS_DIR}/${FILENAME} \
                              --model.init_args.dataset_cfgs.fill_memory.memory_length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
#### 2. Post-processing geheugenbank


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode postprocess_memory \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --ckpt_path ${RESULTS_DIR}/memory.ckpt \
                              --out_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --trainer.devices 1
```
#### 3. Inferentie op doelfoto's


```bash
python run_lightening.py test --config $CONFIG  \
                              --ckpt_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --model.init_args.test_mode test \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.model_cfg.dataset_name $CLASS_SPLIT \
                              --model.init_args.dataset_cfgs.test.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
Als je de inferentieresultaten online wilt zien (terwijl ze worden berekend), haal dan het commentaar weg bij regels 1746-1749 in `no_time_to_train/models/Sam2MatchingBaseline_noAMG.py` [hier](https://github.com/miquel-espinosa/no-time-to-train/blob/main/no_time_to_train/models/Sam2MatchingBaseline_noAMG.py#L1746).
Pas de score drempelparameter `score_thr` aan indien nodig om meer of minder gesegmenteerde instanties te zien.
Afbeeldingen worden nu opgeslagen in `results_analysis/few_shot_classes/`. De afbeelding links toont de ground truth, de afbeelding rechts toont de gesegmenteerde instanties die door onze training-vrije methode zijn gevonden.

Let op dat we in dit voorbeeld de `few_shot_classes` split gebruiken, daarom mogen we alleen gesegmenteerde instanties verwachten van de klassen in deze split (niet alle klassen in COCO).

#### Resultaten

Na het verwerken van alle afbeeldingen in de validatieset, zou je het volgende moeten verkrijgen:


```
BBOX RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.368

SEGM RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.342
```
---

## 🔍 Aangepaste dataset

We geven de instructies voor het uitvoeren van onze pipeline op een aangepaste dataset. Het annotatieformaat is altijd in COCO-formaat.

> **TLDR;** Om direct te zien hoe je de volledige pipeline draait op *aangepaste datasets*, kijk naar `scripts/matching_cdfsod_pipeline.sh` samen met voorbeeldscripts van CD-FSOD-datasets (bijv. `scripts/dior_fish.sh`)

### 0. Bereid een aangepaste dataset voor ⛵🐦

Stel dat we **boten**⛵ en **vogels**🐦 willen detecteren in een aangepaste dataset. Om onze methode te gebruiken hebben we nodig:
- Minimaal 1 *geannoteerde* referentieafbeelding per klasse (d.w.z. 1 referentieafbeelding voor boot en 1 referentieafbeelding voor vogel)
- Meerdere doelafbeeldingen om instanties van onze gewenste klassen te vinden.

We hebben een voorbeeldscript voorbereid om een aangepaste dataset te maken met coco-afbeeldingen, voor een **1-shot** setting.
```bash
python scripts/make_custom_dataset.py
```
Dit zal een aangepaste dataset aanmaken met de volgende mappenstructuur:
```
data/my_custom_dataset/
    ├── annotations/
    │   ├── custom_references.json
    │   ├── custom_targets.json
    │   └── references_visualisations/
    │       ├── bird_1.jpg
    │       └── boat_1.jpg
    └── images/
        ├── 429819.jpg
        ├── 101435.jpg
        └── (all target and reference images)
```
**Referentieafbeeldingen visualisatie (1-shot):**

| 1-shot Referentieafbeelding voor VOGEL 🐦 | 1-shot Referentieafbeelding voor BOOT ⛵ |
|:-----------------------------------------:|:----------------------------------------:|
| <img src="https://github.com/user-attachments/assets/e59e580d-a7db-42ac-b386-892af211fc85" alt="bird_1" width="500"/> | <img src="https://github.com/user-attachments/assets/f94ee025-ae37-4a45-9c3e-0cfe8f8cd2bc" alt="boat_1" width="500"/> |


### 0.1 Als alleen bbox-annotaties beschikbaar zijn

We bieden ook een script om instance-level segmentatiemaskers te genereren met SAM2. Dit is handig als je alleen bounding box-annotaties hebt voor de referentieafbeeldingen.


```bash
# Download sam_h checkpoint. Feel free to use more recent checkpoints (note: code might need to be adapted)
wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth -O checkpoints/sam_vit_h_4b8939.pth
# Run automatic instance segmentation from ground truth bounding boxes.
python no_time_to_train/dataset/sam_bbox_to_segm_batch.py \
    --input_json data/my_custom_dataset/annotations/custom_references.json \
    --image_dir data/my_custom_dataset/images \
    --sam_checkpoint checkpoints/sam_vit_h_4b8939.pth \
    --model_type vit_h \
    --device cuda \
    --batch_size 8 \
    --visualize
```
**Referentieafbeeldingen met segmentatiemaskers op instantie-niveau (gegenereerd door SAM2 uit gt bounding boxes, 1-shot):**

Visualisaties van de gegenereerde segmentatiemaskers zijn opgeslagen in `data/my_custom_dataset/annotations/custom_references_with_SAM_segm/references_visualisations/`.


| 1-shot Referentieafbeelding voor VOGEL 🐦 (automatisch gesegmenteerd met SAM) | 1-shot Referentieafbeelding voor BOOT ⛵ (automatisch gesegmenteerd met SAM) |
|:---------------------------------:|:----------------------------------:|
| <img src="https://github.com/user-attachments/assets/65d38dc4-1454-43cd-9600-e8efc67b3a82" alt="bird_1_with_SAM_segm" width="500"/> | <img src="https://github.com/user-attachments/assets/43a558ad-50ca-4715-8285-9aa3268843c6" alt="boat_1_with_SAM_segm" width="500"/> |


### 0.2 Zet coco-annotaties om naar pickle-bestand


```bash
python no_time_to_train/dataset/coco_to_pkl.py \
    data/my_custom_dataset/annotations/custom_references_with_segm.json \
    data/my_custom_dataset/annotations/custom_references_with_segm.pkl \
    1
```
### 1. Vul het geheugen met referenties

Definieer eerst bruikbare variabelen en maak een map aan voor de resultaten. Voor een correcte visualisatie van labels moeten de class-namen geordend zijn op categorie-id, zoals deze voorkomt in het json-bestand. Bijvoorbeeld: `bird` heeft categorie-id `16`, `boat` heeft categorie-id `9`. Dus, `CAT_NAMES=boat,bird`.


```bash
DATASET_NAME=my_custom_dataset
DATASET_PATH=data/my_custom_dataset
CAT_NAMES=boat,bird
CATEGORY_NUM=2
SHOT=1
YAML_PATH=no_time_to_train/pl_configs/matching_cdfsod_template.yaml
PATH_TO_SAVE_CKPTS=./tmp_ckpts/my_custom_dataset
mkdir -p $PATH_TO_SAVE_CKPTS
```
Voer stap 1 uit:

```bash
python run_lightening.py test --config $YAML_PATH \
    --model.test_mode fill_memory \
    --out_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory.pth \
    --model.init_args.dataset_cfgs.fill_memory.root $DATASET_PATH/images \
    --model.init_args.dataset_cfgs.fill_memory.json_file $DATASET_PATH/annotations/custom_references_with_segm.json \
    --model.init_args.dataset_cfgs.fill_memory.memory_pkl $DATASET_PATH/annotations/custom_references_with_segm.pkl \
    --model.init_args.dataset_cfgs.fill_memory.memory_length $SHOT \
    --model.init_args.dataset_cfgs.fill_memory.cat_names $CAT_NAMES \
    --model.init_args.model_cfg.dataset_name $DATASET_NAME \
    --model.init_args.model_cfg.memory_bank_cfg.length $SHOT \
    --model.init_args.model_cfg.memory_bank_cfg.category_num $CATEGORY_NUM \
    --trainer.devices 1
```
### 2. Geheugenbank na verwerking


```bash
python run_lightening.py test --config $YAML_PATH \
    --model.test_mode postprocess_memory \
    --ckpt_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory.pth \
    --out_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory_postprocessed.pth \
    --model.init_args.model_cfg.dataset_name $DATASET_NAME \
    --model.init_args.model_cfg.memory_bank_cfg.length $SHOT \
    --model.init_args.model_cfg.memory_bank_cfg.category_num $CATEGORY_NUM \
    --trainer.devices 1
```
### 3. Inferentie op doelafbeeldingen

Als `ONLINE_VIS` is ingesteld op True, worden de voorspelde resultaten opgeslagen in `results_analysis/my_custom_dataset/` en getoond zodra ze zijn berekend. LET OP dat het uitvoeren met online visualisatie veel trager is.

Voel je vrij om de score drempel `VIS_THR` aan te passen om meer of minder gesegmenteerde instanties te zien.

```bash
ONLINE_VIS=True
VIS_THR=0.4
python run_lightening.py test --config $YAML_PATH \
    --model.test_mode test \
    --ckpt_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory_postprocessed.pth \
    --model.init_args.model_cfg.dataset_name $DATASET_NAME \
    --model.init_args.model_cfg.memory_bank_cfg.length $SHOT \
    --model.init_args.model_cfg.memory_bank_cfg.category_num $CATEGORY_NUM \
    --model.init_args.model_cfg.test.imgs_path $DATASET_PATH/images \
    --model.init_args.model_cfg.test.online_vis $ONLINE_VIS \
    --model.init_args.model_cfg.test.vis_thr $VIS_THR \
    --model.init_args.dataset_cfgs.test.root $DATASET_PATH/images \
    --model.init_args.dataset_cfgs.test.json_file $DATASET_PATH/annotations/custom_targets.json \
    --model.init_args.dataset_cfgs.test.cat_names $CAT_NAMES \
    --trainer.devices 1
```
### Resultaten

Prestatie-indicatoren (met exact dezelfde parameters als de bovenstaande commando's) zouden moeten zijn:


```
BBOX RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.478

SEGM RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.458
```
Visuele resultaten worden opgeslagen in `results_analysis/my_custom_dataset/`. Let op: onze methode werkt voor false negatives, dat wil zeggen, afbeeldingen die geen enkele instantie van de gewenste klassen bevatten.

*Klik op de afbeeldingen om te vergroten ⬇️*

| Doelafbeelding met boten ⛵ (links GT, rechts voorspellingen) | Doelafbeelding met vogels 🐦 (links GT, rechts voorspellingen) |
|:----------------------:|:----------------------:|
| ![000000459673](https://github.com/user-attachments/assets/678dc15a-dd3b-49d5-9287-6290da16aa6b) | ![000000407180](https://github.com/user-attachments/assets/fe306e48-af49-4d83-ac82-76fac6c456d1) |

| Doelafbeelding met boten en vogels ⛵🐦 (links GT, rechts voorspellingen) | Doelafbeelding zonder boten of vogels 🚫 (links GT, rechts voorspellingen) |
|:---------------------------------:|:----------------------------------:|
| ![000000517410](https://github.com/user-attachments/assets/9849b227-7f43-43d7-81ea-58010a623ad5) | ![000000460598](https://github.com/user-attachments/assets/7587700c-e09d-4cf6-8590-3df129c2568e) |


## 📚 Referentie

Als u dit werk gebruikt, citeer ons dan alsjeblieft:


```bibtex
@article{espinosa2025notimetotrain,
  title={No time to train! Training-Free Reference-Based Instance Segmentation},
  author={Miguel Espinosa and Chenhongyi Yang and Linus Ericsson and Steven McDonagh and Elliot J. Crowley},
  journal={arXiv preprint arXiv:2507.02798},
  year={2025},
  primaryclass={cs.CV}
}
```


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-23

---