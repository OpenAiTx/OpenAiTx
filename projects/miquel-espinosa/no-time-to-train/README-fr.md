<div align="right">
  <details>
    <summary >üåê Langue</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-TW">ÁπÅÈ´î‰∏≠Êñá</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=hi">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=th">‡πÑ‡∏ó‡∏¢</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fr">Fran√ßais</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=es">Espa√±ol</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=it">Italiano</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ru">–†—É—Å—Å–∫–∏–π</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pt">Portugu√™s</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ar">ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fa">ŸÅÿßÿ±ÿ≥€å</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=tr">T√ºrk√ße</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=vi">Ti·∫øng Vi·ªát</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=id">Bahasa Indonesia</a>
      </div>
    </div>
  </details>
</div>

<div align="center">

# üöÄ Pas le temps de s'entra√Æner !  
### Segmentation d'instances bas√©e sur des r√©f√©rences, sans entra√Ænement  
[![GitHub](https://img.shields.io/badge/%E2%80%8B-No%20Time%20To%20Train-black?logo=github)](https://github.com/miquel-espinosa/no-time-to-train)
[![Website](https://img.shields.io/badge/üåê-Project%20Page-grey)](https://miquel-espinosa.github.io/no-time-to-train/)
[![arXiv](https://img.shields.io/badge/arXiv-2507.02798-b31b1b)](https://arxiv.org/abs/2507.02798)

**√âtat de l'art (Papers with Code)**

[**_1-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-1-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference)

[**_10-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-10-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference)

[**_30-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-30-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference)

</div>

---

> üîî **Mise √† jour (juillet 2025) :** Le code a √©t√© mis √† jour avec des instructions !

---

## üìã Table des mati√®res

- [üéØ Points forts](#-points-forts)
- [üìú R√©sum√©](#-r√©sum√©)
- [üß† Architecture](#-architecture)
- [üõ†Ô∏è Instructions d'installation](#Ô∏è-instructions-dinstallation)
  - [1. Cloner le d√©p√¥t](#1-cloner-le-d√©p√¥t)
  - [2. Cr√©er un environnement conda](#2-cr√©er-un-environnement-conda)
  - [3. Installer SAM2 et DinoV2](#3-installer-sam2-et-dinov2)
  - [4. T√©l√©charger les jeux de donn√©es](#4-t√©l√©charger-les-jeux-de-donn√©es)
  - [5. T√©l√©charger les checkpoints SAM2 et DinoV2](#5-t√©l√©charger-les-checkpoints-sam2-et-dinov2)
- [üìä Code d'inf√©rence : Reproduire les r√©sultats SOTA 30-shot sur Few-shot COCO](#-code-dinf√©rence)
  - [0. Cr√©er un ensemble de r√©f√©rences](#0-cr√©er-un-ensemble-de-r√©f√©rences)
  - [1. Remplir la m√©moire avec les r√©f√©rences](#1-remplir-la-m√©moire-avec-les-r√©f√©rences)
  - [2. Post-traiter la banque de m√©moire](#2-post-traiter-la-banque-de-m√©moire)
  - [3. Inf√©rence sur les images cibles](#3-inf√©rence-sur-les-images-cibles)
  - [R√©sultats](#r√©sultats)
- [üîç Citation](#-citation)


## üéØ Points forts
- üí° **Sans entra√Ænement** : Pas de fine-tuning, pas d‚Äôing√©nierie de prompt‚Äîjuste une image de r√©f√©rence.  
- üñºÔ∏è **Bas√© sur la r√©f√©rence** : Segmenter de nouveaux objets avec seulement quelques exemples.  
- üî• **Performance SOTA** : Surpasse les approches sans entra√Ænement pr√©c√©dentes sur COCO, PASCAL VOC, et Cross-Domain FSOD.

**Liens :**
- üßæ [**Article arXiv**](https://arxiv.org/abs/2507.02798)  
- üåê [**Site du projet**](https://miquel-espinosa.github.io/no-time-to-train/)  
- üìà [**Papers with Code**](https://paperswithcode.com/paper/no-time-to-train-training-free-reference)

## üìú R√©sum√©

> Les performances des mod√®les de segmentation d‚Äôimages ont historiquement √©t√© limit√©es par le co√ªt √©lev√© de la collecte de donn√©es annot√©es √† grande √©chelle. Le Segment Anything Model (SAM) att√©nue ce probl√®me initial gr√¢ce √† un paradigme de segmentation activable par prompt, ind√©pendant de la s√©mantique, mais n√©cessite encore des prompts visuels manuels ou des r√®gles complexes de g√©n√©ration de prompts d√©pendantes du domaine pour traiter une nouvelle image. Dans le but de r√©duire ce nouveau fardeau, notre travail √©tudie la t√¢che de segmentation d‚Äôobjets lorsque, √† la place, un petit ensemble d‚Äôimages de r√©f√©rence est fourni. Notre intuition cl√© est de tirer parti de forts a priori s√©mantiques, appris par les mod√®les fondamentaux, pour identifier les r√©gions correspondantes entre une image de r√©f√©rence et une image cible. Nous constatons que les correspondances permettent la g√©n√©ration automatique de masques de segmentation au niveau des instances pour des t√¢ches avales et mettons en ≈ìuvre nos id√©es via une m√©thode multi-√©tapes, sans entra√Ænement, incorporant (1) la construction d‚Äôune banque de m√©moire ; (2) l‚Äôagr√©gation des repr√©sentations et (3) l‚Äôappariement de caract√©ristiques tenant compte de la s√©mantique. Nos exp√©riences montrent des am√©liorations significatives sur les m√©triques de segmentation, menant √† des performances √©tat de l‚Äôart sur COCO FSOD (36.8% nAP), PASCAL VOC Few-Shot (71.2% nAP50) et surpassant les approches existantes sans entra√Ænement sur le benchmark Cross-Domain FSOD (22.4% nAP).

![cdfsod-results-final-comic-sans-min](https://github.com/user-attachments/assets/ab302c02-c080-4042-99fc-0e181ba8abb9)


## üß† Architecture

![training-free-architecture-comic-sans-min](https://github.com/user-attachments/assets/d84dd83a-505e-45a0-8ce3-98e1838017f9)


## üõ†Ô∏è Instructions d'installation

### 1. Cloner le d√©p√¥t


```bash
git clone https://github.com/miquel-espinosa/no-time-to-train.git
cd no-time-to-train
```
### 2. Cr√©er un environnement conda

Nous allons cr√©er un environnement conda avec les paquets requis.

```bash
conda env create -f environment.yml
conda activate no-time-to-train
```
### 3. Installer SAM2 et DinoV2

Nous allons installer SAM2 et DinoV2 √† partir du code source.

```bash
pip install -e .
cd dinov2
pip install -e .
cd ..
```
### 4. T√©l√©charger les jeux de donn√©es

Veuillez t√©l√©charger le jeu de donn√©es COCO et le placer dans `data/coco`

### 5. T√©l√©charger les points de contr√¥le SAM2 et DinoV2

Nous allons t√©l√©charger exactement les points de contr√¥le SAM2 utilis√©s dans l'article.
(Notez cependant que les points de contr√¥le SAM2.1 sont d√©j√† disponibles et pourraient offrir de meilleures performances.)


```bash
mkdir -p checkpoints/dinov2
cd checkpoints
wget https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt
cd dinov2
wget https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth
cd ../..
```
## üìä Code d'inf√©rence

‚ö†Ô∏è Avertissement : Ceci est un code de recherche ‚Äî attendez-vous √† un peu de chaos !

### Reproduire les r√©sultats SOTA √† 30 essais sur Few-shot COCO

D√©finissez des variables utiles et cr√©ez un dossier pour les r√©sultats :



```bash
CONFIG=./dev_hongyi/new_exps/coco_fewshot_10shot_Sam2L.yaml
CLASS_SPLIT="few_shot_classes"
RESULTS_DIR=work_dirs/few_shot_results
SHOTS=30
SEED=33
GPUS=4

mkdir -p $RESULTS_DIR
FILENAME=few_shot_${SHOTS}shot_seed${SEED}.pkl
```
#### 0. Cr√©er un ensemble de r√©f√©rence


```bash
python dev_hongyi/dataset/few_shot_sampling.py \
        --n-shot $SHOTS \
        --out-path ${RESULTS_DIR}/${FILENAME} \
        --seed $SEED \
        --dataset $CLASS_SPLIT
```
#### 1. Remplir la m√©moire avec des r√©f√©rences


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode fill_memory \
                              --out_path ${RESULTS_DIR}/memory.ckpt \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.memory_pkl ${RESULTS_DIR}/${FILENAME} \
                              --model.init_args.dataset_cfgs.fill_memory.memory_length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
#### 2. Post-traitement de la banque de m√©moire


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode postprocess_memory \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --ckpt_path ${RESULTS_DIR}/memory.ckpt \
                              --out_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --trainer.devices 1
```
#### 3. Inf√©rence sur les images cibles


```bash
python run_lightening.py test --config $CONFIG  \
                              --ckpt_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --model.init_args.test_mode test \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.model_cfg.dataset_name $CLASS_SPLIT \
                              --model.init_args.dataset_cfgs.test.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
Si vous souhaitez voir les r√©sultats d'inf√©rence en ligne (au fur et √† mesure qu'ils sont calcul√©s), d√©commentez les lignes 1746-1749 dans `dev_hongyi/models/Sam2MatchingBaseline_noAMG.py` [ici](https://github.com/miquel-espinosa/no-time-to-train/blob/main/dev_hongyi/models/Sam2MatchingBaseline_noAMG.py#L1746).
Ajustez le param√®tre du seuil de score `score_thr` selon vos besoins pour voir plus ou moins d'instances segment√©es.
Les images seront d√©sormais sauvegard√©es dans `results_analysis/few_shot_classes/`. L'image de gauche montre la v√©rit√© terrain, l'image de droite montre les instances segment√©es trouv√©es par notre m√©thode sans entra√Ænement.

Notez que dans cet exemple, nous utilisons la division `few_shot_classes`, ainsi, nous ne devrions nous attendre √† voir que des instances segment√©es des classes de cette division (et non toutes les classes de COCO).

#### R√©sultats

Apr√®s avoir trait√© toutes les images de l'ensemble de validation, vous devriez obtenir :


```
BBOX RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.368

SEGM RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.342
```
---


## üîç Citation

If you use this work, please cite us:

```bibtex
@article{espinosa2025notimetotrain,
  title={No time to train! Training-Free Reference-Based Instance Segmentation},
  author={Miguel Espinosa and Chenhongyi Yang and Linus Ericsson and Steven McDonagh and Elliot J. Crowley},
  journal={arXiv preprint arXiv:2507.02798},
  year={2025},
  primaryclass={cs.CV}
}
```

---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-22

---