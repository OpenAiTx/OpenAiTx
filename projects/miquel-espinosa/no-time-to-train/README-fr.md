
<div align="right">
  <details>
    <summary >üåê Langue</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-TW">ÁπÅÈ´î‰∏≠Êñá</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=hi">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=th">‡πÑ‡∏ó‡∏¢</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fr">Fran√ßais</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=es">Espa√±ol</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=it">Italiano</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ru">–†—É—Å—Å–∫–∏–π</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pt">Portugu√™s</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ar">ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fa">ŸÅÿßÿ±ÿ≥€å</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=tr">T√ºrk√ße</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=vi">Ti·∫øng Vi·ªát</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=id">Bahasa Indonesia</a>
      </div>
    </div>
  </details>
</div>

<div align="center">

# üöÄ Pas le temps d‚Äôentra√Æner !  
### Segmentation d‚Äôinstance bas√©e sur la r√©f√©rence sans entra√Ænement  
[![GitHub](https://img.shields.io/badge/%E2%80%8B-No%20Time%20To%20Train-black?logo=github)](https://github.com/miquel-espinosa/no-time-to-train)
[![Website](https://img.shields.io/badge/üåê-Project%20Page-grey)](https://miquel-espinosa.github.io/no-time-to-train/)
[![arXiv](https://img.shields.io/badge/arXiv-2507.02798-b31b1b)](https://arxiv.org/abs/2507.02798)

**√âtat de l‚Äôart (Papers with Code)**

[**_SOTA 1-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/badge/State%20of%20the%20Art-Few--Shot%20Object%20Detection%20on%20MS--COCO%20(1--shot)-21CBCE?style=flat&logo=paperswithcode)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference)

[**_SOTA 10-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/badge/State%20of%20the%20Art-Few--Shot%20Object%20Detection%20on%20MS--COCO%20(10--shot)-21CBCE?style=flat&logo=paperswithcode)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference)

[**_SOTA 30-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/badge/State%20of%20the%20Art-Few--Shot%20Object%20Detection%20on%20MS--COCO%20(30--shot)-21CBCE?style=flat&logo=paperswithcode)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference)

<!-- [**_SOTA 1-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-1-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference)

[**_SOTA 10-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-10-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference)

[**_SOTA 30-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-30-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference) -->

</div>

---

> üö® **Mise √† jour (22 juillet 2025) :** Instructions pour les jeux de donn√©es personnalis√©s ajout√©es !
> 
> üîî **Mise √† jour (16 juillet 2025) :** Le code a √©t√© mis √† jour avec les instructions !

---

## üìã Table des mati√®res

- [üéØ Points forts](#-highlights)
- [üìú R√©sum√©](#-abstract)
- [üß† Architecture](#-architecture)
- [üõ†Ô∏è Instructions d‚Äôinstallation](#Ô∏è-installation-instructions)
  - [1. Cloner le d√©p√¥t](#1-clone-the-repository)
  - [2. Cr√©er l‚Äôenvironnement conda](#2-create-conda-environment)
  - [3. Installer SAM2 et DinoV2](#3-install-sam2-and-dinov2)
  - [4. T√©l√©charger les jeux de donn√©es](#4-download-datasets)
  - [5. T√©l√©charger les checkpoints SAM2 et DinoV2](#5-download-sam2-and-dinov2-checkpoints)
- [üìä Code d‚Äôinf√©rence : Reproduire les r√©sultats SOTA 30-shot sur Few-shot COCO](#-inference-code)
  - [0. Cr√©er un ensemble de r√©f√©rences](#0-create-reference-set)
  - [1. Remplir la m√©moire avec des r√©f√©rences](#1-fill-memory-with-references)
  - [2. Post-traiter la banque de m√©moire](#2-post-process-memory-bank)
  - [3. Inf√©rence sur les images cibles](#3-inference-on-target-images)
  - [R√©sultats](#results)
- [üîç Jeu de donn√©es personnalis√©](#-custom-dataset)
  - [0. Pr√©parer un jeu de donn√©es personnalis√© ‚õµüê¶](#0-prepare-a-custom-dataset)
  - [0.1 Si seules des annotations bbox sont disponibles](#01-if-only-bbox-annotations-are-available)
  - [0.2 Convertir les annotations coco en fichier pickle](#02-convert-coco-annotations-to-pickle-file)
  - [1. Remplir la m√©moire avec des r√©f√©rences](#1-fill-memory-with-references)
  - [2. Post-traiter la banque de m√©moire](#2-post-process-memory-bank)
- [üìö Citation](#-citation)


## üéØ Points forts
- üí° **Sans entra√Ænement** : Aucun fine-tuning, aucune ing√©nierie de prompt ‚Äî juste une image de r√©f√©rence.  
- üñºÔ∏è **Bas√© sur la r√©f√©rence** : Segmentez de nouveaux objets avec seulement quelques exemples.  
- üî• **Performance SOTA** : Surpasse les approches sans entra√Ænement pr√©c√©dentes sur COCO, PASCAL VOC et Cross-Domain FSOD.

**Liens :**
- üßæ [**Article arXiv**](https://arxiv.org/abs/2507.02798)  
- üåê [**Site du projet**](https://miquel-espinosa.github.io/no-time-to-train/)  
- üìà [**Papers with Code**](https://paperswithcode.com/paper/no-time-to-train-training-free-reference)

## üìú R√©sum√©

> The performance of image segmentation models has historically been constrained by the high cost of collecting large-scale annotated data. The Segment Anything Model (SAM) alleviates this original problem through a promptable, semantics-agnostic, segmentation paradigm and yet still requires manual visual-prompts or complex domain-dependent prompt-generation rules to process a new image. Towards reducing this new burden, our work investigates the task of object segmentation when provided with, alternatively, only a small set of reference images. Our key insight is to leverage strong semantic priors, as learned by foundation models, to identify corresponding regions between a reference and a target image. We find that correspondences enable automatic generation of instance-level segmentation masks for downstream tasks and instantiate our ideas via a multi-stage, training-free method incorporating (1) memory bank construction; (2) representation aggregation and (3) semantic-aware feature matching. Our experiments show significant improvements on segmentation metrics, leading to state-of-the-art performance on COCO FSOD (36.8% nAP), PASCAL VOC Few-Shot (71.2% nAP50) and outperforming existing training-free approaches on the Cross-Domain FSOD benchmark (22.4% nAP).

![cdfsod-results-final-comic-sans-min](https://github.com/user-attachments/assets/ab302c02-c080-4042-99fc-0e181ba8abb9)


## üß† Architecture

![training-free-architecture-comic-sans-min](https://github.com/user-attachments/assets/d84dd83a-505e-45a0-8ce3-98e1838017f9)


## üõ†Ô∏è Installation instructions

### 1. Clone the repository

```bash
git clone https://github.com/miquel-espinosa/no-time-to-train.git
cd no-time-to-train
```
### 2. Cr√©er un environnement conda

Nous allons cr√©er un environnement conda avec les paquets requis.

```bash
conda env create -f environment.yml
conda activate no-time-to-train
```
### 3. Installer SAM2 et DinoV2

Nous allons installer SAM2 et DinoV2 √† partir du code source.

```bash
pip install -e .
cd dinov2
pip install -e .
cd ..
```
### 4. T√©l√©charger les jeux de donn√©es

Veuillez t√©l√©charger le jeu de donn√©es COCO et le placer dans `data/coco`

### 5. T√©l√©charger les points de contr√¥le SAM2 et DinoV2

Nous allons t√©l√©charger exactement les points de contr√¥le SAM2 utilis√©s dans l'article.
(Notez cependant que les points de contr√¥le SAM2.1 sont d√©j√† disponibles et pourraient offrir de meilleures performances.)


```bash
mkdir -p checkpoints/dinov2
cd checkpoints
wget https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt
cd dinov2
wget https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth
cd ../..
```
## üìä Code d'inf√©rence

‚ö†Ô∏è Avertissement : Ceci est un code de recherche ‚Äî attendez-vous √† un peu de chaos !

### Reproduire les r√©sultats SOTA √† 30 essais sur Few-shot COCO

D√©finissez des variables utiles et cr√©ez un dossier pour les r√©sultats :



```bash
CONFIG=./no_time_to_train/new_exps/coco_fewshot_10shot_Sam2L.yaml
CLASS_SPLIT="few_shot_classes"
RESULTS_DIR=work_dirs/few_shot_results
SHOTS=30
SEED=33
GPUS=4

mkdir -p $RESULTS_DIR
FILENAME=few_shot_${SHOTS}shot_seed${SEED}.pkl
```
#### 0. Cr√©er un ensemble de r√©f√©rence


```bash
python no_time_to_train/dataset/few_shot_sampling.py \
        --n-shot $SHOTS \
        --out-path ${RESULTS_DIR}/${FILENAME} \
        --seed $SEED \
        --dataset $CLASS_SPLIT
```
#### 1. Remplir la m√©moire avec des r√©f√©rences


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode fill_memory \
                              --out_path ${RESULTS_DIR}/memory.ckpt \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.memory_pkl ${RESULTS_DIR}/${FILENAME} \
                              --model.init_args.dataset_cfgs.fill_memory.memory_length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
#### 2. Post-traitement de la banque de m√©moire


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode postprocess_memory \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --ckpt_path ${RESULTS_DIR}/memory.ckpt \
                              --out_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --trainer.devices 1
```
#### 3. Inf√©rence sur les images cibles


```bash
python run_lightening.py test --config $CONFIG  \
                              --ckpt_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --model.init_args.test_mode test \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.model_cfg.dataset_name $CLASS_SPLIT \
                              --model.init_args.dataset_cfgs.test.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
Si vous souhaitez voir les r√©sultats d'inf√©rence en ligne (au fur et √† mesure qu'ils sont calcul√©s), d√©commentez les lignes 1746-1749 dans `no_time_to_train/models/Sam2MatchingBaseline_noAMG.py` [ici](https://github.com/miquel-espinosa/no-time-to-train/blob/main/no_time_to_train/models/Sam2MatchingBaseline_noAMG.py#L1746).
Ajustez le param√®tre du seuil de score `score_thr` selon vos besoins pour voir plus ou moins d'instances segment√©es.
Les images seront d√©sormais enregistr√©es dans `results_analysis/few_shot_classes/`. L'image de gauche montre la v√©rit√© terrain, celle de droite montre les instances segment√©es trouv√©es par notre m√©thode sans entra√Ænement.

Notez que dans cet exemple, nous utilisons la division `few_shot_classes`, nous ne devons donc nous attendre qu'√† voir des instances segment√©es des classes de cette division (et non pas toutes les classes de COCO).

#### R√©sultats

Apr√®s avoir trait√© toutes les images du jeu de validation, vous devriez obtenir :


```
BBOX RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.368

SEGM RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.342
```
---

## üîç Jeu de donn√©es personnalis√©

Nous fournissons les instructions pour ex√©cuter notre pipeline sur un jeu de donn√©es personnalis√©. Les formats d'annotation sont toujours au format COCO.

> **TLDR ;** Pour voir directement comment ex√©cuter le pipeline complet sur des *jeux de donn√©es personnalis√©s*, consultez `scripts/matching_cdfsod_pipeline.sh` ainsi que des exemples de scripts pour les jeux de donn√©es CD-FSOD (par exemple `scripts/dior_fish.sh`)

### 0. Pr√©parer un jeu de donn√©es personnalis√© ‚õµüê¶

Imaginons que nous voulons d√©tecter des **bateaux**‚õµ et des **oiseaux**üê¶ dans un jeu de donn√©es personnalis√©. Pour utiliser notre m√©thode, il nous faudra :
- Au moins 1 image de r√©f√©rence *annot√©e* pour chaque classe (c'est-√†-dire 1 image de r√©f√©rence pour bateau et 1 image de r√©f√©rence pour oiseau)
- Plusieurs images cibles pour trouver des instances des classes souhait√©es.

Nous avons pr√©par√© un script d'exemple pour cr√©er un jeu de donn√©es personnalis√© avec des images coco, pour un sc√©nario **1-shot**.
```bash
python scripts/make_custom_dataset.py
```
Cela cr√©era un jeu de donn√©es personnalis√© avec la structure de dossiers suivante :
```
data/my_custom_dataset/
    ‚îú‚îÄ‚îÄ annotations/
    ‚îÇ   ‚îú‚îÄ‚îÄ custom_references.json
    ‚îÇ   ‚îú‚îÄ‚îÄ custom_targets.json
    ‚îÇ   ‚îî‚îÄ‚îÄ references_visualisations/
    ‚îÇ       ‚îú‚îÄ‚îÄ bird_1.jpg
    ‚îÇ       ‚îî‚îÄ‚îÄ boat_1.jpg
    ‚îî‚îÄ‚îÄ images/
        ‚îú‚îÄ‚îÄ 429819.jpg
        ‚îú‚îÄ‚îÄ 101435.jpg
        ‚îî‚îÄ‚îÄ (all target and reference images)
```
**Visualisation des images de r√©f√©rence (1-shot) :**

| Image de r√©f√©rence 1-shot pour OISEAU üê¶ | Image de r√©f√©rence 1-shot pour BATEAU ‚õµ |
|:---------------------------------------:|:----------------------------------------:|
| <img src="https://github.com/user-attachments/assets/e59e580d-a7db-42ac-b386-892af211fc85" alt="bird_1" width="500"/> | <img src="https://github.com/user-attachments/assets/f94ee025-ae37-4a45-9c3e-0cfe8f8cd2bc" alt="boat_1" width="500"/> |


### 0.1 Si seules des annotations bbox sont disponibles

Nous fournissons √©galement un script pour g√©n√©rer des masques de segmentation au niveau instance en utilisant SAM2. Ceci est utile si vous ne disposez que d‚Äôannotations de bo√Ætes englobantes pour les images de r√©f√©rence.


```bash
# Download sam_h checkpoint. Feel free to use more recent checkpoints (note: code might need to be adapted)
wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth -O checkpoints/sam_vit_h_4b8939.pth
# Run automatic instance segmentation from ground truth bounding boxes.
python no_time_to_train/dataset/sam_bbox_to_segm_batch.py \
    --input_json data/my_custom_dataset/annotations/custom_references.json \
    --image_dir data/my_custom_dataset/images \
    --sam_checkpoint checkpoints/sam_vit_h_4b8939.pth \
    --model_type vit_h \
    --device cuda \
    --batch_size 8 \
    --visualize
```
**Images de r√©f√©rence avec masques de segmentation au niveau des instances (g√©n√©r√©s par SAM2 √† partir des bo√Ætes englobantes gt, 1-shot) :**

La visualisation des masques de segmentation g√©n√©r√©s est enregistr√©e dans `data/my_custom_dataset/annotations/custom_references_with_SAM_segm/references_visualisations/`.


| Image de r√©f√©rence 1-shot pour OISEAU üê¶ (segment√©e automatiquement avec SAM) | Image de r√©f√©rence 1-shot pour BATEAU ‚õµ (segment√©e automatiquement avec SAM) |
|:---------------------------------:|:----------------------------------:|
| <img src="https://github.com/user-attachments/assets/65d38dc4-1454-43cd-9600-e8efc67b3a82" alt="bird_1_with_SAM_segm" width="500"/> | <img src="https://github.com/user-attachments/assets/43a558ad-50ca-4715-8285-9aa3268843c6" alt="boat_1_with_SAM_segm" width="500"/> |


### 0.2 Convertir les annotations coco en fichier pickle


```bash
python no_time_to_train/dataset/coco_to_pkl.py \
    data/my_custom_dataset/annotations/custom_references_with_segm.json \
    data/my_custom_dataset/annotations/custom_references_with_segm.pkl \
    1
```
### 1. Remplir la m√©moire avec des r√©f√©rences

Tout d'abord, d√©finissez les variables utiles et cr√©ez un dossier pour les r√©sultats. Pour une visualisation correcte des √©tiquettes, les noms de classes doivent √™tre ordonn√©s par identifiant de cat√©gorie comme dans le fichier json. Par exemple, `bird` a l'identifiant de cat√©gorie `16`, `boat` a l'identifiant de cat√©gorie `9`. Ainsi, `CAT_NAMES=boat,bird`.


```bash
DATASET_NAME=my_custom_dataset
DATASET_PATH=data/my_custom_dataset
CAT_NAMES=boat,bird
CATEGORY_NUM=2
SHOT=1
YAML_PATH=no_time_to_train/pl_configs/matching_cdfsod_template.yaml
PATH_TO_SAVE_CKPTS=./tmp_ckpts/my_custom_dataset
mkdir -p $PATH_TO_SAVE_CKPTS
```
Ex√©cutez l‚Äô√©tape 1 :

```bash
python run_lightening.py test --config $YAML_PATH \
    --model.test_mode fill_memory \
    --out_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory.pth \
    --model.init_args.dataset_cfgs.fill_memory.root $DATASET_PATH/images \
    --model.init_args.dataset_cfgs.fill_memory.json_file $DATASET_PATH/annotations/custom_references_with_segm.json \
    --model.init_args.dataset_cfgs.fill_memory.memory_pkl $DATASET_PATH/annotations/custom_references_with_segm.pkl \
    --model.init_args.dataset_cfgs.fill_memory.memory_length $SHOT \
    --model.init_args.dataset_cfgs.fill_memory.cat_names $CAT_NAMES \
    --model.init_args.model_cfg.dataset_name $DATASET_NAME \
    --model.init_args.model_cfg.memory_bank_cfg.length $SHOT \
    --model.init_args.model_cfg.memory_bank_cfg.category_num $CATEGORY_NUM \
    --trainer.devices 1
```
### 2. Banque de m√©moire post-traitement


```bash
python run_lightening.py test --config $YAML_PATH \
    --model.test_mode postprocess_memory \
    --ckpt_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory.pth \
    --out_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory_postprocessed.pth \
    --model.init_args.model_cfg.dataset_name $DATASET_NAME \
    --model.init_args.model_cfg.memory_bank_cfg.length $SHOT \
    --model.init_args.model_cfg.memory_bank_cfg.category_num $CATEGORY_NUM \
    --trainer.devices 1
```
### 3. Inf√©rence sur les images cibles

Si `ONLINE_VIS` est r√©gl√© sur True, les r√©sultats de pr√©diction seront sauvegard√©s dans `results_analysis/my_custom_dataset/` et affich√©s au fur et √† mesure de leur calcul. NOTEZ que l'ex√©cution avec la visualisation en ligne est beaucoup plus lente.

N'h√©sitez pas √† modifier le seuil de score `VIS_THR` pour voir plus ou moins d'instances segment√©es.

```bash
ONLINE_VIS=True
VIS_THR=0.4
python run_lightening.py test --config $YAML_PATH \
    --model.test_mode test \
    --ckpt_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory_postprocessed.pth \
    --model.init_args.model_cfg.dataset_name $DATASET_NAME \
    --model.init_args.model_cfg.memory_bank_cfg.length $SHOT \
    --model.init_args.model_cfg.memory_bank_cfg.category_num $CATEGORY_NUM \
    --model.init_args.model_cfg.test.imgs_path $DATASET_PATH/images \
    --model.init_args.model_cfg.test.online_vis $ONLINE_VIS \
    --model.init_args.model_cfg.test.vis_thr $VIS_THR \
    --model.init_args.dataset_cfgs.test.root $DATASET_PATH/images \
    --model.init_args.dataset_cfgs.test.json_file $DATASET_PATH/annotations/custom_targets.json \
    --model.init_args.dataset_cfgs.test.cat_names $CAT_NAMES \
    --trainer.devices 1
```
### R√©sultats

Les m√©triques de performance (avec exactement les m√™mes param√®tres que les commandes ci-dessus) devraient √™tre :


```
BBOX RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.478

SEGM RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.458
```
Les r√©sultats visuels sont enregistr√©s dans `results_analysis/my_custom_dataset/`. Notez que notre m√©thode fonctionne pour les faux n√©gatifs, c'est-√†-dire les images qui ne contiennent aucune instance des classes souhait√©es.

*Cliquez sur les images pour agrandir ‚¨áÔ∏è*

| Image cible avec des bateaux ‚õµ (gauche GT, droite pr√©dictions) | Image cible avec des oiseaux üê¶ (gauche GT, droite pr√©dictions) |
|:----------------------:|:----------------------:|
| ![000000459673](https://github.com/user-attachments/assets/678dc15a-dd3b-49d5-9287-6290da16aa6b) | ![000000407180](https://github.com/user-attachments/assets/fe306e48-af49-4d83-ac82-76fac6c456d1) |

| Image cible avec des bateaux et des oiseaux ‚õµüê¶ (gauche GT, droite pr√©dictions) | Image cible sans bateaux ni oiseaux üö´ (gauche GT, droite pr√©dictions) |
|:---------------------------------:|:----------------------------------:|
| ![000000517410](https://github.com/user-attachments/assets/9849b227-7f43-43d7-81ea-58010a623ad5) | ![000000460598](https://github.com/user-attachments/assets/7587700c-e09d-4cf6-8590-3df129c2568e) |


## üìö Citation

Si vous utilisez ce travail, veuillez nous citer :


```bibtex
@article{espinosa2025notimetotrain,
  title={No time to train! Training-Free Reference-Based Instance Segmentation},
  author={Miguel Espinosa and Chenhongyi Yang and Linus Ericsson and Steven McDonagh and Elliot J. Crowley},
  journal={arXiv preprint arXiv:2507.02798},
  year={2025},
  primaryclass={cs.CV}
}
```


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-24

---