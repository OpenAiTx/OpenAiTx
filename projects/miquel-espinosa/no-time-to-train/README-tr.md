<div align="right">
  <details>
    <summary >ğŸŒ Dil</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-CN">ç®€ä½“ä¸­æ–‡</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-TW">ç¹é«”ä¸­æ–‡</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ja">æ—¥æœ¬èª</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ko">í•œêµ­ì–´</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=hi">à¤¹à¤¿à¤¨à¥à¤¦à¥€</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=th">à¹„à¸—à¸¢</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fr">FranÃ§ais</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=es">EspaÃ±ol</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=it">Italiano</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ru">Ğ ÑƒÑÑĞºĞ¸Ğ¹</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pt">PortuguÃªs</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ar">Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fa">ÙØ§Ø±Ø³ÛŒ</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=tr">TÃ¼rkÃ§e</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=vi">Tiáº¿ng Viá»‡t</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=id">Bahasa Indonesia</a>
      </div>
    </div>
  </details>
</div>

<div align="center">

# ğŸš€ EÄŸitime Zaman Yok!  
### EÄŸitimsiz Referans TabanlÄ± Nesne BÃ¶lÃ¼tleme  
[![GitHub](https://img.shields.io/badge/%E2%80%8B-No%20Time%20To%20Train-black?logo=github)](https://github.com/miquel-espinosa/no-time-to-train)
[![Website](https://img.shields.io/badge/ğŸŒ-Project%20Page-grey)](https://miquel-espinosa.github.io/no-time-to-train/)
[![arXiv](https://img.shields.io/badge/arXiv-2507.02798-b31b1b)](https://arxiv.org/abs/2507.02798)

**En Ä°yi SonuÃ§lar (Papers with Code)**

[**_1-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-1-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference)

[**_10-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-10-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference)

[**_30-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-30-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference)

</div>

---

> ğŸ”” **GÃ¼ncelleme (Temmuz 2025):** Kod, talimatlarla birlikte gÃ¼ncellendi!

---

## ğŸ“‹ Ä°Ã§indekiler

- [ğŸ¯ Ã–ne Ã‡Ä±kanlar](#-Ã¶ne-Ã§Ä±kanlar)
- [ğŸ“œ Ã–z](#-Ã¶z)
- [ğŸ§  Mimari](#-mimari)
- [ğŸ› ï¸ Kurulum talimatlarÄ±](#ï¸-kurulum-talimati)
  - [1. Depoyu klonla](#1-depoyu-klonla)
  - [2. Conda ortamÄ± oluÅŸtur](#2-conda-ortamÄ±-oluÅŸtur)
  - [3. SAM2 ve DinoV2'yi kur](#3-sam2-ve-dinov2yi-kur)
  - [4. Veri setlerini indir](#4-veri-setlerini-indir)
  - [5. SAM2 ve DinoV2 kontrol noktalarÄ±nÄ± indir](#5-sam2-ve-dinov2-kontrol-noktalarÄ±nÄ±-indir)
- [ğŸ“Š Ã‡Ä±karÄ±m kodu: Few-shot COCO'da 30-shot SOTA SonuÃ§larÄ±nÄ± TekrarlayÄ±n](#-Ã§Ä±karÄ±m-kodu)
  - [0. Referans seti oluÅŸtur](#0-referans-seti-oluÅŸtur)
  - [1. HafÄ±zayÄ± referanslarla doldur](#1-hafÄ±zayÄ±-referanslarla-doldur)
  - [2. HafÄ±za bankasÄ±nÄ± sonradan iÅŸle](#2-hafÄ±za-bankasÄ±nÄ±-sonradan-iÅŸle)
  - [3. Hedef gÃ¶rsellerde Ã§Ä±karÄ±m yap](#3-hedef-gÃ¶rsellerde-Ã§Ä±karÄ±m-yap)
  - [SonuÃ§lar](#sonuÃ§lar)
- [ğŸ” AtÄ±f](#-atÄ±f)


## ğŸ¯ Ã–ne Ã‡Ä±kanlar
- ğŸ’¡ **EÄŸitimsiz**: Ä°nce ayar yok, prompt mÃ¼hendisliÄŸi yokâ€”sadece bir referans gÃ¶rseli yeterli.  
- ğŸ–¼ï¸ **Referans TabanlÄ±**: YalnÄ±zca birkaÃ§ Ã¶rnekle yeni nesneleri bÃ¶lÃ¼tleyin.  
- ğŸ”¥ **SOTA PerformansÄ±**: COCO, PASCAL VOC ve Cross-Domain FSOD Ã¼zerinde Ã¶nceki eÄŸitimsiz yaklaÅŸÄ±mlardan daha iyi sonuÃ§lar.

**BaÄŸlantÄ±lar:**
- ğŸ§¾ [**arXiv Makalesi**](https://arxiv.org/abs/2507.02798)  
- ğŸŒ [**Proje Web Sitesi**](https://miquel-espinosa.github.io/no-time-to-train/)  
- ğŸ“ˆ [**Papers with Code**](https://paperswithcode.com/paper/no-time-to-train-training-free-reference)

## ğŸ“œ Ã–z

> GÃ¶rÃ¼ntÃ¼ bÃ¶lÃ¼tleme modellerinin performansÄ±, tarihsel olarak, bÃ¼yÃ¼k Ã¶lÃ§ekli etiketli veri toplamanÄ±n yÃ¼ksek maliyetiyle sÄ±nÄ±rlanmÄ±ÅŸtÄ±r. Segment Anything Model (SAM), bu temel problemi, prompt ile Ã§alÄ±ÅŸabilen, anlamsal olarak baÄŸÄ±msÄ±z bir bÃ¶lÃ¼tleme yaklaÅŸÄ±mÄ±yla hafifletmiÅŸtir; ancak yeni bir gÃ¶rseli iÅŸlemek iÃ§in hala manuel gÃ¶rsel promptlara veya karmaÅŸÄ±k, alana Ã¶zgÃ¼ prompt oluÅŸturma kurallarÄ±na ihtiyaÃ§ duyar. Bu yeni yÃ¼kÃ¼ azaltmak iÃ§in Ã§alÄ±ÅŸmamÄ±z, yalnÄ±zca kÃ¼Ã§Ã¼k bir referans gÃ¶rsel seti saÄŸlandÄ±ÄŸÄ±nda nesne bÃ¶lÃ¼tleme gÃ¶revini araÅŸtÄ±rÄ±yor. Temel Ã§Ä±karÄ±mÄ±mÄ±z, temel modellerin Ã¶ÄŸrendiÄŸi gÃ¼Ã§lÃ¼ anlamsal Ã¶ncÃ¼lleri kullanarak, bir referans ve hedef gÃ¶rsel arasÄ±ndaki karÅŸÄ±lÄ±k gelen bÃ¶lgeleri tespit etmektir. KarÅŸÄ±lÄ±klarÄ±n, ileri gÃ¶revler iÃ§in otomatik olarak Ã¶rnek dÃ¼zeyinde bÃ¶lÃ¼tleme maskeleri Ã¼retmeyi saÄŸladÄ±ÄŸÄ±nÄ± bulduk ve fikrimizi Ã§ok aÅŸamalÄ±, eÄŸitimsiz bir yÃ¶ntemle (1) hafÄ±za bankasÄ± oluÅŸturma; (2) temsil toplama ve (3) anlamsal farkÄ±ndalÄ±klÄ± Ã¶zellik eÅŸleÅŸtirme adÄ±mlarÄ±yla hayata geÃ§irdik. Deneylerimiz, bÃ¶lÃ¼tleme metriklerinde Ã¶nemli iyileÅŸmeler gÃ¶stererek COCO FSOD'da (36.8% nAP), PASCAL VOC Few-Shot'ta (71.2% nAP50) ve Cross-Domain FSOD kÄ±yaslamasÄ±nda mevcut eÄŸitimsiz yaklaÅŸÄ±mlardan daha iyi sonuÃ§lara (22.4% nAP) ulaÅŸmaktadÄ±r.

![cdfsod-results-final-comic-sans-min](https://github.com/user-attachments/assets/ab302c02-c080-4042-99fc-0e181ba8abb9)


## ğŸ§  Mimari

![training-free-architecture-comic-sans-min](https://github.com/user-attachments/assets/d84dd83a-505e-45a0-8ce3-98e1838017f9)


## ğŸ› ï¸ Kurulum talimatlarÄ±

### 1. Depoyu klonla


```bash
git clone https://github.com/miquel-espinosa/no-time-to-train.git
cd no-time-to-train
```
### 2. Conda ortamÄ± oluÅŸturun

Gerekli paketlerle bir conda ortamÄ± oluÅŸturacaÄŸÄ±z.

```bash
conda env create -f environment.yml
conda activate no-time-to-train
```
### 3. SAM2 ve DinoV2'yi Kurun

SAM2 ve DinoV2'yi kaynaktan kuracaÄŸÄ±z.

```bash
pip install -e .
cd dinov2
pip install -e .
cd ..
```
### 4. Veri setlerini indirin

LÃ¼tfen COCO veri setini indirin ve `data/coco` klasÃ¶rÃ¼ne yerleÅŸtirin

### 5. SAM2 ve DinoV2 kontrol noktalarÄ±nÄ± indirin

Makaledekiyle tam olarak aynÄ± SAM2 kontrol noktalarÄ±nÄ± indireceÄŸiz.
(Ancak, SAM2.1 kontrol noktalarÄ±nÄ±n zaten mevcut olduÄŸunu ve daha iyi performans gÃ¶sterebileceÄŸini unutmayÄ±n.)


```bash
mkdir -p checkpoints/dinov2
cd checkpoints
wget https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt
cd dinov2
wget https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth
cd ../..
```
## ğŸ“Š Ã‡Ä±karÄ±m kodu

âš ï¸ UyarÄ±: Bu bir araÅŸtÄ±rma kodudur â€” biraz karmaÅŸa bekleyin!

### Few-shot COCO'da 30-shot SOTA sonuÃ§larÄ±nÄ± Ã§oÄŸaltma

KullanÄ±ÅŸlÄ± deÄŸiÅŸkenleri tanÄ±mlayÄ±n ve sonuÃ§lar iÃ§in bir klasÃ¶r oluÅŸturun:



```bash
CONFIG=./dev_hongyi/new_exps/coco_fewshot_10shot_Sam2L.yaml
CLASS_SPLIT="few_shot_classes"
RESULTS_DIR=work_dirs/few_shot_results
SHOTS=30
SEED=33
GPUS=4

mkdir -p $RESULTS_DIR
FILENAME=few_shot_${SHOTS}shot_seed${SEED}.pkl
```
#### 0. Referans seti oluÅŸturun


```bash
python dev_hongyi/dataset/few_shot_sampling.py \
        --n-shot $SHOTS \
        --out-path ${RESULTS_DIR}/${FILENAME} \
        --seed $SEED \
        --dataset $CLASS_SPLIT
```
#### 1. BelleÄŸi referanslarla doldurun


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode fill_memory \
                              --out_path ${RESULTS_DIR}/memory.ckpt \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.memory_pkl ${RESULTS_DIR}/${FILENAME} \
                              --model.init_args.dataset_cfgs.fill_memory.memory_length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
#### 2. Bellek bankasÄ±nÄ± sonradan iÅŸleme


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode postprocess_memory \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --ckpt_path ${RESULTS_DIR}/memory.ckpt \
                              --out_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --trainer.devices 1
```
#### 3. Hedef gÃ¶rÃ¼ntÃ¼lerde Ã§Ä±karÄ±m yapma


```bash
python run_lightening.py test --config $CONFIG  \
                              --ckpt_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --model.init_args.test_mode test \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.model_cfg.dataset_name $CLASS_SPLIT \
                              --model.init_args.dataset_cfgs.test.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
EÄŸer Ã§Ä±karÄ±m sonuÃ§larÄ±nÄ± Ã§evrimiÃ§i olarak (hesaplandÄ±kÃ§a) gÃ¶rmek isterseniz, `dev_hongyi/models/Sam2MatchingBaseline_noAMG.py` dosyasÄ±ndaki 1746-1749. satÄ±rlarÄ±n yorumunu kaldÄ±rÄ±n [burada](https://github.com/miquel-espinosa/no-time-to-train/blob/main/dev_hongyi/models/Sam2MatchingBaseline_noAMG.py#L1746).
Daha fazla veya daha az segmentli nesne gÃ¶rmek iÃ§in `score_thr` puan eÅŸiÄŸi parametresini ihtiyaca gÃ¶re ayarlayÄ±n.
GÃ¶rseller artÄ±k `results_analysis/few_shot_classes/` klasÃ¶rÃ¼ne kaydedilecektir. Soldaki gÃ¶rsel yer gerÃ§eÄŸini, saÄŸdaki gÃ¶rsel ise eÄŸitim gerektirmeyen yÃ¶ntemimizle bulunan segmentli nesneleri gÃ¶sterir.

Bu Ã¶rnekte `few_shot_classes` bÃ¶lmesini kullandÄ±ÄŸÄ±mÄ±zÄ± unutmayÄ±n, bu nedenle yalnÄ±zca bu bÃ¶lmede bulunan sÄ±nÄ±flarÄ±n segmentli nesnelerini gÃ¶rmeyi beklemeliyiz (COCO'daki tÃ¼m sÄ±nÄ±flarÄ± deÄŸil).

#### SonuÃ§lar

DoÄŸrulama setindeki tÃ¼m gÃ¶rselleri Ã§alÄ±ÅŸtÄ±rdÄ±ktan sonra, aÅŸaÄŸÄ±dakileri elde etmelisiniz:


```
BBOX RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.368

SEGM RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.342
```
---


## ğŸ” Citation

If you use this work, please cite us:

```bibtex
@article{espinosa2025notimetotrain,
  title={No time to train! Training-Free Reference-Based Instance Segmentation},
  author={Miguel Espinosa and Chenhongyi Yang and Linus Ericsson and Steven McDonagh and Elliot J. Crowley},
  journal={arXiv preprint arXiv:2507.02798},
  year={2025},
  primaryclass={cs.CV}
}
```

---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-22

---