
<div align="right">
  <details>
    <summary >ğŸŒ Dil</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-CN">ç®€ä½“ä¸­æ–‡</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-TW">ç¹é«”ä¸­æ–‡</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ja">æ—¥æœ¬èª</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ko">í•œêµ­ì–´</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=hi">à¤¹à¤¿à¤¨à¥à¤¦à¥€</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=th">à¹„à¸—à¸¢</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fr">FranÃ§ais</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=es">EspaÃ±ol</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=it">Italiano</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ru">Ğ ÑƒÑÑĞºĞ¸Ğ¹</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pt">PortuguÃªs</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ar">Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fa">ÙØ§Ø±Ø³ÛŒ</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=tr">TÃ¼rkÃ§e</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=vi">Tiáº¿ng Viá»‡t</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=id">Bahasa Indonesia</a>
      </div>
    </div>
  </details>
</div>

<div align="center">

# ğŸš€ EÄŸitime Zaman Yok!  
### EÄŸitimsiz Referans TabanlÄ± Nesne Segmentasyonu  
[![GitHub](https://img.shields.io/badge/%E2%80%8B-No%20Time%20To%20Train-black?logo=github)](https://github.com/miquel-espinosa/no-time-to-train)
[![Website](https://img.shields.io/badge/ğŸŒ-Project%20Page-grey)](https://miquel-espinosa.github.io/no-time-to-train/)
[![arXiv](https://img.shields.io/badge/arXiv-2507.02798-b31b1b)](https://arxiv.org/abs/2507.02798)

**En Son Teknoloji (Papers with Code)**
[**_SOTA 1-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/badge/State%20of%20the%20Art-Few--Shot%20Object%20Detection%20on%20MS--COCO%20(1--shot)-21CBCE?style=flat&logo=paperswithcode)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference)

[**_SOTA 10-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/badge/State%20of%20the%20Art-Few--Shot%20Object%20Detection%20on%20MS--COCO%20(10--shot)-21CBCE?style=flat&logo=paperswithcode)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference)

[**_SOTA 30-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/badge/State%20of%20the%20Art-Few--Shot%20Object%20Detection%20on%20MS--COCO%20(30--shot)-21CBCE?style=flat&logo=paperswithcode)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference)

<!-- [**_SOTA 1-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-1-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference)

[**_SOTA 10-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-10-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference)

[**_SOTA 30-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-30-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference) -->

</div>

---

> ğŸš¨ **GÃ¼ncelleme (22 Temmuz 2025):** Ã–zel veri kÃ¼meleri iÃ§in talimatlar eklendi!
> 
> ğŸ”” **GÃ¼ncelleme (16 Temmuz 2025):** Kod, talimatlarla birlikte gÃ¼ncellendi!

---

## ğŸ“‹ Ä°Ã§indekiler

- [ğŸ¯ Ã–ne Ã‡Ä±kanlar](#-highlights)
- [ğŸ“œ Ã–zet](#-abstract)
- [ğŸ§  Mimari](#-architecture)
- [ğŸ› ï¸ Kurulum talimatlarÄ±](#ï¸-installation-instructions)
  - [1. Depoyu klonlayÄ±n](#1-clone-the-repository)
  - [2. Conda ortamÄ± oluÅŸturun](#2-create-conda-environment)
  - [3. SAM2 ve DinoV2'yi kurun](#3-install-sam2-and-dinov2)
  - [4. Veri kÃ¼melerini indirin](#4-download-datasets)
  - [5. SAM2 ve DinoV2 kontrol noktalarÄ±nÄ± indirin](#5-download-sam2-and-dinov2-checkpoints)
- [ğŸ“Š Ã‡Ä±karÄ±m kodu: Few-shot COCO'da 30-shot SOTA sonuÃ§larÄ±nÄ± Ã§oÄŸaltÄ±n](#-inference-code)
  - [0. Referans seti oluÅŸturun](#0-create-reference-set)
  - [1. BelleÄŸi referanslarla doldurun](#1-fill-memory-with-references)
  - [2. Bellek bankasÄ±nÄ± son iÅŸlemden geÃ§irin](#2-post-process-memory-bank)
  - [3. Hedef gÃ¶rÃ¼ntÃ¼lerde Ã§Ä±karÄ±m yapÄ±n](#3-inference-on-target-images)
  - [SonuÃ§lar](#results)

- [ğŸ” Ã–zel veri seti](#-custom-dataset)
  - [0. Ã–zel bir veri seti hazÄ±rlayÄ±n â›µğŸ¦](#0-prepare-a-custom-dataset)
  - [0.1 Sadece bbox anotasyonlarÄ± varsa](#01-if-only-bbox-annotations-are-available)
  - [0.2 COCO anotasyonlarÄ±nÄ± pickle dosyasÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼n](#02-convert-coco-annotations-to-pickle-file)
  - [1. BelleÄŸi referanslarla doldurun](#1-fill-memory-with-references)
  - [2. Bellek bankasÄ±nÄ± sonradan iÅŸleyin](#2-post-process-memory-bank)
- [ğŸ“š AtÄ±f](#-citation)


## ğŸ¯ Ã–ne Ã‡Ä±kanlar
- ğŸ’¡ **EÄŸitimsiz**: Ä°nce ayar yok, prompt mÃ¼hendisliÄŸi yokâ€”sadece bir referans gÃ¶rÃ¼ntÃ¼sÃ¼.  
- ğŸ–¼ï¸ **Referans TabanlÄ±**: Yeni nesneleri sadece birkaÃ§ Ã¶rnek ile segmentleyin.  
- ğŸ”¥ **SOTA PerformansÄ±**: Ã–nceki eÄŸitimsiz yaklaÅŸÄ±mlara gÃ¶re COCO, PASCAL VOC ve Cross-Domain FSOD'da daha iyi sonuÃ§lar.

**BaÄŸlantÄ±lar:**
- ğŸ§¾ [**arXiv Makalesi**](https://arxiv.org/abs/2507.02798)  
- ğŸŒ [**Proje Web Sitesi**](https://miquel-espinosa.github.io/no-time-to-train/)  
- ğŸ“ˆ [**Kodlu Makaleler**](https://paperswithcode.com/paper/no-time-to-train-training-free-reference)

## ğŸ“œ Ã–zet

> The performance of image segmentation models has historically been constrained by the high cost of collecting large-scale annotated data. The Segment Anything Model (SAM) alleviates this original problem through a promptable, semantics-agnostic, segmentation paradigm and yet still requires manual visual-prompts or complex domain-dependent prompt-generation rules to process a new image. Towards reducing this new burden, our work investigates the task of object segmentation when provided with, alternatively, only a small set of reference images. Our key insight is to leverage strong semantic priors, as learned by foundation models, to identify corresponding regions between a reference and a target image. We find that correspondences enable automatic generation of instance-level segmentation masks for downstream tasks and instantiate our ideas via a multi-stage, training-free method incorporating (1) memory bank construction; (2) representation aggregation and (3) semantic-aware feature matching. Our experiments show significant improvements on segmentation metrics, leading to state-of-the-art performance on COCO FSOD (36.8% nAP), PASCAL VOC Few-Shot (71.2% nAP50) and outperforming existing training-free approaches on the Cross-Domain FSOD benchmark (22.4% nAP).

![cdfsod-results-final-comic-sans-min](https://github.com/user-attachments/assets/ab302c02-c080-4042-99fc-0e181ba8abb9)


## ğŸ§  Architecture

![training-free-architecture-comic-sans-min](https://github.com/user-attachments/assets/d84dd83a-505e-45a0-8ce3-98e1838017f9)


## ğŸ› ï¸ Installation instructions

### 1. Clone the repository

```bash
git clone https://github.com/miquel-espinosa/no-time-to-train.git
cd no-time-to-train
```
### 2. Conda ortamÄ± oluÅŸturun

Gerekli paketlerle bir conda ortamÄ± oluÅŸturacaÄŸÄ±z.

```bash
conda env create -f environment.yml
conda activate no-time-to-train
```
### 3. SAM2 ve DinoV2'yi Kurun

SAM2 ve DinoV2'yi kaynaktan kuracaÄŸÄ±z.

```bash
pip install -e .
cd dinov2
pip install -e .
cd ..
```
### 4. Veri setlerini indirin

LÃ¼tfen COCO veri setini indirin ve `data/coco` klasÃ¶rÃ¼ne yerleÅŸtirin

### 5. SAM2 ve DinoV2 kontrol noktalarÄ±nÄ± indirin

Makaledekiyle tam olarak aynÄ± SAM2 kontrol noktalarÄ±nÄ± indireceÄŸiz.
(Ancak, SAM2.1 kontrol noktalarÄ±nÄ±n zaten mevcut olduÄŸunu ve daha iyi performans gÃ¶sterebileceÄŸini unutmayÄ±n.)


```bash
mkdir -p checkpoints/dinov2
cd checkpoints
wget https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt
cd dinov2
wget https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth
cd ../..
```
## ğŸ“Š Ã‡Ä±karÄ±m kodu

âš ï¸ UyarÄ±: Bu bir araÅŸtÄ±rma kodudur â€” biraz karmaÅŸa bekleyin!

### Few-shot COCO'da 30-shot SOTA sonuÃ§larÄ±nÄ± Ã§oÄŸaltma

KullanÄ±ÅŸlÄ± deÄŸiÅŸkenleri tanÄ±mlayÄ±n ve sonuÃ§lar iÃ§in bir klasÃ¶r oluÅŸturun:



```bash
CONFIG=./no_time_to_train/new_exps/coco_fewshot_10shot_Sam2L.yaml
CLASS_SPLIT="few_shot_classes"
RESULTS_DIR=work_dirs/few_shot_results
SHOTS=30
SEED=33
GPUS=4

mkdir -p $RESULTS_DIR
FILENAME=few_shot_${SHOTS}shot_seed${SEED}.pkl
```
#### 0. Referans seti oluÅŸturun


```bash
python no_time_to_train/dataset/few_shot_sampling.py \
        --n-shot $SHOTS \
        --out-path ${RESULTS_DIR}/${FILENAME} \
        --seed $SEED \
        --dataset $CLASS_SPLIT
```
#### 1. BelleÄŸi referanslarla doldurun


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode fill_memory \
                              --out_path ${RESULTS_DIR}/memory.ckpt \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.memory_pkl ${RESULTS_DIR}/${FILENAME} \
                              --model.init_args.dataset_cfgs.fill_memory.memory_length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
#### 2. Bellek bankasÄ±nÄ± sonradan iÅŸleme


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode postprocess_memory \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --ckpt_path ${RESULTS_DIR}/memory.ckpt \
                              --out_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --trainer.devices 1
```
#### 3. Hedef gÃ¶rÃ¼ntÃ¼lerde Ã§Ä±karÄ±m yapma


```bash
python run_lightening.py test --config $CONFIG  \
                              --ckpt_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --model.init_args.test_mode test \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.model_cfg.dataset_name $CLASS_SPLIT \
                              --model.init_args.dataset_cfgs.test.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
EÄŸer Ã§Ä±karÄ±m sonuÃ§larÄ±nÄ± Ã§evrimiÃ§i olarak (hesaplandÄ±kÃ§a) gÃ¶rmek isterseniz, ÅŸu argÃ¼manÄ± ekleyin:

```bash
    --model.init_args.model_cfg.test.online_vis True
```
Puan eÅŸiÄŸi `score_thr` parametresini ayarlamak iÃ§in, argÃ¼manÄ± ekleyin (Ã¶rneÄŸin, skoru `0.4`'ten yÃ¼ksek olan tÃ¼m Ã¶rnekleri gÃ¶rselleÅŸtirmek iÃ§in):
```bash
    --model.init_args.model_cfg.test.vis_thr 0.4
```
GÃ¶rseller artÄ±k `results_analysis/few_shot_classes/` klasÃ¶rÃ¼ne kaydedilecektir. Soldaki gÃ¶rsel gerÃ§ek durumu, saÄŸdaki gÃ¶rsel ise eÄŸitim gerektirmeyen yÃ¶ntemimizle bulunan bÃ¶lÃ¼nmÃ¼ÅŸ Ã¶rnekleri gÃ¶stermektedir.

Bu Ã¶rnekte `few_shot_classes` bÃ¶lÃ¼mÃ¼nÃ¼ kullandÄ±ÄŸÄ±mÄ±zÄ± unutmayÄ±n, bu nedenle yalnÄ±zca bu bÃ¶lÃ¼mdeki sÄ±nÄ±flarÄ±n bÃ¶lÃ¼nmÃ¼ÅŸ Ã¶rneklerini gÃ¶rmeyi beklemeliyiz (COCOâ€™daki tÃ¼m sÄ±nÄ±flarÄ± deÄŸil).

#### SonuÃ§lar

DoÄŸrulama setindeki tÃ¼m gÃ¶rselleri Ã§alÄ±ÅŸtÄ±rdÄ±ktan sonra ÅŸunlarÄ± elde etmelisiniz:

```
BBOX RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.368

SEGM RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.342
```
---

## ğŸ” Ã–zel veri seti

Kendi veri setinizde boru hattÄ±mÄ±zÄ± Ã§alÄ±ÅŸtÄ±rmak iÃ§in talimatlarÄ± saÄŸlÄ±yoruz. AÃ§Ä±klama formatÄ± her zaman COCO formatÄ±ndadÄ±r.

> **Ã–zetle;** *Ã–zel veri setlerinde* tam boru hattÄ±nÄ±n nasÄ±l Ã§alÄ±ÅŸtÄ±rÄ±lacaÄŸÄ±nÄ± doÄŸrudan gÃ¶rmek iÃ§in `scripts/matching_cdfsod_pipeline.sh` dosyasÄ±na ve CD-FSOD veri setlerinin Ã¶rnek betiklerine (Ã¶rn. `scripts/dior_fish.sh`) bakabilirsiniz.

### 0. Ã–zel bir veri seti hazÄ±rlama â›µğŸ¦

Diyelim ki Ã¶zel bir veri setinde **tekne**â›µ ve **kuÅŸ**ğŸ¦ tespiti yapmak istiyoruz. YÃ¶ntemimizi kullanmak iÃ§in ÅŸunlara ihtiyacÄ±mÄ±z olacak:
- Her sÄ±nÄ±f iÃ§in en az 1 *etiketlenmiÅŸ* referans gÃ¶rseli (yani tekne iÃ§in 1 referans gÃ¶rseli ve kuÅŸ iÃ§in 1 referans gÃ¶rseli)
- Ä°stediÄŸimiz sÄ±nÄ±flarÄ±n Ã¶rneklerini bulmak iÃ§in birden fazla hedef gÃ¶rsel.

COCO gÃ¶rselleriyle **1-shot** ayarÄ±nda Ã¶zel bir veri seti oluÅŸturmak iÃ§in basit bir betik hazÄ±rladÄ±k.
```bash
mkdir -p data/my_custom_dataset
python scripts/make_custom_dataset.py
```
Bu, aÅŸaÄŸÄ±daki klasÃ¶r yapÄ±sÄ±na sahip Ã¶zel bir veri kÃ¼mesi oluÅŸturacaktÄ±r:
```
data/my_custom_dataset/
    â”œâ”€â”€ annotations/
    â”‚   â”œâ”€â”€ custom_references.json
    â”‚   â”œâ”€â”€ custom_targets.json
    â”‚   â””â”€â”€ references_visualisations/
    â”‚       â”œâ”€â”€ bird_1.jpg
    â”‚       â””â”€â”€ boat_1.jpg
    â””â”€â”€ images/
        â”œâ”€â”€ 429819.jpg
        â”œâ”€â”€ 101435.jpg
        â””â”€â”€ (all target and reference images)
```
**Referans gÃ¶rsellerin gÃ¶rselleÅŸtirilmesi (1-atÄ±ÅŸ):**

| KUÅ ğŸ¦ iÃ§in 1-atÄ±ÅŸ Referans GÃ¶rseli | TEKNE â›µ iÃ§in 1-atÄ±ÅŸ Referans GÃ¶rseli |
|:----------------------------------:|:------------------------------------:|
| <img src="https://github.com/user-attachments/assets/e59e580d-a7db-42ac-b386-892af211fc85" alt="bird_1" width="500"/> | <img src="https://github.com/user-attachments/assets/f94ee025-ae37-4a45-9c3e-0cfe8f8cd2bc" alt="boat_1" width="500"/> |


### 0.1 Sadece bbox anotasyonlarÄ± mevcutsa

YalnÄ±zca referans gÃ¶rseller iÃ§in sÄ±nÄ±rlayÄ±cÄ± kutu (bbox) anotasyonlarÄ±nÄ±z varsa, SAM2 kullanarak Ã¶rnek dÃ¼zeyinde segmentasyon maskeleri oluÅŸturmak iÃ§in de bir betik saÄŸlÄ±yoruz. Bu, yalnÄ±zca sÄ±nÄ±rlayÄ±cÄ± kutu anotasyonlarÄ± mevcutsa faydalÄ±dÄ±r.


```bash
# Download sam_h checkpoint. Feel free to use more recent checkpoints (note: code might need to be adapted)
wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth -O checkpoints/sam_vit_h_4b8939.pth
# Run automatic instance segmentation from ground truth bounding boxes.
python no_time_to_train/dataset/sam_bbox_to_segm_batch.py \
    --input_json data/my_custom_dataset/annotations/custom_references.json \
    --image_dir data/my_custom_dataset/images \
    --sam_checkpoint checkpoints/sam_vit_h_4b8939.pth \
    --model_type vit_h \
    --device cuda \
    --batch_size 8 \
    --visualize
```
**Ã–rnek dÃ¼zeyinde segmentasyon maskeleriyle referans gÃ¶rÃ¼ntÃ¼ler (gt bounding box'lardan SAM2 ile oluÅŸturuldu, 1-shot):**

OluÅŸturulan segmentasyon maskelerinin gÃ¶rselleÅŸtirmeleri `data/my_custom_dataset/annotations/custom_references_with_SAM_segm/references_visualisations/` dizininde kaydedilmiÅŸtir.


| KUÅ ğŸ¦ iÃ§in 1-shot Referans GÃ¶rÃ¼ntÃ¼sÃ¼ (SAM ile otomatik segmentasyonlu) | TEKNE â›µ iÃ§in 1-shot Referans GÃ¶rÃ¼ntÃ¼sÃ¼ (SAM ile otomatik segmentasyonlu) |
|:---------------------------------:|:----------------------------------:|
| <img src="https://github.com/user-attachments/assets/65d38dc4-1454-43cd-9600-e8efc67b3a82" alt="bird_1_with_SAM_segm" width="500"/> | <img src="https://github.com/user-attachments/assets/43a558ad-50ca-4715-8285-9aa3268843c6" alt="boat_1_with_SAM_segm" width="500"/> |


### 0.2 Coco aÃ§Ä±klamalarÄ±nÄ± pickle dosyasÄ±na dÃ¶nÃ¼ÅŸtÃ¼r


```bash
python no_time_to_train/dataset/coco_to_pkl.py \
    data/my_custom_dataset/annotations/custom_references_with_segm.json \
    data/my_custom_dataset/annotations/custom_references_with_segm.pkl \
    1
```
### 1. BelleÄŸi referanslarla doldurun

Ã–ncelikle, kullanÄ±ÅŸlÄ± deÄŸiÅŸkenler tanÄ±mlayÄ±n ve sonuÃ§lar iÃ§in bir klasÃ¶r oluÅŸturun. Etiketlerin doÄŸru gÃ¶rselleÅŸtirilmesi iÃ§in, sÄ±nÄ±f isimleri json dosyasÄ±nda gÃ¶rÃ¼ndÃ¼ÄŸÃ¼ gibi kategori kimliÄŸine gÃ¶re sÄ±ralanmalÄ±dÄ±r. Ã–rneÄŸin, `bird` kategori kimliÄŸi `16`, `boat` ise kategori kimliÄŸi `9`'dur. Bu nedenle, `CAT_NAMES=boat,bird`.


```bash
DATASET_NAME=my_custom_dataset
DATASET_PATH=data/my_custom_dataset
CAT_NAMES=boat,bird
CATEGORY_NUM=2
SHOT=1
YAML_PATH=no_time_to_train/pl_configs/matching_cdfsod_template.yaml
PATH_TO_SAVE_CKPTS=./tmp_ckpts/my_custom_dataset
mkdir -p $PATH_TO_SAVE_CKPTS
```
AdÄ±m 1'i Ã§alÄ±ÅŸtÄ±rÄ±n:

```bash
python run_lightening.py test --config $YAML_PATH \
    --model.test_mode fill_memory \
    --out_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory.pth \
    --model.init_args.dataset_cfgs.fill_memory.root $DATASET_PATH/images \
    --model.init_args.dataset_cfgs.fill_memory.json_file $DATASET_PATH/annotations/custom_references_with_segm.json \
    --model.init_args.dataset_cfgs.fill_memory.memory_pkl $DATASET_PATH/annotations/custom_references_with_segm.pkl \
    --model.init_args.dataset_cfgs.fill_memory.memory_length $SHOT \
    --model.init_args.dataset_cfgs.fill_memory.cat_names $CAT_NAMES \
    --model.init_args.model_cfg.dataset_name $DATASET_NAME \
    --model.init_args.model_cfg.memory_bank_cfg.length $SHOT \
    --model.init_args.model_cfg.memory_bank_cfg.category_num $CATEGORY_NUM \
    --trainer.devices 1
```
### 2. Bellek bankasÄ±nÄ± sonradan iÅŸleme


```bash
python run_lightening.py test --config $YAML_PATH \
    --model.test_mode postprocess_memory \
    --ckpt_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory.pth \
    --out_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory_postprocessed.pth \
    --model.init_args.model_cfg.dataset_name $DATASET_NAME \
    --model.init_args.model_cfg.memory_bank_cfg.length $SHOT \
    --model.init_args.model_cfg.memory_bank_cfg.category_num $CATEGORY_NUM \
    --trainer.devices 1
```

#### 2.1 Ä°ÅŸlenmiÅŸ bellek bankasÄ±nÄ± gÃ¶rselleÅŸtir

```bash
python run_lightening.py test --config $YAML_PATH \
    --model.test_mode vis_memory \
    --ckpt_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory_postprocessed.pth \
    --model.init_args.dataset_cfgs.fill_memory.root $DATASET_PATH/images \
    --model.init_args.dataset_cfgs.fill_memory.json_file $DATASET_PATH/annotations/custom_references_with_segm.json \
    --model.init_args.dataset_cfgs.fill_memory.memory_pkl $DATASET_PATH/annotations/custom_references_with_segm.pkl \
    --model.init_args.dataset_cfgs.fill_memory.memory_length $SHOT \
    --model.init_args.dataset_cfgs.fill_memory.cat_names $CAT_NAMES \
    --model.init_args.model_cfg.dataset_name $DATASET_NAME \
    --model.init_args.model_cfg.memory_bank_cfg.length $SHOT \
    --model.init_args.model_cfg.memory_bank_cfg.category_num $CATEGORY_NUM \
    --trainer.devices 1
```
Bellek bankasÄ± gÃ¶rÃ¼ntÃ¼leri iÃ§in PCA ve K-means gÃ¶rselleÅŸtirmeleri `results_analysis/memory_vis/my_custom_dataset` klasÃ¶rÃ¼nde saklanÄ±r.

### 3. Hedef gÃ¶rÃ¼ntÃ¼lerde Ã§Ä±karÄ±m

EÄŸer `ONLINE_VIS` True olarak ayarlanÄ±rsa, tahmin sonuÃ§larÄ± `results_analysis/my_custom_dataset/` klasÃ¶rÃ¼ne kaydedilir ve hesaplandÄ±kÃ§a gÃ¶rÃ¼ntÃ¼lenir. ONLINE gÃ¶rselleÅŸtirme ile Ã§alÄ±ÅŸmanÄ±n Ã§ok daha yavaÅŸ olduÄŸunu UNUTMAYIN.

Daha fazla veya daha az segmentlenmiÅŸ Ã¶rnek gÃ¶rmek iÃ§in skor eÅŸiÄŸi olan `VIS_THR` deÄŸerini istediÄŸiniz gibi deÄŸiÅŸtirebilirsiniz.
```bash
ONLINE_VIS=True
VIS_THR=0.4
python run_lightening.py test --config $YAML_PATH \
    --model.test_mode test \
    --ckpt_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory_postprocessed.pth \
    --model.init_args.model_cfg.dataset_name $DATASET_NAME \
    --model.init_args.model_cfg.memory_bank_cfg.length $SHOT \
    --model.init_args.model_cfg.memory_bank_cfg.category_num $CATEGORY_NUM \
    --model.init_args.model_cfg.test.imgs_path $DATASET_PATH/images \
    --model.init_args.model_cfg.test.online_vis $ONLINE_VIS \
    --model.init_args.model_cfg.test.vis_thr $VIS_THR \
    --model.init_args.dataset_cfgs.test.root $DATASET_PATH/images \
    --model.init_args.dataset_cfgs.test.json_file $DATASET_PATH/annotations/custom_targets.json \
    --model.init_args.dataset_cfgs.test.cat_names $CAT_NAMES \
    --trainer.devices 1
```
### SonuÃ§lar

Performans metrikleri (yukarÄ±daki komutlarla tamamen aynÄ± parametrelerle) ÅŸu ÅŸekilde olmalÄ±dÄ±r:


```
BBOX RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.478

SEGM RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.458
```
GÃ¶rsel sonuÃ§lar `results_analysis/my_custom_dataset/` dizininde kaydedilir. YÃ¶ntemimizin yanlÄ±ÅŸ negatifler iÃ§in Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± unutmayÄ±n, yani istenen sÄ±nÄ±flarÄ±n hiÃ§bir Ã¶rneÄŸini iÃ§ermeyen gÃ¶rsellerde de Ã§alÄ±ÅŸÄ±r.

*Resimleri bÃ¼yÃ¼tmek iÃ§in tÄ±klayÄ±n â¬‡ï¸*

| Hedef gÃ¶rÃ¼ntÃ¼de tekneler â›µ (sol GT, saÄŸ tahminler) | Hedef gÃ¶rÃ¼ntÃ¼de kuÅŸlar ğŸ¦ (sol GT, saÄŸ tahminler) |
|:----------------------:|:----------------------:|
| ![000000459673](https://github.com/user-attachments/assets/678dc15a-dd3b-49d5-9287-6290da16aa6b) | ![000000407180](https://github.com/user-attachments/assets/fe306e48-af49-4d83-ac82-76fac6c456d1) |

| Hedef gÃ¶rÃ¼ntÃ¼de tekneler ve kuÅŸlar â›µğŸ¦ (sol GT, saÄŸ tahminler) | Hedef gÃ¶rÃ¼ntÃ¼de tekne veya kuÅŸ yok ğŸš« (sol GT, saÄŸ tahminler) |
|:---------------------------------:|:----------------------------------:|
| ![000000517410](https://github.com/user-attachments/assets/9849b227-7f43-43d7-81ea-58010a623ad5) | ![000000460598](https://github.com/user-attachments/assets/7587700c-e09d-4cf6-8590-3df129c2568e) |


## ğŸ“š AtÄ±f

Bu Ã§alÄ±ÅŸmayÄ± kullanÄ±rsanÄ±z, lÃ¼tfen bize atÄ±fta bulunun:


```bibtex
@article{espinosa2025notimetotrain,
  title={No time to train! Training-Free Reference-Based Instance Segmentation},
  author={Miguel Espinosa and Chenhongyi Yang and Linus Ericsson and Steven McDonagh and Elliot J. Crowley},
  journal={arXiv preprint arXiv:2507.02798},
  year={2025},
  primaryclass={cs.CV}
}
```


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2026-01-15

---