<div align="right">
  <details>
    <summary >🌐 Dil</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-CN">简体中文</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-TW">繁體中文</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ko">한국어</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=hi">हिन्दी</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=th">ไทย</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fr">Français</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=es">Español</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=it">Italiano</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ru">Русский</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pt">Português</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ar">العربية</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fa">فارسی</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=tr">Türkçe</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=vi">Tiếng Việt</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=id">Bahasa Indonesia</a>
      </div>
    </div>
  </details>
</div>

<div align="center">

# 🚀 Eğitime Zaman Yok!  
### Eğitimsiz Referans Tabanlı Nesne Bölütleme  
[![GitHub](https://img.shields.io/badge/%E2%80%8B-No%20Time%20To%20Train-black?logo=github)](https://github.com/miquel-espinosa/no-time-to-train)
[![Website](https://img.shields.io/badge/🌐-Project%20Page-grey)](https://miquel-espinosa.github.io/no-time-to-train/)
[![arXiv](https://img.shields.io/badge/arXiv-2507.02798-b31b1b)](https://arxiv.org/abs/2507.02798)

**En İyi Sonuçlar (Papers with Code)**

[**_1-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-1-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference)

[**_10-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-10-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference)

[**_30-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-30-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference)

</div>

---

> 🔔 **Güncelleme (Temmuz 2025):** Kod, talimatlarla birlikte güncellendi!

---

## 📋 İçindekiler

- [🎯 Öne Çıkanlar](#-öne-çıkanlar)
- [📜 Öz](#-öz)
- [🧠 Mimari](#-mimari)
- [🛠️ Kurulum talimatları](#️-kurulum-talimati)
  - [1. Depoyu klonla](#1-depoyu-klonla)
  - [2. Conda ortamı oluştur](#2-conda-ortamı-oluştur)
  - [3. SAM2 ve DinoV2'yi kur](#3-sam2-ve-dinov2yi-kur)
  - [4. Veri setlerini indir](#4-veri-setlerini-indir)
  - [5. SAM2 ve DinoV2 kontrol noktalarını indir](#5-sam2-ve-dinov2-kontrol-noktalarını-indir)
- [📊 Çıkarım kodu: Few-shot COCO'da 30-shot SOTA Sonuçlarını Tekrarlayın](#-çıkarım-kodu)
  - [0. Referans seti oluştur](#0-referans-seti-oluştur)
  - [1. Hafızayı referanslarla doldur](#1-hafızayı-referanslarla-doldur)
  - [2. Hafıza bankasını sonradan işle](#2-hafıza-bankasını-sonradan-işle)
  - [3. Hedef görsellerde çıkarım yap](#3-hedef-görsellerde-çıkarım-yap)
  - [Sonuçlar](#sonuçlar)
- [🔍 Atıf](#-atıf)


## 🎯 Öne Çıkanlar
- 💡 **Eğitimsiz**: İnce ayar yok, prompt mühendisliği yok—sadece bir referans görseli yeterli.  
- 🖼️ **Referans Tabanlı**: Yalnızca birkaç örnekle yeni nesneleri bölütleyin.  
- 🔥 **SOTA Performansı**: COCO, PASCAL VOC ve Cross-Domain FSOD üzerinde önceki eğitimsiz yaklaşımlardan daha iyi sonuçlar.

**Bağlantılar:**
- 🧾 [**arXiv Makalesi**](https://arxiv.org/abs/2507.02798)  
- 🌐 [**Proje Web Sitesi**](https://miquel-espinosa.github.io/no-time-to-train/)  
- 📈 [**Papers with Code**](https://paperswithcode.com/paper/no-time-to-train-training-free-reference)

## 📜 Öz

> Görüntü bölütleme modellerinin performansı, tarihsel olarak, büyük ölçekli etiketli veri toplamanın yüksek maliyetiyle sınırlanmıştır. Segment Anything Model (SAM), bu temel problemi, prompt ile çalışabilen, anlamsal olarak bağımsız bir bölütleme yaklaşımıyla hafifletmiştir; ancak yeni bir görseli işlemek için hala manuel görsel promptlara veya karmaşık, alana özgü prompt oluşturma kurallarına ihtiyaç duyar. Bu yeni yükü azaltmak için çalışmamız, yalnızca küçük bir referans görsel seti sağlandığında nesne bölütleme görevini araştırıyor. Temel çıkarımımız, temel modellerin öğrendiği güçlü anlamsal öncülleri kullanarak, bir referans ve hedef görsel arasındaki karşılık gelen bölgeleri tespit etmektir. Karşılıkların, ileri görevler için otomatik olarak örnek düzeyinde bölütleme maskeleri üretmeyi sağladığını bulduk ve fikrimizi çok aşamalı, eğitimsiz bir yöntemle (1) hafıza bankası oluşturma; (2) temsil toplama ve (3) anlamsal farkındalıklı özellik eşleştirme adımlarıyla hayata geçirdik. Deneylerimiz, bölütleme metriklerinde önemli iyileşmeler göstererek COCO FSOD'da (36.8% nAP), PASCAL VOC Few-Shot'ta (71.2% nAP50) ve Cross-Domain FSOD kıyaslamasında mevcut eğitimsiz yaklaşımlardan daha iyi sonuçlara (22.4% nAP) ulaşmaktadır.

![cdfsod-results-final-comic-sans-min](https://github.com/user-attachments/assets/ab302c02-c080-4042-99fc-0e181ba8abb9)


## 🧠 Mimari

![training-free-architecture-comic-sans-min](https://github.com/user-attachments/assets/d84dd83a-505e-45a0-8ce3-98e1838017f9)


## 🛠️ Kurulum talimatları

### 1. Depoyu klonla


```bash
git clone https://github.com/miquel-espinosa/no-time-to-train.git
cd no-time-to-train
```
### 2. Conda ortamı oluşturun

Gerekli paketlerle bir conda ortamı oluşturacağız.

```bash
conda env create -f environment.yml
conda activate no-time-to-train
```
### 3. SAM2 ve DinoV2'yi Kurun

SAM2 ve DinoV2'yi kaynaktan kuracağız.

```bash
pip install -e .
cd dinov2
pip install -e .
cd ..
```
### 4. Veri setlerini indirin

Lütfen COCO veri setini indirin ve `data/coco` klasörüne yerleştirin

### 5. SAM2 ve DinoV2 kontrol noktalarını indirin

Makaledekiyle tam olarak aynı SAM2 kontrol noktalarını indireceğiz.
(Ancak, SAM2.1 kontrol noktalarının zaten mevcut olduğunu ve daha iyi performans gösterebileceğini unutmayın.)


```bash
mkdir -p checkpoints/dinov2
cd checkpoints
wget https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt
cd dinov2
wget https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth
cd ../..
```
## 📊 Çıkarım kodu

⚠️ Uyarı: Bu bir araştırma kodudur — biraz karmaşa bekleyin!

### Few-shot COCO'da 30-shot SOTA sonuçlarını çoğaltma

Kullanışlı değişkenleri tanımlayın ve sonuçlar için bir klasör oluşturun:



```bash
CONFIG=./dev_hongyi/new_exps/coco_fewshot_10shot_Sam2L.yaml
CLASS_SPLIT="few_shot_classes"
RESULTS_DIR=work_dirs/few_shot_results
SHOTS=30
SEED=33
GPUS=4

mkdir -p $RESULTS_DIR
FILENAME=few_shot_${SHOTS}shot_seed${SEED}.pkl
```
#### 0. Referans seti oluşturun


```bash
python dev_hongyi/dataset/few_shot_sampling.py \
        --n-shot $SHOTS \
        --out-path ${RESULTS_DIR}/${FILENAME} \
        --seed $SEED \
        --dataset $CLASS_SPLIT
```
#### 1. Belleği referanslarla doldurun


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode fill_memory \
                              --out_path ${RESULTS_DIR}/memory.ckpt \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.memory_pkl ${RESULTS_DIR}/${FILENAME} \
                              --model.init_args.dataset_cfgs.fill_memory.memory_length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
#### 2. Bellek bankasını sonradan işleme


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode postprocess_memory \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --ckpt_path ${RESULTS_DIR}/memory.ckpt \
                              --out_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --trainer.devices 1
```
#### 3. Hedef görüntülerde çıkarım yapma


```bash
python run_lightening.py test --config $CONFIG  \
                              --ckpt_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --model.init_args.test_mode test \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.model_cfg.dataset_name $CLASS_SPLIT \
                              --model.init_args.dataset_cfgs.test.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
Eğer çıkarım sonuçlarını çevrimiçi olarak (hesaplandıkça) görmek isterseniz, `dev_hongyi/models/Sam2MatchingBaseline_noAMG.py` dosyasındaki 1746-1749. satırların yorumunu kaldırın [burada](https://github.com/miquel-espinosa/no-time-to-train/blob/main/dev_hongyi/models/Sam2MatchingBaseline_noAMG.py#L1746).
Daha fazla veya daha az segmentli nesne görmek için `score_thr` puan eşiği parametresini ihtiyaca göre ayarlayın.
Görseller artık `results_analysis/few_shot_classes/` klasörüne kaydedilecektir. Soldaki görsel yer gerçeğini, sağdaki görsel ise eğitim gerektirmeyen yöntemimizle bulunan segmentli nesneleri gösterir.

Bu örnekte `few_shot_classes` bölmesini kullandığımızı unutmayın, bu nedenle yalnızca bu bölmede bulunan sınıfların segmentli nesnelerini görmeyi beklemeliyiz (COCO'daki tüm sınıfları değil).

#### Sonuçlar

Doğrulama setindeki tüm görselleri çalıştırdıktan sonra, aşağıdakileri elde etmelisiniz:


```
BBOX RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.368

SEGM RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.342
```
---


## 🔍 Citation

If you use this work, please cite us:

```bibtex
@article{espinosa2025notimetotrain,
  title={No time to train! Training-Free Reference-Based Instance Segmentation},
  author={Miguel Espinosa and Chenhongyi Yang and Linus Ericsson and Steven McDonagh and Elliot J. Crowley},
  journal={arXiv preprint arXiv:2507.02798},
  year={2025},
  primaryclass={cs.CV}
}
```

---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-22

---