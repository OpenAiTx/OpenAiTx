<div align="right">
  <details>
    <summary >🌐 Dil</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-CN">简体中文</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-TW">繁體中文</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ko">한국어</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=hi">हिन्दी</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=th">ไทย</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fr">Français</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=es">Español</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=it">Italiano</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ru">Русский</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pt">Português</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ar">العربية</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fa">فارسی</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=tr">Türkçe</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=vi">Tiếng Việt</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=id">Bahasa Indonesia</a>
      </div>
    </div>
  </details>
</div>

<div align="center">

# 🚀 Eğitime Zaman Yok!  
### Eğitimsiz Referans Tabanlı Nesne Segmentasyonu  
[![GitHub](https://img.shields.io/badge/%E2%80%8B-No%20Time%20To%20Train-black?logo=github)](https://github.com/miquel-espinosa/no-time-to-train)
[![Website](https://img.shields.io/badge/🌐-Project%20Page-grey)](https://miquel-espinosa.github.io/no-time-to-train/)
[![arXiv](https://img.shields.io/badge/arXiv-2507.02798-b31b1b)](https://arxiv.org/abs/2507.02798)

**En İyi Sonuçlar (Papers with Code)**

[**_SOTA 1-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-1-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference)

[**_SOTA 10-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-10-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference)

[**_SOTA 30-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-30-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference)

</div>

---

> 🚨 **Güncelleme (22 Temmuz 2025):** Özel veri setleri için talimatlar eklendi!
> 
> 🔔 **Güncelleme (16 Temmuz 2025):** Kod, talimatlarla birlikte güncellendi!

---

## 📋 İçindekiler

- [🎯 Öne Çıkanlar](#-öne-çıkanlar)
- [📜 Özet](#-özet)
- [🧠 Mimari](#-mimari)
- [🛠️ Kurulum talimatları](#️-kurulum-talimati)
  - [1. Depoyu klonlayın](#1-depoyu-klonlayın)
  - [2. Conda ortamı oluşturun](#2-conda-ortamı-oluşturun)
  - [3. SAM2 ve DinoV2'yi yükleyin](#3-sam2-ve-dinov2yi-yükleyin)
  - [4. Veri setlerini indirin](#4-veri-setlerini-indirin)
  - [5. SAM2 ve DinoV2 kontrol noktalarını indirin](#5-sam2-ve-dinov2-kontrol-noktalarını-indirin)
- [📊 Çıkarım kodu: Few-shot COCO'da 30-shot SOTA sonuçları çoğaltın](#-çıkarım-kodu)
  - [0. Referans seti oluşturun](#0-referans-seti-oluşturun)
  - [1. Hafızayı referanslarla doldurun](#1-hafızayı-referanslarla-doldurun)
  - [2. Hafıza bankasını sonradan işleyin](#2-hafıza-bankasını-sonradan-işleyin)
  - [3. Hedef görüntülerde çıkarım](#3-hedef-görüntülerde-çıkarım)
  - [Sonuçlar](#sonuçlar)
- [🔍 Özel veri seti](#-özel-veri-seti)
  - [0. Özel bir veri seti hazırlayın ⛵🐦](#0-özel-bir-veri-seti-hazırlayın)
  - [0.1 Sadece bbox anotasyonları mevcutsa](#01-sadece-bbox-anotasyonları-mevcutsa)
  - [0.2 Coco anotasyonlarını pickle dosyasına dönüştürün](#02-coco-anotasyonlarını-pickle-dosyasına-dönüştürün)
  - [1. Hafızayı referanslarla doldurun](#1-hafızayı-referanslarla-doldurun)
  - [2. Hafıza bankasını sonradan işleyin](#2-hafıza-bankasını-sonradan-işleyin)
- [📚 Atıf](#-atıf)


## 🎯 Öne Çıkanlar
- 💡 **Eğitimsiz**: İnce ayar yok, prompt mühendisliği yok—yalnızca bir referans görseli.  
- 🖼️ **Referans Tabanlı**: Sadece birkaç örnekle yeni nesneleri segmente edin.  
- 🔥 **SOTA Performansı**: COCO, PASCAL VOC ve Cross-Domain FSOD'da önceki eğitimsiz yöntemleri geride bırakır.

**Bağlantılar:**
- 🧾 [**arXiv Makalesi**](https://arxiv.org/abs/2507.02798)  
- 🌐 [**Proje Web Sitesi**](https://miquel-espinosa.github.io/no-time-to-train/)  
- 📈 [**Papers with Code**](https://paperswithcode.com/paper/no-time-to-train-training-free-reference)

## 📜 Özet

> Görüntü segmentasyon modellerinin performansı, tarihsel olarak, büyük ölçekli anotasyonlu veri toplamanın yüksek maliyetiyle sınırlandırılmıştır. Segment Anything Model (SAM), bu orijinal sorunu prompt ile çalıştırılabilen, semantik bağımsız, segmentasyon paradigmasıyla hafifletir; fakat yeni bir görüntüyü işlemek için hâlâ manuel görsel komutlara veya karmaşık alan bağımlı komut üretim kurallarına ihtiyaç duyar. Bu yeni yükü azaltmak amacıyla, çalışmamız nesne segmentasyonu görevini, bunun yerine yalnızca küçük bir referans görsel setinin sağlandığı durumda inceler. Temel öngörümüz, temel modellerin öğrendiği güçlü semantik önbilgileri kullanarak bir referans ve hedef görüntü arasında karşılık gelen bölgeleri bulmaktır. Karşılıkların, aşağı akış görevleri için örnek düzeyinde segmentasyon maskelerinin otomatik üretimini mümkün kıldığını bulduk ve fikirlerimizi çok aşamalı, eğitimsiz bir yöntemle hayata geçiriyoruz: (1) hafıza bankası oluşturma; (2) temsili toplama ve (3) semantik bilinçli özellik eşleştirme. Deneylerimiz, segmentasyon metriklerinde önemli iyileşmeler gösteriyor ve COCO FSOD’da (36.8% nAP), PASCAL VOC Few-Shot’ta (71.2% nAP50) ve Cross-Domain FSOD benchmark’ında mevcut eğitimsiz yaklaşımları aşan (22.4% nAP) alanında en iyi performansa ulaşıyor.

![cdfsod-results-final-comic-sans-min](https://github.com/user-attachments/assets/ab302c02-c080-4042-99fc-0e181ba8abb9)




## 🧠 Mimari

![training-free-architecture-comic-sans-min](https://github.com/user-attachments/assets/d84dd83a-505e-45a0-8ce3-98e1838017f9)


## 🛠️ Kurulum talimatları

### 1. Depoyu klonlayın

```bash
git clone https://github.com/miquel-espinosa/no-time-to-train.git
cd no-time-to-train
```
### 2. Conda ortamı oluşturun

Gerekli paketlerle bir conda ortamı oluşturacağız.

```bash
conda env create -f environment.yml
conda activate no-time-to-train
```
### 3. SAM2 ve DinoV2'yi Kurun

SAM2 ve DinoV2'yi kaynaktan kuracağız.

```bash
pip install -e .
cd dinov2
pip install -e .
cd ..
```
### 4. Veri setlerini indirin

Lütfen COCO veri setini indirin ve `data/coco` klasörüne yerleştirin

### 5. SAM2 ve DinoV2 kontrol noktalarını indirin

Makaledekiyle tam olarak aynı SAM2 kontrol noktalarını indireceğiz.
(Ancak, SAM2.1 kontrol noktalarının zaten mevcut olduğunu ve daha iyi performans gösterebileceğini unutmayın.)


```bash
mkdir -p checkpoints/dinov2
cd checkpoints
wget https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt
cd dinov2
wget https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth
cd ../..
```
## 📊 Çıkarım kodu

⚠️ Uyarı: Bu bir araştırma kodudur — biraz karmaşa bekleyin!

### Few-shot COCO'da 30-shot SOTA sonuçlarını çoğaltma

Kullanışlı değişkenleri tanımlayın ve sonuçlar için bir klasör oluşturun:



```bash
CONFIG=./no_time_to_train/new_exps/coco_fewshot_10shot_Sam2L.yaml
CLASS_SPLIT="few_shot_classes"
RESULTS_DIR=work_dirs/few_shot_results
SHOTS=30
SEED=33
GPUS=4

mkdir -p $RESULTS_DIR
FILENAME=few_shot_${SHOTS}shot_seed${SEED}.pkl
```
#### 0. Referans seti oluşturun


```bash
python no_time_to_train/dataset/few_shot_sampling.py \
        --n-shot $SHOTS \
        --out-path ${RESULTS_DIR}/${FILENAME} \
        --seed $SEED \
        --dataset $CLASS_SPLIT
```
#### 1. Belleği referanslarla doldurun


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode fill_memory \
                              --out_path ${RESULTS_DIR}/memory.ckpt \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.memory_pkl ${RESULTS_DIR}/${FILENAME} \
                              --model.init_args.dataset_cfgs.fill_memory.memory_length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
#### 2. Bellek bankasını sonradan işleme


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode postprocess_memory \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --ckpt_path ${RESULTS_DIR}/memory.ckpt \
                              --out_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --trainer.devices 1
```
#### 3. Hedef görüntülerde çıkarım yapma


```bash
python run_lightening.py test --config $CONFIG  \
                              --ckpt_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --model.init_args.test_mode test \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.model_cfg.dataset_name $CLASS_SPLIT \
                              --model.init_args.dataset_cfgs.test.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
Eğer çıkarım sonuçlarını çevrimiçi olarak (hesaplandıkça) görmek isterseniz, `no_time_to_train/models/Sam2MatchingBaseline_noAMG.py` dosyasında 1746-1749. satırların yorumunu kaldırın [buradan bakabilirsiniz](https://github.com/miquel-espinosa/no-time-to-train/blob/main/no_time_to_train/models/Sam2MatchingBaseline_noAMG.py#L1746).
Daha fazla ya da daha az segmentlenmiş örnek görmek için `score_thr` skor eşiği parametresini ihtiyacınıza göre ayarlayın.
Görseller artık `results_analysis/few_shot_classes/` klasöründe kaydedilecektir. Soldaki görsel yer gerçeğini, sağdaki görsel ise eğitim gerektirmeyen yöntemimizle bulunan segmentlenmiş örnekleri göstermektedir.

Bu örnekte `few_shot_classes` bölmesini kullandığımızı unutmayın, bu nedenle yalnızca bu bölmedeki sınıfların segmentlenmiş örneklerini görmeyi beklemeliyiz (COCO'daki tüm sınıfları değil).

#### Sonuçlar

Doğrulama kümesindeki tüm görselleri çalıştırdıktan sonra şunları elde etmelisiniz:


```
BBOX RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.368

SEGM RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.342
```
---

## 🔍 Özel veri seti

Kendi veri setinizde boru hattımızı çalıştırmak için talimatları sağlıyoruz. Açıklama formatı her zaman COCO formatındadır.

> **Özetle;** *Özel veri setlerinde* tam boru hattının nasıl çalıştırılacağını doğrudan görmek için `scripts/matching_cdfsod_pipeline.sh` dosyasına ve CD-FSOD veri setlerinin örnek betiklerine (örn. `scripts/dior_fish.sh`) bakabilirsiniz.

### 0. Özel bir veri seti hazırlama ⛵🐦

Diyelim ki özel bir veri setinde **tekne**⛵ ve **kuş**🐦 tespiti yapmak istiyoruz. Yöntemimizi kullanmak için şunlara ihtiyacımız olacak:
- Her sınıf için en az 1 *etiketlenmiş* referans görseli (yani tekne için 1 referans görseli ve kuş için 1 referans görseli)
- İstediğimiz sınıfların örneklerini bulmak için birden fazla hedef görsel.

COCO görselleriyle **1-shot** ayarında özel bir veri seti oluşturmak için basit bir betik hazırladık.
```bash
python scripts/make_custom_dataset.py
```
Bu, aşağıdaki klasör yapısına sahip özel bir veri kümesi oluşturacaktır:
```
data/my_custom_dataset/
    ├── annotations/
    │   ├── custom_references.json
    │   ├── custom_targets.json
    │   └── references_visualisations/
    │       ├── bird_1.jpg
    │       └── boat_1.jpg
    └── images/
        ├── 429819.jpg
        ├── 101435.jpg
        └── (all target and reference images)
```
**Referans görsellerin görselleştirilmesi (1-atış):**

| KUŞ 🐦 için 1-atış Referans Görseli | TEKNE ⛵ için 1-atış Referans Görseli |
|:----------------------------------:|:------------------------------------:|
| <img src="https://github.com/user-attachments/assets/e59e580d-a7db-42ac-b386-892af211fc85" alt="bird_1" width="500"/> | <img src="https://github.com/user-attachments/assets/f94ee025-ae37-4a45-9c3e-0cfe8f8cd2bc" alt="boat_1" width="500"/> |


### 0.1 Sadece bbox anotasyonları mevcutsa

Yalnızca referans görseller için sınırlayıcı kutu (bbox) anotasyonlarınız varsa, SAM2 kullanarak örnek düzeyinde segmentasyon maskeleri oluşturmak için de bir betik sağlıyoruz. Bu, yalnızca sınırlayıcı kutu anotasyonları mevcutsa faydalıdır.


```bash
# Download sam_h checkpoint. Feel free to use more recent checkpoints (note: code might need to be adapted)
wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth -O checkpoints/sam_vit_h_4b8939.pth
# Run automatic instance segmentation from ground truth bounding boxes.
python no_time_to_train/dataset/sam_bbox_to_segm_batch.py \
    --input_json data/my_custom_dataset/annotations/custom_references.json \
    --image_dir data/my_custom_dataset/images \
    --sam_checkpoint checkpoints/sam_vit_h_4b8939.pth \
    --model_type vit_h \
    --device cuda \
    --batch_size 8 \
    --visualize
```
**Örnek düzeyinde segmentasyon maskeleriyle referans görüntüler (gt bounding box'lardan SAM2 ile oluşturuldu, 1-shot):**

Oluşturulan segmentasyon maskelerinin görselleştirmeleri `data/my_custom_dataset/annotations/custom_references_with_SAM_segm/references_visualisations/` dizininde kaydedilmiştir.


| KUŞ 🐦 için 1-shot Referans Görüntüsü (SAM ile otomatik segmentasyonlu) | TEKNE ⛵ için 1-shot Referans Görüntüsü (SAM ile otomatik segmentasyonlu) |
|:---------------------------------:|:----------------------------------:|
| <img src="https://github.com/user-attachments/assets/65d38dc4-1454-43cd-9600-e8efc67b3a82" alt="bird_1_with_SAM_segm" width="500"/> | <img src="https://github.com/user-attachments/assets/43a558ad-50ca-4715-8285-9aa3268843c6" alt="boat_1_with_SAM_segm" width="500"/> |


### 0.2 Coco açıklamalarını pickle dosyasına dönüştür


```bash
python no_time_to_train/dataset/coco_to_pkl.py \
    data/my_custom_dataset/annotations/custom_references_with_segm.json \
    data/my_custom_dataset/annotations/custom_references_with_segm.pkl \
    1
```
### 1. Belleği referanslarla doldurun

Öncelikle, kullanışlı değişkenler tanımlayın ve sonuçlar için bir klasör oluşturun. Etiketlerin doğru görselleştirilmesi için, sınıf isimleri json dosyasında göründüğü gibi kategori kimliğine göre sıralanmalıdır. Örneğin, `bird` kategori kimliği `16`, `boat` ise kategori kimliği `9`'dur. Bu nedenle, `CAT_NAMES=boat,bird`.


```bash
DATASET_NAME=my_custom_dataset
DATASET_PATH=data/my_custom_dataset
CAT_NAMES=boat,bird
CATEGORY_NUM=2
SHOT=1
YAML_PATH=no_time_to_train/pl_configs/matching_cdfsod_template.yaml
PATH_TO_SAVE_CKPTS=./tmp_ckpts/my_custom_dataset
mkdir -p $PATH_TO_SAVE_CKPTS
```
Adım 1'i çalıştırın:

```bash
python run_lightening.py test --config $YAML_PATH \
    --model.test_mode fill_memory \
    --out_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory.pth \
    --model.init_args.dataset_cfgs.fill_memory.root $DATASET_PATH/images \
    --model.init_args.dataset_cfgs.fill_memory.json_file $DATASET_PATH/annotations/custom_references_with_segm.json \
    --model.init_args.dataset_cfgs.fill_memory.memory_pkl $DATASET_PATH/annotations/custom_references_with_segm.pkl \
    --model.init_args.dataset_cfgs.fill_memory.memory_length $SHOT \
    --model.init_args.dataset_cfgs.fill_memory.cat_names $CAT_NAMES \
    --model.init_args.model_cfg.dataset_name $DATASET_NAME \
    --model.init_args.model_cfg.memory_bank_cfg.length $SHOT \
    --model.init_args.model_cfg.memory_bank_cfg.category_num $CATEGORY_NUM \
    --trainer.devices 1
```
### 2. Bellek bankasını sonradan işleme


```bash
python run_lightening.py test --config $YAML_PATH \
    --model.test_mode postprocess_memory \
    --ckpt_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory.pth \
    --out_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory_postprocessed.pth \
    --model.init_args.model_cfg.dataset_name $DATASET_NAME \
    --model.init_args.model_cfg.memory_bank_cfg.length $SHOT \
    --model.init_args.model_cfg.memory_bank_cfg.category_num $CATEGORY_NUM \
    --trainer.devices 1
```
### 3. Hedef görüntülerde çıkarım

Eğer `ONLINE_VIS` True olarak ayarlanırsa, tahmin sonuçları `results_analysis/my_custom_dataset/` dizinine kaydedilecek ve hesaplandıkça görüntülenecektir. ONLINE görselleştirme ile çalışmak çok daha yavaştır, DİKKAT EDİN.

Daha fazla ya da daha az segmentlenmiş örnek görmek için skor eşiği olan `VIS_THR` değerini değiştirmekten çekinmeyin.

```bash
ONLINE_VIS=True
VIS_THR=0.4
python run_lightening.py test --config $YAML_PATH \
    --model.test_mode test \
    --ckpt_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory_postprocessed.pth \
    --model.init_args.model_cfg.dataset_name $DATASET_NAME \
    --model.init_args.model_cfg.memory_bank_cfg.length $SHOT \
    --model.init_args.model_cfg.memory_bank_cfg.category_num $CATEGORY_NUM \
    --model.init_args.model_cfg.test.imgs_path $DATASET_PATH/images \
    --model.init_args.model_cfg.test.online_vis $ONLINE_VIS \
    --model.init_args.model_cfg.test.vis_thr $VIS_THR \
    --model.init_args.dataset_cfgs.test.root $DATASET_PATH/images \
    --model.init_args.dataset_cfgs.test.json_file $DATASET_PATH/annotations/custom_targets.json \
    --model.init_args.dataset_cfgs.test.cat_names $CAT_NAMES \
    --trainer.devices 1
```
### Sonuçlar

Performans metrikleri (yukarıdaki komutlarla tamamen aynı parametrelerle) şu şekilde olmalıdır:


```
BBOX RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.478

SEGM RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.458
```
Görsel sonuçlar `results_analysis/my_custom_dataset/` dizininde kaydedilir. Yöntemimizin yanlış negatifler için çalıştığını unutmayın, yani istenen sınıfların hiçbir örneğini içermeyen görsellerde de çalışır.

*Resimleri büyütmek için tıklayın ⬇️*

| Hedef görüntüde tekneler ⛵ (sol GT, sağ tahminler) | Hedef görüntüde kuşlar 🐦 (sol GT, sağ tahminler) |
|:----------------------:|:----------------------:|
| ![000000459673](https://github.com/user-attachments/assets/678dc15a-dd3b-49d5-9287-6290da16aa6b) | ![000000407180](https://github.com/user-attachments/assets/fe306e48-af49-4d83-ac82-76fac6c456d1) |

| Hedef görüntüde tekneler ve kuşlar ⛵🐦 (sol GT, sağ tahminler) | Hedef görüntüde tekne veya kuş yok 🚫 (sol GT, sağ tahminler) |
|:---------------------------------:|:----------------------------------:|
| ![000000517410](https://github.com/user-attachments/assets/9849b227-7f43-43d7-81ea-58010a623ad5) | ![000000460598](https://github.com/user-attachments/assets/7587700c-e09d-4cf6-8590-3df129c2568e) |


## 📚 Atıf

Bu çalışmayı kullanırsanız, lütfen bize atıfta bulunun:


```bibtex
@article{espinosa2025notimetotrain,
  title={No time to train! Training-Free Reference-Based Instance Segmentation},
  author={Miguel Espinosa and Chenhongyi Yang and Linus Ericsson and Steven McDonagh and Elliot J. Crowley},
  journal={arXiv preprint arXiv:2507.02798},
  year={2025},
  primaryclass={cs.CV}
}
```


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-23

---