<div align="right">
  <details>
    <summary >🌐 भाषा</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-CN">简体中文</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-TW">繁體中文</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ko">한국어</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=hi">हिन्दी</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=th">ไทย</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fr">Français</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=es">Español</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=it">Italiano</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ru">Русский</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pt">Português</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ar">العربية</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fa">فارسی</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=tr">Türkçe</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=vi">Tiếng Việt</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=id">Bahasa Indonesia</a>
      </div>
    </div>
  </details>
</div>

<div align="center">

# 🚀 ट्रेनिंग का समय नहीं है!  
### प्रशिक्षण-मुक्त संदर्भ-आधारित इंस्टेंस सेगमेंटेशन  
[![GitHub](https://img.shields.io/badge/%E2%80%8B-No%20Time%20To%20Train-black?logo=github)](https://github.com/miquel-espinosa/no-time-to-train)
[![Website](https://img.shields.io/badge/🌐-Project%20Page-grey)](https://miquel-espinosa.github.io/no-time-to-train/)
[![arXiv](https://img.shields.io/badge/arXiv-2507.02798-b31b1b)](https://arxiv.org/abs/2507.02798)

**राज्य-का-श्रेष्ठ (Papers with Code)**

[**_SOTA 1-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-1-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference)

[**_SOTA 10-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-10-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference)

[**_SOTA 30-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-30-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference)

</div>

---

> 🚨 **अपडेट (22 जुलाई 2025):** कस्टम डेटासेट के लिए निर्देश जोड़ दिए गए हैं!
> 
> 🔔 **अपडेट (16 जुलाई 2025):** कोड को निर्देशों के साथ अपडेट किया गया है!

---

## 📋 विषय सूची

- [🎯 मुख्य विशेषताएँ](#-highlights)
- [📜 सारांश](#-abstract)
- [🧠 संरचना](#-architecture)
- [🛠️ इंस्टॉलेशन निर्देश](#️-installation-instructions)
  - [1. रिपॉजिटरी क्लोन करें](#1-clone-the-repository)
  - [2. कॉन्डा वातावरण बनाएँ](#2-create-conda-environment)
  - [3. SAM2 और DinoV2 स्थापित करें](#3-install-sam2-and-dinov2)
  - [4. डेटासेट डाउनलोड करें](#4-download-datasets)
  - [5. SAM2 और DinoV2 चेकपॉइंट डाउनलोड करें](#5-download-sam2-and-dinov2-checkpoints)
- [📊 इनफेरेंस कोड: Few-shot COCO में 30-shot SOTA परिणाम पुन: उत्पन्न करें](#-inference-code)
  - [0. संदर्भ सेट बनाएँ](#0-create-reference-set)
  - [1. संदर्भों के साथ मेमोरी भरें](#1-fill-memory-with-references)
  - [2. मेमोरी बैंक का पोस्ट-प्रोसेस करें](#2-post-process-memory-bank)
  - [3. लक्ष्य छवियों पर इनफेरेंस करें](#3-inference-on-target-images)
  - [परिणाम](#results)
- [🔍 कस्टम डेटासेट](#-custom-dataset)
  - [0. कस्टम डेटासेट तैयार करें ⛵🐦](#0-prepare-a-custom-dataset)
  - [0.1 यदि केवल bbox एनोटेशन उपलब्ध हैं](#01-if-only-bbox-annotations-are-available)
  - [0.2 कोको एनोटेशन को पिकल फ़ाइल में बदलें](#02-convert-coco-annotations-to-pickle-file)
  - [1. संदर्भों के साथ मेमोरी भरें](#1-fill-memory-with-references)
  - [2. मेमोरी बैंक का पोस्ट-प्रोसेस करें](#2-post-process-memory-bank)
- [📚 संदर्भ](#-citation)


## 🎯 मुख्य विशेषताएँ
- 💡 **प्रशिक्षण-मुक्त**: कोई फाइन-ट्यूनिंग नहीं, कोई प्रॉम्प्ट इंजीनियरिंग नहीं—सिर्फ एक संदर्भ छवि।  
- 🖼️ **संदर्भ-आधारित**: केवल कुछ उदाहरणों से नए ऑब्जेक्ट्स को सेगमेंट करें।  
- 🔥 **SOTA प्रदर्शन**: COCO, PASCAL VOC, और Cross-Domain FSOD पर पिछले प्रशिक्षण-मुक्त तरीकों से बेहतर।

**लिंक:**
- 🧾 [**arXiv पेपर**](https://arxiv.org/abs/2507.02798)  
- 🌐 [**प्रोजेक्ट वेबसाइट**](https://miquel-espinosa.github.io/no-time-to-train/)  
- 📈 [**Papers with Code**](https://paperswithcode.com/paper/no-time-to-train-training-free-reference)

## 📜 सारांश

> छवि सेगमेंटेशन मॉडलों का प्रदर्शन ऐतिहासिक रूप से बड़े पैमाने पर एनोटेटेड डेटा एकत्र करने की उच्च लागत से सीमित रहा है। सेगमेंट एनीथिंग मॉडल (SAM) इस मूल समस्या को एक प्रॉम्प्टेबल, सेमांटिक्स-अज्ञेय, सेगमेंटेशन पैरेडाइम के माध्यम से कम करता है, फिर भी एक नई छवि को संसाधित करने के लिए मैनुअल विज़ुअल-प्रॉम्प्ट्स या जटिल डोमेन-निर्भर प्रॉम्प्ट-जनरेशन नियमों की आवश्यकता होती है। इस नई समस्या को कम करने की दिशा में, हमारा कार्य वस्तु सेगमेंटेशन के कार्य का अध्ययन करता है जब वैकल्पिक रूप से केवल संदर्भ छवियों का एक छोटा सेट उपलब्ध कराया जाता है। हमारी मुख्य अंतर्दृष्टि यह है कि फाउंडेशन मॉडलों द्वारा सीखे गए मजबूत सेमांटिक प्रायर का लाभ उठाया जाए, ताकि संदर्भ और लक्ष्य छवि के बीच संबंधित क्षेत्रों की पहचान की जा सके। हमें पता चला है कि ये संबंध डाउनस्ट्रीम कार्यों के लिए स्वचालित रूप से इंस्टेंस-स्तरीय सेगमेंटेशन मास्क उत्पन्न करने में सक्षम बनाते हैं और हम अपने विचारों को एक बहु-चरण, प्रशिक्षण-मुक्त विधि के माध्यम से प्रस्तुत करते हैं, जिसमें (1) मेमोरी बैंक निर्माण; (2) प्रतिनिधित्व एकत्रीकरण और (3) सेमांटिक-अवेयर फीचर मिलान शामिल है। हमारे प्रयोगों से सेगमेंटेशन मेट्रिक्स पर महत्वपूर्ण सुधार दिखते हैं, जिससे COCO FSOD (36.8% nAP), PASCAL VOC Few-Shot (71.2% nAP50) पर राज्य-का-श्रेष्ठ प्रदर्शन और Cross-Domain FSOD बेंचमार्क (22.4% nAP) पर मौजूदा प्रशिक्षण-मुक्त तरीकों से बेहतर परिणाम मिलते हैं।

![cdfsod-results-final-comic-sans-min](https://github.com/user-attachments/assets/ab302c02-c080-4042-99fc-0e181ba8abb9)




## 🧠 वास्तुकला

![training-free-architecture-comic-sans-min](https://github.com/user-attachments/assets/d84dd83a-505e-45a0-8ce3-98e1838017f9)


## 🛠️ स्थापना निर्देश

### 1. रिपॉजिटरी को क्लोन करें

```bash
git clone https://github.com/miquel-espinosa/no-time-to-train.git
cd no-time-to-train
```
### 2. कॉन्डा वातावरण बनाएं

हम आवश्यक पैकेजों के साथ एक कॉन्डा वातावरण बनाएंगे।

```bash
conda env create -f environment.yml
conda activate no-time-to-train
```
### 3. SAM2 और DinoV2 स्थापित करें

हम स्रोत से SAM2 और DinoV2 स्थापित करेंगे।

```bash
pip install -e .
cd dinov2
pip install -e .
cd ..
```
### 4. डेटासेट डाउनलोड करें

कृपया COCO डेटासेट डाउनलोड करें और इसे `data/coco` में रखें

### 5. SAM2 और DinoV2 चेकपॉइंट्स डाउनलोड करें

हम वही SAM2 चेकपॉइंट्स डाउनलोड करेंगे जो पेपर में उपयोग किए गए थे।
(ध्यान दें, हालांकि, SAM2.1 चेकपॉइंट्स पहले से उपलब्ध हैं और संभवतः बेहतर प्रदर्शन कर सकते हैं।)


```bash
mkdir -p checkpoints/dinov2
cd checkpoints
wget https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt
cd dinov2
wget https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth
cd ../..
```
## 📊 अनुमान कोड

⚠️ अस्वीकरण: यह अनुसंधान कोड है — इसमें थोड़ी अव्यवस्था हो सकती है!

### Few-shot COCO में 30-shot SOTA परिणामों की पुनरावृत्ति

उपयोगी वेरिएबल्स परिभाषित करें और परिणामों के लिए एक फ़ोल्डर बनाएं:



```bash
CONFIG=./no_time_to_train/new_exps/coco_fewshot_10shot_Sam2L.yaml
CLASS_SPLIT="few_shot_classes"
RESULTS_DIR=work_dirs/few_shot_results
SHOTS=30
SEED=33
GPUS=4

mkdir -p $RESULTS_DIR
FILENAME=few_shot_${SHOTS}shot_seed${SEED}.pkl
```
#### 0. संदर्भ सेट बनाएं


```bash
python no_time_to_train/dataset/few_shot_sampling.py \
        --n-shot $SHOTS \
        --out-path ${RESULTS_DIR}/${FILENAME} \
        --seed $SEED \
        --dataset $CLASS_SPLIT
```
#### 1. मेमोरी को रेफरेंस के साथ भरें


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode fill_memory \
                              --out_path ${RESULTS_DIR}/memory.ckpt \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.memory_pkl ${RESULTS_DIR}/${FILENAME} \
                              --model.init_args.dataset_cfgs.fill_memory.memory_length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
#### 2. पोस्ट-प्रोसेस मेमोरी बैंक


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode postprocess_memory \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --ckpt_path ${RESULTS_DIR}/memory.ckpt \
                              --out_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --trainer.devices 1
```
#### 3. लक्ष्य चित्रों पर अनुकरण (इन्फेरेंस)


```bash
python run_lightening.py test --config $CONFIG  \
                              --ckpt_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --model.init_args.test_mode test \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.model_cfg.dataset_name $CLASS_SPLIT \
                              --model.init_args.dataset_cfgs.test.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
यदि आप अनुमान परिणामों को ऑनलाइन (जैसे ही वे गणना होते हैं) देखना चाहते हैं, तो `no_time_to_train/models/Sam2MatchingBaseline_noAMG.py` की पंक्तियाँ 1746-1749 को अनकमेंट करें [यहाँ](https://github.com/miquel-espinosa/no-time-to-train/blob/main/no_time_to_train/models/Sam2MatchingBaseline_noAMG.py#L1746)।
स्कोर थ्रेशोल्ड `score_thr` पैरामीटर को आवश्यकतानुसार समायोजित करें, ताकि आप अधिक या कम सेगमेंटेड उदाहरण देख सकें।
अब छवियाँ `results_analysis/few_shot_classes/` में सहेजी जाएंगी। बाईं ओर की छवि ग्राउंड ट्रुथ दिखाती है, दाईं ओर की छवि हमारे प्रशिक्षण-मुक्त तरीके से पाए गए सेगमेंटेड उदाहरण दिखाती है।

ध्यान दें कि इस उदाहरण में हम `few_shot_classes` विभाजन का उपयोग कर रहे हैं, अतः हमें केवल इस विभाजन की कक्षाओं के सेगमेंटेड उदाहरण ही देखने की अपेक्षा करनी चाहिए (COCO की सभी कक्षाओं के नहीं)।

#### परिणाम

सत्यापन सेट में सभी छवियों को चलाने के बाद, आपको प्राप्त होना चाहिए:


```
BBOX RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.368

SEGM RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.342
```
---

## 🔍 कस्टम डेटासेट

हम अपने पाइपलाइन को कस्टम डेटासेट पर चलाने के लिए निर्देश प्रदान करते हैं। एनोटेशन फॉर्मेट हमेशा COCO फॉर्मेट में ही होता है।

> **संक्षेप में;** यदि आप सीधे *कस्टम डेटासेट्स* पर पूरी पाइपलाइन कैसे चलाएं देखना चाहते हैं, तो `scripts/matching_cdfsod_pipeline.sh` देखें, साथ ही CD-FSOD डेटासेट्स के उदाहरण स्क्रिप्ट्स (जैसे `scripts/dior_fish.sh`) भी देखें।

### 0. एक कस्टम डेटासेट तैयार करें ⛵🐦

मान लीजिए हम एक कस्टम डेटासेट में **नावें**⛵ और **पक्षी**🐦 का पता लगाना चाहते हैं। हमारी विधि का उपयोग करने के लिए आपको आवश्यकता होगी:
- प्रत्येक श्रेणी के लिए कम से कम 1 *एनोटेटेड* रेफरेंस इमेज (जैसे नाव के लिए 1 और पक्षी के लिए 1 रेफरेंस इमेज)
- कई लक्ष्य इमेज जिनमें हमारे वांछित श्रेणियों के इंस्टेंस मिल सकें।

हमने COCO इमेजेस के साथ कस्टम डेटासेट बनाने के लिए एक टॉय स्क्रिप्ट तैयार की है, **1-शॉट** सेटिंग के लिए।
```bash
python scripts/make_custom_dataset.py
```
यह निम्नलिखित फ़ोल्डर संरचना के साथ एक कस्टम डेटासेट बनाएगा:
```
data/my_custom_dataset/
    ├── annotations/
    │   ├── custom_references.json
    │   ├── custom_targets.json
    │   └── references_visualisations/
    │       ├── bird_1.jpg
    │       └── boat_1.jpg
    └── images/
        ├── 429819.jpg
        ├── 101435.jpg
        └── (all target and reference images)
```
**संदर्भ छवियों का दृश्यांकन (1-शॉट):**

| BIRD 🐦 के लिए 1-शॉट संदर्भ छवि | BOAT ⛵ के लिए 1-शॉट संदर्भ छवि |
|:-------------------------------:|:-------------------------------:|
| <img src="https://github.com/user-attachments/assets/e59e580d-a7db-42ac-b386-892af211fc85" alt="bird_1" width="500"/> | <img src="https://github.com/user-attachments/assets/f94ee025-ae37-4a45-9c3e-0cfe8f8cd2bc" alt="boat_1" width="500"/> |


### 0.1 यदि केवल bbox एनोटेशन उपलब्ध हैं

हम एक स्क्रिप्ट भी प्रदान करते हैं जो SAM2 का उपयोग करके इंस्टेंस-लेवल सेगमेंटेशन मास्क उत्पन्न करती है। यह तब उपयोगी है जब आपके पास संदर्भ छवियों के लिए केवल बॉक्स एनोटेशन उपलब्ध हों।


```bash
# Download sam_h checkpoint. Feel free to use more recent checkpoints (note: code might need to be adapted)
wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth -O checkpoints/sam_vit_h_4b8939.pth
# Run automatic instance segmentation from ground truth bounding boxes.
python no_time_to_train/dataset/sam_bbox_to_segm_batch.py \
    --input_json data/my_custom_dataset/annotations/custom_references.json \
    --image_dir data/my_custom_dataset/images \
    --sam_checkpoint checkpoints/sam_vit_h_4b8939.pth \
    --model_type vit_h \
    --device cuda \
    --batch_size 8 \
    --visualize
```
**इंस्टेंस-स्तरीय सेगमेंटेशन मास्क के साथ संदर्भ छवियाँ (gt बॉक्स से SAM2 द्वारा उत्पन्न, 1-शॉट):**

उत्पन्न सेगमेंटेशन मास्क का दृश्यावलोकन `data/my_custom_dataset/annotations/custom_references_with_SAM_segm/references_visualisations/` में सहेजा गया है।

| BIRD 🐦 के लिए 1-शॉट संदर्भ छवि (SAM द्वारा स्वचालित सेगमेंटेशन के साथ) | BOAT ⛵ के लिए 1-शॉट संदर्भ छवि (SAM द्वारा स्वचालित सेगमेंटेशन के साथ) |
|:---------------------------------:|:----------------------------------:|
| <img src="https://github.com/user-attachments/assets/65d38dc4-1454-43cd-9600-e8efc67b3a82" alt="bird_1_with_SAM_segm" width="500"/> | <img src="https://github.com/user-attachments/assets/43a558ad-50ca-4715-8285-9aa3268843c6" alt="boat_1_with_SAM_segm" width="500"/> |

### 0.2 कोको एनोटेशन को पिकल फ़ाइल में बदलें




```bash
python no_time_to_train/dataset/coco_to_pkl.py \
    data/my_custom_dataset/annotations/custom_references_with_segm.json \
    data/my_custom_dataset/annotations/custom_references_with_segm.pkl \
    1
```
### 1. मेमोरी को संदर्भों से भरें

सबसे पहले, उपयोगी वेरिएबल्स परिभाषित करें और परिणामों के लिए एक फ़ोल्डर बनाएं। लेबल्स का सही दृश्यकरण सुनिश्चित करने के लिए, वर्ग नामों को उसी क्रम में रखना चाहिए जैसा कि वे json फ़ाइल में category id के अनुसार हैं। उदाहरण के लिए, `bird` का category id `16` है, `boat` का category id `9` है। अतः, `CAT_NAMES=boat,bird`।


```bash
DATASET_NAME=my_custom_dataset
DATASET_PATH=data/my_custom_dataset
CAT_NAMES=boat,bird
CATEGORY_NUM=2
SHOT=1
YAML_PATH=no_time_to_train/pl_configs/matching_cdfsod_template.yaml
PATH_TO_SAVE_CKPTS=./tmp_ckpts/my_custom_dataset
mkdir -p $PATH_TO_SAVE_CKPTS
```
चरण 1 चलाएँ:

```bash
python run_lightening.py test --config $YAML_PATH \
    --model.test_mode fill_memory \
    --out_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory.pth \
    --model.init_args.dataset_cfgs.fill_memory.root $DATASET_PATH/images \
    --model.init_args.dataset_cfgs.fill_memory.json_file $DATASET_PATH/annotations/custom_references_with_segm.json \
    --model.init_args.dataset_cfgs.fill_memory.memory_pkl $DATASET_PATH/annotations/custom_references_with_segm.pkl \
    --model.init_args.dataset_cfgs.fill_memory.memory_length $SHOT \
    --model.init_args.dataset_cfgs.fill_memory.cat_names $CAT_NAMES \
    --model.init_args.model_cfg.dataset_name $DATASET_NAME \
    --model.init_args.model_cfg.memory_bank_cfg.length $SHOT \
    --model.init_args.model_cfg.memory_bank_cfg.category_num $CATEGORY_NUM \
    --trainer.devices 1
```
### 2. पोस्ट-प्रोसेस मेमोरी बैंक


```bash
python run_lightening.py test --config $YAML_PATH \
    --model.test_mode postprocess_memory \
    --ckpt_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory.pth \
    --out_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory_postprocessed.pth \
    --model.init_args.model_cfg.dataset_name $DATASET_NAME \
    --model.init_args.model_cfg.memory_bank_cfg.length $SHOT \
    --model.init_args.model_cfg.memory_bank_cfg.category_num $CATEGORY_NUM \
    --trainer.devices 1
```
### 3. लक्ष्य इमेजों पर अनुमान

यदि `ONLINE_VIS` को True पर सेट किया गया है, तो भविष्यवाणी के परिणाम `results_analysis/my_custom_dataset/` में सहेजे जाएंगे और जैसे ही वे गणना किए जाते हैं, वैसे ही प्रदर्शित होंगे। ध्यान दें कि ऑनलाइन विज़ुअलाइज़ेशन के साथ चलाना काफी धीमा होता है।

आप स्कोर थ्रेशोल्ड `VIS_THR` को बदल सकते हैं ताकि अधिक या कम सेगमेंटेड इंस्टेंस देख सकें।

```bash
ONLINE_VIS=True
VIS_THR=0.4
python run_lightening.py test --config $YAML_PATH \
    --model.test_mode test \
    --ckpt_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory_postprocessed.pth \
    --model.init_args.model_cfg.dataset_name $DATASET_NAME \
    --model.init_args.model_cfg.memory_bank_cfg.length $SHOT \
    --model.init_args.model_cfg.memory_bank_cfg.category_num $CATEGORY_NUM \
    --model.init_args.model_cfg.test.imgs_path $DATASET_PATH/images \
    --model.init_args.model_cfg.test.online_vis $ONLINE_VIS \
    --model.init_args.model_cfg.test.vis_thr $VIS_THR \
    --model.init_args.dataset_cfgs.test.root $DATASET_PATH/images \
    --model.init_args.dataset_cfgs.test.json_file $DATASET_PATH/annotations/custom_targets.json \
    --model.init_args.dataset_cfgs.test.cat_names $CAT_NAMES \
    --trainer.devices 1
```
### परिणाम

प्रदर्शन मीट्रिक (ऊपर दिए गए कमांड्स के समान सटीक पैरामीटर के साथ) निम्नलिखित होनी चाहिए:


```
BBOX RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.478

SEGM RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.458
```
दृश्य परिणाम `results_analysis/my_custom_dataset/` में सहेजे जाते हैं। ध्यान दें कि हमारी विधि झूठे नकारात्मक मामलों के लिए काम करती है, यानी वे छवियाँ जिनमें वांछित वर्गों की कोई भी इकाई नहीं होती है।

*छवियों पर क्लिक करें बड़ा देखने के लिए ⬇️*

| नावों वाली लक्षित छवि ⛵ (बाएं GT, दाएं पूर्वानुमान) | पक्षियों वाली लक्षित छवि 🐦 (बाएं GT, दाएं पूर्वानुमान) |
|:----------------------:|:----------------------:|
| ![000000459673](https://github.com/user-attachments/assets/678dc15a-dd3b-49d5-9287-6290da16aa6b) | ![000000407180](https://github.com/user-attachments/assets/fe306e48-af49-4d83-ac82-76fac6c456d1) |

| नावों और पक्षियों वाली लक्षित छवि ⛵🐦 (बाएं GT, दाएं पूर्वानुमान) | बिना नाव या पक्षियों वाली लक्षित छवि 🚫 (बाएं GT, दाएं पूर्वानुमान) |
|:---------------------------------:|:----------------------------------:|
| ![000000517410](https://github.com/user-attachments/assets/9849b227-7f43-43d7-81ea-58010a623ad5) | ![000000460598](https://github.com/user-attachments/assets/7587700c-e09d-4cf6-8590-3df129c2568e) |


## 📚 संदर्भ

यदि आप इस कार्य का उपयोग करते हैं, तो कृपया हमें उद्धृत करें:


```bibtex
@article{espinosa2025notimetotrain,
  title={No time to train! Training-Free Reference-Based Instance Segmentation},
  author={Miguel Espinosa and Chenhongyi Yang and Linus Ericsson and Steven McDonagh and Elliot J. Crowley},
  journal={arXiv preprint arXiv:2507.02798},
  year={2025},
  primaryclass={cs.CV}
}
```


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-23

---