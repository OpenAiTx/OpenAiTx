<div align="right">
  <details>
    <summary >üåê Idioma</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-TW">ÁπÅÈ´î‰∏≠Êñá</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=hi">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=th">‡πÑ‡∏ó‡∏¢</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fr">Fran√ßais</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=es">Espa√±ol</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=it">Italiano</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ru">–†—É—Å—Å–∫–∏–π</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pt">Portugu√™s</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ar">ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fa">ŸÅÿßÿ±ÿ≥€å</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=tr">T√ºrk√ße</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=vi">Ti·∫øng Vi·ªát</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=id">Bahasa Indonesia</a>
      </div>
    </div>
  </details>
</div>

<div align="center">

# üöÄ ¬°No Time to Train!  
### Segmentaci√≥n de Instancias Basada en Referencias y Sin Entrenamiento  
[![GitHub](https://img.shields.io/badge/%E2%80%8B-No%20Time%20To%20Train-black?logo=github)](https://github.com/miquel-espinosa/no-time-to-train)
[![Website](https://img.shields.io/badge/üåê-Project%20Page-grey)](https://miquel-espinosa.github.io/no-time-to-train/)
[![arXiv](https://img.shields.io/badge/arXiv-2507.02798-b31b1b)](https://arxiv.org/abs/2507.02798)

**Estado del arte (Papers with Code)**

[**_1-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-1-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference)

[**_10-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-10-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference)

[**_30-shot_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-30-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference)

</div>

---

> üîî **Actualizaci√≥n (julio 2025):** ¬°El c√≥digo ha sido actualizado con instrucciones!

---

## üìã Tabla de contenidos

- [üéØ Aspectos destacados](#-aspectos-destacados)
- [üìú Resumen](#-resumen)
- [üß† Arquitectura](#-arquitectura)
- [üõ†Ô∏è Instrucciones de instalaci√≥n](#Ô∏è-instrucciones-de-instalacion)
  - [1. Clonar el repositorio](#1-clonar-el-repositorio)
  - [2. Crear entorno conda](#2-crear-entorno-conda)
  - [3. Instalar SAM2 y DinoV2](#3-instalar-sam2-y-dinov2)
  - [4. Descargar conjuntos de datos](#4-descargar-conjuntos-de-datos)
  - [5. Descargar puntos de control de SAM2 y DinoV2](#5-descargar-puntos-de-control-de-sam2-y-dinov2)
- [üìä C√≥digo de inferencia: Reproducir resultados SOTA de 30-shot en Few-shot COCO](#-codigo-de-inferencia)
  - [0. Crear conjunto de referencia](#0-crear-conjunto-de-referencia)
  - [1. Llenar la memoria con referencias](#1-llenar-la-memoria-con-referencias)
  - [2. Post-procesar banco de memoria](#2-post-procesar-banco-de-memoria)
  - [3. Inferencia en im√°genes objetivo](#3-inferencia-en-imagenes-objetivo)
  - [Resultados](#resultados)
- [üîç Citaci√≥n](#-citacion)


## üéØ Aspectos destacados
- üí° **Sin entrenamiento**: Sin fine-tuning, sin ingenier√≠a de prompts‚Äîsolo una imagen de referencia.  
- üñºÔ∏è **Basado en referencia**: Segmenta nuevos objetos usando solo unos pocos ejemplos.  
- üî• **Rendimiento SOTA**: Supera a los enfoques sin entrenamiento previos en COCO, PASCAL VOC y Cross-Domain FSOD.

**Enlaces:**
- üßæ [**Art√≠culo en arXiv**](https://arxiv.org/abs/2507.02798)  
- üåê [**Sitio web del proyecto**](https://miquel-espinosa.github.io/no-time-to-train/)  
- üìà [**Papers with Code**](https://paperswithcode.com/paper/no-time-to-train-training-free-reference)

## üìú Resumen

> El rendimiento de los modelos de segmentaci√≥n de im√°genes ha estado hist√≥ricamente limitado por el alto coste de recopilar grandes vol√∫menes de datos anotados. El Segment Anything Model (SAM) alivia este problema original mediante un paradigma de segmentaci√≥n con prompts, independiente de la sem√°ntica, aunque a√∫n requiere prompts visuales manuales o reglas complejas de generaci√≥n de prompts dependientes del dominio para procesar una nueva imagen. Para reducir esta nueva carga, nuestro trabajo investiga la tarea de segmentaci√≥n de objetos cuando se proporciona, de forma alternativa, solo un peque√±o conjunto de im√°genes de referencia. Nuestra principal idea es aprovechar fuertes priors sem√°nticos, aprendidos por modelos fundacionales, para identificar regiones correspondientes entre una imagen de referencia y una imagen objetivo. Encontramos que las correspondencias permiten la generaci√≥n autom√°tica de m√°scaras de segmentaci√≥n a nivel de instancia para tareas posteriores e implementamos nuestras ideas mediante un m√©todo de m√∫ltiples etapas y sin entrenamiento que incorpora (1) construcci√≥n de banco de memoria; (2) agregaci√≥n de representaciones y (3) emparejamiento de caracter√≠sticas con conciencia sem√°ntica. Nuestros experimentos muestran mejoras significativas en las m√©tricas de segmentaci√≥n, logrando un rendimiento de vanguardia en COCO FSOD (36.8% nAP), PASCAL VOC Few-Shot (71.2% nAP50) y superando los enfoques sin entrenamiento existentes en el benchmark Cross-Domain FSOD (22.4% nAP).

![cdfsod-results-final-comic-sans-min](https://github.com/user-attachments/assets/ab302c02-c080-4042-99fc-0e181ba8abb9)


## üß† Arquitectura

![training-free-architecture-comic-sans-min](https://github.com/user-attachments/assets/d84dd83a-505e-45a0-8ce3-98e1838017f9)


## üõ†Ô∏è Instrucciones de instalaci√≥n

### 1. Clonar el repositorio


```bash
git clone https://github.com/miquel-espinosa/no-time-to-train.git
cd no-time-to-train
```
### 2. Crear un entorno conda

Crearemos un entorno conda con los paquetes requeridos.

```bash
conda env create -f environment.yml
conda activate no-time-to-train
```
### 3. Instalar SAM2 y DinoV2

Instalaremos SAM2 y DinoV2 desde el c√≥digo fuente.

```bash
pip install -e .
cd dinov2
pip install -e .
cd ..
```
### 4. Descargar conjuntos de datos

Por favor, descargue el conjunto de datos COCO y col√≥quelo en `data/coco`

### 5. Descargar puntos de control de SAM2 y DinoV2

Descargaremos exactamente los puntos de control de SAM2 utilizados en el art√≠culo.
(Sin embargo, tenga en cuenta que los puntos de control SAM2.1 ya est√°n disponibles y podr√≠an tener un mejor rendimiento.)


```bash
mkdir -p checkpoints/dinov2
cd checkpoints
wget https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt
cd dinov2
wget https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth
cd ../..
```
## üìä C√≥digo de inferencia

‚ö†Ô∏è Descargo de responsabilidad: Este es c√≥digo de investigaci√≥n ‚Äî ¬°espere un poco de caos!

### Reproducci√≥n de los resultados SOTA de 30 ejemplos en Few-shot COCO

Defina variables √∫tiles y cree una carpeta para los resultados:



```bash
CONFIG=./dev_hongyi/new_exps/coco_fewshot_10shot_Sam2L.yaml
CLASS_SPLIT="few_shot_classes"
RESULTS_DIR=work_dirs/few_shot_results
SHOTS=30
SEED=33
GPUS=4

mkdir -p $RESULTS_DIR
FILENAME=few_shot_${SHOTS}shot_seed${SEED}.pkl
```
#### 0. Crear conjunto de referencia


```bash
python dev_hongyi/dataset/few_shot_sampling.py \
        --n-shot $SHOTS \
        --out-path ${RESULTS_DIR}/${FILENAME} \
        --seed $SEED \
        --dataset $CLASS_SPLIT
```
#### 1. Llenar la memoria con referencias


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode fill_memory \
                              --out_path ${RESULTS_DIR}/memory.ckpt \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.memory_pkl ${RESULTS_DIR}/${FILENAME} \
                              --model.init_args.dataset_cfgs.fill_memory.memory_length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
#### 2. Posprocesar banco de memoria


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode postprocess_memory \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --ckpt_path ${RESULTS_DIR}/memory.ckpt \
                              --out_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --trainer.devices 1
```
#### 3. Inferencia en im√°genes objetivo


```bash
python run_lightening.py test --config $CONFIG  \
                              --ckpt_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --model.init_args.test_mode test \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.model_cfg.dataset_name $CLASS_SPLIT \
                              --model.init_args.dataset_cfgs.test.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
Si deseas ver los resultados de inferencia en l√≠nea (a medida que se calculan), descomenta las l√≠neas 1746-1749 en `dev_hongyi/models/Sam2MatchingBaseline_noAMG.py` [aqu√≠](https://github.com/miquel-espinosa/no-time-to-train/blob/main/dev_hongyi/models/Sam2MatchingBaseline_noAMG.py#L1746).
Ajusta el par√°metro de umbral de puntuaci√≥n `score_thr` seg√∫n sea necesario para ver m√°s o menos instancias segmentadas.
Las im√°genes ahora se guardar√°n en `results_analysis/few_shot_classes/`. La imagen de la izquierda muestra la verdad de terreno, la imagen de la derecha muestra las instancias segmentadas encontradas por nuestro m√©todo sin entrenamiento.

Ten en cuenta que en este ejemplo estamos usando la partici√≥n `few_shot_classes`, por lo tanto, solo deber√≠amos esperar ver instancias segmentadas de las clases en esta partici√≥n (no todas las clases de COCO).

#### Resultados

Despu√©s de ejecutar todas las im√°genes en el conjunto de validaci√≥n, deber√≠as obtener:


```
BBOX RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.368

SEGM RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.342
```
---


## üîç Citation

If you use this work, please cite us:

```bibtex
@article{espinosa2025notimetotrain,
  title={No time to train! Training-Free Reference-Based Instance Segmentation},
  author={Miguel Espinosa and Chenhongyi Yang and Linus Ericsson and Steven McDonagh and Elliot J. Crowley},
  journal={arXiv preprint arXiv:2507.02798},
  year={2025},
  primaryclass={cs.CV}
}
```

---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-22

---