<div align="right">
  <details>
    <summary >🌐 زبان</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=en">انگلیسی</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-CN">简体中文</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-TW">繁體中文</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ko">한국어</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=hi">हिन्दी</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=th">ไทย</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fr">Français</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=es">Español</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=it">Italiano</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ru">Русский</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pt">Português</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ar">العربية</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fa">فارسی</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=tr">Türkçe</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=vi">Tiếng Việt</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=id">Bahasa Indonesia</a>
      </div>
    </div>
  </details>
</div>

<div align="center">

# 🚀 زمان برای آموزش نیست!  
### تقسیم‌بندی نمونه مبتنی بر مرجع بدون نیاز به آموزش  
[![GitHub](https://img.shields.io/badge/%E2%80%8B-No%20Time%20To%20Train-black?logo=github)](https://github.com/miquel-espinosa/no-time-to-train)
[![Website](https://img.shields.io/badge/🌐-Project%20Page-grey)](https://miquel-espinosa.github.io/no-time-to-train/)
[![arXiv](https://img.shields.io/badge/arXiv-2507.02798-b31b1b)](https://arxiv.org/abs/2507.02798)

**پیشرفته‌ترین روش‌های روز (Papers with Code)**

[**_1-نمونه‌ای_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-1-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference)

[**_10-نمونه‌ای_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-10-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference)

[**_30-نمونه‌ای_**](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-30-shot)](https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference)

</div>

---

> 🔔 **به‌روزرسانی (ژوئیه ۲۰۲۵):** کد با دستورالعمل‌ها به‌روزرسانی شد!

---

## 📋 فهرست مطالب

- [🎯 نکات برجسته](#-highlights)
- [📜 چکیده](#-abstract)
- [🧠 معماری](#-architecture)
- [🛠️ دستورالعمل‌های نصب](#️-installation-instructions)
  - [1. کلون کردن مخزن](#1-clone-the-repository)
  - [2. ساخت محیط کاندا](#2-create-conda-environment)
  - [3. نصب SAM2 و DinoV2](#3-install-sam2-and-dinov2)
  - [4. دانلود داده‌ها](#4-download-datasets)
  - [5. دانلود چک‌پوینت‌های SAM2 و DinoV2](#5-download-sam2-and-dinov2-checkpoints)
- [📊 کد استنتاج: بازتولید نتایج ۳۰-نمونه‌ای SOTA در Few-shot COCO](#-inference-code)
  - [0. ساخت مجموعه مرجع](#0-create-reference-set)
  - [1. پر کردن حافظه با مراجع](#1-fill-memory-with-references)
  - [2. پردازش پسین بانک حافظه](#2-post-process-memory-bank)
  - [3. استنتاج روی تصاویر هدف](#3-inference-on-target-images)
  - [نتایج](#results)
- [🔍 استناد](#-citation)


## 🎯 نکات برجسته
- 💡 **بدون نیاز به آموزش:** بدون یادگیری مجدد، بدون مهندسی پرامپت—تنها یک تصویر مرجع کافی است.  
- 🖼️ **مبتنی بر مرجع:** تقسیم‌بندی اشیاء جدید تنها با چند نمونه مرجع.  
- 🔥 **عملکرد SOTA:** بهتر از روش‌های بدون آموزش قبلی روی COCO، PASCAL VOC و Cross-Domain FSOD.

**لینک‌ها:**
- 🧾 [**مقاله arXiv**](https://arxiv.org/abs/2507.02798)  
- 🌐 [**وب‌سایت پروژه**](https://miquel-espinosa.github.io/no-time-to-train/)  
- 📈 [**Papers with Code**](https://paperswithcode.com/paper/no-time-to-train-training-free-reference)

## 📜 چکیده

> عملکرد مدل‌های تقسیم‌بندی تصویر همواره با هزینه بالای گردآوری داده‌های برچسب‌خورده در مقیاس وسیع محدود بوده است. مدل Segment Anything (SAM) این مشکل اولیه را از طریق پارادایم تقسیم‌بندی مبتنی بر پرامپت و بی‌تفاوت به معنا تا حدی حل می‌کند، اما همچنان نیازمند پرامپت‌های بصری دستی یا قوانین پیچیده تولید پرامپت وابسته به دامنه برای پردازش یک تصویر جدید است. به منظور کاهش این بار جدید، کار ما به بررسی وظیفه تقسیم‌بندی اشیاء با تنها یک مجموعه کوچک از تصاویر مرجع می‌پردازد. بینش کلیدی ما استفاده از پیش‌فرض‌های معنایی قوی است که توسط مدل‌های پایه آموخته شده‌اند تا نواحی متناظر بین یک تصویر مرجع و تصویر هدف را شناسایی کنیم. ما دریافتیم که این تطابق‌ها امکان تولید خودکار ماسک‌های تقسیم‌بندی نمونه‌ای را برای وظایف پایین‌دستی فراهم می‌کنند و ایده‌های خود را از طریق یک روش چندمرحله‌ای، بدون آموزش، شامل (۱) ساخت بانک حافظه؛ (۲) تجمیع نمایش‌ها و (۳) تطبیق ویژگی معنایی پیاده‌سازی می‌کنیم. آزمایش‌های ما بهبود قابل توجهی در معیارهای تقسیم‌بندی نشان داده و به عملکرد پیشرفته در COCO FSOD (۳۶.۸٪ nAP)، PASCAL VOC Few-Shot (۷۱.۲٪ nAP50) و پیشی گرفتن از روش‌های بدون آموزش موجود در محک Cross-Domain FSOD (۲۲.۴٪ nAP) منجر شده است.

![cdfsod-results-final-comic-sans-min](https://github.com/user-attachments/assets/ab302c02-c080-4042-99fc-0e181ba8abb9)


## 🧠 معماری

![training-free-architecture-comic-sans-min](https://github.com/user-attachments/assets/d84dd83a-505e-45a0-8ce3-98e1838017f9)


## 🛠️ دستورالعمل‌های نصب

### 1. کلون کردن مخزن


```bash
git clone https://github.com/miquel-espinosa/no-time-to-train.git
cd no-time-to-train
```
### ۲. ایجاد محیط conda

ما یک محیط conda با بسته‌های مورد نیاز ایجاد خواهیم کرد.

```bash
conda env create -f environment.yml
conda activate no-time-to-train
```
### 3. نصب SAM2 و DinoV2

ما SAM2 و DinoV2 را از سورس نصب خواهیم کرد.

```bash
pip install -e .
cd dinov2
pip install -e .
cd ..
```
### ۴. دانلود دیتاست‌ها

لطفاً دیتاست COCO را دانلود کرده و در مسیر `data/coco` قرار دهید

### ۵. دانلود چک‌پوینت‌های SAM2 و DinoV2

ما دقیقاً همان چک‌پوینت‌های SAM2 که در مقاله استفاده شده را دانلود خواهیم کرد.
(با این حال توجه داشته باشید که چک‌پوینت‌های SAM2.1 هم‌اکنون در دسترس هستند و ممکن است عملکرد بهتری داشته باشند.)


```bash
mkdir -p checkpoints/dinov2
cd checkpoints
wget https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt
cd dinov2
wget https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth
cd ../..
```
## 📊 کد استنتاج

⚠️ سلب مسئولیت: این کد پژوهشی است — انتظار کمی بی‌نظمی داشته باشید!

### بازتولید نتایج 30-شات SOTA در Few-shot COCO

متغیرهای مفید را تعریف کنید و یک پوشه برای نتایج ایجاد نمایید:



```bash
CONFIG=./dev_hongyi/new_exps/coco_fewshot_10shot_Sam2L.yaml
CLASS_SPLIT="few_shot_classes"
RESULTS_DIR=work_dirs/few_shot_results
SHOTS=30
SEED=33
GPUS=4

mkdir -p $RESULTS_DIR
FILENAME=few_shot_${SHOTS}shot_seed${SEED}.pkl
```
#### ۰. ایجاد مجموعه مرجع


```bash
python dev_hongyi/dataset/few_shot_sampling.py \
        --n-shot $SHOTS \
        --out-path ${RESULTS_DIR}/${FILENAME} \
        --seed $SEED \
        --dataset $CLASS_SPLIT
```
#### 1. حافظه را با ارجاعات پر کنید


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode fill_memory \
                              --out_path ${RESULTS_DIR}/memory.ckpt \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.memory_pkl ${RESULTS_DIR}/${FILENAME} \
                              --model.init_args.dataset_cfgs.fill_memory.memory_length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
#### 2. پردازش پس‌ازآن بانک حافظه


```bash
python run_lightening.py test --config $CONFIG \
                              --model.test_mode postprocess_memory \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --ckpt_path ${RESULTS_DIR}/memory.ckpt \
                              --out_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --trainer.devices 1
```
#### 3. استنتاج بر روی تصاویر هدف


```bash
python run_lightening.py test --config $CONFIG  \
                              --ckpt_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --model.init_args.test_mode test \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.model_cfg.dataset_name $CLASS_SPLIT \
                              --model.init_args.dataset_cfgs.test.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS
```
اگر می‌خواهید نتایج استنتاج را به صورت آنلاین (در حین محاسبه) مشاهده کنید، خطوط 1746 تا 1749 را در فایل `dev_hongyi/models/Sam2MatchingBaseline_noAMG.py` [اینجا](https://github.com/miquel-espinosa/no-time-to-train/blob/main/dev_hongyi/models/Sam2MatchingBaseline_noAMG.py#L1746) از حالت کامنت خارج کنید.
پارامتر آستانه امتیاز `score_thr` را بسته به نیاز تنظیم کنید تا نمونه‌های بخش‌بندی شده بیشتری یا کمتری را مشاهده کنید.
تصاویر اکنون در مسیر `results_analysis/few_shot_classes/` ذخیره خواهند شد. تصویر سمت چپ حقیقت زمین را نشان می‌دهد و تصویر سمت راست نمونه‌های بخش‌بندی شده‌ای را که توسط روش بدون آموزش ما یافته شده‌اند، نمایش می‌دهد.

توجه داشته باشید که در این مثال ما از تقسیم‌بندی `few_shot_classes` استفاده می‌کنیم، بنابراین فقط باید انتظار داشته باشیم نمونه‌های بخش‌بندی شده‌ی کلاس‌های این تقسیم‌بندی را ببینیم (نه همه کلاس‌های COCO).

#### نتایج

پس از اجرای تمام تصاویر در مجموعه اعتبارسنجی، باید موارد زیر را به دست آورید:


```
BBOX RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.368

SEGM RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.342
```
---


## 🔍 Citation

If you use this work, please cite us:

```bibtex
@article{espinosa2025notimetotrain,
  title={No time to train! Training-Free Reference-Based Instance Segmentation},
  author={Miguel Espinosa and Chenhongyi Yang and Linus Ericsson and Steven McDonagh and Elliot J. Crowley},
  journal={arXiv preprint arXiv:2507.02798},
  year={2025},
  primaryclass={cs.CV}
}
```

---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-22

---