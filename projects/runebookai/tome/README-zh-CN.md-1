{
  "id": 1,
  "origin": "# Tome - Magical AI Spellbook\n\n<img src=\"https://raw.githubusercontent.com/runebookai/tome/main/static/images/repo-header.png\" alt=\"Tome\" />\n\n<p align=\"center\">\n    <code>a magical desktop app that puts the power of LLMs and MCP in the hands of everyone</code>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://discord.gg/9CH6us29YA\" target=\"_blank\"><img src=\"https://img.shields.io/discord/1365100902561742868?logo=discord&logoColor=fff&label=Join%20Us!&color=9D7CD8\" alt=\"Join Us on Discord\" /></a>\n    <a href=\"https://opensource.org/licenses/Apache-2.0\" target=\"_blank\"><img src=\"https://img.shields.io/badge/License-Apache_2.0-blue.svg\" alt=\"License: Apache 2.0\" /></a>\n    <a href=\"https://github.com/runebookai/tome/releases\" target=\"_blank\"><img src=\"https://img.shields.io/github/v/release/runebookai/tome\" alt=\"GitHub Release\" /></a>\n</p>\n\n<p align=\"center\">\n    🔮 Download the Tome Desktop App: <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_x64-setup.exe\">Windows</a> | <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_aarch64.dmg\">MacOS</a>\n</p>\n\n# Tome\n\nTome is a desktop app that lets **anyone** harness the magic of LLMs and MCP. Download Tome, connect any local or remote LLM and hook it up to thousands of MCP servers to create your own magical AI-powered spellbook.\n\n🫥 Want it to be 100% local, 100% private? Use Ollama and Qwen3 with only local MCP servers to cast spells in your own pocket universe. ⚡ Want state of the art cloud models with the latest remote MCP servers? You can have that too. It's all up to you!\n\n🏗️ This is a Technical Preview so bear in mind things will be rough around the edges. [Join us on Discord](https://discord.gg/9CH6us29YA) to share tips, tricks, and issues you run into. Star this repo to stay on top of updates and feature releases!\n\n## 🪄 Features\n\n- 🧙 **Streamlined Beginner Friendly Experience**\n  - Simply download and install Tome and hook up the LLM of your choice\n  - No fiddling with JSON, Docker, python or node\n- 🤖 **AI Model Support**\n  - **Remote**: Google Gemini, OpenAI, any OpenAI API-compatible endpoint\n  - **Local**: Ollama, LM Studio, Cortex, any OpenAI API-compatible endpoint\n- 🔮 **Enhanced MCP support**\n  - UI to install, remove, turn on/off MCP servers\n  - npm, uvx, node, python MCP servers all supported out of box\n- 🏪 **Integration into [Smithery.ai](https://smithery.ai) registry**\n  - Thousands of MCP servers available via one-click installation\n- ✏️ **Customization of context windows and temperature**\n- 🧰 **Native support for tool calls and reasoning models**\n  - UI enhancements that clearly delineate tool calls and thinking messages\n\n## Demo\n\nhttps://github.com/user-attachments/assets/0775d100-3eba-4219-9e2f-360a01f28cce\n\n# Getting Started\n\n## Requirements\n\n- MacOS or Windows (Linux coming soon!)\n- LLM Provider of your choice: [Ollama](https://ollama.com/) or [Gemini API key](https://aistudio.google.com/app/apikey) are easy/free\n- [Download the latest release of Tome](https://github.com/runebookai/tome/releases)\n\n## Quickstart\n\n1. Install [Tome](https://github.com/runebookai/tome/releases)\n2. Connect your preferred LLM provider - OpenAI, Ollama and Gemini are preset but you can also add providers like LM Studio by using http://localhost:1234/v1 as the URL\n3. Open the MCP tab in Tome and install your first [MCP server](https://github.com/modelcontextprotocol/servers) (Fetch is an easy one to get started with, just paste `uvx mcp-server-fetch` into the server field).\n4. Chat with your MCP-powered model! Ask it to fetch the top story on Hacker News.\n\n# Vision\n\nWe want to make local LLMs and MCP accessible to everyone. We're building a tool that allows you to be creative with LLMs, regardless\nof whether you're an engineer, tinkerer, hobbyist, or anyone in between.\n\n## Core Principles\n\n- **Tome is local first:** You are in control of where your data goes.\n- **Tome is for everyone:** You shouldn't have to manage programming languages, package managers, or json config files.\n\n## What's Next\n\nWe've gotten a lot of amazing feedback in the last few weeks since releasing Tome but we've got big plans for the future. We want to break LLMs out of their chatbox, and we've got a lot of features coming to help y'all do that.\n\n- Scheduled tasks: LLMs should be doing helpful things even when you're not in front of the computer.\n- Native integrations: MCP servers are a great way to access tools and information, but we want to add more powerful integrations to interact with LLMs in unique. ways\n- App builder: we believe long term that the best experiences will not be in a chat interface. We have plans to add additional tools that will enable you to create powerful applications and workflows.\n- ??? Let us know what you'd like to see! Join our community via the links below, we'd love to hear from you.\n\n# Community\n\n[Discord](https://discord.gg/9CH6us29YA) [Blog](https://blog.runebook.ai) [Bluesky](https://bsky.app/profile/gettome.app) [Twitter](https://twitter.com/get_tome) \n",
  "origin_sha": "rWLnVvqJwvr9v1fqiTn2LWjwaQbT+x1VplsLjSlpRbU=",
  "translate": "# Tome - 魔法 AI 魔法书\n\n<img src=\"https://raw.githubusercontent.com/runebookai/tome/main/static/images/repo-header.png\" alt=\"Tome\" />\n\n<p align=\"center\">\n    <code>一个神奇的桌面应用，让每个人都能掌控 LLM 和 MCP 的力量</code>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://discord.gg/9CH6us29YA\" target=\"_blank\"><img src=\"https://img.shields.io/discord/1365100902561742868?logo=discord&logoColor=fff&label=Join%20Us!&color=9D7CD8\" alt=\"Join Us on Discord\" /></a>\n    <a href=\"https://opensource.org/licenses/Apache-2.0\" target=\"_blank\"><img src=\"https://img.shields.io/badge/License-Apache_2.0-blue.svg\" alt=\"License: Apache 2.0\" /></a>\n    <a href=\"https://github.com/runebookai/tome/releases\" target=\"_blank\"><img src=\"https://img.shields.io/github/v/release/runebookai/tome\" alt=\"GitHub Release\" /></a>\n</p>\n\n<p align=\"center\">\n    🔮 下载 Tome 桌面应用: <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_x64-setup.exe\">Windows</a> | <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_aarch64.dmg\">MacOS</a>\n</p>\n\n# Tome\n\nTome 是一个桌面应用，让**任何人**都能驾驭 LLM 和 MCP 的魔法。下载 Tome，连接任意本地或远程 LLM，并将其挂接到数千个 MCP 服务器，打造你自己的魔法 AI 魔法书。\n\n🫥 想要 100% 本地、100% 私密？只用本地 MCP 服务器配合 Ollama 和 Qwen3，在你的私有宇宙中施展魔法。⚡ 想要最新的云端模型和最新的远程 MCP 服务器？你也可以做到。这一切都由你决定！\n\n🏗️ 这是技术预览版，所以请注意，部分功能可能还不够完善。[加入我们的 Discord](https://discord.gg/9CH6us29YA) 分享你的经验、技巧和遇到的问题。给本仓库加星，获取最新更新和功能发布！\n\n## 🪄 功能特性\n\n- 🧙 **简洁新手友好体验**\n  - 只需下载并安装 Tome，然后连接你喜欢的 LLM\n  - 无需折腾 JSON、Docker、python 或 node\n- 🤖 **AI 模型支持**\n  - **远程**：Google Gemini、OpenAI、任何兼容 OpenAI API 的端点\n  - **本地**：Ollama、LM Studio、Cortex、任何兼容 OpenAI API 的端点\n- 🔮 **增强的 MCP 支持**\n  - 图形界面安装、移除、开启/关闭 MCP 服务器\n  - 原生支持 npm、uvx、node、python MCP 服务器\n- 🏪 **集成到 [Smithery.ai](https://smithery.ai) 注册表**\n  - 数千个 MCP 服务器一键安装\n- ✏️ **上下文窗口和温度可自定义**\n- 🧰 **原生支持工具调用和推理模型**\n  - 界面增强，清晰区分工具调用和思考消息\n\n## 演示\n\nhttps://github.com/user-attachments/assets/0775d100-3eba-4219-9e2f-360a01f28cce\n\n# 入门指南\n\n## 系统要求\n\n- MacOS 或 Windows（Linux 即将支持！）\n- 你选择的 LLM 提供商：[Ollama](https://ollama.com/) 或 [Gemini API 密钥](https://aistudio.google.com/app/apikey) 简单/免费\n- [下载 Tome 的最新版本](https://github.com/runebookai/tome/releases)\n\n## 快速开始\n\n1. 安装 [Tome](https://github.com/runebookai/tome/releases)\n2. 连接你喜欢的 LLM 提供商 —— OpenAI、Ollama 和 Gemini 已预设，但你也可以通过使用 http://localhost:1234/v1 作为 URL 添加如 LM Studio 等其他提供商\n3. 在 Tome 中打开 MCP 选项卡，安装你的第一个 [MCP 服务器](https://github.com/modelcontextprotocol/servers)（Fetch 是个简单的入门选择，只需将 `uvx mcp-server-fetch` 粘贴到服务器字段即可）。\n4. 与你的 MCP 驱动模型聊天！让它帮你获取 Hacker News 上的头条新闻。\n\n# 愿景\n\n我们希望让本地 LLM 和 MCP 对每个人都易于访问。我们正在打造一个工具，无论你是工程师、极客、爱好者还是其他任何人，都能用 LLM 发挥创意。\n\n## 核心原则\n\n- **Tome 首先本地化：** 你掌控你的数据流向。\n- **Tome 面向所有人：** 你无需管理编程语言、包管理器或 json 配置文件。\n\n## 未来规划\n\n自从发布 Tome 以来，我们收获了大量宝贵反馈，但我们对未来还有宏伟计划。我们希望将 LLM 从聊天框中解放出来，并为大家带来更多新功能。\n\n- 定时任务：即使你不在电脑前，LLM 也能帮你做有用的事情。\n- 原生集成：MCP 服务器是访问工具和信息的绝佳途径，但我们还想添加更强大的集成，以独特方式与 LLM 互动。\n- 应用构建器：我们相信，从长远来看，最佳体验不会局限在聊天界面。我们计划添加更多工具，助你创建强大的应用和工作流。\n- ??? 告诉我们你希望看到什么！通过下方链接加入我们的社区，我们期待你的声音。\n\n# 社区\n\n[Discord](https://discord.gg/9CH6us29YA) [博客](https://blog.runebook.ai) [Bluesky](https://bsky.app/profile/gettome.app) [Twitter](https://twitter.com/get_tome) \n",
  "status": "ok"
}