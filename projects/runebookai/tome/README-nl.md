# Tome - Magisch AI Spellbook

<img src="https://raw.githubusercontent.com/runebookai/tome/main/static/images/repo-header.png" alt="Tome" />

<p align="center">
    <code>een magische desktop-app die de kracht van LLMs en MCP in de handen van iedereen legt</code>
</p>

<p align="center">
    <a href="https://discord.gg/9CH6us29YA" target="_blank"><img src="https://img.shields.io/discord/1365100902561742868?logo=discord&logoColor=fff&label=Join%20Us!&color=9D7CD8" alt="Join Us on Discord" /></a>
    <a href="https://opensource.org/licenses/Apache-2.0" target="_blank"><img src="https://img.shields.io/badge/License-Apache_2.0-blue.svg" alt="License: Apache 2.0" /></a>
    <a href="https://github.com/runebookai/tome/releases" target="_blank"><img src="https://img.shields.io/github/v/release/runebookai/tome" alt="GitHub Release" /></a>
</p>

<p align="center">
    🔮 Download de Tome Desktop App: <a href="https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_x64-setup.exe">Windows</a> | <a href="https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_aarch64.dmg">MacOS</a>
</p>

# Tome

Tome is een desktop-app waarmee **iedereen** de magie van LLMs en MCP kan benutten. Download Tome, verbind elke lokale of externe LLM en koppel hem aan duizenden MCP-servers om je eigen magische AI-aangedreven spellbook te creëren.

🫥 Wil je het 100% lokaal, 100% privé? Gebruik Ollama en Qwen3 alleen met lokale MCP-servers om spreuken uit te spreken in je eigen pocketuniversum. ⚡ Wil je state-of-the-art cloudmodellen met de nieuwste externe MCP-servers? Dat kan ook. Het is helemaal aan jou!

🏗️ Dit is een Technische Preview, dus houd er rekening mee dat sommige dingen nog ruw kunnen zijn. [Word lid van onze Discord](https://discord.gg/9CH6us29YA) om tips, trucs en problemen te delen. Ster deze repo om op de hoogte te blijven van updates en nieuwe functies!

## 🪄 Functies

- 🧙 **Gestroomlijnde, gebruiksvriendelijke ervaring**
  - Download en installeer Tome en koppel eenvoudig de LLM van jouw keuze
  - Geen gepruts met JSON, Docker, python of node
- 🤖 **Ondersteuning voor AI-modellen**
  - **Extern**: Google Gemini, OpenAI, elke OpenAI API-compatibele endpoint
  - **Lokaal**: Ollama, LM Studio, Cortex, elke OpenAI API-compatibele endpoint
- 🔮 **Uitgebreide MCP-ondersteuning**
  - UI om MCP-servers te installeren, verwijderen, aan/uit te zetten
  - npm, uvx, node, python MCP-servers worden standaard ondersteund
- 🏪 **Integratie met [Smithery.ai](https://smithery.ai) registry**
  - Duizenden MCP-servers beschikbaar via installatie met één klik
- ✏️ **Aanpassen van contextwindows en temperatuur**
- 🧰 **Native ondersteuning voor tool calls en redeneermodellen**
  - UI-verbeteringen die tool calls en denkberichten duidelijk onderscheiden

## Demo

https://github.com/user-attachments/assets/0775d100-3eba-4219-9e2f-360a01f28cce

# Aan de slag

## Vereisten

- MacOS of Windows (Linux binnenkort!)
- LLM-provider naar keuze: [Ollama](https://ollama.com/) of [Gemini API key](https://aistudio.google.com/app/apikey) zijn eenvoudig/gratis
- [Download de laatste release van Tome](https://github.com/runebookai/tome/releases)

## Quickstart

1. Installeer [Tome](https://github.com/runebookai/tome/releases)
2. Verbind jouw favoriete LLM-provider - OpenAI, Ollama en Gemini zijn vooringesteld, maar je kunt ook providers zoals LM Studio toevoegen door http://localhost:1234/v1 als URL te gebruiken
3. Open het MCP-tabblad in Tome en installeer je eerste [MCP-server](https://github.com/modelcontextprotocol/servers) (Fetch is een makkelijke om mee te beginnen, plak gewoon `uvx mcp-server-fetch` in het serverveld).
4. Chat met je MCP-aangedreven model! Vraag bijvoorbeeld naar het belangrijkste nieuws op Hacker News.

# Visie

Wij willen lokale LLMs en MCP toegankelijk maken voor iedereen. We bouwen een tool waarmee je creatief kunt zijn met LLMs, ongeacht
of je een ingenieur, knutselaar, hobbyist of iets daartussen bent.

## Kernprincipes

- **Tome is lokaal eerst:** Jij bepaalt waar je data naartoe gaat.
- **Tome is voor iedereen:** Je zou geen programmeertalen, pakketmanagers of json-configuratiebestanden hoeven te beheren.

## Wat komt eraan

We hebben de afgelopen weken veel geweldig feedback ontvangen sinds de release van Tome, maar we hebben grote plannen voor de toekomst. We willen LLMs uit hun chatbox bevrijden, en we hebben veel functies op komst om jullie daarbij te helpen.

- Geplande taken: LLMs moeten ook nuttige dingen doen als je niet achter de computer zit.
- Native integraties: MCP-servers zijn een geweldige manier om tools en informatie te benaderen, maar we willen krachtigere integraties toevoegen om op unieke manieren met LLMs te interageren.
- App builder: we geloven dat de beste ervaringen op de lange termijn niet in een chatinterface zullen zijn. We zijn van plan extra tools toe te voegen waarmee je krachtige applicaties en workflows kunt creëren.
- ??? Laat ons weten wat jij graag zou willen zien! Word lid van onze community via de links hieronder, we horen graag van je.

# Community

[Discord](https://discord.gg/9CH6us29YA) [Blog](https://blog.runebook.ai) [Bluesky](https://bsky.app/profile/gettome.app) [Twitter](https://twitter.com/get_tome) 

---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-03

---