{
  "id": 1,
  "origin": "# Tome - Magical AI Spellbook\n\n<img src=\"https://raw.githubusercontent.com/runebookai/tome/main/static/images/repo-header.png\" alt=\"Tome\" />\n\n<p align=\"center\">\n    <code>a magical desktop app that puts the power of LLMs and MCP in the hands of everyone</code>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://discord.gg/9CH6us29YA\" target=\"_blank\"><img src=\"https://img.shields.io/discord/1365100902561742868?logo=discord&logoColor=fff&label=Join%20Us!&color=9D7CD8\" alt=\"Join Us on Discord\" /></a>\n    <a href=\"https://opensource.org/licenses/Apache-2.0\" target=\"_blank\"><img src=\"https://img.shields.io/badge/License-Apache_2.0-blue.svg\" alt=\"License: Apache 2.0\" /></a>\n    <a href=\"https://github.com/runebookai/tome/releases\" target=\"_blank\"><img src=\"https://img.shields.io/github/v/release/runebookai/tome\" alt=\"GitHub Release\" /></a>\n</p>\n\n<p align=\"center\">\n    ğŸ”® Download the Tome Desktop App: <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_x64-setup.exe\">Windows</a> | <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_aarch64.dmg\">MacOS</a>\n</p>\n\n# Tome\n\nTome is a desktop app that lets **anyone** harness the magic of LLMs and MCP. Download Tome, connect any local or remote LLM and hook it up to thousands of MCP servers to create your own magical AI-powered spellbook.\n\nğŸ«¥ Want it to be 100% local, 100% private? Use Ollama and Qwen3 with only local MCP servers to cast spells in your own pocket universe. âš¡ Want state of the art cloud models with the latest remote MCP servers? You can have that too. It's all up to you!\n\nğŸ—ï¸ This is a Technical Preview so bear in mind things will be rough around the edges. [Join us on Discord](https://discord.gg/9CH6us29YA) to share tips, tricks, and issues you run into. Star this repo to stay on top of updates and feature releases!\n\n## ğŸª„ Features\n\n- ğŸ§™ **Streamlined Beginner Friendly Experience**\n  - Simply download and install Tome and hook up the LLM of your choice\n  - No fiddling with JSON, Docker, python or node\n- ğŸ¤– **AI Model Support**\n  - **Remote**: Google Gemini, OpenAI, any OpenAI API-compatible endpoint\n  - **Local**: Ollama, LM Studio, Cortex, any OpenAI API-compatible endpoint\n- ğŸ”® **Enhanced MCP support**\n  - UI to install, remove, turn on/off MCP servers\n  - npm, uvx, node, python MCP servers all supported out of box\n- ğŸª **Integration into [Smithery.ai](https://smithery.ai) registry**\n  - Thousands of MCP servers available via one-click installation\n- âœï¸ **Customization of context windows and temperature**\n- ğŸ§° **Native support for tool calls and reasoning models**\n  - UI enhancements that clearly delineate tool calls and thinking messages\n\n## Demo\n\nhttps://github.com/user-attachments/assets/0775d100-3eba-4219-9e2f-360a01f28cce\n\n# Getting Started\n\n## Requirements\n\n- MacOS or Windows (Linux coming soon!)\n- LLM Provider of your choice: [Ollama](https://ollama.com/) or [Gemini API key](https://aistudio.google.com/app/apikey) are easy/free\n- [Download the latest release of Tome](https://github.com/runebookai/tome/releases)\n\n## Quickstart\n\n1. Install [Tome](https://github.com/runebookai/tome/releases)\n2. Connect your preferred LLM provider - OpenAI, Ollama and Gemini are preset but you can also add providers like LM Studio by using http://localhost:1234/v1 as the URL\n3. Open the MCP tab in Tome and install your first [MCP server](https://github.com/modelcontextprotocol/servers) (Fetch is an easy one to get started with, just paste `uvx mcp-server-fetch` into the server field).\n4. Chat with your MCP-powered model! Ask it to fetch the top story on Hacker News.\n\n# Vision\n\nWe want to make local LLMs and MCP accessible to everyone. We're building a tool that allows you to be creative with LLMs, regardless\nof whether you're an engineer, tinkerer, hobbyist, or anyone in between.\n\n## Core Principles\n\n- **Tome is local first:** You are in control of where your data goes.\n- **Tome is for everyone:** You shouldn't have to manage programming languages, package managers, or json config files.\n\n## What's Next\n\nWe've gotten a lot of amazing feedback in the last few weeks since releasing Tome but we've got big plans for the future. We want to break LLMs out of their chatbox, and we've got a lot of features coming to help y'all do that.\n\n- Scheduled tasks: LLMs should be doing helpful things even when you're not in front of the computer.\n- Native integrations: MCP servers are a great way to access tools and information, but we want to add more powerful integrations to interact with LLMs in unique. ways\n- App builder: we believe long term that the best experiences will not be in a chat interface. We have plans to add additional tools that will enable you to create powerful applications and workflows.\n- ??? Let us know what you'd like to see! Join our community via the links below, we'd love to hear from you.\n\n# Community\n\n[Discord](https://discord.gg/9CH6us29YA) [Blog](https://blog.runebook.ai) [Bluesky](https://bsky.app/profile/gettome.app) [Twitter](https://twitter.com/get_tome) \n",
  "origin_sha": "rWLnVvqJwvr9v1fqiTn2LWjwaQbT+x1VplsLjSlpRbU=",
  "translate": "# Tome - Magiczna KsiÄ™ga ZaklÄ™Ä‡ AI\n\n<img src=\"https://raw.githubusercontent.com/runebookai/tome/main/static/images/repo-header.png\" alt=\"Tome\" />\n\n<p align=\"center\">\n    <code>magiczna aplikacja desktopowa, ktÃ³ra oddaje moc LLM i MCP w rÄ™ce kaÅ¼dego</code>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://discord.gg/9CH6us29YA\" target=\"_blank\"><img src=\"https://img.shields.io/discord/1365100902561742868?logo=discord&logoColor=fff&label=Join%20Us!&color=9D7CD8\" alt=\"DoÅ‚Ä…cz do nas na Discordzie\" /></a>\n    <a href=\"https://opensource.org/licenses/Apache-2.0\" target=\"_blank\"><img src=\"https://img.shields.io/badge/License-Apache_2.0-blue.svg\" alt=\"Licencja: Apache 2.0\" /></a>\n    <a href=\"https://github.com/runebookai/tome/releases\" target=\"_blank\"><img src=\"https://img.shields.io/github/v/release/runebookai/tome\" alt=\"Wydanie GitHub\" /></a>\n</p>\n\n<p align=\"center\">\n    ğŸ”® Pobierz aplikacjÄ™ Tome Desktop: <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_x64-setup.exe\">Windows</a> | <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_aarch64.dmg\">MacOS</a>\n</p>\n\n# Tome\n\nTome to aplikacja desktopowa, ktÃ³ra pozwala **kaÅ¼demu** korzystaÄ‡ z magii LLM i MCP. Pobierz Tome, poÅ‚Ä…cz dowolny lokalny lub zdalny LLM i podÅ‚Ä…cz go do tysiÄ™cy serwerÃ³w MCP, aby stworzyÄ‡ wÅ‚asnÄ… magicznÄ… ksiÄ™gÄ™ zaklÄ™Ä‡ zasilanÄ… przez AI.\n\nğŸ«¥ Chcesz, aby wszystko dziaÅ‚aÅ‚o w 100% lokalnie i w 100% prywatnie? UÅ¼yj Ollama i Qwen3 wyÅ‚Ä…cznie z lokalnymi serwerami MCP, aby rzucaÄ‡ zaklÄ™cia w swoim wÅ‚asnym kieszonkowym uniwersum. âš¡ Chcesz korzystaÄ‡ z najnowoczeÅ›niejszych modeli w chmurze z najnowszymi zdalnymi serwerami MCP? TeÅ¼ moÅ¼esz to mieÄ‡. WybÃ³r naleÅ¼y do Ciebie!\n\nğŸ—ï¸ To jest Wersja Techniczna Preview, wiÄ™c miej na uwadze, Å¼e nie wszystko jest jeszcze dopracowane. [DoÅ‚Ä…cz do nas na Discordzie](https://discord.gg/9CH6us29YA), aby dzieliÄ‡ siÄ™ wskazÃ³wkami, trikami i zgÅ‚aszaÄ‡ napotkane problemy. Oznacz repozytorium gwiazdkÄ…, aby byÄ‡ na bieÅ¼Ä…co z aktualizacjami i nowoÅ›ciami!\n\n## ğŸª„ Funkcje\n\n- ğŸ§™ **Uproszczone, przyjazne dla poczÄ…tkujÄ…cych doÅ›wiadczenie**\n  - Po prostu pobierz i zainstaluj Tome oraz podÅ‚Ä…cz wybrany przez siebie LLM\n  - Bez grzebania w JSON, Dockerze, pythonie czy node\n- ğŸ¤– **Wsparcie dla modeli AI**\n  - **Zdalne**: Google Gemini, OpenAI, dowolny endpoint kompatybilny z API OpenAI\n  - **Lokalne**: Ollama, LM Studio, Cortex, dowolny endpoint kompatybilny z API OpenAI\n- ğŸ”® **Zaawansowane wsparcie MCP**\n  - Interfejs do instalowania, usuwania, wÅ‚Ä…czania/wyÅ‚Ä…czania serwerÃ³w MCP\n  - Serwery MCP npm, uvx, node, python obsÅ‚ugiwane od razu po instalacji\n- ğŸª **Integracja z rejestrem [Smithery.ai](https://smithery.ai)**\n  - TysiÄ…ce serwerÃ³w MCP dostÄ™pnych poprzez instalacjÄ™ jednym klikniÄ™ciem\n- âœï¸ **Dostosowanie okien kontekstowych i temperatury**\n- ğŸ§° **Nattywne wsparcie dla wywoÅ‚aÅ„ narzÄ™dzi i modeli rozumowania**\n  - Ulepszenia UI, ktÃ³re wyraÅºnie rozrÃ³Å¼niajÄ… wywoÅ‚ania narzÄ™dzi i komunikaty myÅ›lowe\n\n## Demo\n\nhttps://github.com/user-attachments/assets/0775d100-3eba-4219-9e2f-360a01f28cce\n\n# Pierwsze Kroki\n\n## Wymagania\n\n- MacOS lub Windows (Linux juÅ¼ wkrÃ³tce!)\n- Dostawca LLM wedÅ‚ug wyboru: [Ollama](https://ollama.com/) lub [klucz API Gemini](https://aistudio.google.com/app/apikey) sÄ… Å‚atwe/darmowe\n- [Pobierz najnowszÄ… wersjÄ™ Tome](https://github.com/runebookai/tome/releases)\n\n## Szybki start\n\n1. Zainstaluj [Tome](https://github.com/runebookai/tome/releases)\n2. PodÅ‚Ä…cz preferowanego dostawcÄ™ LLM - OpenAI, Ollama i Gemini sÄ… ustawione domyÅ›lnie, ale moÅ¼esz teÅ¼ dodaÄ‡ dostawcÃ³w takich jak LM Studio, uÅ¼ywajÄ…c URL http://localhost:1234/v1\n3. OtwÃ³rz zakÅ‚adkÄ™ MCP w Tome i zainstaluj swÃ³j pierwszy [serwer MCP](https://github.com/modelcontextprotocol/servers) (Fetch to prosty na poczÄ…tek, po prostu wklej `uvx mcp-server-fetch` w polu serwera).\n4. Rozmawiaj z modelem zasilanym MCP! PoproÅ› go o pobranie najnowszej wiadomoÅ›ci z Hacker News.\n\n# Wizja\n\nChcemy, aby lokalne LLM i MCP byÅ‚y dostÄ™pne dla kaÅ¼dego. Budujemy narzÄ™dzie, ktÃ³re pozwoli Ci byÄ‡ kreatywnym z LLM, niezaleÅ¼nie od tego, czy jesteÅ› inÅ¼ynierem, majsterkowiczem, hobbystÄ… czy kimkolwiek pomiÄ™dzy.\n\n## Kluczowe zasady\n\n- **Tome jest w pierwszej kolejnoÅ›ci lokalny:** Ty decydujesz, gdzie trafiajÄ… Twoje dane.\n- **Tome jest dla wszystkich:** Nie musisz zarzÄ…dzaÄ‡ jÄ™zykami programowania, menedÅ¼erami pakietÃ³w ani plikami konfiguracyjnymi json.\n\n## Co dalej\n\nOtrzymaliÅ›my mnÃ³stwo niesamowitych opinii od czasu wydania Tome, ale mamy wielkie plany na przyszÅ‚oÅ›Ä‡. Chcemy uwolniÄ‡ LLM-y z ich okna czatu i mamy w planach wiele funkcji, ktÃ³re pomogÄ… Wam to osiÄ…gnÄ…Ä‡.\n\n- Zadania zaplanowane: LLM-y powinny robiÄ‡ pomocne rzeczy nawet, gdy nie siedzisz przy komputerze.\n- Natywne integracje: Serwery MCP to Å›wietny sposÃ³b na dostÄ™p do narzÄ™dzi i informacji, ale chcemy dodaÄ‡ jeszcze potÄ™Å¼niejsze integracje do unikalnej interakcji z LLM.\n- Kreator aplikacji: Wierzymy, Å¼e docelowo najlepsze doÅ›wiadczenia nie bÄ™dÄ… w interfejsie czatu. Planujemy dodaÄ‡ dodatkowe narzÄ™dzia, ktÃ³re pozwolÄ… Ci tworzyÄ‡ zaawansowane aplikacje i przepÅ‚ywy pracy.\n- ??? Daj nam znaÄ‡, co chciaÅ‚byÅ› zobaczyÄ‡! DoÅ‚Ä…cz do naszej spoÅ‚ecznoÅ›ci przez linki poniÅ¼ej, chÄ™tnie poznamy TwojÄ… opiniÄ™.\n\n# SpoÅ‚ecznoÅ›Ä‡\n\n[Discord](https://discord.gg/9CH6us29YA) [Blog](https://blog.runebook.ai) [Bluesky](https://bsky.app/profile/gettome.app) [Twitter](https://twitter.com/get_tome) \n",
  "status": "ok"
}