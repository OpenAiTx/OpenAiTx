# Tome - Grimoire Magique dâ€™IA

<img src="https://raw.githubusercontent.com/runebookai/tome/main/static/images/repo-header.png" alt="Tome" />

<p align="center">
    <code>une application de bureau magique qui met la puissance des LLM et MCP entre les mains de tous</code>
</p>

<p align="center">
    <a href="https://discord.gg/9CH6us29YA" target="_blank"><img src="https://img.shields.io/discord/1365100902561742868?logo=discord&logoColor=fff&label=Rejoignez-nous!&color=9D7CD8" alt="Rejoignez-nous sur Discord" /></a>
    <a href="https://opensource.org/licenses/Apache-2.0" target="_blank"><img src="https://img.shields.io/badge/License-Apache_2.0-blue.svg" alt="Licence : Apache 2.0" /></a>
    <a href="https://github.com/runebookai/tome/releases" target="_blank"><img src="https://img.shields.io/github/v/release/runebookai/tome" alt="Version GitHub" /></a>
</p>

<p align="center">
    ğŸ”® TÃ©lÃ©chargez lâ€™application de bureau Tome : <a href="https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_x64-setup.exe">Windows</a> | <a href="https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_aarch64.dmg">MacOS</a>
</p>

# Tome

Tome est une application de bureau qui permet Ã  **tout le monde** de maÃ®triser la magie des LLM et MCP. TÃ©lÃ©chargez Tome, connectez n'importe quel LLM local ou distant et reliez-le Ã  des milliers de serveurs MCP pour crÃ©er votre propre grimoire magique alimentÃ© par lâ€™IA.

ğŸ«¥ Vous voulez que ce soit 100% local, 100% privÃ© ? Utilisez Ollama et Qwen3 avec uniquement des serveurs MCP locaux pour lancer des sorts dans votre propre univers de poche. âš¡ Vous souhaitez des modÃ¨les cloud de pointe avec les derniers serveurs MCP distants ? Câ€™est possible aussi. Câ€™est vous qui dÃ©cidez !

ğŸ—ï¸ Ceci est une prÃ©version technique, donc gardez Ã  l'esprit que tout n'est pas parfaitement abouti. [Rejoignez-nous sur Discord](https://discord.gg/9CH6us29YA) pour partager vos astuces, conseils et signaler les problÃ¨mes rencontrÃ©s. Ajoutez une Ã©toile Ã  ce dÃ©pÃ´t pour suivre les mises Ã  jour et nouveautÃ©s !

## ğŸª„ FonctionnalitÃ©s

- ğŸ§™ **ExpÃ©rience fluide et accessible aux dÃ©butants**
  - TÃ©lÃ©chargez et installez simplement Tome, puis connectez le LLM de votre choix
  - Pas besoin de manipuler JSON, Docker, python ou node
- ğŸ¤– **Prise en charge des modÃ¨les dâ€™IA**
  - **Distant** : Google Gemini, OpenAI, tout point de terminaison compatible API OpenAI
  - **Local** : Ollama, LM Studio, Cortex, tout point de terminaison compatible API OpenAI
- ğŸ”® **Support MCP amÃ©liorÃ©**
  - Interface pour installer, supprimer, activer/dÃ©sactiver les serveurs MCP
  - Serveurs MCP npm, uvx, node, python pris en charge nativement
- ğŸª **IntÃ©gration au registre [Smithery.ai](https://smithery.ai)**
  - Des milliers de serveurs MCP disponibles en un clic
- âœï¸ **Personnalisation des fenÃªtres de contexte et de la tempÃ©rature**
- ğŸ§° **Prise en charge native des appels dâ€™outils et des modÃ¨les de raisonnement**
  - AmÃ©liorations de lâ€™interface qui distinguent clairement les appels dâ€™outils et les messages de rÃ©flexion

## DÃ©mo

https://github.com/user-attachments/assets/0775d100-3eba-4219-9e2f-360a01f28cce

# DÃ©marrage

## PrÃ©requis

- MacOS ou Windows (Linux bientÃ´t disponible !)
- Fournisseur de LLM au choix : [Ollama](https://ollama.com/) ou [clÃ© API Gemini](https://aistudio.google.com/app/apikey) sont faciles/gratuits
- [TÃ©lÃ©chargez la derniÃ¨re version de Tome](https://github.com/runebookai/tome/releases)

## DÃ©marrage rapide

1. Installez [Tome](https://github.com/runebookai/tome/releases)
2. Connectez votre fournisseur LLM prÃ©fÃ©rÃ© â€“ OpenAI, Ollama et Gemini sont prÃ©configurÃ©s, mais vous pouvez aussi ajouter des fournisseurs comme LM Studio en utilisant http://localhost:1234/v1 comme URL
3. Ouvrez lâ€™onglet MCP dans Tome et installez votre premier [serveur MCP](https://github.com/modelcontextprotocol/servers) (Fetch est un bon point de dÃ©part, il suffit de coller `uvx mcp-server-fetch` dans le champ serveur).
4. Discutez avec votre modÃ¨le propulsÃ© par MCP ! Demandez-lui de rÃ©cupÃ©rer la meilleure actualitÃ© sur Hacker News.

# Vision

Nous voulons rendre les LLM locaux et MCP accessibles Ã  tous. Nous construisons un outil qui vous permet dâ€™Ãªtre crÃ©atif avec les LLM, que vous soyez ingÃ©nieur, bricoleur, amateur ou toute autre personne.

## Principes fondamentaux

- **Tome est local avant tout :** Vous contrÃ´lez oÃ¹ vont vos donnÃ©es.
- **Tome est pour tous :** Vous ne devriez pas avoir Ã  gÃ©rer des langages de programmation, des gestionnaires de paquets ou des fichiers de configuration json.

## Prochaines Ã©tapes

Nous avons reÃ§u de nombreux retours incroyables ces derniÃ¨res semaines depuis la sortie de Tome, mais nous avons de grands projets pour lâ€™avenir. Nous voulons libÃ©rer les LLM de leur boÃ®te de dialogue, et de nombreuses fonctionnalitÃ©s arrivent pour vous aider Ã  y parvenir.

- TÃ¢ches planifiÃ©es : les LLM devraient effectuer des tÃ¢ches utiles mÃªme lorsque vous nâ€™Ãªtes pas devant lâ€™ordinateur.
- IntÃ©grations natives : les serveurs MCP sont un excellent moyen dâ€™accÃ©der Ã  des outils et informations, mais nous voulons ajouter des intÃ©grations encore plus puissantes pour interagir avec les LLM de maniÃ¨re unique.
- GÃ©nÃ©rateur dâ€™applications : Ã  long terme, nous pensons que les meilleures expÃ©riences ne seront pas dans une interface de chat. Nous prÃ©voyons dâ€™ajouter des outils supplÃ©mentaires pour vous permettre de crÃ©er des applications et des workflows puissants.
- ??? Dites-nous ce que vous aimeriez voir ! Rejoignez notre communautÃ© via les liens ci-dessous, nous serions ravis dâ€™avoir vos retours.

# CommunautÃ©

[Discord](https://discord.gg/9CH6us29YA) [Blog](https://blog.runebook.ai) [Bluesky](https://bsky.app/profile/gettome.app) [Twitter](https://twitter.com/get_tome)

---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-03

---