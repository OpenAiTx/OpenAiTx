{
  "id": 1,
  "origin": "# Tome - Magical AI Spellbook\n\n<img src=\"https://raw.githubusercontent.com/runebookai/tome/main/static/images/repo-header.png\" alt=\"Tome\" />\n\n<p align=\"center\">\n    <code>a magical desktop app that puts the power of LLMs and MCP in the hands of everyone</code>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://discord.gg/9CH6us29YA\" target=\"_blank\"><img src=\"https://img.shields.io/discord/1365100902561742868?logo=discord&logoColor=fff&label=Join%20Us!&color=9D7CD8\" alt=\"Join Us on Discord\" /></a>\n    <a href=\"https://opensource.org/licenses/Apache-2.0\" target=\"_blank\"><img src=\"https://img.shields.io/badge/License-Apache_2.0-blue.svg\" alt=\"License: Apache 2.0\" /></a>\n    <a href=\"https://github.com/runebookai/tome/releases\" target=\"_blank\"><img src=\"https://img.shields.io/github/v/release/runebookai/tome\" alt=\"GitHub Release\" /></a>\n</p>\n\n<p align=\"center\">\n    üîÆ Download the Tome Desktop App: <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_x64-setup.exe\">Windows</a> | <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_aarch64.dmg\">MacOS</a>\n</p>\n\n# Tome\n\nTome is a desktop app that lets **anyone** harness the magic of LLMs and MCP. Download Tome, connect any local or remote LLM and hook it up to thousands of MCP servers to create your own magical AI-powered spellbook.\n\nü´• Want it to be 100% local, 100% private? Use Ollama and Qwen3 with only local MCP servers to cast spells in your own pocket universe. ‚ö° Want state of the art cloud models with the latest remote MCP servers? You can have that too. It's all up to you!\n\nüèóÔ∏è This is a Technical Preview so bear in mind things will be rough around the edges. [Join us on Discord](https://discord.gg/9CH6us29YA) to share tips, tricks, and issues you run into. Star this repo to stay on top of updates and feature releases!\n\n## ü™Ñ Features\n\n- üßô **Streamlined Beginner Friendly Experience**\n  - Simply download and install Tome and hook up the LLM of your choice\n  - No fiddling with JSON, Docker, python or node\n- ü§ñ **AI Model Support**\n  - **Remote**: Google Gemini, OpenAI, any OpenAI API-compatible endpoint\n  - **Local**: Ollama, LM Studio, Cortex, any OpenAI API-compatible endpoint\n- üîÆ **Enhanced MCP support**\n  - UI to install, remove, turn on/off MCP servers\n  - npm, uvx, node, python MCP servers all supported out of box\n- üè™ **Integration into [Smithery.ai](https://smithery.ai) registry**\n  - Thousands of MCP servers available via one-click installation\n- ‚úèÔ∏è **Customization of context windows and temperature**\n- üß∞ **Native support for tool calls and reasoning models**\n  - UI enhancements that clearly delineate tool calls and thinking messages\n\n## Demo\n\nhttps://github.com/user-attachments/assets/0775d100-3eba-4219-9e2f-360a01f28cce\n\n# Getting Started\n\n## Requirements\n\n- MacOS or Windows (Linux coming soon!)\n- LLM Provider of your choice: [Ollama](https://ollama.com/) or [Gemini API key](https://aistudio.google.com/app/apikey) are easy/free\n- [Download the latest release of Tome](https://github.com/runebookai/tome/releases)\n\n## Quickstart\n\n1. Install [Tome](https://github.com/runebookai/tome/releases)\n2. Connect your preferred LLM provider - OpenAI, Ollama and Gemini are preset but you can also add providers like LM Studio by using http://localhost:1234/v1 as the URL\n3. Open the MCP tab in Tome and install your first [MCP server](https://github.com/modelcontextprotocol/servers) (Fetch is an easy one to get started with, just paste `uvx mcp-server-fetch` into the server field).\n4. Chat with your MCP-powered model! Ask it to fetch the top story on Hacker News.\n\n# Vision\n\nWe want to make local LLMs and MCP accessible to everyone. We're building a tool that allows you to be creative with LLMs, regardless\nof whether you're an engineer, tinkerer, hobbyist, or anyone in between.\n\n## Core Principles\n\n- **Tome is local first:** You are in control of where your data goes.\n- **Tome is for everyone:** You shouldn't have to manage programming languages, package managers, or json config files.\n\n## What's Next\n\nWe've gotten a lot of amazing feedback in the last few weeks since releasing Tome but we've got big plans for the future. We want to break LLMs out of their chatbox, and we've got a lot of features coming to help y'all do that.\n\n- Scheduled tasks: LLMs should be doing helpful things even when you're not in front of the computer.\n- Native integrations: MCP servers are a great way to access tools and information, but we want to add more powerful integrations to interact with LLMs in unique. ways\n- App builder: we believe long term that the best experiences will not be in a chat interface. We have plans to add additional tools that will enable you to create powerful applications and workflows.\n- ??? Let us know what you'd like to see! Join our community via the links below, we'd love to hear from you.\n\n# Community\n\n[Discord](https://discord.gg/9CH6us29YA) [Blog](https://blog.runebook.ai) [Bluesky](https://bsky.app/profile/gettome.app) [Twitter](https://twitter.com/get_tome) \n",
  "origin_sha": "rWLnVvqJwvr9v1fqiTn2LWjwaQbT+x1VplsLjSlpRbU=",
  "translate": "# Tome - Grimoire Magique d‚ÄôIA\n\n<img src=\"https://raw.githubusercontent.com/runebookai/tome/main/static/images/repo-header.png\" alt=\"Tome\" />\n\n<p align=\"center\">\n    <code>une application de bureau magique qui met la puissance des LLM et MCP entre les mains de tous</code>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://discord.gg/9CH6us29YA\" target=\"_blank\"><img src=\"https://img.shields.io/discord/1365100902561742868?logo=discord&logoColor=fff&label=Rejoignez-nous!&color=9D7CD8\" alt=\"Rejoignez-nous sur Discord\" /></a>\n    <a href=\"https://opensource.org/licenses/Apache-2.0\" target=\"_blank\"><img src=\"https://img.shields.io/badge/License-Apache_2.0-blue.svg\" alt=\"Licence : Apache 2.0\" /></a>\n    <a href=\"https://github.com/runebookai/tome/releases\" target=\"_blank\"><img src=\"https://img.shields.io/github/v/release/runebookai/tome\" alt=\"Version GitHub\" /></a>\n</p>\n\n<p align=\"center\">\n    üîÆ T√©l√©chargez l‚Äôapplication de bureau Tome : <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_x64-setup.exe\">Windows</a> | <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_aarch64.dmg\">MacOS</a>\n</p>\n\n# Tome\n\nTome est une application de bureau qui permet √† **tout le monde** de ma√Ætriser la magie des LLM et MCP. T√©l√©chargez Tome, connectez n'importe quel LLM local ou distant et reliez-le √† des milliers de serveurs MCP pour cr√©er votre propre grimoire magique aliment√© par l‚ÄôIA.\n\nü´• Vous voulez que ce soit 100% local, 100% priv√© ? Utilisez Ollama et Qwen3 avec uniquement des serveurs MCP locaux pour lancer des sorts dans votre propre univers de poche. ‚ö° Vous souhaitez des mod√®les cloud de pointe avec les derniers serveurs MCP distants ? C‚Äôest possible aussi. C‚Äôest vous qui d√©cidez !\n\nüèóÔ∏è Ceci est une pr√©version technique, donc gardez √† l'esprit que tout n'est pas parfaitement abouti. [Rejoignez-nous sur Discord](https://discord.gg/9CH6us29YA) pour partager vos astuces, conseils et signaler les probl√®mes rencontr√©s. Ajoutez une √©toile √† ce d√©p√¥t pour suivre les mises √† jour et nouveaut√©s !\n\n## ü™Ñ Fonctionnalit√©s\n\n- üßô **Exp√©rience fluide et accessible aux d√©butants**\n  - T√©l√©chargez et installez simplement Tome, puis connectez le LLM de votre choix\n  - Pas besoin de manipuler JSON, Docker, python ou node\n- ü§ñ **Prise en charge des mod√®les d‚ÄôIA**\n  - **Distant** : Google Gemini, OpenAI, tout point de terminaison compatible API OpenAI\n  - **Local** : Ollama, LM Studio, Cortex, tout point de terminaison compatible API OpenAI\n- üîÆ **Support MCP am√©lior√©**\n  - Interface pour installer, supprimer, activer/d√©sactiver les serveurs MCP\n  - Serveurs MCP npm, uvx, node, python pris en charge nativement\n- üè™ **Int√©gration au registre [Smithery.ai](https://smithery.ai)**\n  - Des milliers de serveurs MCP disponibles en un clic\n- ‚úèÔ∏è **Personnalisation des fen√™tres de contexte et de la temp√©rature**\n- üß∞ **Prise en charge native des appels d‚Äôoutils et des mod√®les de raisonnement**\n  - Am√©liorations de l‚Äôinterface qui distinguent clairement les appels d‚Äôoutils et les messages de r√©flexion\n\n## D√©mo\n\nhttps://github.com/user-attachments/assets/0775d100-3eba-4219-9e2f-360a01f28cce\n\n# D√©marrage\n\n## Pr√©requis\n\n- MacOS ou Windows (Linux bient√¥t disponible !)\n- Fournisseur de LLM au choix : [Ollama](https://ollama.com/) ou [cl√© API Gemini](https://aistudio.google.com/app/apikey) sont faciles/gratuits\n- [T√©l√©chargez la derni√®re version de Tome](https://github.com/runebookai/tome/releases)\n\n## D√©marrage rapide\n\n1. Installez [Tome](https://github.com/runebookai/tome/releases)\n2. Connectez votre fournisseur LLM pr√©f√©r√© ‚Äì OpenAI, Ollama et Gemini sont pr√©configur√©s, mais vous pouvez aussi ajouter des fournisseurs comme LM Studio en utilisant http://localhost:1234/v1 comme URL\n3. Ouvrez l‚Äôonglet MCP dans Tome et installez votre premier [serveur MCP](https://github.com/modelcontextprotocol/servers) (Fetch est un bon point de d√©part, il suffit de coller `uvx mcp-server-fetch` dans le champ serveur).\n4. Discutez avec votre mod√®le propuls√© par MCP ! Demandez-lui de r√©cup√©rer la meilleure actualit√© sur Hacker News.\n\n# Vision\n\nNous voulons rendre les LLM locaux et MCP accessibles √† tous. Nous construisons un outil qui vous permet d‚Äô√™tre cr√©atif avec les LLM, que vous soyez ing√©nieur, bricoleur, amateur ou toute autre personne.\n\n## Principes fondamentaux\n\n- **Tome est local avant tout :** Vous contr√¥lez o√π vont vos donn√©es.\n- **Tome est pour tous :** Vous ne devriez pas avoir √† g√©rer des langages de programmation, des gestionnaires de paquets ou des fichiers de configuration json.\n\n## Prochaines √©tapes\n\nNous avons re√ßu de nombreux retours incroyables ces derni√®res semaines depuis la sortie de Tome, mais nous avons de grands projets pour l‚Äôavenir. Nous voulons lib√©rer les LLM de leur bo√Æte de dialogue, et de nombreuses fonctionnalit√©s arrivent pour vous aider √† y parvenir.\n\n- T√¢ches planifi√©es : les LLM devraient effectuer des t√¢ches utiles m√™me lorsque vous n‚Äô√™tes pas devant l‚Äôordinateur.\n- Int√©grations natives : les serveurs MCP sont un excellent moyen d‚Äôacc√©der √† des outils et informations, mais nous voulons ajouter des int√©grations encore plus puissantes pour interagir avec les LLM de mani√®re unique.\n- G√©n√©rateur d‚Äôapplications : √† long terme, nous pensons que les meilleures exp√©riences ne seront pas dans une interface de chat. Nous pr√©voyons d‚Äôajouter des outils suppl√©mentaires pour vous permettre de cr√©er des applications et des workflows puissants.\n- ??? Dites-nous ce que vous aimeriez voir ! Rejoignez notre communaut√© via les liens ci-dessous, nous serions ravis d‚Äôavoir vos retours.\n\n# Communaut√©\n\n[Discord](https://discord.gg/9CH6us29YA) [Blog](https://blog.runebook.ai) [Bluesky](https://bsky.app/profile/gettome.app) [Twitter](https://twitter.com/get_tome)",
  "status": "ok"
}