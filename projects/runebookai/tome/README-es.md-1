{
  "id": 1,
  "origin": "# Tome - Magical AI Spellbook\n\n<img src=\"https://raw.githubusercontent.com/runebookai/tome/main/static/images/repo-header.png\" alt=\"Tome\" />\n\n<p align=\"center\">\n    <code>a magical desktop app that puts the power of LLMs and MCP in the hands of everyone</code>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://discord.gg/9CH6us29YA\" target=\"_blank\"><img src=\"https://img.shields.io/discord/1365100902561742868?logo=discord&logoColor=fff&label=Join%20Us!&color=9D7CD8\" alt=\"Join Us on Discord\" /></a>\n    <a href=\"https://opensource.org/licenses/Apache-2.0\" target=\"_blank\"><img src=\"https://img.shields.io/badge/License-Apache_2.0-blue.svg\" alt=\"License: Apache 2.0\" /></a>\n    <a href=\"https://github.com/runebookai/tome/releases\" target=\"_blank\"><img src=\"https://img.shields.io/github/v/release/runebookai/tome\" alt=\"GitHub Release\" /></a>\n</p>\n\n<p align=\"center\">\n    üîÆ Download the Tome Desktop App: <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_x64-setup.exe\">Windows</a> | <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_aarch64.dmg\">MacOS</a>\n</p>\n\n# Tome\n\nTome is a desktop app that lets **anyone** harness the magic of LLMs and MCP. Download Tome, connect any local or remote LLM and hook it up to thousands of MCP servers to create your own magical AI-powered spellbook.\n\nü´• Want it to be 100% local, 100% private? Use Ollama and Qwen3 with only local MCP servers to cast spells in your own pocket universe. ‚ö° Want state of the art cloud models with the latest remote MCP servers? You can have that too. It's all up to you!\n\nüèóÔ∏è This is a Technical Preview so bear in mind things will be rough around the edges. [Join us on Discord](https://discord.gg/9CH6us29YA) to share tips, tricks, and issues you run into. Star this repo to stay on top of updates and feature releases!\n\n## ü™Ñ Features\n\n- üßô **Streamlined Beginner Friendly Experience**\n  - Simply download and install Tome and hook up the LLM of your choice\n  - No fiddling with JSON, Docker, python or node\n- ü§ñ **AI Model Support**\n  - **Remote**: Google Gemini, OpenAI, any OpenAI API-compatible endpoint\n  - **Local**: Ollama, LM Studio, Cortex, any OpenAI API-compatible endpoint\n- üîÆ **Enhanced MCP support**\n  - UI to install, remove, turn on/off MCP servers\n  - npm, uvx, node, python MCP servers all supported out of box\n- üè™ **Integration into [Smithery.ai](https://smithery.ai) registry**\n  - Thousands of MCP servers available via one-click installation\n- ‚úèÔ∏è **Customization of context windows and temperature**\n- üß∞ **Native support for tool calls and reasoning models**\n  - UI enhancements that clearly delineate tool calls and thinking messages\n\n## Demo\n\nhttps://github.com/user-attachments/assets/0775d100-3eba-4219-9e2f-360a01f28cce\n\n# Getting Started\n\n## Requirements\n\n- MacOS or Windows (Linux coming soon!)\n- LLM Provider of your choice: [Ollama](https://ollama.com/) or [Gemini API key](https://aistudio.google.com/app/apikey) are easy/free\n- [Download the latest release of Tome](https://github.com/runebookai/tome/releases)\n\n## Quickstart\n\n1. Install [Tome](https://github.com/runebookai/tome/releases)\n2. Connect your preferred LLM provider - OpenAI, Ollama and Gemini are preset but you can also add providers like LM Studio by using http://localhost:1234/v1 as the URL\n3. Open the MCP tab in Tome and install your first [MCP server](https://github.com/modelcontextprotocol/servers) (Fetch is an easy one to get started with, just paste `uvx mcp-server-fetch` into the server field).\n4. Chat with your MCP-powered model! Ask it to fetch the top story on Hacker News.\n\n# Vision\n\nWe want to make local LLMs and MCP accessible to everyone. We're building a tool that allows you to be creative with LLMs, regardless\nof whether you're an engineer, tinkerer, hobbyist, or anyone in between.\n\n## Core Principles\n\n- **Tome is local first:** You are in control of where your data goes.\n- **Tome is for everyone:** You shouldn't have to manage programming languages, package managers, or json config files.\n\n## What's Next\n\nWe've gotten a lot of amazing feedback in the last few weeks since releasing Tome but we've got big plans for the future. We want to break LLMs out of their chatbox, and we've got a lot of features coming to help y'all do that.\n\n- Scheduled tasks: LLMs should be doing helpful things even when you're not in front of the computer.\n- Native integrations: MCP servers are a great way to access tools and information, but we want to add more powerful integrations to interact with LLMs in unique. ways\n- App builder: we believe long term that the best experiences will not be in a chat interface. We have plans to add additional tools that will enable you to create powerful applications and workflows.\n- ??? Let us know what you'd like to see! Join our community via the links below, we'd love to hear from you.\n\n# Community\n\n[Discord](https://discord.gg/9CH6us29YA) [Blog](https://blog.runebook.ai) [Bluesky](https://bsky.app/profile/gettome.app) [Twitter](https://twitter.com/get_tome) \n",
  "origin_sha": "rWLnVvqJwvr9v1fqiTn2LWjwaQbT+x1VplsLjSlpRbU=",
  "translate": "# Tome - Libro de Hechizos M√°gico de IA\n\n<img src=\"https://raw.githubusercontent.com/runebookai/tome/main/static/images/repo-header.png\" alt=\"Tome\" />\n\n<p align=\"center\">\n    <code>una aplicaci√≥n de escritorio m√°gica que pone el poder de los LLMs y MCP en manos de todos</code>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://discord.gg/9CH6us29YA\" target=\"_blank\"><img src=\"https://img.shields.io/discord/1365100902561742868?logo=discord&logoColor=fff&label=Join%20Us!&color=9D7CD8\" alt=\"√önete a nosotros en Discord\" /></a>\n    <a href=\"https://opensource.org/licenses/Apache-2.0\" target=\"_blank\"><img src=\"https://img.shields.io/badge/License-Apache_2.0-blue.svg\" alt=\"Licencia: Apache 2.0\" /></a>\n    <a href=\"https://github.com/runebookai/tome/releases\" target=\"_blank\"><img src=\"https://img.shields.io/github/v/release/runebookai/tome\" alt=\"GitHub Release\" /></a>\n</p>\n\n<p align=\"center\">\n    üîÆ Descarga la aplicaci√≥n de escritorio Tome: <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_x64-setup.exe\">Windows</a> | <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_aarch64.dmg\">MacOS</a>\n</p>\n\n# Tome\n\nTome es una aplicaci√≥n de escritorio que permite que **cualquiera** aproveche la magia de los LLMs y MCP. Descarga Tome, conecta cualquier LLM local o remoto y enl√°zalo con miles de servidores MCP para crear tu propio libro de hechizos impulsado por IA.\n\nü´• ¬øQuieres que sea 100% local, 100% privado? Usa Ollama y Qwen3 solo con servidores MCP locales para lanzar hechizos en tu propio universo de bolsillo. ‚ö° ¬øQuieres modelos en la nube de √∫ltima generaci√≥n con los servidores MCP remotos m√°s recientes? Tambi√©n puedes tener eso. ¬°Todo depende de ti!\n\nüèóÔ∏è Esta es una Vista Previa T√©cnica, as√≠ que ten en cuenta que habr√° aspectos por pulir. [√önete a nosotros en Discord](https://discord.gg/9CH6us29YA) para compartir consejos, trucos y problemas que encuentres. ¬°Dale una estrella a este repositorio para estar al tanto de actualizaciones y lanzamientos de nuevas funciones!\n\n## ü™Ñ Caracter√≠sticas\n\n- üßô **Experiencia simplificada y apta para principiantes**\n  - Simplemente descarga e instala Tome y conecta el LLM de tu elecci√≥n\n  - Sin complicaciones con JSON, Docker, python o node\n- ü§ñ **Soporte para modelos de IA**\n  - **Remoto**: Google Gemini, OpenAI, cualquier endpoint compatible con la API de OpenAI\n  - **Local**: Ollama, LM Studio, Cortex, cualquier endpoint compatible con la API de OpenAI\n- üîÆ **Soporte mejorado para MCP**\n  - Interfaz para instalar, eliminar, activar/desactivar servidores MCP\n  - Soporte inmediato para servidores MCP basados en npm, uvx, node y python\n- üè™ **Integraci√≥n en el registro de [Smithery.ai](https://smithery.ai)**\n  - Miles de servidores MCP disponibles mediante instalaci√≥n con un solo clic\n- ‚úèÔ∏è **Personalizaci√≥n de ventanas de contexto y temperatura**\n- üß∞ **Soporte nativo para llamadas a herramientas y modelos de razonamiento**\n  - Mejoras en la interfaz que distinguen claramente las llamadas a herramientas y los mensajes de razonamiento\n\n## Demo\n\nhttps://github.com/user-attachments/assets/0775d100-3eba-4219-9e2f-360a01f28cce\n\n# Primeros Pasos\n\n## Requisitos\n\n- MacOS o Windows (¬°Linux pr√≥ximamente!)\n- Proveedor de LLM de tu elecci√≥n: [Ollama](https://ollama.com/) o [clave de API de Gemini](https://aistudio.google.com/app/apikey) son f√°ciles/gratis\n- [Descarga la √∫ltima versi√≥n de Tome](https://github.com/runebookai/tome/releases)\n\n## Inicio R√°pido\n\n1. Instala [Tome](https://github.com/runebookai/tome/releases)\n2. Conecta tu proveedor LLM preferido: OpenAI, Ollama y Gemini ya est√°n preconfigurados, pero tambi√©n puedes a√±adir proveedores como LM Studio usando http://localhost:1234/v1 como URL\n3. Abre la pesta√±a MCP en Tome e instala tu primer [servidor MCP](https://github.com/modelcontextprotocol/servers) (Fetch es uno f√°cil para empezar, solo pega `uvx mcp-server-fetch` en el campo del servidor).\n4. ¬°Chatea con tu modelo potenciado por MCP! P√≠dele que obtenga la noticia principal en Hacker News.\n\n# Visi√≥n\n\nQueremos hacer que los LLMs locales y MCP sean accesibles para todos. Estamos construyendo una herramienta que te permite ser creativo con los LLMs, sin importar si eres ingeniero, aficionado, entusiasta o cualquier persona intermedia.\n\n## Principios Fundamentales\n\n- **Tome es primero local:** T√∫ tienes el control de a d√≥nde va tu informaci√≥n.\n- **Tome es para todos:** No deber√≠as tener que manejar lenguajes de programaci√≥n, gestores de paquetes o archivos de configuraci√≥n json.\n\n## Pr√≥ximos Pasos\n\nHemos recibido muchos comentarios incre√≠bles en las √∫ltimas semanas desde que lanzamos Tome, pero tenemos grandes planes para el futuro. Queremos sacar a los LLMs de su caja de chat, y tenemos muchas funciones en camino para ayudarles a hacer eso.\n\n- Tareas programadas: Los LLMs deber√≠an hacer cosas √∫tiles incluso cuando no est√©s frente a la computadora.\n- Integraciones nativas: Los servidores MCP son una excelente forma de acceder a herramientas e informaci√≥n, pero queremos a√±adir integraciones m√°s potentes para interactuar con los LLMs de formas √∫nicas.\n- Constructor de aplicaciones: creemos que a largo plazo las mejores experiencias no estar√°n en una interfaz de chat. Tenemos planes para a√±adir herramientas adicionales que te permitir√°n crear aplicaciones y flujos de trabajo potentes.\n- ??? ¬°Cu√©ntanos qu√© te gustar√≠a ver! √önete a nuestra comunidad a trav√©s de los enlaces de abajo, nos encantar√≠a escucharte.\n\n# Comunidad\n\n[Discord](https://discord.gg/9CH6us29YA) [Blog](https://blog.runebook.ai) [Bluesky](https://bsky.app/profile/gettome.app) [Twitter](https://twitter.com/get_tome) \n",
  "status": "ok"
}