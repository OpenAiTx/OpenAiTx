{
  "id": 1,
  "origin": "# Tome - Magical AI Spellbook\n\n<img src=\"https://raw.githubusercontent.com/runebookai/tome/main/static/images/repo-header.png\" alt=\"Tome\" />\n\n<p align=\"center\">\n    <code>a magical desktop app that puts the power of LLMs and MCP in the hands of everyone</code>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://discord.gg/9CH6us29YA\" target=\"_blank\"><img src=\"https://img.shields.io/discord/1365100902561742868?logo=discord&logoColor=fff&label=Join%20Us!&color=9D7CD8\" alt=\"Join Us on Discord\" /></a>\n    <a href=\"https://opensource.org/licenses/Apache-2.0\" target=\"_blank\"><img src=\"https://img.shields.io/badge/License-Apache_2.0-blue.svg\" alt=\"License: Apache 2.0\" /></a>\n    <a href=\"https://github.com/runebookai/tome/releases\" target=\"_blank\"><img src=\"https://img.shields.io/github/v/release/runebookai/tome\" alt=\"GitHub Release\" /></a>\n</p>\n\n<p align=\"center\">\n    🔮 Download the Tome Desktop App: <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_x64-setup.exe\">Windows</a> | <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_aarch64.dmg\">MacOS</a>\n</p>\n\n# Tome\n\nTome is a desktop app that lets **anyone** harness the magic of LLMs and MCP. Download Tome, connect any local or remote LLM and hook it up to thousands of MCP servers to create your own magical AI-powered spellbook.\n\n🫥 Want it to be 100% local, 100% private? Use Ollama and Qwen3 with only local MCP servers to cast spells in your own pocket universe. ⚡ Want state of the art cloud models with the latest remote MCP servers? You can have that too. It's all up to you!\n\n🏗️ This is a Technical Preview so bear in mind things will be rough around the edges. [Join us on Discord](https://discord.gg/9CH6us29YA) to share tips, tricks, and issues you run into. Star this repo to stay on top of updates and feature releases!\n\n## 🪄 Features\n\n- 🧙 **Streamlined Beginner Friendly Experience**\n  - Simply download and install Tome and hook up the LLM of your choice\n  - No fiddling with JSON, Docker, python or node\n- 🤖 **AI Model Support**\n  - **Remote**: Google Gemini, OpenAI, any OpenAI API-compatible endpoint\n  - **Local**: Ollama, LM Studio, Cortex, any OpenAI API-compatible endpoint\n- 🔮 **Enhanced MCP support**\n  - UI to install, remove, turn on/off MCP servers\n  - npm, uvx, node, python MCP servers all supported out of box\n- 🏪 **Integration into [Smithery.ai](https://smithery.ai) registry**\n  - Thousands of MCP servers available via one-click installation\n- ✏️ **Customization of context windows and temperature**\n- 🧰 **Native support for tool calls and reasoning models**\n  - UI enhancements that clearly delineate tool calls and thinking messages\n\n## Demo\n\nhttps://github.com/user-attachments/assets/0775d100-3eba-4219-9e2f-360a01f28cce\n\n# Getting Started\n\n## Requirements\n\n- MacOS or Windows (Linux coming soon!)\n- LLM Provider of your choice: [Ollama](https://ollama.com/) or [Gemini API key](https://aistudio.google.com/app/apikey) are easy/free\n- [Download the latest release of Tome](https://github.com/runebookai/tome/releases)\n\n## Quickstart\n\n1. Install [Tome](https://github.com/runebookai/tome/releases)\n2. Connect your preferred LLM provider - OpenAI, Ollama and Gemini are preset but you can also add providers like LM Studio by using http://localhost:1234/v1 as the URL\n3. Open the MCP tab in Tome and install your first [MCP server](https://github.com/modelcontextprotocol/servers) (Fetch is an easy one to get started with, just paste `uvx mcp-server-fetch` into the server field).\n4. Chat with your MCP-powered model! Ask it to fetch the top story on Hacker News.\n\n# Vision\n\nWe want to make local LLMs and MCP accessible to everyone. We're building a tool that allows you to be creative with LLMs, regardless\nof whether you're an engineer, tinkerer, hobbyist, or anyone in between.\n\n## Core Principles\n\n- **Tome is local first:** You are in control of where your data goes.\n- **Tome is for everyone:** You shouldn't have to manage programming languages, package managers, or json config files.\n\n## What's Next\n\nWe've gotten a lot of amazing feedback in the last few weeks since releasing Tome but we've got big plans for the future. We want to break LLMs out of their chatbox, and we've got a lot of features coming to help y'all do that.\n\n- Scheduled tasks: LLMs should be doing helpful things even when you're not in front of the computer.\n- Native integrations: MCP servers are a great way to access tools and information, but we want to add more powerful integrations to interact with LLMs in unique. ways\n- App builder: we believe long term that the best experiences will not be in a chat interface. We have plans to add additional tools that will enable you to create powerful applications and workflows.\n- ??? Let us know what you'd like to see! Join our community via the links below, we'd love to hear from you.\n\n# Community\n\n[Discord](https://discord.gg/9CH6us29YA) [Blog](https://blog.runebook.ai) [Bluesky](https://bsky.app/profile/gettome.app) [Twitter](https://twitter.com/get_tome) \n",
  "origin_sha": "rWLnVvqJwvr9v1fqiTn2LWjwaQbT+x1VplsLjSlpRbU=",
  "translate": "# Tome - Buku Mantra AI Ajaib\n\n<img src=\"https://raw.githubusercontent.com/runebookai/tome/main/static/images/repo-header.png\" alt=\"Tome\" />\n\n<p align=\"center\">\n    <code>aplikasi desktop ajaib yang memberikan kekuatan LLM dan MCP ke tangan semua orang</code>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://discord.gg/9CH6us29YA\" target=\"_blank\"><img src=\"https://img.shields.io/discord/1365100902561742868?logo=discord&logoColor=fff&label=Join%20Us!&color=9D7CD8\" alt=\"Bergabunglah dengan Kami di Discord\" /></a>\n    <a href=\"https://opensource.org/licenses/Apache-2.0\" target=\"_blank\"><img src=\"https://img.shields.io/badge/License-Apache_2.0-blue.svg\" alt=\"Lisensi: Apache 2.0\" /></a>\n    <a href=\"https://github.com/runebookai/tome/releases\" target=\"_blank\"><img src=\"https://img.shields.io/github/v/release/runebookai/tome\" alt=\"Rilis GitHub\" /></a>\n</p>\n\n<p align=\"center\">\n    🔮 Unduh Aplikasi Desktop Tome: <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_x64-setup.exe\">Windows</a> | <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_aarch64.dmg\">MacOS</a>\n</p>\n\n# Tome\n\nTome adalah aplikasi desktop yang memungkinkan **siapa saja** memanfaatkan keajaiban LLM dan MCP. Unduh Tome, hubungkan LLM lokal atau jarak jauh apa pun, dan sambungkan ke ribuan server MCP untuk membuat buku mantra AI ajaib milikmu sendiri.\n\n🫥 Ingin semuanya 100% lokal, 100% privat? Gunakan Ollama dan Qwen3 hanya dengan server MCP lokal untuk merapal mantra di semesta saku milikmu sendiri. ⚡ Ingin model cloud tercanggih dengan server MCP jarak jauh terbaru? Kamu juga bisa memilikinya. Semuanya terserah kamu!\n\n🏗️ Ini adalah Pratinjau Teknis jadi harap maklum jika masih ada kekurangan di sana-sini. [Bergabunglah dengan kami di Discord](https://discord.gg/9CH6us29YA) untuk berbagi tips, trik, dan masalah yang kamu temui. Bintangi repo ini untuk terus mendapat pembaruan dan rilis fitur!\n\n## 🪄 Fitur\n\n- 🧙 **Pengalaman Ramah Pemula yang Sederhana**\n  - Cukup unduh dan instal Tome lalu hubungkan LLM pilihanmu\n  - Tanpa repot mengatur JSON, Docker, python, atau node\n- 🤖 **Dukungan Model AI**\n  - **Jarak Jauh**: Google Gemini, OpenAI, endpoint kompatibel API OpenAI lainnya\n  - **Lokal**: Ollama, LM Studio, Cortex, endpoint kompatibel API OpenAI lainnya\n- 🔮 **Dukungan MCP yang Ditingkatkan**\n  - UI untuk instalasi, menghapus, mengaktifkan/nonaktifkan server MCP\n  - Server MCP npm, uvx, node, python semuanya didukung secara langsung\n- 🏪 **Integrasi dengan [Smithery.ai](https://smithery.ai) registry**\n  - Ribuan server MCP tersedia melalui instalasi sekali klik\n- ✏️ **Kustomisasi jendela konteks dan temperatur**\n- 🧰 **Dukungan native untuk tool call dan reasoning model**\n  - Peningkatan UI yang membedakan dengan jelas antara tool call dan pesan berpikir\n\n## Demo\n\nhttps://github.com/user-attachments/assets/0775d100-3eba-4219-9e2f-360a01f28cce\n\n# Memulai\n\n## Persyaratan\n\n- MacOS atau Windows (Linux segera hadir!)\n- Penyedia LLM pilihanmu: [Ollama](https://ollama.com/) atau [Gemini API key](https://aistudio.google.com/app/apikey) adalah yang mudah/gratis\n- [Unduh rilis terbaru Tome](https://github.com/runebookai/tome/releases)\n\n## Quickstart\n\n1. Instal [Tome](https://github.com/runebookai/tome/releases)\n2. Hubungkan penyedia LLM pilihanmu - OpenAI, Ollama dan Gemini sudah tersedia secara preset, tetapi kamu juga dapat menambah penyedia seperti LM Studio dengan menggunakan http://localhost:1234/v1 sebagai URL-nya\n3. Buka tab MCP di Tome dan instal [server MCP](https://github.com/modelcontextprotocol/servers) pertamamu (Fetch adalah yang mudah untuk memulai, cukup tempelkan `uvx mcp-server-fetch` ke kolom server).\n4. Chat dengan model berbasis MCP milikmu! Minta untuk mengambil berita utama di Hacker News.\n\n# Visi\n\nKami ingin membuat LLM lokal dan MCP dapat diakses oleh semua orang. Kami membangun alat yang memungkinkan kamu berkreasi dengan LLM, terlepas dari apakah kamu seorang engineer, tukang oprek, hobiis, atau siapa pun di antaranya.\n\n## Prinsip Inti\n\n- **Tome mengutamakan lokal:** Kamu mengendalikan ke mana data milikmu pergi.\n- **Tome untuk semua orang:** Kamu tidak perlu mengatur bahasa pemrograman, package manager, atau file konfigurasi json.\n\n## Selanjutnya\n\nKami telah menerima banyak masukan luar biasa dalam beberapa minggu terakhir sejak merilis Tome, tetapi kami punya rencana besar untuk ke depannya. Kami ingin membawa LLM keluar dari kotak chat-nya, dan banyak fitur akan hadir untuk membantu kalian melakukan hal itu.\n\n- Tugas terjadwal: LLM harus bisa melakukan hal-hal bermanfaat bahkan saat kamu tidak di depan komputer.\n- Integrasi native: Server MCP adalah cara hebat mengakses alat dan informasi, tapi kami ingin menambahkan integrasi yang lebih kuat untuk berinteraksi dengan LLM dengan cara unik.\n- Pembuat aplikasi: kami percaya, dalam jangka panjang, pengalaman terbaik tidak akan berada di antarmuka chat. Kami berencana menambah alat tambahan yang memungkinkan kamu membuat aplikasi dan alur kerja yang kuat.\n- ??? Beri tahu kami apa yang ingin kamu lihat! Bergabunglah dengan komunitas kami melalui tautan di bawah ini, kami ingin mendengar pendapatmu.\n\n# Komunitas\n\n[Discord](https://discord.gg/9CH6us29YA) [Blog](https://blog.runebook.ai) [Bluesky](https://bsky.app/profile/gettome.app) [Twitter](https://twitter.com/get_tome) ",
  "status": "ok"
}