{
  "id": 1,
  "origin": "# Tome - Magical AI Spellbook\n\n<img src=\"https://raw.githubusercontent.com/runebookai/tome/main/static/images/repo-header.png\" alt=\"Tome\" />\n\n<p align=\"center\">\n    <code>a magical desktop app that puts the power of LLMs and MCP in the hands of everyone</code>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://discord.gg/9CH6us29YA\" target=\"_blank\"><img src=\"https://img.shields.io/discord/1365100902561742868?logo=discord&logoColor=fff&label=Join%20Us!&color=9D7CD8\" alt=\"Join Us on Discord\" /></a>\n    <a href=\"https://opensource.org/licenses/Apache-2.0\" target=\"_blank\"><img src=\"https://img.shields.io/badge/License-Apache_2.0-blue.svg\" alt=\"License: Apache 2.0\" /></a>\n    <a href=\"https://github.com/runebookai/tome/releases\" target=\"_blank\"><img src=\"https://img.shields.io/github/v/release/runebookai/tome\" alt=\"GitHub Release\" /></a>\n</p>\n\n<p align=\"center\">\n    üîÆ Download the Tome Desktop App: <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_x64-setup.exe\">Windows</a> | <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_aarch64.dmg\">MacOS</a>\n</p>\n\n# Tome\n\nTome is a desktop app that lets **anyone** harness the magic of LLMs and MCP. Download Tome, connect any local or remote LLM and hook it up to thousands of MCP servers to create your own magical AI-powered spellbook.\n\nü´• Want it to be 100% local, 100% private? Use Ollama and Qwen3 with only local MCP servers to cast spells in your own pocket universe. ‚ö° Want state of the art cloud models with the latest remote MCP servers? You can have that too. It's all up to you!\n\nüèóÔ∏è This is a Technical Preview so bear in mind things will be rough around the edges. [Join us on Discord](https://discord.gg/9CH6us29YA) to share tips, tricks, and issues you run into. Star this repo to stay on top of updates and feature releases!\n\n## ü™Ñ Features\n\n- üßô **Streamlined Beginner Friendly Experience**\n  - Simply download and install Tome and hook up the LLM of your choice\n  - No fiddling with JSON, Docker, python or node\n- ü§ñ **AI Model Support**\n  - **Remote**: Google Gemini, OpenAI, any OpenAI API-compatible endpoint\n  - **Local**: Ollama, LM Studio, Cortex, any OpenAI API-compatible endpoint\n- üîÆ **Enhanced MCP support**\n  - UI to install, remove, turn on/off MCP servers\n  - npm, uvx, node, python MCP servers all supported out of box\n- üè™ **Integration into [Smithery.ai](https://smithery.ai) registry**\n  - Thousands of MCP servers available via one-click installation\n- ‚úèÔ∏è **Customization of context windows and temperature**\n- üß∞ **Native support for tool calls and reasoning models**\n  - UI enhancements that clearly delineate tool calls and thinking messages\n\n## Demo\n\nhttps://github.com/user-attachments/assets/0775d100-3eba-4219-9e2f-360a01f28cce\n\n# Getting Started\n\n## Requirements\n\n- MacOS or Windows (Linux coming soon!)\n- LLM Provider of your choice: [Ollama](https://ollama.com/) or [Gemini API key](https://aistudio.google.com/app/apikey) are easy/free\n- [Download the latest release of Tome](https://github.com/runebookai/tome/releases)\n\n## Quickstart\n\n1. Install [Tome](https://github.com/runebookai/tome/releases)\n2. Connect your preferred LLM provider - OpenAI, Ollama and Gemini are preset but you can also add providers like LM Studio by using http://localhost:1234/v1 as the URL\n3. Open the MCP tab in Tome and install your first [MCP server](https://github.com/modelcontextprotocol/servers) (Fetch is an easy one to get started with, just paste `uvx mcp-server-fetch` into the server field).\n4. Chat with your MCP-powered model! Ask it to fetch the top story on Hacker News.\n\n# Vision\n\nWe want to make local LLMs and MCP accessible to everyone. We're building a tool that allows you to be creative with LLMs, regardless\nof whether you're an engineer, tinkerer, hobbyist, or anyone in between.\n\n## Core Principles\n\n- **Tome is local first:** You are in control of where your data goes.\n- **Tome is for everyone:** You shouldn't have to manage programming languages, package managers, or json config files.\n\n## What's Next\n\nWe've gotten a lot of amazing feedback in the last few weeks since releasing Tome but we've got big plans for the future. We want to break LLMs out of their chatbox, and we've got a lot of features coming to help y'all do that.\n\n- Scheduled tasks: LLMs should be doing helpful things even when you're not in front of the computer.\n- Native integrations: MCP servers are a great way to access tools and information, but we want to add more powerful integrations to interact with LLMs in unique. ways\n- App builder: we believe long term that the best experiences will not be in a chat interface. We have plans to add additional tools that will enable you to create powerful applications and workflows.\n- ??? Let us know what you'd like to see! Join our community via the links below, we'd love to hear from you.\n\n# Community\n\n[Discord](https://discord.gg/9CH6us29YA) [Blog](https://blog.runebook.ai) [Bluesky](https://bsky.app/profile/gettome.app) [Twitter](https://twitter.com/get_tome) \n",
  "origin_sha": "rWLnVvqJwvr9v1fqiTn2LWjwaQbT+x1VplsLjSlpRbU=",
  "translate": "# Tome - Grim√≥rio M√°gico de IA\n\n<img src=\"https://raw.githubusercontent.com/runebookai/tome/main/static/images/repo-header.png\" alt=\"Tome\" />\n\n<p align=\"center\">\n    <code>um aplicativo m√°gico de desktop que coloca o poder de LLMs e MCP nas m√£os de todos</code>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://discord.gg/9CH6us29YA\" target=\"_blank\"><img src=\"https://img.shields.io/discord/1365100902561742868?logo=discord&logoColor=fff&label=Join%20Us!&color=9D7CD8\" alt=\"Junte-se a n√≥s no Discord\" /></a>\n    <a href=\"https://opensource.org/licenses/Apache-2.0\" target=\"_blank\"><img src=\"https://img.shields.io/badge/License-Apache_2.0-blue.svg\" alt=\"Licen√ßa: Apache 2.0\" /></a>\n    <a href=\"https://github.com/runebookai/tome/releases\" target=\"_blank\"><img src=\"https://img.shields.io/github/v/release/runebookai/tome\" alt=\"Lan√ßamento no GitHub\" /></a>\n</p>\n\n<p align=\"center\">\n    üîÆ Baixe o aplicativo Tome Desktop: <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_x64-setup.exe\">Windows</a> | <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_aarch64.dmg\">MacOS</a>\n</p>\n\n# Tome\n\nO Tome √© um aplicativo de desktop que permite que **qualquer pessoa** aproveite a magia dos LLMs e MCP. Baixe o Tome, conecte qualquer LLM local ou remoto e ligue-o a milhares de servidores MCP para criar seu pr√≥prio grim√≥rio m√°gico alimentado por IA.\n\nü´• Quer que seja 100% local, 100% privado? Use Ollama e Qwen3 apenas com servidores MCP locais para lan√ßar feiti√ßos em seu pr√≥prio universo de bolso. ‚ö° Quer modelos de nuvem de ponta com os servidores MCP remotos mais recentes? Voc√™ tamb√©m pode ter isso. A escolha √© toda sua!\n\nüèóÔ∏è Esta √© uma Visualiza√ß√£o T√©cnica, ent√£o lembre-se de que as coisas podem estar inacabadas. [Junte-se a n√≥s no Discord](https://discord.gg/9CH6us29YA) para compartilhar dicas, truques e problemas encontrados. D√™ uma estrela neste reposit√≥rio para ficar por dentro das novidades e lan√ßamentos de funcionalidades!\n\n## ü™Ñ Funcionalidades\n\n- üßô **Experi√™ncia Facilitada para Iniciantes**\n  - Basta baixar e instalar o Tome e conectar o LLM de sua escolha\n  - Sem complica√ß√µes com JSON, Docker, python ou node\n- ü§ñ **Suporte a Modelos de IA**\n  - **Remoto**: Google Gemini, OpenAI, qualquer endpoint compat√≠vel com API OpenAI\n  - **Local**: Ollama, LM Studio, Cortex, qualquer endpoint compat√≠vel com API OpenAI\n- üîÆ **Suporte Aprimorado a MCP**\n  - Interface para instalar, remover, ligar/desligar servidores MCP\n  - Servidores MCP npm, uvx, node, python todos suportados nativamente\n- üè™ **Integra√ß√£o com o registro [Smithery.ai](https://smithery.ai)**\n  - Milhares de servidores MCP dispon√≠veis via instala√ß√£o com um clique\n- ‚úèÔ∏è **Personaliza√ß√£o de janelas de contexto e temperatura**\n- üß∞ **Suporte nativo a chamadas de ferramentas e modelos de racioc√≠nio**\n  - Melhorias na interface que destacam claramente chamadas de ferramentas e mensagens de pensamento\n\n## Demonstra√ß√£o\n\nhttps://github.com/user-attachments/assets/0775d100-3eba-4219-9e2f-360a01f28cce\n\n# Primeiros Passos\n\n## Requisitos\n\n- MacOS ou Windows (Linux em breve!)\n- Provedor de LLM de sua escolha: [Ollama](https://ollama.com/) ou [chave da API Gemini](https://aistudio.google.com/app/apikey) s√£o f√°ceis/gr√°tis\n- [Baixe a vers√£o mais recente do Tome](https://github.com/runebookai/tome/releases)\n\n## In√≠cio R√°pido\n\n1. Instale o [Tome](https://github.com/runebookai/tome/releases)\n2. Conecte seu provedor de LLM preferido - OpenAI, Ollama e Gemini j√° v√™m pr√©-configurados, mas voc√™ tamb√©m pode adicionar provedores como o LM Studio usando http://localhost:1234/v1 como URL\n3. Abra a aba MCP no Tome e instale seu primeiro [servidor MCP](https://github.com/modelcontextprotocol/servers) (Fetch √© um f√°cil para come√ßar, basta colar `uvx mcp-server-fetch` no campo do servidor).\n4. Converse com seu modelo alimentado por MCP! Pe√ßa para buscar a principal not√≠cia do Hacker News.\n\n# Vis√£o\n\nQueremos tornar os LLMs locais e MCP acess√≠veis para todos. Estamos construindo uma ferramenta que permite que voc√™ seja criativo com LLMs, independentemente\nde ser engenheiro, entusiasta, hobbyista ou qualquer pessoa entre eles.\n\n## Princ√≠pios Fundamentais\n\n- **Tome √© local primeiro:** Voc√™ est√° no controle de para onde seus dados v√£o.\n- **Tome √© para todos:** Voc√™ n√£o deve precisar gerenciar linguagens de programa√ß√£o, gerenciadores de pacotes ou arquivos de configura√ß√£o json.\n\n## O que vem a seguir\n\nRecebemos muitos feedbacks incr√≠veis nas √∫ltimas semanas desde o lan√ßamento do Tome, mas temos grandes planos para o futuro. Queremos libertar os LLMs da sua caixa de chat, e temos muitos recursos chegando para ajudar voc√™ a fazer isso.\n\n- Tarefas agendadas: LLMs devem realizar tarefas √∫teis mesmo quando voc√™ n√£o est√° na frente do computador.\n- Integra√ß√µes nativas: Servidores MCP s√£o uma √≥tima maneira de acessar ferramentas e informa√ß√µes, mas queremos adicionar integra√ß√µes ainda mais poderosas para interagir com os LLMs de maneiras √∫nicas.\n- Construtor de aplicativos: acreditamos que, a longo prazo, as melhores experi√™ncias n√£o estar√£o em uma interface de chat. Temos planos para adicionar ferramentas adicionais que permitir√£o criar aplicativos e fluxos de trabalho poderosos.\n- ??? Conte-nos o que voc√™ gostaria de ver! Participe da nossa comunidade pelos links abaixo, adorar√≠amos ouvir voc√™.\n\n# Comunidade\n\n[Discord](https://discord.gg/9CH6us29YA) [Blog](https://blog.runebook.ai) [Bluesky](https://bsky.app/profile/gettome.app) [Twitter](https://twitter.com/get_tome) \n",
  "status": "ok"
}