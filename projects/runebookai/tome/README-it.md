# Tome - Libro degli Incantesimi Magico con IA

<img src="https://raw.githubusercontent.com/runebookai/tome/main/static/images/repo-header.png" alt="Tome" />

<p align="center">
    <code>un'app desktop magica che mette il potere degli LLM e MCP nelle mani di tutti</code>
</p>

<p align="center">
    <a href="https://discord.gg/9CH6us29YA" target="_blank"><img src="https://img.shields.io/discord/1365100902561742868?logo=discord&logoColor=fff&label=Join%20Us!&color=9D7CD8" alt="Join Us on Discord" /></a>
    <a href="https://opensource.org/licenses/Apache-2.0" target="_blank"><img src="https://img.shields.io/badge/License-Apache_2.0-blue.svg" alt="License: Apache 2.0" /></a>
    <a href="https://github.com/runebookai/tome/releases" target="_blank"><img src="https://img.shields.io/github/v/release/runebookai/tome" alt="GitHub Release" /></a>
</p>

<p align="center">
    üîÆ Scarica l'app desktop Tome: <a href="https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_x64-setup.exe">Windows</a> | <a href="https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_aarch64.dmg">MacOS</a>
</p>

# Tome

Tome √® un'app desktop che permette a **chiunque** di sfruttare la magia degli LLM e MCP. Scarica Tome, collega qualsiasi LLM locale o remoto e connettilo a migliaia di server MCP per creare il tuo magico libro degli incantesimi alimentato dall'IA.

ü´• Vuoi che sia al 100% locale, al 100% privato? Usa Ollama e Qwen3 solo con server MCP locali per lanciare incantesimi nel tuo universo tascabile. ‚ö° Vuoi modelli cloud all'avanguardia con gli ultimi server MCP remoti? Puoi avere anche questo. Dipende tutto da te!

üèóÔ∏è Questa √® un'Anteprima Tecnica, quindi tieni presente che alcune cose potrebbero essere grezze. [Unisciti a noi su Discord](https://discord.gg/9CH6us29YA) per condividere suggerimenti, trucchi e problemi che incontri. Metti una stella a questo repo per rimanere aggiornato su novit√† e nuove funzionalit√†!

## ü™Ñ Funzionalit√†

- üßô **Esperienza Semplificata per Principianti**
  - Basta scaricare e installare Tome e collegare l'LLM che preferisci
  - Niente complicazioni con JSON, Docker, python o node
- ü§ñ **Supporto Modelli AI**
  - **Remoti**: Google Gemini, OpenAI, qualsiasi endpoint compatibile con OpenAI API
  - **Locali**: Ollama, LM Studio, Cortex, qualsiasi endpoint compatibile con OpenAI API
- üîÆ **Supporto MCP Avanzato**
  - Interfaccia per installare, rimuovere, accendere/spegnere server MCP
  - Server MCP npm, uvx, node, python tutti supportati nativamente
- üè™ **Integrazione nel registro [Smithery.ai](https://smithery.ai)**
  - Migliaia di server MCP disponibili tramite installazione con un clic
- ‚úèÔ∏è **Personalizzazione delle finestre di contesto e della temperatura**
- üß∞ **Supporto nativo per chiamate a strumenti e modelli di ragionamento**
  - Migliorie UI che distinguono chiaramente tra chiamate a strumenti e messaggi di pensiero

## Demo

https://github.com/user-attachments/assets/0775d100-3eba-4219-9e2f-360a01f28cce

# Per Iniziare

## Requisiti

- MacOS o Windows (Linux in arrivo!)
- Un provider LLM a scelta: [Ollama](https://ollama.com/) o [chiave API Gemini](https://aistudio.google.com/app/apikey) sono semplici/gratuiti
- [Scarica l'ultima versione di Tome](https://github.com/runebookai/tome/releases)

## Avvio Rapido

1. Installa [Tome](https://github.com/runebookai/tome/releases)
2. Collega il tuo provider LLM preferito - OpenAI, Ollama e Gemini sono preimpostati ma puoi anche aggiungere provider come LM Studio utilizzando http://localhost:1234/v1 come URL
3. Apri la scheda MCP in Tome e installa il tuo primo [server MCP](https://github.com/modelcontextprotocol/servers) (Fetch √® uno semplice per iniziare, basta incollare `uvx mcp-server-fetch` nel campo del server).
4. Chatta con il tuo modello potenziato da MCP! Chiedigli di recuperare la notizia principale su Hacker News.

# Visione

Vogliamo rendere gli LLM locali e MCP accessibili a tutti. Stiamo costruendo uno strumento che ti permette di essere creativo con gli LLM, indipendentemente dal fatto che tu sia un ingegnere, un maker, un hobbista o chiunque altro.

## Principi Fondamentali

- **Tome √® locale prima di tutto:** Hai il controllo su dove vanno i tuoi dati.
- **Tome √® per tutti:** Non dovresti dover gestire linguaggi di programmazione, package manager o file di configurazione json.

## Cosa ci aspetta

Abbiamo ricevuto molti feedback fantastici nelle ultime settimane dal rilascio di Tome ma abbiamo grandi progetti per il futuro. Vogliamo liberare gli LLM dalla loro chatbox, e abbiamo in arrivo molte funzionalit√† per aiutarti a farlo.

- Attivit√† programmate: gli LLM dovrebbero fare cose utili anche quando non sei davanti al computer.
- Integrazioni native: i server MCP sono un ottimo modo per accedere a strumenti e informazioni, ma vogliamo aggiungere integrazioni ancora pi√π potenti per interagire con gli LLM in modi unici.
- Costruttore di app: crediamo che, a lungo termine, le migliori esperienze non saranno in un'interfaccia di chat. Abbiamo in programma di aggiungere ulteriori strumenti che ti permetteranno di creare potenti applicazioni e flussi di lavoro.
- ??? Facci sapere cosa vorresti vedere! Unisciti alla nostra community tramite i link qui sotto, ci piacerebbe sentirti.

# Community

[Discord](https://discord.gg/9CH6us29YA) [Blog](https://blog.runebook.ai) [Bluesky](https://bsky.app/profile/gettome.app) [Twitter](https://twitter.com/get_tome)

---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-03

---