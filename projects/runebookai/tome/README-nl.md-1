{
  "id": 1,
  "origin": "# Tome - Magical AI Spellbook\n\n<img src=\"https://raw.githubusercontent.com/runebookai/tome/main/static/images/repo-header.png\" alt=\"Tome\" />\n\n<p align=\"center\">\n    <code>a magical desktop app that puts the power of LLMs and MCP in the hands of everyone</code>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://discord.gg/9CH6us29YA\" target=\"_blank\"><img src=\"https://img.shields.io/discord/1365100902561742868?logo=discord&logoColor=fff&label=Join%20Us!&color=9D7CD8\" alt=\"Join Us on Discord\" /></a>\n    <a href=\"https://opensource.org/licenses/Apache-2.0\" target=\"_blank\"><img src=\"https://img.shields.io/badge/License-Apache_2.0-blue.svg\" alt=\"License: Apache 2.0\" /></a>\n    <a href=\"https://github.com/runebookai/tome/releases\" target=\"_blank\"><img src=\"https://img.shields.io/github/v/release/runebookai/tome\" alt=\"GitHub Release\" /></a>\n</p>\n\n<p align=\"center\">\n    üîÆ Download the Tome Desktop App: <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_x64-setup.exe\">Windows</a> | <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_aarch64.dmg\">MacOS</a>\n</p>\n\n# Tome\n\nTome is a desktop app that lets **anyone** harness the magic of LLMs and MCP. Download Tome, connect any local or remote LLM and hook it up to thousands of MCP servers to create your own magical AI-powered spellbook.\n\nü´• Want it to be 100% local, 100% private? Use Ollama and Qwen3 with only local MCP servers to cast spells in your own pocket universe. ‚ö° Want state of the art cloud models with the latest remote MCP servers? You can have that too. It's all up to you!\n\nüèóÔ∏è This is a Technical Preview so bear in mind things will be rough around the edges. [Join us on Discord](https://discord.gg/9CH6us29YA) to share tips, tricks, and issues you run into. Star this repo to stay on top of updates and feature releases!\n\n## ü™Ñ Features\n\n- üßô **Streamlined Beginner Friendly Experience**\n  - Simply download and install Tome and hook up the LLM of your choice\n  - No fiddling with JSON, Docker, python or node\n- ü§ñ **AI Model Support**\n  - **Remote**: Google Gemini, OpenAI, any OpenAI API-compatible endpoint\n  - **Local**: Ollama, LM Studio, Cortex, any OpenAI API-compatible endpoint\n- üîÆ **Enhanced MCP support**\n  - UI to install, remove, turn on/off MCP servers\n  - npm, uvx, node, python MCP servers all supported out of box\n- üè™ **Integration into [Smithery.ai](https://smithery.ai) registry**\n  - Thousands of MCP servers available via one-click installation\n- ‚úèÔ∏è **Customization of context windows and temperature**\n- üß∞ **Native support for tool calls and reasoning models**\n  - UI enhancements that clearly delineate tool calls and thinking messages\n\n## Demo\n\nhttps://github.com/user-attachments/assets/0775d100-3eba-4219-9e2f-360a01f28cce\n\n# Getting Started\n\n## Requirements\n\n- MacOS or Windows (Linux coming soon!)\n- LLM Provider of your choice: [Ollama](https://ollama.com/) or [Gemini API key](https://aistudio.google.com/app/apikey) are easy/free\n- [Download the latest release of Tome](https://github.com/runebookai/tome/releases)\n\n## Quickstart\n\n1. Install [Tome](https://github.com/runebookai/tome/releases)\n2. Connect your preferred LLM provider - OpenAI, Ollama and Gemini are preset but you can also add providers like LM Studio by using http://localhost:1234/v1 as the URL\n3. Open the MCP tab in Tome and install your first [MCP server](https://github.com/modelcontextprotocol/servers) (Fetch is an easy one to get started with, just paste `uvx mcp-server-fetch` into the server field).\n4. Chat with your MCP-powered model! Ask it to fetch the top story on Hacker News.\n\n# Vision\n\nWe want to make local LLMs and MCP accessible to everyone. We're building a tool that allows you to be creative with LLMs, regardless\nof whether you're an engineer, tinkerer, hobbyist, or anyone in between.\n\n## Core Principles\n\n- **Tome is local first:** You are in control of where your data goes.\n- **Tome is for everyone:** You shouldn't have to manage programming languages, package managers, or json config files.\n\n## What's Next\n\nWe've gotten a lot of amazing feedback in the last few weeks since releasing Tome but we've got big plans for the future. We want to break LLMs out of their chatbox, and we've got a lot of features coming to help y'all do that.\n\n- Scheduled tasks: LLMs should be doing helpful things even when you're not in front of the computer.\n- Native integrations: MCP servers are a great way to access tools and information, but we want to add more powerful integrations to interact with LLMs in unique. ways\n- App builder: we believe long term that the best experiences will not be in a chat interface. We have plans to add additional tools that will enable you to create powerful applications and workflows.\n- ??? Let us know what you'd like to see! Join our community via the links below, we'd love to hear from you.\n\n# Community\n\n[Discord](https://discord.gg/9CH6us29YA) [Blog](https://blog.runebook.ai) [Bluesky](https://bsky.app/profile/gettome.app) [Twitter](https://twitter.com/get_tome) \n",
  "origin_sha": "rWLnVvqJwvr9v1fqiTn2LWjwaQbT+x1VplsLjSlpRbU=",
  "translate": "# Tome - Magisch AI Spellbook\n\n<img src=\"https://raw.githubusercontent.com/runebookai/tome/main/static/images/repo-header.png\" alt=\"Tome\" />\n\n<p align=\"center\">\n    <code>een magische desktop-app die de kracht van LLMs en MCP in de handen van iedereen legt</code>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://discord.gg/9CH6us29YA\" target=\"_blank\"><img src=\"https://img.shields.io/discord/1365100902561742868?logo=discord&logoColor=fff&label=Join%20Us!&color=9D7CD8\" alt=\"Join Us on Discord\" /></a>\n    <a href=\"https://opensource.org/licenses/Apache-2.0\" target=\"_blank\"><img src=\"https://img.shields.io/badge/License-Apache_2.0-blue.svg\" alt=\"License: Apache 2.0\" /></a>\n    <a href=\"https://github.com/runebookai/tome/releases\" target=\"_blank\"><img src=\"https://img.shields.io/github/v/release/runebookai/tome\" alt=\"GitHub Release\" /></a>\n</p>\n\n<p align=\"center\">\n    üîÆ Download de Tome Desktop App: <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_x64-setup.exe\">Windows</a> | <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_aarch64.dmg\">MacOS</a>\n</p>\n\n# Tome\n\nTome is een desktop-app waarmee **iedereen** de magie van LLMs en MCP kan benutten. Download Tome, verbind elke lokale of externe LLM en koppel hem aan duizenden MCP-servers om je eigen magische AI-aangedreven spellbook te cre√´ren.\n\nü´• Wil je het 100% lokaal, 100% priv√©? Gebruik Ollama en Qwen3 alleen met lokale MCP-servers om spreuken uit te spreken in je eigen pocketuniversum. ‚ö° Wil je state-of-the-art cloudmodellen met de nieuwste externe MCP-servers? Dat kan ook. Het is helemaal aan jou!\n\nüèóÔ∏è Dit is een Technische Preview, dus houd er rekening mee dat sommige dingen nog ruw kunnen zijn. [Word lid van onze Discord](https://discord.gg/9CH6us29YA) om tips, trucs en problemen te delen. Ster deze repo om op de hoogte te blijven van updates en nieuwe functies!\n\n## ü™Ñ Functies\n\n- üßô **Gestroomlijnde, gebruiksvriendelijke ervaring**\n  - Download en installeer Tome en koppel eenvoudig de LLM van jouw keuze\n  - Geen gepruts met JSON, Docker, python of node\n- ü§ñ **Ondersteuning voor AI-modellen**\n  - **Extern**: Google Gemini, OpenAI, elke OpenAI API-compatibele endpoint\n  - **Lokaal**: Ollama, LM Studio, Cortex, elke OpenAI API-compatibele endpoint\n- üîÆ **Uitgebreide MCP-ondersteuning**\n  - UI om MCP-servers te installeren, verwijderen, aan/uit te zetten\n  - npm, uvx, node, python MCP-servers worden standaard ondersteund\n- üè™ **Integratie met [Smithery.ai](https://smithery.ai) registry**\n  - Duizenden MCP-servers beschikbaar via installatie met √©√©n klik\n- ‚úèÔ∏è **Aanpassen van contextwindows en temperatuur**\n- üß∞ **Native ondersteuning voor tool calls en redeneermodellen**\n  - UI-verbeteringen die tool calls en denkberichten duidelijk onderscheiden\n\n## Demo\n\nhttps://github.com/user-attachments/assets/0775d100-3eba-4219-9e2f-360a01f28cce\n\n# Aan de slag\n\n## Vereisten\n\n- MacOS of Windows (Linux binnenkort!)\n- LLM-provider naar keuze: [Ollama](https://ollama.com/) of [Gemini API key](https://aistudio.google.com/app/apikey) zijn eenvoudig/gratis\n- [Download de laatste release van Tome](https://github.com/runebookai/tome/releases)\n\n## Quickstart\n\n1. Installeer [Tome](https://github.com/runebookai/tome/releases)\n2. Verbind jouw favoriete LLM-provider - OpenAI, Ollama en Gemini zijn vooringesteld, maar je kunt ook providers zoals LM Studio toevoegen door http://localhost:1234/v1 als URL te gebruiken\n3. Open het MCP-tabblad in Tome en installeer je eerste [MCP-server](https://github.com/modelcontextprotocol/servers) (Fetch is een makkelijke om mee te beginnen, plak gewoon `uvx mcp-server-fetch` in het serverveld).\n4. Chat met je MCP-aangedreven model! Vraag bijvoorbeeld naar het belangrijkste nieuws op Hacker News.\n\n# Visie\n\nWij willen lokale LLMs en MCP toegankelijk maken voor iedereen. We bouwen een tool waarmee je creatief kunt zijn met LLMs, ongeacht\nof je een ingenieur, knutselaar, hobbyist of iets daartussen bent.\n\n## Kernprincipes\n\n- **Tome is lokaal eerst:** Jij bepaalt waar je data naartoe gaat.\n- **Tome is voor iedereen:** Je zou geen programmeertalen, pakketmanagers of json-configuratiebestanden hoeven te beheren.\n\n## Wat komt eraan\n\nWe hebben de afgelopen weken veel geweldig feedback ontvangen sinds de release van Tome, maar we hebben grote plannen voor de toekomst. We willen LLMs uit hun chatbox bevrijden, en we hebben veel functies op komst om jullie daarbij te helpen.\n\n- Geplande taken: LLMs moeten ook nuttige dingen doen als je niet achter de computer zit.\n- Native integraties: MCP-servers zijn een geweldige manier om tools en informatie te benaderen, maar we willen krachtigere integraties toevoegen om op unieke manieren met LLMs te interageren.\n- App builder: we geloven dat de beste ervaringen op de lange termijn niet in een chatinterface zullen zijn. We zijn van plan extra tools toe te voegen waarmee je krachtige applicaties en workflows kunt cre√´ren.\n- ??? Laat ons weten wat jij graag zou willen zien! Word lid van onze community via de links hieronder, we horen graag van je.\n\n# Community\n\n[Discord](https://discord.gg/9CH6us29YA) [Blog](https://blog.runebook.ai) [Bluesky](https://bsky.app/profile/gettome.app) [Twitter](https://twitter.com/get_tome) ",
  "status": "ok"
}