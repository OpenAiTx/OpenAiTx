{
  "id": 1,
  "origin": "# Tome - Magical AI Spellbook\n\n<img src=\"https://raw.githubusercontent.com/runebookai/tome/main/static/images/repo-header.png\" alt=\"Tome\" />\n\n<p align=\"center\">\n    <code>a magical desktop app that puts the power of LLMs and MCP in the hands of everyone</code>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://discord.gg/9CH6us29YA\" target=\"_blank\"><img src=\"https://img.shields.io/discord/1365100902561742868?logo=discord&logoColor=fff&label=Join%20Us!&color=9D7CD8\" alt=\"Join Us on Discord\" /></a>\n    <a href=\"https://opensource.org/licenses/Apache-2.0\" target=\"_blank\"><img src=\"https://img.shields.io/badge/License-Apache_2.0-blue.svg\" alt=\"License: Apache 2.0\" /></a>\n    <a href=\"https://github.com/runebookai/tome/releases\" target=\"_blank\"><img src=\"https://img.shields.io/github/v/release/runebookai/tome\" alt=\"GitHub Release\" /></a>\n</p>\n\n<p align=\"center\">\n    🔮 Download the Tome Desktop App: <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_x64-setup.exe\">Windows</a> | <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_aarch64.dmg\">MacOS</a>\n</p>\n\n# Tome\n\nTome is a desktop app that lets **anyone** harness the magic of LLMs and MCP. Download Tome, connect any local or remote LLM and hook it up to thousands of MCP servers to create your own magical AI-powered spellbook.\n\n🫥 Want it to be 100% local, 100% private? Use Ollama and Qwen3 with only local MCP servers to cast spells in your own pocket universe. ⚡ Want state of the art cloud models with the latest remote MCP servers? You can have that too. It's all up to you!\n\n🏗️ This is a Technical Preview so bear in mind things will be rough around the edges. [Join us on Discord](https://discord.gg/9CH6us29YA) to share tips, tricks, and issues you run into. Star this repo to stay on top of updates and feature releases!\n\n## 🪄 Features\n\n- 🧙 **Streamlined Beginner Friendly Experience**\n  - Simply download and install Tome and hook up the LLM of your choice\n  - No fiddling with JSON, Docker, python or node\n- 🤖 **AI Model Support**\n  - **Remote**: Google Gemini, OpenAI, any OpenAI API-compatible endpoint\n  - **Local**: Ollama, LM Studio, Cortex, any OpenAI API-compatible endpoint\n- 🔮 **Enhanced MCP support**\n  - UI to install, remove, turn on/off MCP servers\n  - npm, uvx, node, python MCP servers all supported out of box\n- 🏪 **Integration into [Smithery.ai](https://smithery.ai) registry**\n  - Thousands of MCP servers available via one-click installation\n- ✏️ **Customization of context windows and temperature**\n- 🧰 **Native support for tool calls and reasoning models**\n  - UI enhancements that clearly delineate tool calls and thinking messages\n\n## Demo\n\nhttps://github.com/user-attachments/assets/0775d100-3eba-4219-9e2f-360a01f28cce\n\n# Getting Started\n\n## Requirements\n\n- MacOS or Windows (Linux coming soon!)\n- LLM Provider of your choice: [Ollama](https://ollama.com/) or [Gemini API key](https://aistudio.google.com/app/apikey) are easy/free\n- [Download the latest release of Tome](https://github.com/runebookai/tome/releases)\n\n## Quickstart\n\n1. Install [Tome](https://github.com/runebookai/tome/releases)\n2. Connect your preferred LLM provider - OpenAI, Ollama and Gemini are preset but you can also add providers like LM Studio by using http://localhost:1234/v1 as the URL\n3. Open the MCP tab in Tome and install your first [MCP server](https://github.com/modelcontextprotocol/servers) (Fetch is an easy one to get started with, just paste `uvx mcp-server-fetch` into the server field).\n4. Chat with your MCP-powered model! Ask it to fetch the top story on Hacker News.\n\n# Vision\n\nWe want to make local LLMs and MCP accessible to everyone. We're building a tool that allows you to be creative with LLMs, regardless\nof whether you're an engineer, tinkerer, hobbyist, or anyone in between.\n\n## Core Principles\n\n- **Tome is local first:** You are in control of where your data goes.\n- **Tome is for everyone:** You shouldn't have to manage programming languages, package managers, or json config files.\n\n## What's Next\n\nWe've gotten a lot of amazing feedback in the last few weeks since releasing Tome but we've got big plans for the future. We want to break LLMs out of their chatbox, and we've got a lot of features coming to help y'all do that.\n\n- Scheduled tasks: LLMs should be doing helpful things even when you're not in front of the computer.\n- Native integrations: MCP servers are a great way to access tools and information, but we want to add more powerful integrations to interact with LLMs in unique. ways\n- App builder: we believe long term that the best experiences will not be in a chat interface. We have plans to add additional tools that will enable you to create powerful applications and workflows.\n- ??? Let us know what you'd like to see! Join our community via the links below, we'd love to hear from you.\n\n# Community\n\n[Discord](https://discord.gg/9CH6us29YA) [Blog](https://blog.runebook.ai) [Bluesky](https://bsky.app/profile/gettome.app) [Twitter](https://twitter.com/get_tome) \n",
  "origin_sha": "rWLnVvqJwvr9v1fqiTn2LWjwaQbT+x1VplsLjSlpRbU=",
  "translate": "# Tome - کتاب جادویی هوش مصنوعی\n\n<img src=\"https://raw.githubusercontent.com/runebookai/tome/main/static/images/repo-header.png\" alt=\"Tome\" />\n\n<p align=\"center\">\n    <code>یک اپلیکیشن دسکتاپ جادویی که قدرت LLMها و MCP را در اختیار همه قرار می‌دهد</code>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://discord.gg/9CH6us29YA\" target=\"_blank\"><img src=\"https://img.shields.io/discord/1365100902561742868?logo=discord&logoColor=fff&label=Join%20Us!&color=9D7CD8\" alt=\"Join Us on Discord\" /></a>\n    <a href=\"https://opensource.org/licenses/Apache-2.0\" target=\"_blank\"><img src=\"https://img.shields.io/badge/License-Apache_2.0-blue.svg\" alt=\"License: Apache 2.0\" /></a>\n    <a href=\"https://github.com/runebookai/tome/releases\" target=\"_blank\"><img src=\"https://img.shields.io/github/v/release/runebookai/tome\" alt=\"GitHub Release\" /></a>\n</p>\n\n<p align=\"center\">\n    🔮 دانلود اپلیکیشن دسکتاپ Tome: <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_x64-setup.exe\">ویندوز</a> | <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_aarch64.dmg\">مک‌او‌اس</a>\n</p>\n\n# Tome\n\nTome یک اپلیکیشن دسکتاپ است که به **هر کسی** اجازه می‌دهد جادوی LLMها و MCP را به کار گیرد. Tome را دانلود کنید، هر LLM محلی یا راه‌دور را متصل کنید و آن را به هزاران سرور MCP وصل کنید تا کتاب جادویی هوش مصنوعی خود را بسازید.\n\n🫥 می‌خواهید همه‌چیز ۱۰۰٪ محلی و ۱۰۰٪ خصوصی باشد؟ از Ollama و Qwen3 فقط با سرورهای MCP محلی استفاده کنید تا طلسم‌های خود را در جهان جیبی‌تان اجرا کنید. ⚡ به مدل‌های پیشرفته ابری با جدیدترین سرورهای MCP راه‌دور نیاز دارید؟ این هم ممکن است. انتخاب با شماست!\n\n🏗️ این نسخه پیش‌نمایش فنی است، بنابراین ممکن است همه‌چیز کاملاً بی‌نقص نباشد. [به ما در دیسکورد بپیوندید](https://discord.gg/9CH6us29YA) تا نکات، ترفندها و مشکلاتی که با آن مواجه می‌شوید را به اشتراک بگذارید. این مخزن را ستاره‌دار کنید تا از بروزرسانی‌ها و ویژگی‌های جدید مطلع شوید!\n\n## 🪄 ویژگی‌ها\n\n- 🧙 **تجربه کاربری ساده و مناسب مبتدیان**\n  - فقط Tome را دانلود و نصب کنید و LLM دلخواه خود را وصل کنید\n  - نیازی به کار با JSON، Docker، پایتون یا node نیست\n- 🤖 **پشتیبانی از مدل‌های هوش مصنوعی**\n  - **راه‌دور**: Google Gemini، OpenAI، هر نقطه پایانی سازگار با API OpenAI\n  - **محلی**: Ollama، LM Studio، Cortex، هر نقطه پایانی سازگار با API OpenAI\n- 🔮 **پشتیبانی پیشرفته از MCP**\n  - رابط کاربری برای نصب، حذف، روشن/خاموش کردن سرورهای MCP\n  - سرورهای MCP با npm، uvx، node، python به طور پیش‌فرض پشتیبانی می‌شوند\n- 🏪 **ادغام با رجیستری [Smithery.ai](https://smithery.ai)**\n  - هزاران سرور MCP با نصب یک‌کلیکی در دسترس هستند\n- ✏️ **شخصی‌سازی پنجره‌های کانتکست و دما (temperature)**\n- 🧰 **پشتیبانی بومی از tool calls و مدل‌های استدلال**\n  - بهبود رابط کاربری که تماس ابزارها و پیام‌های فکری را به وضوح نشان می‌دهد\n\n## دمو\n\nhttps://github.com/user-attachments/assets/0775d100-3eba-4219-9e2f-360a01f28cce\n\n# شروع به کار\n\n## پیش‌نیازها\n\n- مک‌او‌اس یا ویندوز (به‌زودی لینوکس!)\n- ارائه‌دهنده LLM مورد نظر شما: [Ollama](https://ollama.com/) یا [کلید API جِمینی](https://aistudio.google.com/app/apikey) به‌راحتی/رایگان در دسترس هستند\n- [آخرین نسخه Tome را دانلود کنید](https://github.com/runebookai/tome/releases)\n\n## راه‌اندازی سریع\n\n1. [Tome را نصب کنید](https://github.com/runebookai/tome/releases)\n2. ارائه‌دهنده LLM دلخواه خود را متصل کنید - OpenAI، Ollama و Gemini از پیش تنظیم شده‌اند اما می‌توانید ارائه‌دهندگانی مانند LM Studio را نیز با وارد کردن http://localhost:1234/v1 به عنوان URL اضافه کنید\n3. تب MCP را در Tome باز کرده و اولین [سرور MCP خود](https://github.com/modelcontextprotocol/servers) را نصب کنید (Fetch برای شروع گزینه ساده‌ای است، کافیست `uvx mcp-server-fetch` را در فیلد سرور جایگذاری کنید).\n4. با مدل مجهز به MCP خود گفتگو کنید! از آن بخواهید برترین خبر Hacker News را بازیابی کند.\n\n# چشم‌انداز\n\nما می‌خواهیم LLMهای محلی و MCP را برای همه قابل دسترس کنیم. ما در حال ساخت ابزاری هستیم که به شما اجازه می‌دهد با LLMها خلاقیت به خرج دهید، فارغ از اینکه مهندس، علاقه‌مند، هابیست یا هر چیز دیگری باشید.\n\n## اصول اصلی\n\n- **Tome اول محلی است:** شما کنترل کامل دارید که داده‌هایتان کجا برود.\n- **Tome برای همه است:** نباید مجبور باشید زبان‌های برنامه‌نویسی، مدیریت بسته‌ها یا فایل‌های پیکربندی json را مدیریت کنید.\n\n## برنامه‌های آینده\n\nاز زمان انتشار Tome بازخوردهای شگفت‌انگیزی دریافت کرده‌ایم اما برنامه‌های بزرگی برای آینده داریم. می‌خواهیم LLMها را از چت‌باکس‌شان خارج کنیم و ویژگی‌های زیادی در راه است تا به شما کمک کنیم این کار را انجام دهید.\n\n- وظایف زمان‌بندی‌شده: LLMها باید حتی زمانی که مقابل کامپیوتر نیستید، کارهای مفیدی انجام دهند.\n- یکپارچه‌سازی بومی: سرورهای MCP راهی عالی برای دسترسی به ابزارها و اطلاعات هستند، اما می‌خواهیم یکپارچه‌سازی‌های قدرتمندتری برای تعامل منحصربه‌فرد با LLMها اضافه کنیم.\n- سازنده اپلیکیشن: ما باور داریم در بلندمدت بهترین تجربه‌ها صرفاً در رابط چت نخواهند بود. برنامه داریم ابزارهای بیشتری اضافه کنیم تا بتوانید اپلیکیشن‌ها و گردش‌کارهای قدرتمندی بسازید.\n- ??? نظرات خود را به ما بگویید! از طریق لینک‌های زیر به جامعه ما بپیوندید، خوشحال می‌شویم از شما بشنویم.\n\n# جامعه\n\n[دیسکورد](https://discord.gg/9CH6us29YA) [بلاگ](https://blog.runebook.ai) [بلوسکای](https://bsky.app/profile/gettome.app) [توییتر](https://twitter.com/get_tome) \n",
  "status": "ok"
}