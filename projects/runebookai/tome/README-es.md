# Tome - Libro de Hechizos M√°gico de IA

<img src="https://raw.githubusercontent.com/runebookai/tome/main/static/images/repo-header.png" alt="Tome" />

<p align="center">
    <code>una aplicaci√≥n de escritorio m√°gica que pone el poder de los LLMs y MCP en manos de todos</code>
</p>

<p align="center">
    <a href="https://discord.gg/9CH6us29YA" target="_blank"><img src="https://img.shields.io/discord/1365100902561742868?logo=discord&logoColor=fff&label=Join%20Us!&color=9D7CD8" alt="√önete a nosotros en Discord" /></a>
    <a href="https://opensource.org/licenses/Apache-2.0" target="_blank"><img src="https://img.shields.io/badge/License-Apache_2.0-blue.svg" alt="Licencia: Apache 2.0" /></a>
    <a href="https://github.com/runebookai/tome/releases" target="_blank"><img src="https://img.shields.io/github/v/release/runebookai/tome" alt="GitHub Release" /></a>
</p>

<p align="center">
    üîÆ Descarga la aplicaci√≥n de escritorio Tome: <a href="https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_x64-setup.exe">Windows</a> | <a href="https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_aarch64.dmg">MacOS</a>
</p>

# Tome

Tome es una aplicaci√≥n de escritorio que permite que **cualquiera** aproveche la magia de los LLMs y MCP. Descarga Tome, conecta cualquier LLM local o remoto y enl√°zalo con miles de servidores MCP para crear tu propio libro de hechizos impulsado por IA.

ü´• ¬øQuieres que sea 100% local, 100% privado? Usa Ollama y Qwen3 solo con servidores MCP locales para lanzar hechizos en tu propio universo de bolsillo. ‚ö° ¬øQuieres modelos en la nube de √∫ltima generaci√≥n con los servidores MCP remotos m√°s recientes? Tambi√©n puedes tener eso. ¬°Todo depende de ti!

üèóÔ∏è Esta es una Vista Previa T√©cnica, as√≠ que ten en cuenta que habr√° aspectos por pulir. [√önete a nosotros en Discord](https://discord.gg/9CH6us29YA) para compartir consejos, trucos y problemas que encuentres. ¬°Dale una estrella a este repositorio para estar al tanto de actualizaciones y lanzamientos de nuevas funciones!

## ü™Ñ Caracter√≠sticas

- üßô **Experiencia simplificada y apta para principiantes**
  - Simplemente descarga e instala Tome y conecta el LLM de tu elecci√≥n
  - Sin complicaciones con JSON, Docker, python o node
- ü§ñ **Soporte para modelos de IA**
  - **Remoto**: Google Gemini, OpenAI, cualquier endpoint compatible con la API de OpenAI
  - **Local**: Ollama, LM Studio, Cortex, cualquier endpoint compatible con la API de OpenAI
- üîÆ **Soporte mejorado para MCP**
  - Interfaz para instalar, eliminar, activar/desactivar servidores MCP
  - Soporte inmediato para servidores MCP basados en npm, uvx, node y python
- üè™ **Integraci√≥n en el registro de [Smithery.ai](https://smithery.ai)**
  - Miles de servidores MCP disponibles mediante instalaci√≥n con un solo clic
- ‚úèÔ∏è **Personalizaci√≥n de ventanas de contexto y temperatura**
- üß∞ **Soporte nativo para llamadas a herramientas y modelos de razonamiento**
  - Mejoras en la interfaz que distinguen claramente las llamadas a herramientas y los mensajes de razonamiento

## Demo

https://github.com/user-attachments/assets/0775d100-3eba-4219-9e2f-360a01f28cce

# Primeros Pasos

## Requisitos

- MacOS o Windows (¬°Linux pr√≥ximamente!)
- Proveedor de LLM de tu elecci√≥n: [Ollama](https://ollama.com/) o [clave de API de Gemini](https://aistudio.google.com/app/apikey) son f√°ciles/gratis
- [Descarga la √∫ltima versi√≥n de Tome](https://github.com/runebookai/tome/releases)

## Inicio R√°pido

1. Instala [Tome](https://github.com/runebookai/tome/releases)
2. Conecta tu proveedor LLM preferido: OpenAI, Ollama y Gemini ya est√°n preconfigurados, pero tambi√©n puedes a√±adir proveedores como LM Studio usando http://localhost:1234/v1 como URL
3. Abre la pesta√±a MCP en Tome e instala tu primer [servidor MCP](https://github.com/modelcontextprotocol/servers) (Fetch es uno f√°cil para empezar, solo pega `uvx mcp-server-fetch` en el campo del servidor).
4. ¬°Chatea con tu modelo potenciado por MCP! P√≠dele que obtenga la noticia principal en Hacker News.

# Visi√≥n

Queremos hacer que los LLMs locales y MCP sean accesibles para todos. Estamos construyendo una herramienta que te permite ser creativo con los LLMs, sin importar si eres ingeniero, aficionado, entusiasta o cualquier persona intermedia.

## Principios Fundamentales

- **Tome es primero local:** T√∫ tienes el control de a d√≥nde va tu informaci√≥n.
- **Tome es para todos:** No deber√≠as tener que manejar lenguajes de programaci√≥n, gestores de paquetes o archivos de configuraci√≥n json.

## Pr√≥ximos Pasos

Hemos recibido muchos comentarios incre√≠bles en las √∫ltimas semanas desde que lanzamos Tome, pero tenemos grandes planes para el futuro. Queremos sacar a los LLMs de su caja de chat, y tenemos muchas funciones en camino para ayudarles a hacer eso.

- Tareas programadas: Los LLMs deber√≠an hacer cosas √∫tiles incluso cuando no est√©s frente a la computadora.
- Integraciones nativas: Los servidores MCP son una excelente forma de acceder a herramientas e informaci√≥n, pero queremos a√±adir integraciones m√°s potentes para interactuar con los LLMs de formas √∫nicas.
- Constructor de aplicaciones: creemos que a largo plazo las mejores experiencias no estar√°n en una interfaz de chat. Tenemos planes para a√±adir herramientas adicionales que te permitir√°n crear aplicaciones y flujos de trabajo potentes.
- ??? ¬°Cu√©ntanos qu√© te gustar√≠a ver! √önete a nuestra comunidad a trav√©s de los enlaces de abajo, nos encantar√≠a escucharte.

# Comunidad

[Discord](https://discord.gg/9CH6us29YA) [Blog](https://blog.runebook.ai) [Bluesky](https://bsky.app/profile/gettome.app) [Twitter](https://twitter.com/get_tome) 


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-03

---