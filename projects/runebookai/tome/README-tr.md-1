{
  "id": 1,
  "origin": "# Tome - Magical AI Spellbook\n\n<img src=\"https://raw.githubusercontent.com/runebookai/tome/main/static/images/repo-header.png\" alt=\"Tome\" />\n\n<p align=\"center\">\n    <code>a magical desktop app that puts the power of LLMs and MCP in the hands of everyone</code>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://discord.gg/9CH6us29YA\" target=\"_blank\"><img src=\"https://img.shields.io/discord/1365100902561742868?logo=discord&logoColor=fff&label=Join%20Us!&color=9D7CD8\" alt=\"Join Us on Discord\" /></a>\n    <a href=\"https://opensource.org/licenses/Apache-2.0\" target=\"_blank\"><img src=\"https://img.shields.io/badge/License-Apache_2.0-blue.svg\" alt=\"License: Apache 2.0\" /></a>\n    <a href=\"https://github.com/runebookai/tome/releases\" target=\"_blank\"><img src=\"https://img.shields.io/github/v/release/runebookai/tome\" alt=\"GitHub Release\" /></a>\n</p>\n\n<p align=\"center\">\n    ğŸ”® Download the Tome Desktop App: <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_x64-setup.exe\">Windows</a> | <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_aarch64.dmg\">MacOS</a>\n</p>\n\n# Tome\n\nTome is a desktop app that lets **anyone** harness the magic of LLMs and MCP. Download Tome, connect any local or remote LLM and hook it up to thousands of MCP servers to create your own magical AI-powered spellbook.\n\nğŸ«¥ Want it to be 100% local, 100% private? Use Ollama and Qwen3 with only local MCP servers to cast spells in your own pocket universe. âš¡ Want state of the art cloud models with the latest remote MCP servers? You can have that too. It's all up to you!\n\nğŸ—ï¸ This is a Technical Preview so bear in mind things will be rough around the edges. [Join us on Discord](https://discord.gg/9CH6us29YA) to share tips, tricks, and issues you run into. Star this repo to stay on top of updates and feature releases!\n\n## ğŸª„ Features\n\n- ğŸ§™ **Streamlined Beginner Friendly Experience**\n  - Simply download and install Tome and hook up the LLM of your choice\n  - No fiddling with JSON, Docker, python or node\n- ğŸ¤– **AI Model Support**\n  - **Remote**: Google Gemini, OpenAI, any OpenAI API-compatible endpoint\n  - **Local**: Ollama, LM Studio, Cortex, any OpenAI API-compatible endpoint\n- ğŸ”® **Enhanced MCP support**\n  - UI to install, remove, turn on/off MCP servers\n  - npm, uvx, node, python MCP servers all supported out of box\n- ğŸª **Integration into [Smithery.ai](https://smithery.ai) registry**\n  - Thousands of MCP servers available via one-click installation\n- âœï¸ **Customization of context windows and temperature**\n- ğŸ§° **Native support for tool calls and reasoning models**\n  - UI enhancements that clearly delineate tool calls and thinking messages\n\n## Demo\n\nhttps://github.com/user-attachments/assets/0775d100-3eba-4219-9e2f-360a01f28cce\n\n# Getting Started\n\n## Requirements\n\n- MacOS or Windows (Linux coming soon!)\n- LLM Provider of your choice: [Ollama](https://ollama.com/) or [Gemini API key](https://aistudio.google.com/app/apikey) are easy/free\n- [Download the latest release of Tome](https://github.com/runebookai/tome/releases)\n\n## Quickstart\n\n1. Install [Tome](https://github.com/runebookai/tome/releases)\n2. Connect your preferred LLM provider - OpenAI, Ollama and Gemini are preset but you can also add providers like LM Studio by using http://localhost:1234/v1 as the URL\n3. Open the MCP tab in Tome and install your first [MCP server](https://github.com/modelcontextprotocol/servers) (Fetch is an easy one to get started with, just paste `uvx mcp-server-fetch` into the server field).\n4. Chat with your MCP-powered model! Ask it to fetch the top story on Hacker News.\n\n# Vision\n\nWe want to make local LLMs and MCP accessible to everyone. We're building a tool that allows you to be creative with LLMs, regardless\nof whether you're an engineer, tinkerer, hobbyist, or anyone in between.\n\n## Core Principles\n\n- **Tome is local first:** You are in control of where your data goes.\n- **Tome is for everyone:** You shouldn't have to manage programming languages, package managers, or json config files.\n\n## What's Next\n\nWe've gotten a lot of amazing feedback in the last few weeks since releasing Tome but we've got big plans for the future. We want to break LLMs out of their chatbox, and we've got a lot of features coming to help y'all do that.\n\n- Scheduled tasks: LLMs should be doing helpful things even when you're not in front of the computer.\n- Native integrations: MCP servers are a great way to access tools and information, but we want to add more powerful integrations to interact with LLMs in unique. ways\n- App builder: we believe long term that the best experiences will not be in a chat interface. We have plans to add additional tools that will enable you to create powerful applications and workflows.\n- ??? Let us know what you'd like to see! Join our community via the links below, we'd love to hear from you.\n\n# Community\n\n[Discord](https://discord.gg/9CH6us29YA) [Blog](https://blog.runebook.ai) [Bluesky](https://bsky.app/profile/gettome.app) [Twitter](https://twitter.com/get_tome) \n",
  "origin_sha": "rWLnVvqJwvr9v1fqiTn2LWjwaQbT+x1VplsLjSlpRbU=",
  "translate": "# Tome - Sihirli Yapay ZekÃ¢ BÃ¼yÃ¼ KitabÄ±\n\n<img src=\"https://raw.githubusercontent.com/runebookai/tome/main/static/images/repo-header.png\" alt=\"Tome\" />\n\n<p align=\"center\">\n    <code>LLM'lerin ve MCP'nin gÃ¼cÃ¼nÃ¼ herkesin eline veren sihirli bir masaÃ¼stÃ¼ uygulamasÄ±</code>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://discord.gg/9CH6us29YA\" target=\"_blank\"><img src=\"https://img.shields.io/discord/1365100902561742868?logo=discord&logoColor=fff&label=Join%20Us!&color=9D7CD8\" alt=\"Join Us on Discord\" /></a>\n    <a href=\"https://opensource.org/licenses/Apache-2.0\" target=\"_blank\"><img src=\"https://img.shields.io/badge/License-Apache_2.0-blue.svg\" alt=\"License: Apache 2.0\" /></a>\n    <a href=\"https://github.com/runebookai/tome/releases\" target=\"_blank\"><img src=\"https://img.shields.io/github/v/release/runebookai/tome\" alt=\"GitHub Release\" /></a>\n</p>\n\n<p align=\"center\">\n    ğŸ”® Tome MasaÃ¼stÃ¼ UygulamasÄ±nÄ± Ä°ndirin: <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_x64-setup.exe\">Windows</a> | <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_aarch64.dmg\">MacOS</a>\n</p>\n\n# Tome\n\nTome, **herkesin** LLM'lerin ve MCP'nin sihrini kullanmasÄ±nÄ± saÄŸlayan bir masaÃ¼stÃ¼ uygulamasÄ±dÄ±r. Tome'u indirin, herhangi bir yerel veya uzak LLM'e baÄŸlanÄ±n ve binlerce MCP sunucusuna eriÅŸerek kendi sihirli yapay zekÃ¢ bÃ¼yÃ¼ kitabÄ±nÄ±zÄ± oluÅŸturun.\n\nğŸ«¥ %100 yerel, %100 Ã¶zel mi olmasÄ±nÄ± istiyorsunuz? Sadece yerel MCP sunucularÄ±yla Ollama ve Qwen3 kullanarak kendi cebinizdeki evrende bÃ¼yÃ¼ler yapabilirsiniz. âš¡ En yeni uzak MCP sunucularÄ± ile son teknoloji bulut modellerini mi istiyorsunuz? Onu da yapabilirsiniz. Her ÅŸey sizin elinizde!\n\nğŸ—ï¸ Bu bir Teknik Ã–nizleme'dir, dolayÄ±sÄ±yla bazÄ± ÅŸeylerin tam olarak oturmamÄ±ÅŸ olabileceÄŸini unutmayÄ±n. [Discord'da bize katÄ±lÄ±n](https://discord.gg/9CH6us29YA) ve karÅŸÄ±laÅŸtÄ±ÄŸÄ±nÄ±z ipuÃ§larÄ±nÄ±, pÃ¼f noktalarÄ±nÄ± ve sorunlarÄ± paylaÅŸÄ±n. Bu depoyu yÄ±ldÄ±zlayarak gÃ¼ncellemelerden ve yeni Ã¶zelliklerden haberdar olun!\n\n## ğŸª„ Ã–zellikler\n\n- ğŸ§™ **KullanÄ±cÄ± Dostu ve BasitleÅŸtirilmiÅŸ Deneyim**\n  - Sadece Tome'u indirip kurun ve istediÄŸiniz LLM'i baÄŸlayÄ±n\n  - JSON, Docker, python veya node ile uÄŸraÅŸmanÄ±za gerek yok\n- ğŸ¤– **Yapay ZekÃ¢ Modeli DesteÄŸi**\n  - **Uzak**: Google Gemini, OpenAI, OpenAI API ile uyumlu herhangi bir uÃ§ nokta\n  - **Yerel**: Ollama, LM Studio, Cortex, OpenAI API ile uyumlu herhangi bir uÃ§ nokta\n- ğŸ”® **GeliÅŸmiÅŸ MCP DesteÄŸi**\n  - MCP sunucularÄ±nÄ± yÃ¼klemek, kaldÄ±rmak, aÃ§Ä±p kapamak iÃ§in arayÃ¼z\n  - npm, uvx, node, python MCP sunucularÄ± kutudan Ã§Ä±ktÄ±ÄŸÄ± gibi desteklenir\n- ğŸª **[Smithery.ai](https://smithery.ai) kayÄ±t sistemine entegrasyon**\n  - Tek tÄ±kla kurulabilen binlerce MCP sunucusu mevcut\n- âœï¸ **BaÄŸlam pencerelerinin ve sÄ±caklÄ±ÄŸÄ±n Ã¶zelleÅŸtirilebilmesi**\n- ğŸ§° **AraÃ§ Ã§aÄŸrÄ±larÄ± ve muhakeme modelleri iÃ§in yerel destek**\n  - AraÃ§ Ã§aÄŸrÄ±larÄ±nÄ± ve dÃ¼ÅŸÃ¼nme mesajlarÄ±nÄ± aÃ§Ä±kÃ§a ayÄ±ran arayÃ¼z iyileÅŸtirmeleri\n\n## Demo\n\nhttps://github.com/user-attachments/assets/0775d100-3eba-4219-9e2f-360a01f28cce\n\n# BaÅŸlarken\n\n## Gereksinimler\n\n- MacOS veya Windows (Linux yakÄ±nda geliyor!)\n- Tercih ettiÄŸiniz LLM SaÄŸlayÄ±cÄ±sÄ±: [Ollama](https://ollama.com/) veya [Gemini API anahtarÄ±](https://aistudio.google.com/app/apikey) kolay/Ã¼cretsizdir\n- [Tome'un en son sÃ¼rÃ¼mÃ¼nÃ¼ indirin](https://github.com/runebookai/tome/releases)\n\n## HÄ±zlÄ± BaÅŸlangÄ±Ã§\n\n1. [Tome'u yÃ¼kleyin](https://github.com/runebookai/tome/releases)\n2. Tercih ettiÄŸiniz LLM saÄŸlayÄ±cÄ±sÄ±nÄ± baÄŸlayÄ±n - OpenAI, Ollama ve Gemini Ã¶nceden ayarlanmÄ±ÅŸtÄ±r, ancak LM Studio gibi saÄŸlayÄ±cÄ±larÄ± da http://localhost:1234/v1 adresini URL olarak kullanarak ekleyebilirsiniz\n3. Tome'da MCP sekmesini aÃ§Ä±n ve ilk [MCP sunucunuzu](https://github.com/modelcontextprotocol/servers) yÃ¼kleyin (BaÅŸlamak iÃ§in en kolayÄ± Fetch'tir, sunucu alanÄ±na sadece `uvx mcp-server-fetch` yapÄ±ÅŸtÄ±rÄ±n).\n4. MCP destekli modelinizle sohbet edin! Ondan Hacker News'daki en popÃ¼ler haberi Ã§ekmesini isteyin.\n\n# Vizyon\n\nYerel LLM'leri ve MCP'yi herkes iÃ§in eriÅŸilebilir kÄ±lmak istiyoruz. Bir mÃ¼hendis, meraklÄ±, hobi sahibi ya da aradaki herhangi biri olsanÄ±z da LLM'lerle yaratÄ±cÄ± olmanÄ±zÄ± saÄŸlayacak bir araÃ§ inÅŸa ediyoruz.\n\n## Temel Prensipler\n\n- **Tome Ã¶ncelikle yereldir:** Verilerinizin nereye gittiÄŸi Ã¼zerinde kontrol sizdedir.\n- **Tome herkes iÃ§indir:** Programlama dilleri, paket yÃ¶neticileri veya json yapÄ±landÄ±rma dosyalarÄ±nÄ± yÃ¶netmek zorunda kalmamalÄ±sÄ±nÄ±z.\n\n## SÄ±rada Ne Var\n\nTome'u yayÄ±nladÄ±ktan sonraki birkaÃ§ haftada harika geri bildirimler aldÄ±k fakat gelecek iÃ§in bÃ¼yÃ¼k planlarÄ±mÄ±z var. LLM'leri sohbet kutularÄ±nÄ±n dÄ±ÅŸÄ±na Ã§Ä±karmak istiyoruz ve bunu baÅŸarmanÄ±za yardÄ±mcÄ± olacak birÃ§ok Ã¶zellik geliyor.\n\n- ZamanlanmÄ±ÅŸ gÃ¶revler: LLM'ler siz bilgisayar baÅŸÄ±nda olmasanÄ±z bile yararlÄ± iÅŸler yapabilmeli.\n- Yerel entegrasyonlar: MCP sunucularÄ± araÃ§lara ve bilgilere eriÅŸmek iÃ§in harika bir yol, fakat LLM'lerle benzersiz ÅŸekilde etkileÅŸime geÃ§mek iÃ§in daha gÃ¼Ã§lÃ¼ entegrasyonlar eklemek istiyoruz.\n- Uygulama oluÅŸturucu: Uzun vadede en iyi deneyimlerin bir sohbet arayÃ¼zÃ¼nde olmayacaÄŸÄ±na inanÄ±yoruz. GÃ¼Ã§lÃ¼ uygulamalar ve iÅŸ akÄ±ÅŸlarÄ± oluÅŸturmanÄ±zÄ± saÄŸlayacak ek araÃ§lar eklemeyi planlÄ±yoruz.\n- ??? GÃ¶rmek istediÄŸiniz ÅŸeyleri bize bildirin! AÅŸaÄŸÄ±daki baÄŸlantÄ±lar aracÄ±lÄ±ÄŸÄ±yla topluluÄŸumuza katÄ±lÄ±n, sizden haber almak isteriz.\n\n# Topluluk\n\n[Discord](https://discord.gg/9CH6us29YA) [Blog](https://blog.runebook.ai) [Bluesky](https://bsky.app/profile/gettome.app) [Twitter](https://twitter.com/get_tome) \n",
  "status": "ok"
}