# Tome - Magisches KI-Zauberbuch

<img src="https://raw.githubusercontent.com/runebookai/tome/main/static/images/repo-header.png" alt="Tome" />

<p align="center">
    <code>eine magische Desktop-App, die die Kraft von LLMs und MCP in die HÃ¤nde aller legt</code>
</p>

<p align="center">
    <a href="https://discord.gg/9CH6us29YA" target="_blank"><img src="https://img.shields.io/discord/1365100902561742868?logo=discord&logoColor=fff&label=Join%20Us!&color=9D7CD8" alt="Join Us on Discord" /></a>
    <a href="https://opensource.org/licenses/Apache-2.0" target="_blank"><img src="https://img.shields.io/badge/License-Apache_2.0-blue.svg" alt="License: Apache 2.0" /></a>
    <a href="https://github.com/runebookai/tome/releases" target="_blank"><img src="https://img.shields.io/github/v/release/runebookai/tome" alt="GitHub Release" /></a>
</p>

<p align="center">
    ğŸ”® Lade die Tome Desktop-App herunter: <a href="https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_x64-setup.exe">Windows</a> | <a href="https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_aarch64.dmg">MacOS</a>
</p>

# Tome

Tome ist eine Desktop-App, die **jedem** ermÃ¶glicht, die Magie von LLMs und MCP zu nutzen. Lade Tome herunter, verbinde ein beliebiges lokales oder entferntes LLM und schlieÃŸe es an Tausende von MCP-Servern an, um dein eigenes magisches, KI-gestÃ¼tztes Zauberbuch zu erschaffen.

ğŸ«¥ Willst du, dass alles 100% lokal und 100% privat bleibt? Nutze Ollama und Qwen3 ausschlieÃŸlich mit lokalen MCP-Servern, um ZaubersprÃ¼che in deinem eigenen Pocket-Universum zu wirken. âš¡ Willst du modernste Cloud-Modelle mit den neuesten entfernten MCP-Servern? Auch das ist mÃ¶glich. Die Wahl liegt ganz bei dir!

ğŸ—ï¸ Dies ist eine technische Vorschau, daher kann es an einigen Stellen noch holprig sein. [Tritt unserem Discord bei](https://discord.gg/9CH6us29YA), um Tipps, Tricks und aufgetretene Probleme zu teilen. Setze einen Stern auf dieses Repo, um immer Ã¼ber Updates und neue Funktionen informiert zu bleiben!

## ğŸª„ Funktionen

- ğŸ§™ **Vereinfachtes, anfÃ¤ngerfreundliches Erlebnis**
  - Einfach Tome herunterladen, installieren und das gewÃ¼nschte LLM anschlieÃŸen
  - Kein Herumhantieren mit JSON, Docker, Python oder Node notwendig
- ğŸ¤– **KI-ModellunterstÃ¼tzung**
  - **Remote:** Google Gemini, OpenAI, jeder OpenAI-API-kompatible Endpunkt
  - **Lokal:** Ollama, LM Studio, Cortex, jeder OpenAI-API-kompatible Endpunkt
- ğŸ”® **Erweiterte MCP-UnterstÃ¼tzung**
  - BenutzeroberflÃ¤che zum Installieren, Entfernen, Ein- und Ausschalten von MCP-Servern
  - npm, uvx, node, python MCP-Server werden direkt unterstÃ¼tzt
- ğŸª **Integration in das [Smithery.ai](https://smithery.ai)-Register**
  - Tausende MCP-Server per Ein-Klick-Installation verfÃ¼gbar
- âœï¸ **Anpassung der Kontextfenster und Temperatur**
- ğŸ§° **Native UnterstÃ¼tzung fÃ¼r Tool-Calls und Reasoning-Modelle**
  - UI-Verbesserungen, die Tool-Calls und Denk-Nachrichten klar kennzeichnen

## Demo

https://github.com/user-attachments/assets/0775d100-3eba-4219-9e2f-360a01f28cce

# Erste Schritte

## Voraussetzungen

- MacOS oder Windows (Linux kommt bald!)
- LLM-Anbieter deiner Wahl: [Ollama](https://ollama.com/) oder [Gemini API-SchlÃ¼ssel](https://aistudio.google.com/app/apikey) sind einfach/kostenlos
- [Lade die neueste Version von Tome herunter](https://github.com/runebookai/tome/releases)

## Schnellstart

1. Installiere [Tome](https://github.com/runebookai/tome/releases)
2. Verbinde deinen bevorzugten LLM-Anbieter â€“ OpenAI, Ollama und Gemini sind voreingestellt, aber du kannst auch Anbieter wie LM Studio hinzufÃ¼gen, indem du http://localhost:1234/v1 als URL verwendest
3. Ã–ffne den MCP-Tab in Tome und installiere deinen ersten [MCP-Server](https://github.com/modelcontextprotocol/servers) (Fetch ist ein einfacher Einstieg, kopiere einfach `uvx mcp-server-fetch` in das Serverfeld).
4. Chatte mit deinem MCP-gestÃ¼tzten Modell! Bitte es, die Top-Story auf Hacker News zu holen.

# Vision

Wir mÃ¶chten lokale LLMs und MCP fÃ¼r alle zugÃ¤nglich machen. Wir bauen ein Tool, das dir ermÃ¶glicht, kreativ mit LLMs zu arbeiten, egal ob du Entwickler, Bastler, Hobbyist oder irgendetwas dazwischen bist.

## Grundprinzipien

- **Tome ist lokal zuerst:** Du kontrollierst, wohin deine Daten gehen.
- **Tome ist fÃ¼r alle:** Du solltest dich nicht mit Programmiersprachen, Paketmanagern oder JSON-Konfigurationsdateien herumschlagen mÃ¼ssen.

## Wie geht es weiter

Wir haben seit der VerÃ¶ffentlichung von Tome in den letzten Wochen viel tolles Feedback erhalten, aber wir haben groÃŸe PlÃ¤ne fÃ¼r die Zukunft. Wir wollen LLMs aus ihrer Chatbox befreien und haben viele neue Funktionen in Arbeit, die euch dabei unterstÃ¼tzen.

- Geplante Aufgaben: LLMs sollten hilfreiche Dinge erledigen, auch wenn du nicht am Computer bist.
- Native Integrationen: MCP-Server sind ein groÃŸartiger Weg, um auf Tools und Informationen zuzugreifen, aber wir wollen noch mÃ¤chtigere Integrationen hinzufÃ¼gen, um auf einzigartige Weise mit LLMs zu interagieren.
- App-Builder: Wir glauben langfristig, dass die besten Erfahrungen nicht in einer Chat-OberflÃ¤che stattfinden. Wir planen, zusÃ¤tzliche Tools bereitzustellen, mit denen du leistungsstarke Anwendungen und Workflows erstellen kannst.
- ??? Sag uns, was du dir wÃ¼nschst! Tritt Ã¼ber die untenstehenden Links unserer Community bei, wir freuen uns auf dein Feedback.

# Community

[Discord](https://discord.gg/9CH6us29YA) [Blog](https://blog.runebook.ai) [Bluesky](https://bsky.app/profile/gettome.app) [Twitter](https://twitter.com/get_tome) 


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-03

---