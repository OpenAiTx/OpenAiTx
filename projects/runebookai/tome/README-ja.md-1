{
  "id": 1,
  "origin": "# Tome - Magical AI Spellbook\n\n<img src=\"https://raw.githubusercontent.com/runebookai/tome/main/static/images/repo-header.png\" alt=\"Tome\" />\n\n<p align=\"center\">\n    <code>a magical desktop app that puts the power of LLMs and MCP in the hands of everyone</code>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://discord.gg/9CH6us29YA\" target=\"_blank\"><img src=\"https://img.shields.io/discord/1365100902561742868?logo=discord&logoColor=fff&label=Join%20Us!&color=9D7CD8\" alt=\"Join Us on Discord\" /></a>\n    <a href=\"https://opensource.org/licenses/Apache-2.0\" target=\"_blank\"><img src=\"https://img.shields.io/badge/License-Apache_2.0-blue.svg\" alt=\"License: Apache 2.0\" /></a>\n    <a href=\"https://github.com/runebookai/tome/releases\" target=\"_blank\"><img src=\"https://img.shields.io/github/v/release/runebookai/tome\" alt=\"GitHub Release\" /></a>\n</p>\n\n<p align=\"center\">\n    🔮 Download the Tome Desktop App: <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_x64-setup.exe\">Windows</a> | <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_aarch64.dmg\">MacOS</a>\n</p>\n\n# Tome\n\nTome is a desktop app that lets **anyone** harness the magic of LLMs and MCP. Download Tome, connect any local or remote LLM and hook it up to thousands of MCP servers to create your own magical AI-powered spellbook.\n\n🫥 Want it to be 100% local, 100% private? Use Ollama and Qwen3 with only local MCP servers to cast spells in your own pocket universe. ⚡ Want state of the art cloud models with the latest remote MCP servers? You can have that too. It's all up to you!\n\n🏗️ This is a Technical Preview so bear in mind things will be rough around the edges. [Join us on Discord](https://discord.gg/9CH6us29YA) to share tips, tricks, and issues you run into. Star this repo to stay on top of updates and feature releases!\n\n## 🪄 Features\n\n- 🧙 **Streamlined Beginner Friendly Experience**\n  - Simply download and install Tome and hook up the LLM of your choice\n  - No fiddling with JSON, Docker, python or node\n- 🤖 **AI Model Support**\n  - **Remote**: Google Gemini, OpenAI, any OpenAI API-compatible endpoint\n  - **Local**: Ollama, LM Studio, Cortex, any OpenAI API-compatible endpoint\n- 🔮 **Enhanced MCP support**\n  - UI to install, remove, turn on/off MCP servers\n  - npm, uvx, node, python MCP servers all supported out of box\n- 🏪 **Integration into [Smithery.ai](https://smithery.ai) registry**\n  - Thousands of MCP servers available via one-click installation\n- ✏️ **Customization of context windows and temperature**\n- 🧰 **Native support for tool calls and reasoning models**\n  - UI enhancements that clearly delineate tool calls and thinking messages\n\n## Demo\n\nhttps://github.com/user-attachments/assets/0775d100-3eba-4219-9e2f-360a01f28cce\n\n# Getting Started\n\n## Requirements\n\n- MacOS or Windows (Linux coming soon!)\n- LLM Provider of your choice: [Ollama](https://ollama.com/) or [Gemini API key](https://aistudio.google.com/app/apikey) are easy/free\n- [Download the latest release of Tome](https://github.com/runebookai/tome/releases)\n\n## Quickstart\n\n1. Install [Tome](https://github.com/runebookai/tome/releases)\n2. Connect your preferred LLM provider - OpenAI, Ollama and Gemini are preset but you can also add providers like LM Studio by using http://localhost:1234/v1 as the URL\n3. Open the MCP tab in Tome and install your first [MCP server](https://github.com/modelcontextprotocol/servers) (Fetch is an easy one to get started with, just paste `uvx mcp-server-fetch` into the server field).\n4. Chat with your MCP-powered model! Ask it to fetch the top story on Hacker News.\n\n# Vision\n\nWe want to make local LLMs and MCP accessible to everyone. We're building a tool that allows you to be creative with LLMs, regardless\nof whether you're an engineer, tinkerer, hobbyist, or anyone in between.\n\n## Core Principles\n\n- **Tome is local first:** You are in control of where your data goes.\n- **Tome is for everyone:** You shouldn't have to manage programming languages, package managers, or json config files.\n\n## What's Next\n\nWe've gotten a lot of amazing feedback in the last few weeks since releasing Tome but we've got big plans for the future. We want to break LLMs out of their chatbox, and we've got a lot of features coming to help y'all do that.\n\n- Scheduled tasks: LLMs should be doing helpful things even when you're not in front of the computer.\n- Native integrations: MCP servers are a great way to access tools and information, but we want to add more powerful integrations to interact with LLMs in unique. ways\n- App builder: we believe long term that the best experiences will not be in a chat interface. We have plans to add additional tools that will enable you to create powerful applications and workflows.\n- ??? Let us know what you'd like to see! Join our community via the links below, we'd love to hear from you.\n\n# Community\n\n[Discord](https://discord.gg/9CH6us29YA) [Blog](https://blog.runebook.ai) [Bluesky](https://bsky.app/profile/gettome.app) [Twitter](https://twitter.com/get_tome) \n",
  "origin_sha": "rWLnVvqJwvr9v1fqiTn2LWjwaQbT+x1VplsLjSlpRbU=",
  "translate": "# Tome - 魔法のAIスペルブック\n\n<img src=\"https://raw.githubusercontent.com/runebookai/tome/main/static/images/repo-header.png\" alt=\"Tome\" />\n\n<p align=\"center\">\n    <code>誰もがLLMとMCPの力を手にできる魔法のデスクトップアプリ</code>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://discord.gg/9CH6us29YA\" target=\"_blank\"><img src=\"https://img.shields.io/discord/1365100902561742868?logo=discord&logoColor=fff&label=Join%20Us!&color=9D7CD8\" alt=\"Join Us on Discord\" /></a>\n    <a href=\"https://opensource.org/licenses/Apache-2.0\" target=\"_blank\"><img src=\"https://img.shields.io/badge/License-Apache_2.0-blue.svg\" alt=\"License: Apache 2.0\" /></a>\n    <a href=\"https://github.com/runebookai/tome/releases\" target=\"_blank\"><img src=\"https://img.shields.io/github/v/release/runebookai/tome\" alt=\"GitHub Release\" /></a>\n</p>\n\n<p align=\"center\">\n    🔮 Tomeデスクトップアプリをダウンロード: <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_x64-setup.exe\">Windows</a> | <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_aarch64.dmg\">MacOS</a>\n</p>\n\n# Tome\n\nTomeは、**誰でも**LLMとMCPの魔法を活用できるデスクトップアプリです。Tomeをダウンロードし、ローカルまたはリモートのLLMを接続し、数千のMCPサーバーと連携して、自分だけの魔法のAIスペルブックを作成できます。\n\n🫥 完全にローカル、完全にプライベートにしたいですか？OllamaとQwen3を使い、ローカルMCPサーバーのみで、自分だけのポケットユニバースで魔法をかけましょう。⚡ 最新のリモートMCPサーバーと最先端のクラウドモデルを使いたいですか？それも可能です。すべてはあなた次第！\n\n🏗️ これはテクニカルプレビュー版ですので、細部が粗い部分があるかもしれません。[Discordに参加](https://discord.gg/9CH6us29YA)して、ヒントやコツ、不具合などを共有してください。このリポジトリにスターを付けて、最新情報や新機能をチェックしましょう！\n\n## 🪄 主な特徴\n\n- 🧙 **初心者に優しいシンプルな体験**\n  - Tomeをダウンロードしてインストールし、お好みのLLMを接続するだけ\n  - JSON、Docker、python、nodeなどの面倒な設定は不要\n- 🤖 **AIモデル対応**\n  - **リモート**: Google Gemini、OpenAI、OpenAI API互換エンドポイント\n  - **ローカル**: Ollama、LM Studio、Cortex、OpenAI API互換エンドポイント\n- 🔮 **強化されたMCPサポート**\n  - MCPサーバーのインストール、削除、オン/オフの切り替え用UI\n  - npm、uvx、node、pythonのMCPサーバーを標準サポート\n- 🏪 **[Smithery.ai](https://smithery.ai) レジストリとの統合**\n  - ワンクリックで数千のMCPサーバーをインストール可能\n- ✏️ **コンテキストウィンドウと温度のカスタマイズ**\n- 🧰 **ツールコールと推論モデルのネイティブサポート**\n  - ツールコールと思考メッセージを明確に区別するUI強化\n\n## デモ\n\nhttps://github.com/user-attachments/assets/0775d100-3eba-4219-9e2f-360a01f28cce\n\n# はじめに\n\n## 必要条件\n\n- MacOS または Windows（Linuxは近日対応予定！）\n- お好みのLLMプロバイダー: [Ollama](https://ollama.com/) または [Gemini APIキー](https://aistudio.google.com/app/apikey)（どちらも簡単＆無料）\n- [Tomeの最新リリースをダウンロード](https://github.com/runebookai/tome/releases)\n\n## クイックスタート\n\n1. [Tome](https://github.com/runebookai/tome/releases)をインストール\n2. お好みのLLMプロバイダーを接続します。OpenAI、Ollama、Geminiはプリセット済みですが、LM Studioなどのプロバイダーも http://localhost:1234/v1 のようなURLで追加可能です\n3. TomeのMCPタブを開き、最初の[MCPサーバー](https://github.com/modelcontextprotocol/servers)をインストールします（Fetchが簡単でおすすめ、`uvx mcp-server-fetch` をサーバーフィールドに貼り付けるだけでOK）\n4. MCP対応モデルとチャット！Hacker Newsのトップ記事を取得するよう依頼してみましょう\n\n# ビジョン\n\n私たちはローカルLLMとMCPをすべての人が使えるようにしたいと考えています。エンジニア、いじり好き、趣味人、その他どんな方でも、LLMで創造的になれるツールを作っています。\n\n## コア原則\n\n- **Tomeはローカルファースト:** データの行き先はあなたが管理します。\n- **Tomeは誰のためにも:** プログラミング言語やパッケージマネージャ、json設定ファイルの管理は不要です。\n\n## 今後の予定\n\nTomeリリース以来、素晴らしいフィードバックをたくさんいただいていますが、今後の大きな計画もあります。LLMをチャットボックスから解放したいと考えており、そのための多くの新機能を準備中です。\n\n- スケジュールタスク: パソコンの前にいなくてもLLMが役立つことをしてくれるように\n- ネイティブ統合: MCPサーバーはツールや情報へのアクセスに最適ですが、より強力な統合機能を追加し、独自の方法でLLMとやりとりできるようにしたい\n- アプリビルダー: 長期的には、最高の体験はチャットインターフェース以外にあると信じています。強力なアプリケーションやワークフローを作成できるツールの追加を計画しています。\n- ??? 皆さんのご要望もぜひお聞かせください！下記リンクからコミュニティに参加し、ご意見をお寄せください。\n\n# コミュニティ\n\n[Discord](https://discord.gg/9CH6us29YA) [Blog](https://blog.runebook.ai) [Bluesky](https://bsky.app/profile/gettome.app) [Twitter](https://twitter.com/get_tome) ",
  "status": "ok"
}