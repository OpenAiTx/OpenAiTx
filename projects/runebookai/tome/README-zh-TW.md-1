{
  "id": 1,
  "origin": "# Tome - Magical AI Spellbook\n\n<img src=\"https://raw.githubusercontent.com/runebookai/tome/main/static/images/repo-header.png\" alt=\"Tome\" />\n\n<p align=\"center\">\n    <code>a magical desktop app that puts the power of LLMs and MCP in the hands of everyone</code>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://discord.gg/9CH6us29YA\" target=\"_blank\"><img src=\"https://img.shields.io/discord/1365100902561742868?logo=discord&logoColor=fff&label=Join%20Us!&color=9D7CD8\" alt=\"Join Us on Discord\" /></a>\n    <a href=\"https://opensource.org/licenses/Apache-2.0\" target=\"_blank\"><img src=\"https://img.shields.io/badge/License-Apache_2.0-blue.svg\" alt=\"License: Apache 2.0\" /></a>\n    <a href=\"https://github.com/runebookai/tome/releases\" target=\"_blank\"><img src=\"https://img.shields.io/github/v/release/runebookai/tome\" alt=\"GitHub Release\" /></a>\n</p>\n\n<p align=\"center\">\n    🔮 Download the Tome Desktop App: <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_x64-setup.exe\">Windows</a> | <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_aarch64.dmg\">MacOS</a>\n</p>\n\n# Tome\n\nTome is a desktop app that lets **anyone** harness the magic of LLMs and MCP. Download Tome, connect any local or remote LLM and hook it up to thousands of MCP servers to create your own magical AI-powered spellbook.\n\n🫥 Want it to be 100% local, 100% private? Use Ollama and Qwen3 with only local MCP servers to cast spells in your own pocket universe. ⚡ Want state of the art cloud models with the latest remote MCP servers? You can have that too. It's all up to you!\n\n🏗️ This is a Technical Preview so bear in mind things will be rough around the edges. [Join us on Discord](https://discord.gg/9CH6us29YA) to share tips, tricks, and issues you run into. Star this repo to stay on top of updates and feature releases!\n\n## 🪄 Features\n\n- 🧙 **Streamlined Beginner Friendly Experience**\n  - Simply download and install Tome and hook up the LLM of your choice\n  - No fiddling with JSON, Docker, python or node\n- 🤖 **AI Model Support**\n  - **Remote**: Google Gemini, OpenAI, any OpenAI API-compatible endpoint\n  - **Local**: Ollama, LM Studio, Cortex, any OpenAI API-compatible endpoint\n- 🔮 **Enhanced MCP support**\n  - UI to install, remove, turn on/off MCP servers\n  - npm, uvx, node, python MCP servers all supported out of box\n- 🏪 **Integration into [Smithery.ai](https://smithery.ai) registry**\n  - Thousands of MCP servers available via one-click installation\n- ✏️ **Customization of context windows and temperature**\n- 🧰 **Native support for tool calls and reasoning models**\n  - UI enhancements that clearly delineate tool calls and thinking messages\n\n## Demo\n\nhttps://github.com/user-attachments/assets/0775d100-3eba-4219-9e2f-360a01f28cce\n\n# Getting Started\n\n## Requirements\n\n- MacOS or Windows (Linux coming soon!)\n- LLM Provider of your choice: [Ollama](https://ollama.com/) or [Gemini API key](https://aistudio.google.com/app/apikey) are easy/free\n- [Download the latest release of Tome](https://github.com/runebookai/tome/releases)\n\n## Quickstart\n\n1. Install [Tome](https://github.com/runebookai/tome/releases)\n2. Connect your preferred LLM provider - OpenAI, Ollama and Gemini are preset but you can also add providers like LM Studio by using http://localhost:1234/v1 as the URL\n3. Open the MCP tab in Tome and install your first [MCP server](https://github.com/modelcontextprotocol/servers) (Fetch is an easy one to get started with, just paste `uvx mcp-server-fetch` into the server field).\n4. Chat with your MCP-powered model! Ask it to fetch the top story on Hacker News.\n\n# Vision\n\nWe want to make local LLMs and MCP accessible to everyone. We're building a tool that allows you to be creative with LLMs, regardless\nof whether you're an engineer, tinkerer, hobbyist, or anyone in between.\n\n## Core Principles\n\n- **Tome is local first:** You are in control of where your data goes.\n- **Tome is for everyone:** You shouldn't have to manage programming languages, package managers, or json config files.\n\n## What's Next\n\nWe've gotten a lot of amazing feedback in the last few weeks since releasing Tome but we've got big plans for the future. We want to break LLMs out of their chatbox, and we've got a lot of features coming to help y'all do that.\n\n- Scheduled tasks: LLMs should be doing helpful things even when you're not in front of the computer.\n- Native integrations: MCP servers are a great way to access tools and information, but we want to add more powerful integrations to interact with LLMs in unique. ways\n- App builder: we believe long term that the best experiences will not be in a chat interface. We have plans to add additional tools that will enable you to create powerful applications and workflows.\n- ??? Let us know what you'd like to see! Join our community via the links below, we'd love to hear from you.\n\n# Community\n\n[Discord](https://discord.gg/9CH6us29YA) [Blog](https://blog.runebook.ai) [Bluesky](https://bsky.app/profile/gettome.app) [Twitter](https://twitter.com/get_tome) \n",
  "origin_sha": "rWLnVvqJwvr9v1fqiTn2LWjwaQbT+x1VplsLjSlpRbU=",
  "translate": "# Tome - 魔法 AI 咒語書\n\n<img src=\"https://raw.githubusercontent.com/runebookai/tome/main/static/images/repo-header.png\" alt=\"Tome\" />\n\n<p align=\"center\">\n    <code>一個神奇的桌面應用程式，讓每個人都能掌握 LLMs 和 MCP 的力量</code>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://discord.gg/9CH6us29YA\" target=\"_blank\"><img src=\"https://img.shields.io/discord/1365100902561742868?logo=discord&logoColor=fff&label=Join%20Us!&color=9D7CD8\" alt=\"Join Us on Discord\" /></a>\n    <a href=\"https://opensource.org/licenses/Apache-2.0\" target=\"_blank\"><img src=\"https://img.shields.io/badge/License-Apache_2.0-blue.svg\" alt=\"License: Apache 2.0\" /></a>\n    <a href=\"https://github.com/runebookai/tome/releases\" target=\"_blank\"><img src=\"https://img.shields.io/github/v/release/runebookai/tome\" alt=\"GitHub Release\" /></a>\n</p>\n\n<p align=\"center\">\n    🔮 下載 Tome 桌面應用程式：<a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_x64-setup.exe\">Windows</a> | <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_aarch64.dmg\">MacOS</a>\n</p>\n\n# Tome\n\nTome 是一款桌面應用程式，讓**任何人**都能利用 LLMs 和 MCP 的魔法。下載 Tome，連接任何本地或遠端 LLM，並將其串接到成千上萬的 MCP 伺服器，打造屬於你自己的 AI 魔法咒語書。\n\n🫥 想要 100% 本地化、100% 私密？只需使用 Ollama 和 Qwen3，並僅搭配本地 MCP 伺服器，就能在屬於你的口袋宇宙中施展魔法。⚡ 想用最先進的雲端模型和最新的遠端 MCP 伺服器？你也可以這麼做。一切由你決定！\n\n🏗️ 這是一個技術預覽版，因此請注意目前仍有許多地方尚未完善。[加入我們的 Discord](https://discord.gg/9CH6us29YA)，分享你的技巧、經驗以及遇到的問題。給本專案加星，隨時掌握最新更新與功能發佈！\n\n## 🪄 特色\n\n- 🧙 **簡化、適合新手的體驗**\n  - 只需下載並安裝 Tome，連接你選擇的 LLM\n  - 無需折騰 JSON、Docker、python 或 node\n- 🤖 **支援多種 AI 模型**\n  - **遠端**：Google Gemini、OpenAI、任何相容 OpenAI API 的端點\n  - **本地**：Ollama、LM Studio、Cortex、任何相容 OpenAI API 的端點\n- 🔮 **強化 MCP 支援**\n  - 介面可安裝、移除、啟用/停用 MCP 伺服器\n  - 原生支援 npm、uvx、node、python MCP 伺服器\n- 🏪 **整合至 [Smithery.ai](https://smithery.ai) 註冊表**\n  - 成千上萬個 MCP 伺服器可一鍵安裝\n- ✏️ **可自訂 context window 和 temperature**\n- 🧰 **原生支援工具呼叫與推理模型**\n  - 介面優化，明確區分工具呼叫與思考訊息\n\n## Demo\n\nhttps://github.com/user-attachments/assets/0775d100-3eba-4219-9e2f-360a01f28cce\n\n# 入門指南\n\n## 系統需求\n\n- MacOS 或 Windows（Linux 即將推出！）\n- 任選一個 LLM 供應商：[Ollama](https://ollama.com/) 或 [Gemini API key](https://aistudio.google.com/app/apikey) 皆簡單且免費\n- [下載 Tome 最新版本](https://github.com/runebookai/tome/releases)\n\n## 快速開始\n\n1. 安裝 [Tome](https://github.com/runebookai/tome/releases)\n2. 連接你偏好的 LLM 供應商 —— OpenAI、Ollama 和 Gemini 已預設提供，你也可以透過 http://localhost:1234/v1 加入像 LM Studio 這樣的供應商\n3. 在 Tome 中開啟 MCP 分頁，安裝你的第一個 [MCP 伺服器](https://github.com/modelcontextprotocol/servers)（Fetch 是入門的好選擇，只需將 `uvx mcp-server-fetch` 貼到伺服器欄位）\n4. 與你的 MCP 驅動模型聊天！請它抓取 Hacker News 的頭條新聞。\n\n# 願景\n\n我們希望讓本地 LLM 和 MCP 對每個人都變得易於使用。我們正在打造一個工具，讓你即使不是工程師、玩家、愛好者，或任何身份，都能發揮 LLM 的創意潛能。\n\n## 核心原則\n\n- **Tome 以本地為優先：** 你能掌控你的資料流向。\n- **Tome 為所有人打造：** 你不需要管理程式語言、套件管理器或 json 設定檔。\n\n## 下一步計畫\n\n自 Tome 發布以來這幾週，我們已收到許多寶貴回饋，未來我們有更多宏大計畫。我們希望將 LLM 從對話框中釋放出來，並將推出更多功能協助大家實現這個目標。\n\n- 排程任務：即使你不在電腦前，LLM 也能自動執行有用的工作。\n- 原生整合：MCP 伺服器是存取工具與資訊的絕佳方式，但我們希望加入更強大的整合，讓你能以獨特方式與 LLM 互動。\n- 應用程式建構器：我們相信長遠來看，最佳體驗不會侷限於對話介面。我們計劃推出更多工具，協助你建立強大的應用與工作流程。\n- ??? 告訴我們你想要什麼！歡迎透過下方連結加入我們的社群，我們很樂意聽取你的意見。\n\n# 社群\n\n[Discord](https://discord.gg/9CH6us29YA) [部落格](https://blog.runebook.ai) [Bluesky](https://bsky.app/profile/gettome.app) [Twitter](https://twitter.com/get_tome) \n",
  "status": "ok"
}