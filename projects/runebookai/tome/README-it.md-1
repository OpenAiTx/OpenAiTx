{
  "id": 1,
  "origin": "# Tome - Magical AI Spellbook\n\n<img src=\"https://raw.githubusercontent.com/runebookai/tome/main/static/images/repo-header.png\" alt=\"Tome\" />\n\n<p align=\"center\">\n    <code>a magical desktop app that puts the power of LLMs and MCP in the hands of everyone</code>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://discord.gg/9CH6us29YA\" target=\"_blank\"><img src=\"https://img.shields.io/discord/1365100902561742868?logo=discord&logoColor=fff&label=Join%20Us!&color=9D7CD8\" alt=\"Join Us on Discord\" /></a>\n    <a href=\"https://opensource.org/licenses/Apache-2.0\" target=\"_blank\"><img src=\"https://img.shields.io/badge/License-Apache_2.0-blue.svg\" alt=\"License: Apache 2.0\" /></a>\n    <a href=\"https://github.com/runebookai/tome/releases\" target=\"_blank\"><img src=\"https://img.shields.io/github/v/release/runebookai/tome\" alt=\"GitHub Release\" /></a>\n</p>\n\n<p align=\"center\">\n    üîÆ Download the Tome Desktop App: <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_x64-setup.exe\">Windows</a> | <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_aarch64.dmg\">MacOS</a>\n</p>\n\n# Tome\n\nTome is a desktop app that lets **anyone** harness the magic of LLMs and MCP. Download Tome, connect any local or remote LLM and hook it up to thousands of MCP servers to create your own magical AI-powered spellbook.\n\nü´• Want it to be 100% local, 100% private? Use Ollama and Qwen3 with only local MCP servers to cast spells in your own pocket universe. ‚ö° Want state of the art cloud models with the latest remote MCP servers? You can have that too. It's all up to you!\n\nüèóÔ∏è This is a Technical Preview so bear in mind things will be rough around the edges. [Join us on Discord](https://discord.gg/9CH6us29YA) to share tips, tricks, and issues you run into. Star this repo to stay on top of updates and feature releases!\n\n## ü™Ñ Features\n\n- üßô **Streamlined Beginner Friendly Experience**\n  - Simply download and install Tome and hook up the LLM of your choice\n  - No fiddling with JSON, Docker, python or node\n- ü§ñ **AI Model Support**\n  - **Remote**: Google Gemini, OpenAI, any OpenAI API-compatible endpoint\n  - **Local**: Ollama, LM Studio, Cortex, any OpenAI API-compatible endpoint\n- üîÆ **Enhanced MCP support**\n  - UI to install, remove, turn on/off MCP servers\n  - npm, uvx, node, python MCP servers all supported out of box\n- üè™ **Integration into [Smithery.ai](https://smithery.ai) registry**\n  - Thousands of MCP servers available via one-click installation\n- ‚úèÔ∏è **Customization of context windows and temperature**\n- üß∞ **Native support for tool calls and reasoning models**\n  - UI enhancements that clearly delineate tool calls and thinking messages\n\n## Demo\n\nhttps://github.com/user-attachments/assets/0775d100-3eba-4219-9e2f-360a01f28cce\n\n# Getting Started\n\n## Requirements\n\n- MacOS or Windows (Linux coming soon!)\n- LLM Provider of your choice: [Ollama](https://ollama.com/) or [Gemini API key](https://aistudio.google.com/app/apikey) are easy/free\n- [Download the latest release of Tome](https://github.com/runebookai/tome/releases)\n\n## Quickstart\n\n1. Install [Tome](https://github.com/runebookai/tome/releases)\n2. Connect your preferred LLM provider - OpenAI, Ollama and Gemini are preset but you can also add providers like LM Studio by using http://localhost:1234/v1 as the URL\n3. Open the MCP tab in Tome and install your first [MCP server](https://github.com/modelcontextprotocol/servers) (Fetch is an easy one to get started with, just paste `uvx mcp-server-fetch` into the server field).\n4. Chat with your MCP-powered model! Ask it to fetch the top story on Hacker News.\n\n# Vision\n\nWe want to make local LLMs and MCP accessible to everyone. We're building a tool that allows you to be creative with LLMs, regardless\nof whether you're an engineer, tinkerer, hobbyist, or anyone in between.\n\n## Core Principles\n\n- **Tome is local first:** You are in control of where your data goes.\n- **Tome is for everyone:** You shouldn't have to manage programming languages, package managers, or json config files.\n\n## What's Next\n\nWe've gotten a lot of amazing feedback in the last few weeks since releasing Tome but we've got big plans for the future. We want to break LLMs out of their chatbox, and we've got a lot of features coming to help y'all do that.\n\n- Scheduled tasks: LLMs should be doing helpful things even when you're not in front of the computer.\n- Native integrations: MCP servers are a great way to access tools and information, but we want to add more powerful integrations to interact with LLMs in unique. ways\n- App builder: we believe long term that the best experiences will not be in a chat interface. We have plans to add additional tools that will enable you to create powerful applications and workflows.\n- ??? Let us know what you'd like to see! Join our community via the links below, we'd love to hear from you.\n\n# Community\n\n[Discord](https://discord.gg/9CH6us29YA) [Blog](https://blog.runebook.ai) [Bluesky](https://bsky.app/profile/gettome.app) [Twitter](https://twitter.com/get_tome) \n",
  "origin_sha": "rWLnVvqJwvr9v1fqiTn2LWjwaQbT+x1VplsLjSlpRbU=",
  "translate": "# Tome - Libro degli Incantesimi Magico con IA\n\n<img src=\"https://raw.githubusercontent.com/runebookai/tome/main/static/images/repo-header.png\" alt=\"Tome\" />\n\n<p align=\"center\">\n    <code>un'app desktop magica che mette il potere degli LLM e MCP nelle mani di tutti</code>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://discord.gg/9CH6us29YA\" target=\"_blank\"><img src=\"https://img.shields.io/discord/1365100902561742868?logo=discord&logoColor=fff&label=Join%20Us!&color=9D7CD8\" alt=\"Join Us on Discord\" /></a>\n    <a href=\"https://opensource.org/licenses/Apache-2.0\" target=\"_blank\"><img src=\"https://img.shields.io/badge/License-Apache_2.0-blue.svg\" alt=\"License: Apache 2.0\" /></a>\n    <a href=\"https://github.com/runebookai/tome/releases\" target=\"_blank\"><img src=\"https://img.shields.io/github/v/release/runebookai/tome\" alt=\"GitHub Release\" /></a>\n</p>\n\n<p align=\"center\">\n    üîÆ Scarica l'app desktop Tome: <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_x64-setup.exe\">Windows</a> | <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_aarch64.dmg\">MacOS</a>\n</p>\n\n# Tome\n\nTome √® un'app desktop che permette a **chiunque** di sfruttare la magia degli LLM e MCP. Scarica Tome, collega qualsiasi LLM locale o remoto e connettilo a migliaia di server MCP per creare il tuo magico libro degli incantesimi alimentato dall'IA.\n\nü´• Vuoi che sia al 100% locale, al 100% privato? Usa Ollama e Qwen3 solo con server MCP locali per lanciare incantesimi nel tuo universo tascabile. ‚ö° Vuoi modelli cloud all'avanguardia con gli ultimi server MCP remoti? Puoi avere anche questo. Dipende tutto da te!\n\nüèóÔ∏è Questa √® un'Anteprima Tecnica, quindi tieni presente che alcune cose potrebbero essere grezze. [Unisciti a noi su Discord](https://discord.gg/9CH6us29YA) per condividere suggerimenti, trucchi e problemi che incontri. Metti una stella a questo repo per rimanere aggiornato su novit√† e nuove funzionalit√†!\n\n## ü™Ñ Funzionalit√†\n\n- üßô **Esperienza Semplificata per Principianti**\n  - Basta scaricare e installare Tome e collegare l'LLM che preferisci\n  - Niente complicazioni con JSON, Docker, python o node\n- ü§ñ **Supporto Modelli AI**\n  - **Remoti**: Google Gemini, OpenAI, qualsiasi endpoint compatibile con OpenAI API\n  - **Locali**: Ollama, LM Studio, Cortex, qualsiasi endpoint compatibile con OpenAI API\n- üîÆ **Supporto MCP Avanzato**\n  - Interfaccia per installare, rimuovere, accendere/spegnere server MCP\n  - Server MCP npm, uvx, node, python tutti supportati nativamente\n- üè™ **Integrazione nel registro [Smithery.ai](https://smithery.ai)**\n  - Migliaia di server MCP disponibili tramite installazione con un clic\n- ‚úèÔ∏è **Personalizzazione delle finestre di contesto e della temperatura**\n- üß∞ **Supporto nativo per chiamate a strumenti e modelli di ragionamento**\n  - Migliorie UI che distinguono chiaramente tra chiamate a strumenti e messaggi di pensiero\n\n## Demo\n\nhttps://github.com/user-attachments/assets/0775d100-3eba-4219-9e2f-360a01f28cce\n\n# Per Iniziare\n\n## Requisiti\n\n- MacOS o Windows (Linux in arrivo!)\n- Un provider LLM a scelta: [Ollama](https://ollama.com/) o [chiave API Gemini](https://aistudio.google.com/app/apikey) sono semplici/gratuiti\n- [Scarica l'ultima versione di Tome](https://github.com/runebookai/tome/releases)\n\n## Avvio Rapido\n\n1. Installa [Tome](https://github.com/runebookai/tome/releases)\n2. Collega il tuo provider LLM preferito - OpenAI, Ollama e Gemini sono preimpostati ma puoi anche aggiungere provider come LM Studio utilizzando http://localhost:1234/v1 come URL\n3. Apri la scheda MCP in Tome e installa il tuo primo [server MCP](https://github.com/modelcontextprotocol/servers) (Fetch √® uno semplice per iniziare, basta incollare `uvx mcp-server-fetch` nel campo del server).\n4. Chatta con il tuo modello potenziato da MCP! Chiedigli di recuperare la notizia principale su Hacker News.\n\n# Visione\n\nVogliamo rendere gli LLM locali e MCP accessibili a tutti. Stiamo costruendo uno strumento che ti permette di essere creativo con gli LLM, indipendentemente dal fatto che tu sia un ingegnere, un maker, un hobbista o chiunque altro.\n\n## Principi Fondamentali\n\n- **Tome √® locale prima di tutto:** Hai il controllo su dove vanno i tuoi dati.\n- **Tome √® per tutti:** Non dovresti dover gestire linguaggi di programmazione, package manager o file di configurazione json.\n\n## Cosa ci aspetta\n\nAbbiamo ricevuto molti feedback fantastici nelle ultime settimane dal rilascio di Tome ma abbiamo grandi progetti per il futuro. Vogliamo liberare gli LLM dalla loro chatbox, e abbiamo in arrivo molte funzionalit√† per aiutarti a farlo.\n\n- Attivit√† programmate: gli LLM dovrebbero fare cose utili anche quando non sei davanti al computer.\n- Integrazioni native: i server MCP sono un ottimo modo per accedere a strumenti e informazioni, ma vogliamo aggiungere integrazioni ancora pi√π potenti per interagire con gli LLM in modi unici.\n- Costruttore di app: crediamo che, a lungo termine, le migliori esperienze non saranno in un'interfaccia di chat. Abbiamo in programma di aggiungere ulteriori strumenti che ti permetteranno di creare potenti applicazioni e flussi di lavoro.\n- ??? Facci sapere cosa vorresti vedere! Unisciti alla nostra community tramite i link qui sotto, ci piacerebbe sentirti.\n\n# Community\n\n[Discord](https://discord.gg/9CH6us29YA) [Blog](https://blog.runebook.ai) [Bluesky](https://bsky.app/profile/gettome.app) [Twitter](https://twitter.com/get_tome)",
  "status": "ok"
}