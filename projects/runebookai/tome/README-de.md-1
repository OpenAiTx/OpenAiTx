{
  "id": 1,
  "origin": "# Tome - Magical AI Spellbook\n\n<img src=\"https://raw.githubusercontent.com/runebookai/tome/main/static/images/repo-header.png\" alt=\"Tome\" />\n\n<p align=\"center\">\n    <code>a magical desktop app that puts the power of LLMs and MCP in the hands of everyone</code>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://discord.gg/9CH6us29YA\" target=\"_blank\"><img src=\"https://img.shields.io/discord/1365100902561742868?logo=discord&logoColor=fff&label=Join%20Us!&color=9D7CD8\" alt=\"Join Us on Discord\" /></a>\n    <a href=\"https://opensource.org/licenses/Apache-2.0\" target=\"_blank\"><img src=\"https://img.shields.io/badge/License-Apache_2.0-blue.svg\" alt=\"License: Apache 2.0\" /></a>\n    <a href=\"https://github.com/runebookai/tome/releases\" target=\"_blank\"><img src=\"https://img.shields.io/github/v/release/runebookai/tome\" alt=\"GitHub Release\" /></a>\n</p>\n\n<p align=\"center\">\n    üîÆ Download the Tome Desktop App: <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_x64-setup.exe\">Windows</a> | <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_aarch64.dmg\">MacOS</a>\n</p>\n\n# Tome\n\nTome is a desktop app that lets **anyone** harness the magic of LLMs and MCP. Download Tome, connect any local or remote LLM and hook it up to thousands of MCP servers to create your own magical AI-powered spellbook.\n\nü´• Want it to be 100% local, 100% private? Use Ollama and Qwen3 with only local MCP servers to cast spells in your own pocket universe. ‚ö° Want state of the art cloud models with the latest remote MCP servers? You can have that too. It's all up to you!\n\nüèóÔ∏è This is a Technical Preview so bear in mind things will be rough around the edges. [Join us on Discord](https://discord.gg/9CH6us29YA) to share tips, tricks, and issues you run into. Star this repo to stay on top of updates and feature releases!\n\n## ü™Ñ Features\n\n- üßô **Streamlined Beginner Friendly Experience**\n  - Simply download and install Tome and hook up the LLM of your choice\n  - No fiddling with JSON, Docker, python or node\n- ü§ñ **AI Model Support**\n  - **Remote**: Google Gemini, OpenAI, any OpenAI API-compatible endpoint\n  - **Local**: Ollama, LM Studio, Cortex, any OpenAI API-compatible endpoint\n- üîÆ **Enhanced MCP support**\n  - UI to install, remove, turn on/off MCP servers\n  - npm, uvx, node, python MCP servers all supported out of box\n- üè™ **Integration into [Smithery.ai](https://smithery.ai) registry**\n  - Thousands of MCP servers available via one-click installation\n- ‚úèÔ∏è **Customization of context windows and temperature**\n- üß∞ **Native support for tool calls and reasoning models**\n  - UI enhancements that clearly delineate tool calls and thinking messages\n\n## Demo\n\nhttps://github.com/user-attachments/assets/0775d100-3eba-4219-9e2f-360a01f28cce\n\n# Getting Started\n\n## Requirements\n\n- MacOS or Windows (Linux coming soon!)\n- LLM Provider of your choice: [Ollama](https://ollama.com/) or [Gemini API key](https://aistudio.google.com/app/apikey) are easy/free\n- [Download the latest release of Tome](https://github.com/runebookai/tome/releases)\n\n## Quickstart\n\n1. Install [Tome](https://github.com/runebookai/tome/releases)\n2. Connect your preferred LLM provider - OpenAI, Ollama and Gemini are preset but you can also add providers like LM Studio by using http://localhost:1234/v1 as the URL\n3. Open the MCP tab in Tome and install your first [MCP server](https://github.com/modelcontextprotocol/servers) (Fetch is an easy one to get started with, just paste `uvx mcp-server-fetch` into the server field).\n4. Chat with your MCP-powered model! Ask it to fetch the top story on Hacker News.\n\n# Vision\n\nWe want to make local LLMs and MCP accessible to everyone. We're building a tool that allows you to be creative with LLMs, regardless\nof whether you're an engineer, tinkerer, hobbyist, or anyone in between.\n\n## Core Principles\n\n- **Tome is local first:** You are in control of where your data goes.\n- **Tome is for everyone:** You shouldn't have to manage programming languages, package managers, or json config files.\n\n## What's Next\n\nWe've gotten a lot of amazing feedback in the last few weeks since releasing Tome but we've got big plans for the future. We want to break LLMs out of their chatbox, and we've got a lot of features coming to help y'all do that.\n\n- Scheduled tasks: LLMs should be doing helpful things even when you're not in front of the computer.\n- Native integrations: MCP servers are a great way to access tools and information, but we want to add more powerful integrations to interact with LLMs in unique. ways\n- App builder: we believe long term that the best experiences will not be in a chat interface. We have plans to add additional tools that will enable you to create powerful applications and workflows.\n- ??? Let us know what you'd like to see! Join our community via the links below, we'd love to hear from you.\n\n# Community\n\n[Discord](https://discord.gg/9CH6us29YA) [Blog](https://blog.runebook.ai) [Bluesky](https://bsky.app/profile/gettome.app) [Twitter](https://twitter.com/get_tome) \n",
  "origin_sha": "rWLnVvqJwvr9v1fqiTn2LWjwaQbT+x1VplsLjSlpRbU=",
  "translate": "# Tome - Magisches KI-Zauberbuch\n\n<img src=\"https://raw.githubusercontent.com/runebookai/tome/main/static/images/repo-header.png\" alt=\"Tome\" />\n\n<p align=\"center\">\n    <code>eine magische Desktop-App, die die Kraft von LLMs und MCP in die H√§nde aller legt</code>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://discord.gg/9CH6us29YA\" target=\"_blank\"><img src=\"https://img.shields.io/discord/1365100902561742868?logo=discord&logoColor=fff&label=Join%20Us!&color=9D7CD8\" alt=\"Join Us on Discord\" /></a>\n    <a href=\"https://opensource.org/licenses/Apache-2.0\" target=\"_blank\"><img src=\"https://img.shields.io/badge/License-Apache_2.0-blue.svg\" alt=\"License: Apache 2.0\" /></a>\n    <a href=\"https://github.com/runebookai/tome/releases\" target=\"_blank\"><img src=\"https://img.shields.io/github/v/release/runebookai/tome\" alt=\"GitHub Release\" /></a>\n</p>\n\n<p align=\"center\">\n    üîÆ Lade die Tome Desktop-App herunter: <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_x64-setup.exe\">Windows</a> | <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_aarch64.dmg\">MacOS</a>\n</p>\n\n# Tome\n\nTome ist eine Desktop-App, die **jedem** erm√∂glicht, die Magie von LLMs und MCP zu nutzen. Lade Tome herunter, verbinde ein beliebiges lokales oder entferntes LLM und schlie√üe es an Tausende von MCP-Servern an, um dein eigenes magisches, KI-gest√ºtztes Zauberbuch zu erschaffen.\n\nü´• Willst du, dass alles 100% lokal und 100% privat bleibt? Nutze Ollama und Qwen3 ausschlie√ülich mit lokalen MCP-Servern, um Zauberspr√ºche in deinem eigenen Pocket-Universum zu wirken. ‚ö° Willst du modernste Cloud-Modelle mit den neuesten entfernten MCP-Servern? Auch das ist m√∂glich. Die Wahl liegt ganz bei dir!\n\nüèóÔ∏è Dies ist eine technische Vorschau, daher kann es an einigen Stellen noch holprig sein. [Tritt unserem Discord bei](https://discord.gg/9CH6us29YA), um Tipps, Tricks und aufgetretene Probleme zu teilen. Setze einen Stern auf dieses Repo, um immer √ºber Updates und neue Funktionen informiert zu bleiben!\n\n## ü™Ñ Funktionen\n\n- üßô **Vereinfachtes, anf√§ngerfreundliches Erlebnis**\n  - Einfach Tome herunterladen, installieren und das gew√ºnschte LLM anschlie√üen\n  - Kein Herumhantieren mit JSON, Docker, Python oder Node notwendig\n- ü§ñ **KI-Modellunterst√ºtzung**\n  - **Remote:** Google Gemini, OpenAI, jeder OpenAI-API-kompatible Endpunkt\n  - **Lokal:** Ollama, LM Studio, Cortex, jeder OpenAI-API-kompatible Endpunkt\n- üîÆ **Erweiterte MCP-Unterst√ºtzung**\n  - Benutzeroberfl√§che zum Installieren, Entfernen, Ein- und Ausschalten von MCP-Servern\n  - npm, uvx, node, python MCP-Server werden direkt unterst√ºtzt\n- üè™ **Integration in das [Smithery.ai](https://smithery.ai)-Register**\n  - Tausende MCP-Server per Ein-Klick-Installation verf√ºgbar\n- ‚úèÔ∏è **Anpassung der Kontextfenster und Temperatur**\n- üß∞ **Native Unterst√ºtzung f√ºr Tool-Calls und Reasoning-Modelle**\n  - UI-Verbesserungen, die Tool-Calls und Denk-Nachrichten klar kennzeichnen\n\n## Demo\n\nhttps://github.com/user-attachments/assets/0775d100-3eba-4219-9e2f-360a01f28cce\n\n# Erste Schritte\n\n## Voraussetzungen\n\n- MacOS oder Windows (Linux kommt bald!)\n- LLM-Anbieter deiner Wahl: [Ollama](https://ollama.com/) oder [Gemini API-Schl√ºssel](https://aistudio.google.com/app/apikey) sind einfach/kostenlos\n- [Lade die neueste Version von Tome herunter](https://github.com/runebookai/tome/releases)\n\n## Schnellstart\n\n1. Installiere [Tome](https://github.com/runebookai/tome/releases)\n2. Verbinde deinen bevorzugten LLM-Anbieter ‚Äì OpenAI, Ollama und Gemini sind voreingestellt, aber du kannst auch Anbieter wie LM Studio hinzuf√ºgen, indem du http://localhost:1234/v1 als URL verwendest\n3. √ñffne den MCP-Tab in Tome und installiere deinen ersten [MCP-Server](https://github.com/modelcontextprotocol/servers) (Fetch ist ein einfacher Einstieg, kopiere einfach `uvx mcp-server-fetch` in das Serverfeld).\n4. Chatte mit deinem MCP-gest√ºtzten Modell! Bitte es, die Top-Story auf Hacker News zu holen.\n\n# Vision\n\nWir m√∂chten lokale LLMs und MCP f√ºr alle zug√§nglich machen. Wir bauen ein Tool, das dir erm√∂glicht, kreativ mit LLMs zu arbeiten, egal ob du Entwickler, Bastler, Hobbyist oder irgendetwas dazwischen bist.\n\n## Grundprinzipien\n\n- **Tome ist lokal zuerst:** Du kontrollierst, wohin deine Daten gehen.\n- **Tome ist f√ºr alle:** Du solltest dich nicht mit Programmiersprachen, Paketmanagern oder JSON-Konfigurationsdateien herumschlagen m√ºssen.\n\n## Wie geht es weiter\n\nWir haben seit der Ver√∂ffentlichung von Tome in den letzten Wochen viel tolles Feedback erhalten, aber wir haben gro√üe Pl√§ne f√ºr die Zukunft. Wir wollen LLMs aus ihrer Chatbox befreien und haben viele neue Funktionen in Arbeit, die euch dabei unterst√ºtzen.\n\n- Geplante Aufgaben: LLMs sollten hilfreiche Dinge erledigen, auch wenn du nicht am Computer bist.\n- Native Integrationen: MCP-Server sind ein gro√üartiger Weg, um auf Tools und Informationen zuzugreifen, aber wir wollen noch m√§chtigere Integrationen hinzuf√ºgen, um auf einzigartige Weise mit LLMs zu interagieren.\n- App-Builder: Wir glauben langfristig, dass die besten Erfahrungen nicht in einer Chat-Oberfl√§che stattfinden. Wir planen, zus√§tzliche Tools bereitzustellen, mit denen du leistungsstarke Anwendungen und Workflows erstellen kannst.\n- ??? Sag uns, was du dir w√ºnschst! Tritt √ºber die untenstehenden Links unserer Community bei, wir freuen uns auf dein Feedback.\n\n# Community\n\n[Discord](https://discord.gg/9CH6us29YA) [Blog](https://blog.runebook.ai) [Bluesky](https://bsky.app/profile/gettome.app) [Twitter](https://twitter.com/get_tome) \n",
  "status": "ok"
}