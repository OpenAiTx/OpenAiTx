{
  "id": 1,
  "origin": "# Tome - Magical AI Spellbook\n\n<img src=\"https://raw.githubusercontent.com/runebookai/tome/main/static/images/repo-header.png\" alt=\"Tome\" />\n\n<p align=\"center\">\n    <code>a magical desktop app that puts the power of LLMs and MCP in the hands of everyone</code>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://discord.gg/9CH6us29YA\" target=\"_blank\"><img src=\"https://img.shields.io/discord/1365100902561742868?logo=discord&logoColor=fff&label=Join%20Us!&color=9D7CD8\" alt=\"Join Us on Discord\" /></a>\n    <a href=\"https://opensource.org/licenses/Apache-2.0\" target=\"_blank\"><img src=\"https://img.shields.io/badge/License-Apache_2.0-blue.svg\" alt=\"License: Apache 2.0\" /></a>\n    <a href=\"https://github.com/runebookai/tome/releases\" target=\"_blank\"><img src=\"https://img.shields.io/github/v/release/runebookai/tome\" alt=\"GitHub Release\" /></a>\n</p>\n\n<p align=\"center\">\n    🔮 Download the Tome Desktop App: <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_x64-setup.exe\">Windows</a> | <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_aarch64.dmg\">MacOS</a>\n</p>\n\n# Tome\n\nTome is a desktop app that lets **anyone** harness the magic of LLMs and MCP. Download Tome, connect any local or remote LLM and hook it up to thousands of MCP servers to create your own magical AI-powered spellbook.\n\n🫥 Want it to be 100% local, 100% private? Use Ollama and Qwen3 with only local MCP servers to cast spells in your own pocket universe. ⚡ Want state of the art cloud models with the latest remote MCP servers? You can have that too. It's all up to you!\n\n🏗️ This is a Technical Preview so bear in mind things will be rough around the edges. [Join us on Discord](https://discord.gg/9CH6us29YA) to share tips, tricks, and issues you run into. Star this repo to stay on top of updates and feature releases!\n\n## 🪄 Features\n\n- 🧙 **Streamlined Beginner Friendly Experience**\n  - Simply download and install Tome and hook up the LLM of your choice\n  - No fiddling with JSON, Docker, python or node\n- 🤖 **AI Model Support**\n  - **Remote**: Google Gemini, OpenAI, any OpenAI API-compatible endpoint\n  - **Local**: Ollama, LM Studio, Cortex, any OpenAI API-compatible endpoint\n- 🔮 **Enhanced MCP support**\n  - UI to install, remove, turn on/off MCP servers\n  - npm, uvx, node, python MCP servers all supported out of box\n- 🏪 **Integration into [Smithery.ai](https://smithery.ai) registry**\n  - Thousands of MCP servers available via one-click installation\n- ✏️ **Customization of context windows and temperature**\n- 🧰 **Native support for tool calls and reasoning models**\n  - UI enhancements that clearly delineate tool calls and thinking messages\n\n## Demo\n\nhttps://github.com/user-attachments/assets/0775d100-3eba-4219-9e2f-360a01f28cce\n\n# Getting Started\n\n## Requirements\n\n- MacOS or Windows (Linux coming soon!)\n- LLM Provider of your choice: [Ollama](https://ollama.com/) or [Gemini API key](https://aistudio.google.com/app/apikey) are easy/free\n- [Download the latest release of Tome](https://github.com/runebookai/tome/releases)\n\n## Quickstart\n\n1. Install [Tome](https://github.com/runebookai/tome/releases)\n2. Connect your preferred LLM provider - OpenAI, Ollama and Gemini are preset but you can also add providers like LM Studio by using http://localhost:1234/v1 as the URL\n3. Open the MCP tab in Tome and install your first [MCP server](https://github.com/modelcontextprotocol/servers) (Fetch is an easy one to get started with, just paste `uvx mcp-server-fetch` into the server field).\n4. Chat with your MCP-powered model! Ask it to fetch the top story on Hacker News.\n\n# Vision\n\nWe want to make local LLMs and MCP accessible to everyone. We're building a tool that allows you to be creative with LLMs, regardless\nof whether you're an engineer, tinkerer, hobbyist, or anyone in between.\n\n## Core Principles\n\n- **Tome is local first:** You are in control of where your data goes.\n- **Tome is for everyone:** You shouldn't have to manage programming languages, package managers, or json config files.\n\n## What's Next\n\nWe've gotten a lot of amazing feedback in the last few weeks since releasing Tome but we've got big plans for the future. We want to break LLMs out of their chatbox, and we've got a lot of features coming to help y'all do that.\n\n- Scheduled tasks: LLMs should be doing helpful things even when you're not in front of the computer.\n- Native integrations: MCP servers are a great way to access tools and information, but we want to add more powerful integrations to interact with LLMs in unique. ways\n- App builder: we believe long term that the best experiences will not be in a chat interface. We have plans to add additional tools that will enable you to create powerful applications and workflows.\n- ??? Let us know what you'd like to see! Join our community via the links below, we'd love to hear from you.\n\n# Community\n\n[Discord](https://discord.gg/9CH6us29YA) [Blog](https://blog.runebook.ai) [Bluesky](https://bsky.app/profile/gettome.app) [Twitter](https://twitter.com/get_tome) \n",
  "origin_sha": "rWLnVvqJwvr9v1fqiTn2LWjwaQbT+x1VplsLjSlpRbU=",
  "translate": "# Tome - Sách Phép Thuật AI\n\n<img src=\"https://raw.githubusercontent.com/runebookai/tome/main/static/images/repo-header.png\" alt=\"Tome\" />\n\n<p align=\"center\">\n    <code>một ứng dụng desktop kỳ diệu giúp mọi người tận dụng sức mạnh của LLMs và MCP</code>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://discord.gg/9CH6us29YA\" target=\"_blank\"><img src=\"https://img.shields.io/discord/1365100902561742868?logo=discord&logoColor=fff&label=Join%20Us!&color=9D7CD8\" alt=\"Tham gia Discord của chúng tôi\" /></a>\n    <a href=\"https://opensource.org/licenses/Apache-2.0\" target=\"_blank\"><img src=\"https://img.shields.io/badge/License-Apache_2.0-blue.svg\" alt=\"Giấy phép: Apache 2.0\" /></a>\n    <a href=\"https://github.com/runebookai/tome/releases\" target=\"_blank\"><img src=\"https://img.shields.io/github/v/release/runebookai/tome\" alt=\"Phát hành GitHub\" /></a>\n</p>\n\n<p align=\"center\">\n    🔮 Tải ứng dụng Tome Desktop: <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_x64-setup.exe\">Windows</a> | <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_aarch64.dmg\">MacOS</a>\n</p>\n\n# Tome\n\nTome là một ứng dụng desktop cho phép **bất kỳ ai** khai thác sức mạnh kỳ diệu của LLMs và MCP. Tải Tome, kết nối với bất kỳ LLM cục bộ hoặc từ xa nào và liên kết với hàng ngàn máy chủ MCP để tạo ra cuốn sách phép thuật AI của riêng bạn.\n\n🫥 Muốn tất cả hoạt động 100% cục bộ, 100% riêng tư? Hãy sử dụng Ollama và Qwen3 chỉ với các máy chủ MCP cục bộ để thi triển phép thuật trong vũ trụ túi của riêng bạn. ⚡ Muốn các mô hình đám mây hiện đại nhất với các máy chủ MCP từ xa mới nhất? Bạn cũng có thể làm được điều đó. Tất cả tùy thuộc vào bạn!\n\n🏗️ Đây là phiên bản Kỹ Thuật Xem Trước nên hãy lưu ý rằng sẽ có một số điểm chưa hoàn thiện. [Tham gia Discord của chúng tôi](https://discord.gg/9CH6us29YA) để chia sẻ mẹo, thủ thuật và các vấn đề bạn gặp phải. Hãy gắn sao repo này để luôn cập nhật các bản phát hành và tính năng mới!\n\n## 🪄 Tính Năng\n\n- 🧙 **Trải nghiệm thân thiện với người mới**\n  - Chỉ cần tải và cài đặt Tome, sau đó kết nối với LLM bạn muốn\n  - Không cần chỉnh sửa JSON, Docker, python hay node\n- 🤖 **Hỗ trợ đa dạng mô hình AI**\n  - **Từ xa**: Google Gemini, OpenAI, bất kỳ endpoint tương thích API OpenAI nào\n  - **Cục bộ**: Ollama, LM Studio, Cortex, bất kỳ endpoint tương thích API OpenAI nào\n- 🔮 **Hỗ trợ MCP nâng cao**\n  - Giao diện cài đặt, gỡ bỏ, bật/tắt máy chủ MCP\n  - Hỗ trợ sẵn các máy chủ MCP npm, uvx, node, python\n- 🏪 **Tích hợp vào registry [Smithery.ai](https://smithery.ai)**\n  - Hàng ngàn máy chủ MCP có sẵn chỉ với một cú nhấp chuột cài đặt\n- ✏️ **Tuỳ chỉnh cửa sổ ngữ cảnh và nhiệt độ**\n- 🧰 **Hỗ trợ gốc cho gọi công cụ và mô hình suy luận**\n  - Giao diện trực quan thể hiện rõ các lệnh gọi công cụ và thông báo suy nghĩ\n\n## Demo\n\nhttps://github.com/user-attachments/assets/0775d100-3eba-4219-9e2f-360a01f28cce\n\n# Bắt Đầu\n\n## Yêu Cầu\n\n- MacOS hoặc Windows (Linux sẽ sớm ra mắt!)\n- Nhà cung cấp LLM bạn chọn: [Ollama](https://ollama.com/) hoặc [Gemini API key](https://aistudio.google.com/app/apikey) đều dễ dàng/miễn phí\n- [Tải bản phát hành mới nhất của Tome](https://github.com/runebookai/tome/releases)\n\n## Bắt Đầu Nhanh\n\n1. Cài đặt [Tome](https://github.com/runebookai/tome/releases)\n2. Kết nối với nhà cung cấp LLM bạn muốn - OpenAI, Ollama và Gemini đã được thiết lập sẵn nhưng bạn cũng có thể thêm các nhà cung cấp như LM Studio bằng cách sử dụng http://localhost:1234/v1 làm URL\n3. Mở tab MCP trong Tome và cài đặt máy chủ [MCP đầu tiên của bạn](https://github.com/modelcontextprotocol/servers) (Fetch là lựa chọn dễ bắt đầu, chỉ cần dán `uvx mcp-server-fetch` vào trường máy chủ).\n4. Trò chuyện với mô hình của bạn được hỗ trợ bởi MCP! Hãy yêu cầu nó lấy tin nổi bật nhất trên Hacker News.\n\n# Tầm Nhìn\n\nChúng tôi muốn giúp mọi người tiếp cận LLM cục bộ và MCP. Chúng tôi đang xây dựng một công cụ cho phép bạn sáng tạo với LLM, bất kể bạn là kỹ sư, người thích mày mò, nhà sáng tạo hay bất kỳ ai.\n\n## Nguyên Tắc Cốt Lõi\n\n- **Tome ưu tiên cục bộ:** Bạn kiểm soát nơi dữ liệu của mình được gửi đi.\n- **Tome dành cho tất cả mọi người:** Bạn không cần phải quản lý các ngôn ngữ lập trình, trình quản lý gói hay các file cấu hình json.\n\n## Kế Hoạch Tiếp Theo\n\nChúng tôi đã nhận được rất nhiều phản hồi tuyệt vời trong vài tuần kể từ khi phát hành Tome, nhưng chúng tôi còn nhiều kế hoạch lớn trong tương lai. Chúng tôi muốn đưa LLM ra khỏi hộp chat, và sẽ có rất nhiều tính năng mới để giúp các bạn làm điều đó.\n\n- Nhiệm vụ theo lịch: LLM nên thực hiện những việc hữu ích ngay cả khi bạn không ngồi trước máy tính.\n- Tích hợp gốc: Máy chủ MCP là cách tuyệt vời để truy cập công cụ và thông tin, nhưng chúng tôi muốn bổ sung các tích hợp mạnh mẽ hơn để tương tác với LLM theo cách độc đáo.\n- Trình xây dựng ứng dụng: chúng tôi tin rằng về lâu dài, trải nghiệm tốt nhất sẽ không chỉ nằm ở giao diện chat. Chúng tôi có kế hoạch bổ sung thêm các công cụ để bạn tạo ra các ứng dụng và quy trình làm việc mạnh mẽ.\n- ??? Hãy cho chúng tôi biết bạn muốn thấy gì! Tham gia cộng đồng qua các liên kết bên dưới, chúng tôi rất mong được lắng nghe ý kiến từ bạn.\n\n# Cộng Đồng\n\n[Discord](https://discord.gg/9CH6us29YA) [Blog](https://blog.runebook.ai) [Bluesky](https://bsky.app/profile/gettome.app) [Twitter](https://twitter.com/get_tome) ",
  "status": "ok"
}