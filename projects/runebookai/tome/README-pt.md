# Tome - GrimÃ³rio MÃ¡gico de IA

<img src="https://raw.githubusercontent.com/runebookai/tome/main/static/images/repo-header.png" alt="Tome" />

<p align="center">
    <code>um aplicativo mÃ¡gico de desktop que coloca o poder de LLMs e MCP nas mÃ£os de todos</code>
</p>

<p align="center">
    <a href="https://discord.gg/9CH6us29YA" target="_blank"><img src="https://img.shields.io/discord/1365100902561742868?logo=discord&logoColor=fff&label=Join%20Us!&color=9D7CD8" alt="Junte-se a nÃ³s no Discord" /></a>
    <a href="https://opensource.org/licenses/Apache-2.0" target="_blank"><img src="https://img.shields.io/badge/License-Apache_2.0-blue.svg" alt="LicenÃ§a: Apache 2.0" /></a>
    <a href="https://github.com/runebookai/tome/releases" target="_blank"><img src="https://img.shields.io/github/v/release/runebookai/tome" alt="LanÃ§amento no GitHub" /></a>
</p>

<p align="center">
    ğŸ”® Baixe o aplicativo Tome Desktop: <a href="https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_x64-setup.exe">Windows</a> | <a href="https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_aarch64.dmg">MacOS</a>
</p>

# Tome

O Tome Ã© um aplicativo de desktop que permite que **qualquer pessoa** aproveite a magia dos LLMs e MCP. Baixe o Tome, conecte qualquer LLM local ou remoto e ligue-o a milhares de servidores MCP para criar seu prÃ³prio grimÃ³rio mÃ¡gico alimentado por IA.

ğŸ«¥ Quer que seja 100% local, 100% privado? Use Ollama e Qwen3 apenas com servidores MCP locais para lanÃ§ar feitiÃ§os em seu prÃ³prio universo de bolso. âš¡ Quer modelos de nuvem de ponta com os servidores MCP remotos mais recentes? VocÃª tambÃ©m pode ter isso. A escolha Ã© toda sua!

ğŸ—ï¸ Esta Ã© uma VisualizaÃ§Ã£o TÃ©cnica, entÃ£o lembre-se de que as coisas podem estar inacabadas. [Junte-se a nÃ³s no Discord](https://discord.gg/9CH6us29YA) para compartilhar dicas, truques e problemas encontrados. DÃª uma estrela neste repositÃ³rio para ficar por dentro das novidades e lanÃ§amentos de funcionalidades!

## ğŸª„ Funcionalidades

- ğŸ§™ **ExperiÃªncia Facilitada para Iniciantes**
  - Basta baixar e instalar o Tome e conectar o LLM de sua escolha
  - Sem complicaÃ§Ãµes com JSON, Docker, python ou node
- ğŸ¤– **Suporte a Modelos de IA**
  - **Remoto**: Google Gemini, OpenAI, qualquer endpoint compatÃ­vel com API OpenAI
  - **Local**: Ollama, LM Studio, Cortex, qualquer endpoint compatÃ­vel com API OpenAI
- ğŸ”® **Suporte Aprimorado a MCP**
  - Interface para instalar, remover, ligar/desligar servidores MCP
  - Servidores MCP npm, uvx, node, python todos suportados nativamente
- ğŸª **IntegraÃ§Ã£o com o registro [Smithery.ai](https://smithery.ai)**
  - Milhares de servidores MCP disponÃ­veis via instalaÃ§Ã£o com um clique
- âœï¸ **PersonalizaÃ§Ã£o de janelas de contexto e temperatura**
- ğŸ§° **Suporte nativo a chamadas de ferramentas e modelos de raciocÃ­nio**
  - Melhorias na interface que destacam claramente chamadas de ferramentas e mensagens de pensamento

## DemonstraÃ§Ã£o

https://github.com/user-attachments/assets/0775d100-3eba-4219-9e2f-360a01f28cce

# Primeiros Passos

## Requisitos

- MacOS ou Windows (Linux em breve!)
- Provedor de LLM de sua escolha: [Ollama](https://ollama.com/) ou [chave da API Gemini](https://aistudio.google.com/app/apikey) sÃ£o fÃ¡ceis/grÃ¡tis
- [Baixe a versÃ£o mais recente do Tome](https://github.com/runebookai/tome/releases)

## InÃ­cio RÃ¡pido

1. Instale o [Tome](https://github.com/runebookai/tome/releases)
2. Conecte seu provedor de LLM preferido - OpenAI, Ollama e Gemini jÃ¡ vÃªm prÃ©-configurados, mas vocÃª tambÃ©m pode adicionar provedores como o LM Studio usando http://localhost:1234/v1 como URL
3. Abra a aba MCP no Tome e instale seu primeiro [servidor MCP](https://github.com/modelcontextprotocol/servers) (Fetch Ã© um fÃ¡cil para comeÃ§ar, basta colar `uvx mcp-server-fetch` no campo do servidor).
4. Converse com seu modelo alimentado por MCP! PeÃ§a para buscar a principal notÃ­cia do Hacker News.

# VisÃ£o

Queremos tornar os LLMs locais e MCP acessÃ­veis para todos. Estamos construindo uma ferramenta que permite que vocÃª seja criativo com LLMs, independentemente
de ser engenheiro, entusiasta, hobbyista ou qualquer pessoa entre eles.

## PrincÃ­pios Fundamentais

- **Tome Ã© local primeiro:** VocÃª estÃ¡ no controle de para onde seus dados vÃ£o.
- **Tome Ã© para todos:** VocÃª nÃ£o deve precisar gerenciar linguagens de programaÃ§Ã£o, gerenciadores de pacotes ou arquivos de configuraÃ§Ã£o json.

## O que vem a seguir

Recebemos muitos feedbacks incrÃ­veis nas Ãºltimas semanas desde o lanÃ§amento do Tome, mas temos grandes planos para o futuro. Queremos libertar os LLMs da sua caixa de chat, e temos muitos recursos chegando para ajudar vocÃª a fazer isso.

- Tarefas agendadas: LLMs devem realizar tarefas Ãºteis mesmo quando vocÃª nÃ£o estÃ¡ na frente do computador.
- IntegraÃ§Ãµes nativas: Servidores MCP sÃ£o uma Ã³tima maneira de acessar ferramentas e informaÃ§Ãµes, mas queremos adicionar integraÃ§Ãµes ainda mais poderosas para interagir com os LLMs de maneiras Ãºnicas.
- Construtor de aplicativos: acreditamos que, a longo prazo, as melhores experiÃªncias nÃ£o estarÃ£o em uma interface de chat. Temos planos para adicionar ferramentas adicionais que permitirÃ£o criar aplicativos e fluxos de trabalho poderosos.
- ??? Conte-nos o que vocÃª gostaria de ver! Participe da nossa comunidade pelos links abaixo, adorarÃ­amos ouvir vocÃª.

# Comunidade

[Discord](https://discord.gg/9CH6us29YA) [Blog](https://blog.runebook.ai) [Bluesky](https://bsky.app/profile/gettome.app) [Twitter](https://twitter.com/get_tome) 


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-03

---