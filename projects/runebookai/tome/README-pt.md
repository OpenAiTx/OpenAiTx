# Tome - Grimório Mágico de IA

<img src="https://raw.githubusercontent.com/runebookai/tome/main/static/images/repo-header.png" alt="Tome" />

<p align="center">
    <code>um aplicativo mágico de desktop que coloca o poder de LLMs e MCP nas mãos de todos</code>
</p>

<p align="center">
    <a href="https://discord.gg/9CH6us29YA" target="_blank"><img src="https://img.shields.io/discord/1365100902561742868?logo=discord&logoColor=fff&label=Join%20Us!&color=9D7CD8" alt="Junte-se a nós no Discord" /></a>
    <a href="https://opensource.org/licenses/Apache-2.0" target="_blank"><img src="https://img.shields.io/badge/License-Apache_2.0-blue.svg" alt="Licença: Apache 2.0" /></a>
    <a href="https://github.com/runebookai/tome/releases" target="_blank"><img src="https://img.shields.io/github/v/release/runebookai/tome" alt="Lançamento no GitHub" /></a>
</p>

<p align="center">
    🔮 Baixe o aplicativo Tome Desktop: <a href="https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_x64-setup.exe">Windows</a> | <a href="https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_aarch64.dmg">MacOS</a>
</p>

# Tome

O Tome é um aplicativo de desktop que permite que **qualquer pessoa** aproveite a magia dos LLMs e MCP. Baixe o Tome, conecte qualquer LLM local ou remoto e ligue-o a milhares de servidores MCP para criar seu próprio grimório mágico alimentado por IA.

🫥 Quer que seja 100% local, 100% privado? Use Ollama e Qwen3 apenas com servidores MCP locais para lançar feitiços em seu próprio universo de bolso. ⚡ Quer modelos de nuvem de ponta com os servidores MCP remotos mais recentes? Você também pode ter isso. A escolha é toda sua!

🏗️ Esta é uma Visualização Técnica, então lembre-se de que as coisas podem estar inacabadas. [Junte-se a nós no Discord](https://discord.gg/9CH6us29YA) para compartilhar dicas, truques e problemas encontrados. Dê uma estrela neste repositório para ficar por dentro das novidades e lançamentos de funcionalidades!

## 🪄 Funcionalidades

- 🧙 **Experiência Facilitada para Iniciantes**
  - Basta baixar e instalar o Tome e conectar o LLM de sua escolha
  - Sem complicações com JSON, Docker, python ou node
- 🤖 **Suporte a Modelos de IA**
  - **Remoto**: Google Gemini, OpenAI, qualquer endpoint compatível com API OpenAI
  - **Local**: Ollama, LM Studio, Cortex, qualquer endpoint compatível com API OpenAI
- 🔮 **Suporte Aprimorado a MCP**
  - Interface para instalar, remover, ligar/desligar servidores MCP
  - Servidores MCP npm, uvx, node, python todos suportados nativamente
- 🏪 **Integração com o registro [Smithery.ai](https://smithery.ai)**
  - Milhares de servidores MCP disponíveis via instalação com um clique
- ✏️ **Personalização de janelas de contexto e temperatura**
- 🧰 **Suporte nativo a chamadas de ferramentas e modelos de raciocínio**
  - Melhorias na interface que destacam claramente chamadas de ferramentas e mensagens de pensamento

## Demonstração

https://github.com/user-attachments/assets/0775d100-3eba-4219-9e2f-360a01f28cce

# Primeiros Passos

## Requisitos

- MacOS ou Windows (Linux em breve!)
- Provedor de LLM de sua escolha: [Ollama](https://ollama.com/) ou [chave da API Gemini](https://aistudio.google.com/app/apikey) são fáceis/grátis
- [Baixe a versão mais recente do Tome](https://github.com/runebookai/tome/releases)

## Início Rápido

1. Instale o [Tome](https://github.com/runebookai/tome/releases)
2. Conecte seu provedor de LLM preferido - OpenAI, Ollama e Gemini já vêm pré-configurados, mas você também pode adicionar provedores como o LM Studio usando http://localhost:1234/v1 como URL
3. Abra a aba MCP no Tome e instale seu primeiro [servidor MCP](https://github.com/modelcontextprotocol/servers) (Fetch é um fácil para começar, basta colar `uvx mcp-server-fetch` no campo do servidor).
4. Converse com seu modelo alimentado por MCP! Peça para buscar a principal notícia do Hacker News.

# Visão

Queremos tornar os LLMs locais e MCP acessíveis para todos. Estamos construindo uma ferramenta que permite que você seja criativo com LLMs, independentemente
de ser engenheiro, entusiasta, hobbyista ou qualquer pessoa entre eles.

## Princípios Fundamentais

- **Tome é local primeiro:** Você está no controle de para onde seus dados vão.
- **Tome é para todos:** Você não deve precisar gerenciar linguagens de programação, gerenciadores de pacotes ou arquivos de configuração json.

## O que vem a seguir

Recebemos muitos feedbacks incríveis nas últimas semanas desde o lançamento do Tome, mas temos grandes planos para o futuro. Queremos libertar os LLMs da sua caixa de chat, e temos muitos recursos chegando para ajudar você a fazer isso.

- Tarefas agendadas: LLMs devem realizar tarefas úteis mesmo quando você não está na frente do computador.
- Integrações nativas: Servidores MCP são uma ótima maneira de acessar ferramentas e informações, mas queremos adicionar integrações ainda mais poderosas para interagir com os LLMs de maneiras únicas.
- Construtor de aplicativos: acreditamos que, a longo prazo, as melhores experiências não estarão em uma interface de chat. Temos planos para adicionar ferramentas adicionais que permitirão criar aplicativos e fluxos de trabalho poderosos.
- ??? Conte-nos o que você gostaria de ver! Participe da nossa comunidade pelos links abaixo, adoraríamos ouvir você.

# Comunidade

[Discord](https://discord.gg/9CH6us29YA) [Blog](https://blog.runebook.ai) [Bluesky](https://bsky.app/profile/gettome.app) [Twitter](https://twitter.com/get_tome) 


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-03

---