{
  "id": 1,
  "origin": "# Tome - Magical AI Spellbook\n\n<img src=\"https://raw.githubusercontent.com/runebookai/tome/main/static/images/repo-header.png\" alt=\"Tome\" />\n\n<p align=\"center\">\n    <code>a magical desktop app that puts the power of LLMs and MCP in the hands of everyone</code>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://discord.gg/9CH6us29YA\" target=\"_blank\"><img src=\"https://img.shields.io/discord/1365100902561742868?logo=discord&logoColor=fff&label=Join%20Us!&color=9D7CD8\" alt=\"Join Us on Discord\" /></a>\n    <a href=\"https://opensource.org/licenses/Apache-2.0\" target=\"_blank\"><img src=\"https://img.shields.io/badge/License-Apache_2.0-blue.svg\" alt=\"License: Apache 2.0\" /></a>\n    <a href=\"https://github.com/runebookai/tome/releases\" target=\"_blank\"><img src=\"https://img.shields.io/github/v/release/runebookai/tome\" alt=\"GitHub Release\" /></a>\n</p>\n\n<p align=\"center\">\n    🔮 Download the Tome Desktop App: <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_x64-setup.exe\">Windows</a> | <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_aarch64.dmg\">MacOS</a>\n</p>\n\n# Tome\n\nTome is a desktop app that lets **anyone** harness the magic of LLMs and MCP. Download Tome, connect any local or remote LLM and hook it up to thousands of MCP servers to create your own magical AI-powered spellbook.\n\n🫥 Want it to be 100% local, 100% private? Use Ollama and Qwen3 with only local MCP servers to cast spells in your own pocket universe. ⚡ Want state of the art cloud models with the latest remote MCP servers? You can have that too. It's all up to you!\n\n🏗️ This is a Technical Preview so bear in mind things will be rough around the edges. [Join us on Discord](https://discord.gg/9CH6us29YA) to share tips, tricks, and issues you run into. Star this repo to stay on top of updates and feature releases!\n\n## 🪄 Features\n\n- 🧙 **Streamlined Beginner Friendly Experience**\n  - Simply download and install Tome and hook up the LLM of your choice\n  - No fiddling with JSON, Docker, python or node\n- 🤖 **AI Model Support**\n  - **Remote**: Google Gemini, OpenAI, any OpenAI API-compatible endpoint\n  - **Local**: Ollama, LM Studio, Cortex, any OpenAI API-compatible endpoint\n- 🔮 **Enhanced MCP support**\n  - UI to install, remove, turn on/off MCP servers\n  - npm, uvx, node, python MCP servers all supported out of box\n- 🏪 **Integration into [Smithery.ai](https://smithery.ai) registry**\n  - Thousands of MCP servers available via one-click installation\n- ✏️ **Customization of context windows and temperature**\n- 🧰 **Native support for tool calls and reasoning models**\n  - UI enhancements that clearly delineate tool calls and thinking messages\n\n## Demo\n\nhttps://github.com/user-attachments/assets/0775d100-3eba-4219-9e2f-360a01f28cce\n\n# Getting Started\n\n## Requirements\n\n- MacOS or Windows (Linux coming soon!)\n- LLM Provider of your choice: [Ollama](https://ollama.com/) or [Gemini API key](https://aistudio.google.com/app/apikey) are easy/free\n- [Download the latest release of Tome](https://github.com/runebookai/tome/releases)\n\n## Quickstart\n\n1. Install [Tome](https://github.com/runebookai/tome/releases)\n2. Connect your preferred LLM provider - OpenAI, Ollama and Gemini are preset but you can also add providers like LM Studio by using http://localhost:1234/v1 as the URL\n3. Open the MCP tab in Tome and install your first [MCP server](https://github.com/modelcontextprotocol/servers) (Fetch is an easy one to get started with, just paste `uvx mcp-server-fetch` into the server field).\n4. Chat with your MCP-powered model! Ask it to fetch the top story on Hacker News.\n\n# Vision\n\nWe want to make local LLMs and MCP accessible to everyone. We're building a tool that allows you to be creative with LLMs, regardless\nof whether you're an engineer, tinkerer, hobbyist, or anyone in between.\n\n## Core Principles\n\n- **Tome is local first:** You are in control of where your data goes.\n- **Tome is for everyone:** You shouldn't have to manage programming languages, package managers, or json config files.\n\n## What's Next\n\nWe've gotten a lot of amazing feedback in the last few weeks since releasing Tome but we've got big plans for the future. We want to break LLMs out of their chatbox, and we've got a lot of features coming to help y'all do that.\n\n- Scheduled tasks: LLMs should be doing helpful things even when you're not in front of the computer.\n- Native integrations: MCP servers are a great way to access tools and information, but we want to add more powerful integrations to interact with LLMs in unique. ways\n- App builder: we believe long term that the best experiences will not be in a chat interface. We have plans to add additional tools that will enable you to create powerful applications and workflows.\n- ??? Let us know what you'd like to see! Join our community via the links below, we'd love to hear from you.\n\n# Community\n\n[Discord](https://discord.gg/9CH6us29YA) [Blog](https://blog.runebook.ai) [Bluesky](https://bsky.app/profile/gettome.app) [Twitter](https://twitter.com/get_tome) \n",
  "origin_sha": "rWLnVvqJwvr9v1fqiTn2LWjwaQbT+x1VplsLjSlpRbU=",
  "translate": "# Tome - كتاب تعويذات الذكاء الاصطناعي السحري\n\n<img src=\"https://raw.githubusercontent.com/runebookai/tome/main/static/images/repo-header.png\" alt=\"Tome\" />\n\n<p align=\"center\">\n    <code>تطبيق سطح مكتب سحري يضع قوة نماذج اللغة الكبيرة (LLMs) وMCP بين يدي الجميع</code>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://discord.gg/9CH6us29YA\" target=\"_blank\"><img src=\"https://img.shields.io/discord/1365100902561742868?logo=discord&logoColor=fff&label=Join%20Us!&color=9D7CD8\" alt=\"انضم إلينا على ديسكورد\" /></a>\n    <a href=\"https://opensource.org/licenses/Apache-2.0\" target=\"_blank\"><img src=\"https://img.shields.io/badge/License-Apache_2.0-blue.svg\" alt=\"الرخصة: Apache 2.0\" /></a>\n    <a href=\"https://github.com/runebookai/tome/releases\" target=\"_blank\"><img src=\"https://img.shields.io/github/v/release/runebookai/tome\" alt=\"إصدار GitHub\" /></a>\n</p>\n\n<p align=\"center\">\n    🔮 حمّل تطبيق Tome لسطح المكتب: <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_x64-setup.exe\">ويندوز</a> | <a href=\"https://github.com/runebookai/tome/releases/download/0.6.0/Tome_0.6.0_aarch64.dmg\">ماك</a>\n</p>\n\n# Tome\n\nTome هو تطبيق سطح مكتب يتيح **لأي شخص** تسخير سحر نماذج اللغة الكبيرة (LLMs) وMCP. قم بتنزيل Tome، وصل أي نموذج LLM محلي أو عن بُعد واربطه بآلاف خوادم MCP لإنشاء كتاب تعويذاتك السحري المدعوم بالذكاء الاصطناعي.\n\n🫥 تريد أن يكون كل شيء محليًا 100% وخاصًا 100%؟ استخدم Ollama وQwen3 مع خوادم MCP المحلية فقط لتلقي التعويذات في عالمك الخاص. ⚡ تريد أحدث النماذج السحابية مع خوادم MCP عن بُعد؟ يمكنك ذلك أيضًا. الخيار لك بالكامل!\n\n🏗️ هذه نسخة تقنية أولية لذا يرجى العلم بأن الأمور قد تكون غير مكتملة. [انضم إلينا على ديسكورد](https://discord.gg/9CH6us29YA) لمشاركة النصائح والحيل والمشاكل التي تواجهك. ضع نجمة على هذا المستودع للبقاء على اطلاع بالتحديثات وميزات الإصدارات!\n\n## 🪄 الميزات\n\n- 🧙 **تجربة سهلة ومبسطة للمبتدئين**\n  - فقط قم بتنزيل وتثبيت Tome ووصّل نموذج اللغة الذي تريده\n  - لا حاجة للتعامل مع JSON أو Docker أو بايثون أو نود\n- 🤖 **دعم نماذج الذكاء الاصطناعي**\n  - **عن بُعد**: Google Gemini، OpenAI، أي نقطة نهاية متوافقة مع OpenAI API\n  - **محلي**: Ollama، LM Studio، Cortex، أي نقطة نهاية متوافقة مع OpenAI API\n- 🔮 **دعم متقدم لـ MCP**\n  - واجهة لتثبيت وإزالة وتشغيل/إيقاف خوادم MCP\n  - دعم خوادم MCP التي تعمل بـ npm وuvx وnode وpython مباشرة من الصندوق\n- 🏪 **تكامل مع سجل [Smithery.ai](https://smithery.ai)**\n  - آلاف خوادم MCP متاحة عبر التثبيت بنقرة واحدة\n- ✏️ **تخصيص نوافذ السياق ودرجة الحرارة**\n- 🧰 **دعم أصلي لاستدعاءات الأدوات ونماذج التفكير**\n  - تحسينات في الواجهة توضح استدعاءات الأدوات ورسائل التفكير بشكل واضح\n\n## العرض التوضيحي\n\nhttps://github.com/user-attachments/assets/0775d100-3eba-4219-9e2f-360a01f28cce\n\n# البدء\n\n## المتطلبات\n\n- ماك أو ويندوز (لينكس قريبًا!)\n- مزود LLM من اختيارك: [Ollama](https://ollama.com/) أو [مفتاح Gemini API](https://aistudio.google.com/app/apikey) سهل/مجاني\n- [حمّل أحدث إصدار من Tome](https://github.com/runebookai/tome/releases)\n\n## البدء السريع\n\n1. ثبّت [Tome](https://github.com/runebookai/tome/releases)\n2. وصّل مزود LLM المفضل لديك - OpenAI وOllama وGemini معدة مسبقًا، ويمكنك أيضًا إضافة مزودين مثل LM Studio باستخدام http://localhost:1234/v1 كعنوان URL\n3. افتح علامة التبويب MCP في Tome وركّب أول [خادم MCP](https://github.com/modelcontextprotocol/servers) (خادم Fetch سهل للبداية، فقط الصق `uvx mcp-server-fetch` في حقل الخادم).\n4. تحدث مع نموذجك المدعوم بـ MCP! اطلب منه جلب الخبر الأعلى على Hacker News.\n\n# الرؤية\n\nنريد أن نجعل النماذج المحلية (LLMs) وMCP متاحة للجميع. نحن نبني أداة تتيح لك الإبداع مع LLMs، بغض النظر\nعما إذا كنت مهندسًا أو هاويًا أو مبتكرًا أو أي شخص بينهما.\n\n## المبادئ الأساسية\n\n- **Tome محلي أولاً:** أنت تتحكم في مكان بياناتك.\n- **Tome للجميع:** لا ينبغي أن تضطر لإدارة لغات البرمجة أو مديري الحزم أو ملفات إعدادات json.\n\n## ماذا بعد\n\nلقد تلقينا الكثير من التعليقات الرائعة في الأسابيع القليلة الماضية منذ إصدار Tome لكن لدينا خطط كبيرة للمستقبل. نريد أن نحرر LLMs من صندوق الدردشة، وهناك العديد من الميزات قادمة لمساعدتكم على ذلك.\n\n- المهام المجدولة: يجب أن تقوم LLMs بأشياء مفيدة حتى عندما لا تكون أمام الكمبيوتر.\n- التكاملات الأصلية: خوادم MCP وسيلة رائعة للوصول إلى الأدوات والمعلومات، لكننا نرغب في إضافة تكاملات أقوى للتفاعل مع LLMs بطرق فريدة.\n- منشئ التطبيقات: نعتقد على المدى الطويل أن أفضل التجارب لن تكون في واجهة الدردشة. لدينا خطط لإضافة أدوات إضافية تتيح لك إنشاء تطبيقات وسير عمل قوية.\n- ??? أخبرنا بما ترغب أن تراه! انضم إلى مجتمعنا عبر الروابط أدناه، يسعدنا الاستماع إليك.\n\n# المجتمع\n\n[ديسكورد](https://discord.gg/9CH6us29YA) [مدونة](https://blog.runebook.ai) [بلوسكاي](https://bsky.app/profile/gettome.app) [تويتر](https://twitter.com/get_tome)",
  "status": "ok"
}