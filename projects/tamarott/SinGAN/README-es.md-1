{
  "id": 1,
  "origin": "# SinGAN\n\n[Project](https://tamarott.github.io/SinGAN.htm) | [Arxiv](https://arxiv.org/pdf/1905.01164.pdf) | [CVF](http://openaccess.thecvf.com/content_ICCV_2019/papers/Shaham_SinGAN_Learning_a_Generative_Model_From_a_Single_Natural_Image_ICCV_2019_paper.pdf) | [Supplementary materials](https://openaccess.thecvf.com/content_ICCV_2019/supplemental/Shaham_SinGAN_Learning_a_ICCV_2019_supplemental.pdf) | [Talk (ICCV`19)](https://youtu.be/mdAcPe74tZI?t=3191) \n### Official pytorch implementation of the paper: \"SinGAN: Learning a Generative Model from a Single Natural Image\"\n#### ICCV 2019 Best paper award (Marr prize)\n\n\n## Random samples from a *single* image\nWith SinGAN, you can train a generative model from a single natural image, and then generate random samples from the given image, for example:\n\n![](https://raw.githubusercontent.com/tamarott/SinGAN/master/imgs/teaser.PNG)\n\n\n## SinGAN's applications\nSinGAN can be also used for a line of image manipulation tasks, for example:\n ![](https://raw.githubusercontent.com/tamarott/SinGAN/master/imgs/manipulation.PNG)\nThis is done by injecting an image to the already trained model. See section 4 in our [paper](https://arxiv.org/pdf/1905.01164.pdf) for more details.\n\n\n### Citation\nIf you use this code for your research, please cite our paper:\n\n```\n@inproceedings{rottshaham2019singan,\n  title={SinGAN: Learning a Generative Model from a Single Natural Image},\n  author={Rott Shaham, Tamar and Dekel, Tali and Michaeli, Tomer},\n  booktitle={Computer Vision (ICCV), IEEE International Conference on},\n  year={2019}\n}\n```\n\n## Code\n\n### Install dependencies\n\n```\npython -m pip install -r requirements.txt\n```\n\nThis code was tested with python 3.6, torch 1.4\n\nPlease note: the code currently only supports torch 1.4 or earlier because of the optimization scheme.\n\nFor later torch versions, you may try this repository: https://github.com/kligvasser/SinGAN (results won't necessarily be identical to the official implementation).\n\n\n###  Train\nTo train SinGAN model on your own image, put the desired training image under Input/Images, and run\n\n```\npython main_train.py --input_name <input_file_name>\n```\n\nThis will also use the resulting trained model to generate random samples starting from the coarsest scale (n=0).\n\nTo run this code on a cpu machine, specify `--not_cuda` when calling `main_train.py`\n\n###  Random samples\nTo generate random samples from any starting generation scale, please first train SinGAN model on the desired image (as described above), then run \n\n```\npython random_samples.py --input_name <training_image_file_name> --mode random_samples --gen_start_scale <generation start scale number>\n```\n\npay attention: for using the full model, specify the generation start scale to be 0, to start the generation from the second scale, specify it to be 1, and so on. \n\n###  Random samples of arbitrary sizes\nTo generate random samples of arbitrary sizes, please first train SinGAN model on the desired image (as described above), then run \n\n```\npython random_samples.py --input_name <training_image_file_name> --mode random_samples_arbitrary_sizes --scale_h <horizontal scaling factor> --scale_v <vertical scaling factor>\n```\n\n###  Animation from a single image\n\nTo generate short animation from a single image, run\n\n```\npython animation.py --input_name <input_file_name> \n```\n\nThis will automatically start a new training phase with noise padding mode.\n\n###  Harmonization\n\nTo harmonize a pasted object into an image (See example in Fig. 13 in [our paper](https://arxiv.org/pdf/1905.01164.pdf)), please first train SinGAN model on the desired background image (as described above), then save the naively pasted reference image and it's binary mask under \"Input/Harmonization\" (see saved images for an example). Run the command\n\n```\npython harmonization.py --input_name <training_image_file_name> --ref_name <naively_pasted_reference_image_file_name> --harmonization_start_scale <scale to inject>\n\n```\n\nPlease note that different injection scale will produce different harmonization effects. The coarsest injection scale equals 1. \n\n###  Editing\n\nTo edit an image, (See example in Fig. 12 in [our paper](https://arxiv.org/pdf/1905.01164.pdf)), please first train SinGAN model on the desired non-edited image (as described above), then save the naive edit as a reference image under \"Input/Editing\" with a corresponding binary map (see saved images for an example). Run the command\n\n```\npython editing.py --input_name <training_image_file_name> --ref_name <edited_image_file_name> --editing_start_scale <scale to inject>",
  "origin_sha": "WBYBAX/rAWvXc6ASkSueAHHWs8nzqFsy7WJit8teMbQ=",
  "translate": "# SinGAN\n\n[Proyecto](https://tamarott.github.io/SinGAN.htm) | [Arxiv](https://arxiv.org/pdf/1905.01164.pdf) | [CVF](http://openaccess.thecvf.com/content_ICCV_2019/papers/Shaham_SinGAN_Learning_a_Generative_Model_From_a_Single_Natural_Image_ICCV_2019_paper.pdf) | [Materiales suplementarios](https://openaccess.thecvf.com/content_ICCV_2019/supplemental/Shaham_SinGAN_Learning_a_ICCV_2019_supplemental.pdf) | [Charla (ICCV`19)](https://youtu.be/mdAcPe74tZI?t=3191) \n### Implementación oficial en pytorch del artículo: \"SinGAN: Learning a Generative Model from a Single Natural Image\"\n#### Premio al mejor artículo ICCV 2019 (Premio Marr)\n\n\n## Muestras aleatorias de una *sola* imagen\nCon SinGAN, puedes entrenar un modelo generativo a partir de una sola imagen natural, y luego generar muestras aleatorias a partir de la imagen dada, por ejemplo:\n\n![](https://raw.githubusercontent.com/tamarott/SinGAN/master/imgs/teaser.PNG)\n\n\n## Aplicaciones de SinGAN\nSinGAN también puede usarse para una variedad de tareas de manipulación de imágenes, por ejemplo:\n ![](https://raw.githubusercontent.com/tamarott/SinGAN/master/imgs/manipulation.PNG)\nEsto se logra inyectando una imagen en el modelo ya entrenado. Consulta la sección 4 de nuestro [artículo](https://arxiv.org/pdf/1905.01164.pdf) para más detalles.\n\n\n### Citación\nSi usas este código para tu investigación, por favor cita nuestro artículo:\n\n```\n@inproceedings{rottshaham2019singan,\n  title={SinGAN: Learning a Generative Model from a Single Natural Image},\n  author={Rott Shaham, Tamar and Dekel, Tali and Michaeli, Tomer},\n  booktitle={Computer Vision (ICCV), IEEE International Conference on},\n  year={2019}\n}\n```\n\n## Código\n\n### Instalar dependencias\n\n```\npython -m pip install -r requirements.txt\n```\n\nEste código fue probado con python 3.6, torch 1.4\n\nNota: actualmente el código solo soporta torch 1.4 o versiones anteriores debido al esquema de optimización.\n\nPara versiones más recientes de torch, puedes intentar este repositorio: https://github.com/kligvasser/SinGAN (los resultados no necesariamente serán idénticos a la implementación oficial).\n\n\n###  Entrenar\nPara entrenar el modelo SinGAN con tu propia imagen, coloca la imagen deseada para entrenamiento en Input/Images y ejecuta\n\n```\npython main_train.py --input_name <input_file_name>\n```\n\nEsto también usará el modelo entrenado resultante para generar muestras aleatorias comenzando desde la escala más gruesa (n=0).\n\nPara ejecutar este código en una máquina cpu, especifica `--not_cuda` al llamar a `main_train.py`\n\n###  Muestras aleatorias\nPara generar muestras aleatorias desde cualquier escala de generación inicial, primero entrena el modelo SinGAN en la imagen deseada (como se describe arriba), luego ejecuta \n\n```\npython random_samples.py --input_name <training_image_file_name> --mode random_samples --gen_start_scale <generation start scale number>\n```\n\natención: para usar el modelo completo, especifica la escala de inicio de generación como 0, para comenzar la generación desde la segunda escala, especifícalo como 1, y así sucesivamente.\n\n###  Muestras aleatorias de tamaños arbitrarios\nPara generar muestras aleatorias de tamaños arbitrarios, primero entrena el modelo SinGAN en la imagen deseada (como se describe arriba), luego ejecuta \n\n```\npython random_samples.py --input_name <training_image_file_name> --mode random_samples_arbitrary_sizes --scale_h <horizontal scaling factor> --scale_v <vertical scaling factor>\n```\n\n###  Animación a partir de una sola imagen\n\nPara generar una animación corta a partir de una sola imagen, ejecuta\n\n```\npython animation.py --input_name <input_file_name> \n```\n\nEsto comenzará automáticamente una nueva fase de entrenamiento con modo de padding de ruido.\n\n###  Armonización\n\nPara armonizar un objeto pegado en una imagen (ver ejemplo en la Fig. 13 en [nuestro artículo](https://arxiv.org/pdf/1905.01164.pdf)), primero entrena el modelo SinGAN en la imagen de fondo deseada (como se describe arriba), luego guarda la imagen de referencia pegada de forma ingenua y su máscara binaria en \"Input/Harmonization\" (ver imágenes guardadas para un ejemplo). Ejecuta el comando\n\n```\npython harmonization.py --input_name <training_image_file_name> --ref_name <naively_pasted_reference_image_file_name> --harmonization_start_scale <scale to inject>\n\n```\n\nTen en cuenta que diferentes escalas de inyección producirán diferentes efectos de armonización. La escala de inyección más gruesa es igual a 1.\n\n###  Edición\n\nPara editar una imagen, (ver ejemplo en la Fig. 12 en [nuestro artículo](https://arxiv.org/pdf/1905.01164.pdf)), primero entrena el modelo SinGAN en la imagen sin editar (como se describe arriba), luego guarda la edición ingenua como imagen de referencia en \"Input/Editing\" con un mapa binario correspondiente (ver imágenes guardadas para un ejemplo). Ejecuta el comando\n\n```\npython editing.py --input_name <training_image_file_name> --ref_name <edited_image_file_name> --editing_start_scale <scale to inject>",
  "status": "ok"
}