{
  "id": 1,
  "origin": "<p align=\"center\">\n<h1 align=\"center\"><strong>TrackVLA: Embodied Visual Tracking in the Wild</strong></h1>\n  <p align=\"center\">\n    <!--   \t<strong>CoRL 2025</strong><br> -->\n    <a href='https://wsakobe.github.io/' target='_blank'>Shaoan Wang</a>&emsp;\n\t<a href='https://jzhzhang.github.io/' target='_blank'>Jiazhao Zhang</a>&emsp;\n    Minghan Li&emsp;\n    Jiahang Liu&emsp;\n    Anqi Li&emsp; <br>\n    Kui Wu&emsp;\n    <a href='https://fangweizhong.xyz/' target='_blank'>Fangwei Zhong</a>&emsp;\n    <a href='https://www.coe.pku.edu.cn/teaching/manufacturing/9993.html' target='_blank'>Junzhi Yu</a>&emsp;\n\t<a href='https://scholar.google.com/citations?user=X7M0I8kAAAAJ&hl=zh-CN' target='_blank'>Zhizheng Zhang</a>&emsp;\n  <a href='https://hughw19.github.io/' target='_blank'>He Wang</a>&emsp;\n    <br>\n    Peking University&emsp; \n    Galbot&emsp; <br>\n    Beihang University&emsp;\n    Beijing Normal University&emsp;\n    Beijing Academy of Artificial Intelligence&emsp;\n    <br>\n  </p>\n</p>\n\n<div id=\"top\" align=\"center\">\n\n[![Project](https://img.shields.io/badge/Project-%239c403d?style=flat&logoColor=%23FA7F6F\n)](https://pku-epic.github.io/TrackVLA-web/)\n[![arXiv](https://img.shields.io/badge/Arxiv-%233b6291?style=flat&logoColor=%23FA7F6F\n)](http://arxiv.org/abs/2505.23189)\n[![Video](https://img.shields.io/badge/Video-%23c97937?style=flat&logoColor=%23FA7F6F\n)](https://youtu.be/v51U3Nk-SK4?si=foz3zbYD8hLHSybC)\n\n</div>\n\n## 🏡 About\n<strong><em>TrackVLA</em></strong> is a vision-language-action model capable of simultaneous object recognition and visual tracking, trained on a dataset of 1.7 million samples. It demonstrates robust tracking, long-horizon tracking, and cross-domain generalization across diverse challenging environments.\n<div style=\"text-align: center;\">\n    <img src=\"https://raw.githubusercontent.com/wsakobe/TrackVLA/main/assets/teaser.png\" alt=\"Dialogue_Teaser\" width=100% >\n</div>\n\n## 📢 News\n\n* [25/07/02]: The EVT-Bench is now available.\n\n## 💡 Installation\n1. **Preparing conda env**\n\n   First, you need to install [conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/). Once conda installed, create a new env:\n   ```bash\n   conda create -n evt_bench python=3.9 cmake=3.14.0\n   conda activate evt_bench\n   ```\n\n2. **Conda install habitat-sim**\n   \n   You need to install habitat-sim v0.3.1\n      ```\n      conda install habitat-sim==0.3.1 withbullet -c conda-forge -c aihabitat\n      ```\n\n3. **Install habitat-lab from source**\n      ```\n      cd habitat-lab\n      pip install -e habitat-lab\n      ```\n\n4. **Prepare datasets**\n\n    Download Habitat Matterport 3D (HM3D) dataset from [here](https://github.com/facebookresearch/habitat-sim/blob/main/DATASETS.md#habitat-matterport-3d-research-dataset-hm3d) and Matterport3D (MP3D) from [here](https://github.com/facebookresearch/habitat-sim/blob/main/DATASETS.md#matterport3d-mp3d-dataset).\n\n    Then move the dataset to `data/scene_datasets`. The structure of the dataset is outlined as follows:\n    ```\n    data/\n     └── scene_datasets/\n        ├── hm3d/\n        │ ├── train/\n        │ │   └── ...\n        │ ├── val/\n        │ │   └── ...\n        │ └── minival\n        │     └── ...\n        └── mp3d/\n          ├── 1LXtFkjw3qL\n          │   └── ...\n          └── ...\n    ```\n\n    Next, run the following code to obtain data for the humanoid avatars:\n      ```\n      python download_humanoid_data.py\n      ```\n\n\n## 🧪 Evaluation\n  Run the script with:\n\n    bash eval.sh\n\n  Results will be saved in the specified SAVE_PATH, which will include a log directory and a video directory. To monitor the results during the evaluation process, run:",
  "origin_sha": "chO/Ywe8xISNVTZxxh2xcDW/cZFvAWrpfNAJZKmMzTw=",
  "translate": "<p align=\"center\">\n<h1 align=\"center\"><strong>TrackVLA: 자연 환경에서의 실체화된 시각 추적</strong></h1>\n  <p align=\"center\">\n    <!--   \t<strong>CoRL 2025</strong><br> -->\n    <a href='https://wsakobe.github.io/' target='_blank'>Shaoan Wang</a>&emsp;\n\t<a href='https://jzhzhang.github.io/' target='_blank'>Jiazhao Zhang</a>&emsp;\n    Minghan Li&emsp;\n    Jiahang Liu&emsp;\n    Anqi Li&emsp; <br>\n    Kui Wu&emsp;\n    <a href='https://fangweizhong.xyz/' target='_blank'>Fangwei Zhong</a>&emsp;\n    <a href='https://www.coe.pku.edu.cn/teaching/manufacturing/9993.html' target='_blank'>Junzhi Yu</a>&emsp;\n\t<a href='https://scholar.google.com/citations?user=X7M0I8kAAAAJ&hl=zh-CN' target='_blank'>Zhizheng Zhang</a>&emsp;\n  <a href='https://hughw19.github.io/' target='_blank'>He Wang</a>&emsp;\n    <br>\n    베이징대학교&emsp; \n    Galbot&emsp; <br>\n    북경항공항천대학교&emsp;\n    베이징사범대학교&emsp;\n    베이징인공지능아카데미&emsp;\n    <br>\n  </p>\n</p>\n\n<div id=\"top\" align=\"center\">\n\n[![Project](https://img.shields.io/badge/Project-%239c403d?style=flat&logoColor=%23FA7F6F\n)](https://pku-epic.github.io/TrackVLA-web/)\n[![arXiv](https://img.shields.io/badge/Arxiv-%233b6291?style=flat&logoColor=%23FA7F6F\n)](http://arxiv.org/abs/2505.23189)\n[![Video](https://img.shields.io/badge/Video-%23c97937?style=flat&logoColor=%23FA7F6F\n)](https://youtu.be/v51U3Nk-SK4?si=foz3zbYD8hLHSybC)\n\n</div>\n\n## 🏡 소개\n<strong><em>TrackVLA</em></strong>는 170만 개 샘플로 학습된, 객체 인식과 시각 추적을 동시에 수행할 수 있는 비전-언어-행동 모델입니다. 이 모델은 다양한 도전적인 환경에서 강인한 추적, 장기 추적, 그리고 도메인 간 일반화 성능을 보여줍니다.\n<div style=\"text-align: center;\">\n    <img src=\"https://raw.githubusercontent.com/wsakobe/TrackVLA/main/assets/teaser.png\" alt=\"Dialogue_Teaser\" width=100% >\n</div>\n\n## 📢 소식\n\n* [25/07/02]: EVT-Bench가 이제 공개되었습니다.\n\n## 💡 설치\n1. **conda 환경 준비**\n\n   먼저 [conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/)를 설치해야 합니다. conda 설치 후, 새로운 환경을 생성하세요:\n   ```bash\n   conda create -n evt_bench python=3.9 cmake=3.14.0\n   conda activate evt_bench\n   ```\n\n2. **Conda를 이용한 habitat-sim 설치**\n   \n   habitat-sim v0.3.1을 설치해야 합니다.\n      ```\n      conda install habitat-sim==0.3.1 withbullet -c conda-forge -c aihabitat\n      ```\n\n3. **habitat-lab 소스에서 설치**\n      ```\n      cd habitat-lab\n      pip install -e habitat-lab\n      ```\n\n4. **데이터셋 준비**\n\n    Habitat Matterport 3D (HM3D) 데이터셋을 [여기](https://github.com/facebookresearch/habitat-sim/blob/main/DATASETS.md#habitat-matterport-3d-research-dataset-hm3d)에서, Matterport3D (MP3D)는 [여기](https://github.com/facebookresearch/habitat-sim/blob/main/DATASETS.md#matterport3d-mp3d-dataset)에서 다운로드하세요.\n\n    다운로드한 데이터셋을 `data/scene_datasets`로 이동합니다. 데이터셋의 구조는 다음과 같습니다:\n    ```\n    data/\n     └── scene_datasets/\n        ├── hm3d/\n        │ ├── train/\n        │ │   └── ...\n        │ ├── val/\n        │ │   └── ...\n        │ └── minival\n        │     └── ...\n        └── mp3d/\n          ├── 1LXtFkjw3qL\n          │   └── ...\n          └── ...\n    ```\n\n    다음으로, 휴머노이드 아바타 데이터를 얻기 위해 아래 코드를 실행하세요:\n      ```\n      python download_humanoid_data.py\n      ```\n\n\n## 🧪 평가\n  다음 스크립트를 실행하세요:\n\n    bash eval.sh\n\n  결과는 지정한 SAVE_PATH에 저장되며, 여기에는 로그 디렉토리와 비디오 디렉토리가 포함됩니다. 평가 진행 중 결과를 모니터링하려면, 다음을 실행하세요:",
  "status": "ok"
}