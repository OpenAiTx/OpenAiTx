{
  "id": 1,
  "origin": "<p align=\"center\">\n<h1 align=\"center\"><strong>TrackVLA: Embodied Visual Tracking in the Wild</strong></h1>\n  <p align=\"center\">\n    <!--   \t<strong>CoRL 2025</strong><br> -->\n    <a href='https://wsakobe.github.io/' target='_blank'>Shaoan Wang</a>&emsp;\n\t<a href='https://jzhzhang.github.io/' target='_blank'>Jiazhao Zhang</a>&emsp;\n    Minghan Li&emsp;\n    Jiahang Liu&emsp;\n    Anqi Li&emsp; <br>\n    Kui Wu&emsp;\n    <a href='https://fangweizhong.xyz/' target='_blank'>Fangwei Zhong</a>&emsp;\n    <a href='https://www.coe.pku.edu.cn/teaching/manufacturing/9993.html' target='_blank'>Junzhi Yu</a>&emsp;\n\t<a href='https://scholar.google.com/citations?user=X7M0I8kAAAAJ&hl=zh-CN' target='_blank'>Zhizheng Zhang</a>&emsp;\n  <a href='https://hughw19.github.io/' target='_blank'>He Wang</a>&emsp;\n    <br>\n    Peking University&emsp; \n    Galbot&emsp; <br>\n    Beihang University&emsp;\n    Beijing Normal University&emsp;\n    Beijing Academy of Artificial Intelligence&emsp;\n    <br>\n  </p>\n</p>\n\n<div id=\"top\" align=\"center\">\n\n[![Project](https://img.shields.io/badge/Project-%239c403d?style=flat&logoColor=%23FA7F6F\n)](https://pku-epic.github.io/TrackVLA-web/)\n[![arXiv](https://img.shields.io/badge/Arxiv-%233b6291?style=flat&logoColor=%23FA7F6F\n)](http://arxiv.org/abs/2505.23189)\n[![Video](https://img.shields.io/badge/Video-%23c97937?style=flat&logoColor=%23FA7F6F\n)](https://youtu.be/v51U3Nk-SK4?si=foz3zbYD8hLHSybC)\n\n</div>\n\n## ğŸ¡ About\n<strong><em>TrackVLA</em></strong> is a vision-language-action model capable of simultaneous object recognition and visual tracking, trained on a dataset of 1.7 million samples. It demonstrates robust tracking, long-horizon tracking, and cross-domain generalization across diverse challenging environments.\n<div style=\"text-align: center;\">\n    <img src=\"https://raw.githubusercontent.com/wsakobe/TrackVLA/main/assets/teaser.png\" alt=\"Dialogue_Teaser\" width=100% >\n</div>\n\n## ğŸ“¢ News\n\n* [25/07/02]: The EVT-Bench is now available.\n\n## ğŸ’¡ Installation\n1. **Preparing conda env**\n\n   First, you need to install [conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/). Once conda installed, create a new env:\n   ```bash\n   conda create -n evt_bench python=3.9 cmake=3.14.0\n   conda activate evt_bench\n   ```\n\n2. **Conda install habitat-sim**\n   \n   You need to install habitat-sim v0.3.1\n      ```\n      conda install habitat-sim==0.3.1 withbullet -c conda-forge -c aihabitat\n      ```\n\n3. **Install habitat-lab from source**\n      ```\n      cd habitat-lab\n      pip install -e habitat-lab\n      ```\n\n4. **Prepare datasets**\n\n    Download Habitat Matterport 3D (HM3D) dataset from [here](https://github.com/facebookresearch/habitat-sim/blob/main/DATASETS.md#habitat-matterport-3d-research-dataset-hm3d) and Matterport3D (MP3D) from [here](https://github.com/facebookresearch/habitat-sim/blob/main/DATASETS.md#matterport3d-mp3d-dataset).\n\n    Then move the dataset to `data/scene_datasets`. The structure of the dataset is outlined as follows:\n    ```\n    data/\n     â””â”€â”€ scene_datasets/\n        â”œâ”€â”€ hm3d/\n        â”‚ â”œâ”€â”€ train/\n        â”‚ â”‚   â””â”€â”€ ...\n        â”‚ â”œâ”€â”€ val/\n        â”‚ â”‚   â””â”€â”€ ...\n        â”‚ â””â”€â”€ minival\n        â”‚     â””â”€â”€ ...\n        â””â”€â”€ mp3d/\n          â”œâ”€â”€ 1LXtFkjw3qL\n          â”‚   â””â”€â”€ ...\n          â””â”€â”€ ...\n    ```\n\n    Next, run the following code to obtain data for the humanoid avatars:\n      ```\n      python download_humanoid_data.py\n      ```\n\n\n## ğŸ§ª Evaluation\n  Run the script with:\n\n    bash eval.sh\n\n  Results will be saved in the specified SAVE_PATH, which will include a log directory and a video directory. To monitor the results during the evaluation process, run:",
  "origin_sha": "chO/Ywe8xISNVTZxxh2xcDW/cZFvAWrpfNAJZKmMzTw=",
  "translate": "<p align=\"center\">\n<h1 align=\"center\"><strong>TrackVLA: é‡å¤–ã«ãŠã‘ã‚‹ã‚¨ãƒ³ãƒœãƒ‡ã‚£ãƒƒãƒ‰ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°</strong></h1>\n  <p align=\"center\">\n    <!--   \t<strong>CoRL 2025</strong><br> -->\n    <a href='https://wsakobe.github.io/' target='_blank'>Shaoan Wang</a>&emsp;\n\t<a href='https://jzhzhang.github.io/' target='_blank'>Jiazhao Zhang</a>&emsp;\n    Minghan Li&emsp;\n    Jiahang Liu&emsp;\n    Anqi Li&emsp; <br>\n    Kui Wu&emsp;\n    <a href='https://fangweizhong.xyz/' target='_blank'>Fangwei Zhong</a>&emsp;\n    <a href='https://www.coe.pku.edu.cn/teaching/manufacturing/9993.html' target='_blank'>Junzhi Yu</a>&emsp;\n\t<a href='https://scholar.google.com/citations?user=X7M0I8kAAAAJ&hl=zh-CN' target='_blank'>Zhizheng Zhang</a>&emsp;\n  <a href='https://hughw19.github.io/' target='_blank'>He Wang</a>&emsp;\n    <br>\n    åŒ—äº¬å¤§å­¦&emsp; \n    Galbot&emsp; <br>\n    åŒ—äº¬èˆªç©ºèˆªå¤©å¤§å­¦&emsp;\n    åŒ—äº¬å¸«ç¯„å¤§å­¦&emsp;\n    åŒ—äº¬äººå·¥çŸ¥èƒ½ã‚¢ã‚«ãƒ‡ãƒŸãƒ¼&emsp;\n    <br>\n  </p>\n</p>\n\n<div id=\"top\" align=\"center\">\n\n[![Project](https://img.shields.io/badge/Project-%239c403d?style=flat&logoColor=%23FA7F6F\n)](https://pku-epic.github.io/TrackVLA-web/)\n[![arXiv](https://img.shields.io/badge/Arxiv-%233b6291?style=flat&logoColor=%23FA7F6F\n)](http://arxiv.org/abs/2505.23189)\n[![Video](https://img.shields.io/badge/Video-%23c97937?style=flat&logoColor=%23FA7F6F\n)](https://youtu.be/v51U3Nk-SK4?si=foz3zbYD8hLHSybC)\n\n</div>\n\n## ğŸ¡ æ¦‚è¦\n<strong><em>TrackVLA</em></strong>ã¯ã€170ä¸‡ã‚µãƒ³ãƒ—ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å­¦ç¿’ã•ã‚ŒãŸã€åŒæ™‚ã«ç‰©ä½“èªè­˜ã¨ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãŒå¯èƒ½ãªãƒ“ã‚¸ãƒ§ãƒ³ãƒ»ãƒ©ãƒ³ã‚²ãƒ¼ã‚¸ãƒ»ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚å¤šæ§˜ã§å›°é›£ãªç’°å¢ƒã«ãŠã„ã¦ã€å …ç‰¢ãªè¿½è·¡ã€é•·æœŸçš„ãªè¿½è·¡ã€ãŠã‚ˆã³ã‚¯ãƒ­ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³ãªæ±åŒ–æ€§èƒ½ã‚’ç¤ºã—ã¾ã™ã€‚\n<div style=\"text-align: center;\">\n    <img src=\"https://raw.githubusercontent.com/wsakobe/TrackVLA/main/assets/teaser.png\" alt=\"Dialogue_Teaser\" width=100% >\n</div>\n\n## ğŸ“¢ ãƒ‹ãƒ¥ãƒ¼ã‚¹\n\n* [25/07/02]: EVT-BenchãŒå…¬é–‹ã•ã‚Œã¾ã—ãŸã€‚\n\n## ğŸ’¡ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n1. **condaç’°å¢ƒã®æº–å‚™**\n\n   ã¾ãšã€[conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/)ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚condaã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¾Œã€æ–°ã—ã„ç’°å¢ƒã‚’ä½œæˆã—ã¾ã™:\n   ```bash\n   conda create -n evt_bench python=3.9 cmake=3.14.0\n   conda activate evt_bench\n   ```\n\n2. **Condaã§habitat-simã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«**\n   \n   habitat-sim v0.3.1ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n      ```\n      conda install habitat-sim==0.3.1 withbullet -c conda-forge -c aihabitat\n      ```\n\n3. **habitat-labã‚’ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«**\n      ```\n      cd habitat-lab\n      pip install -e habitat-lab\n      ```\n\n4. **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™**\n\n    Habitat Matterport 3Dï¼ˆHM3Dï¼‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’[ã“ã¡ã‚‰](https://github.com/facebookresearch/habitat-sim/blob/main/DATASETS.md#habitat-matterport-3d-research-dataset-hm3d)ã‹ã‚‰ã€Matterport3Dï¼ˆMP3Dï¼‰ã‚’[ã“ã¡ã‚‰](https://github.com/facebookresearch/habitat-sim/blob/main/DATASETS.md#matterport3d-mp3d-dataset)ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚\n\n    ãã®å¾Œã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’`data/scene_datasets`ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•ã—ã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ§‹é€ ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™:\n    ```\n    data/\n     â””â”€â”€ scene_datasets/\n        â”œâ”€â”€ hm3d/\n        â”‚ â”œâ”€â”€ train/\n        â”‚ â”‚   â””â”€â”€ ...\n        â”‚ â”œâ”€â”€ val/\n        â”‚ â”‚   â””â”€â”€ ...\n        â”‚ â””â”€â”€ minival\n        â”‚     â””â”€â”€ ...\n        â””â”€â”€ mp3d/\n          â”œâ”€â”€ 1LXtFkjw3qL\n          â”‚   â””â”€â”€ ...\n          â””â”€â”€ ...\n    ```\n\n    æ¬¡ã«ã€ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ãƒ’ãƒ¥ãƒ¼ãƒãƒã‚¤ãƒ‰ã‚¢ãƒã‚¿ãƒ¼ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã™:\n      ```\n      python download_humanoid_data.py\n      ```\n\n\n## ğŸ§ª è©•ä¾¡\n  ä»¥ä¸‹ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œã—ã¾ã™:\n\n    bash eval.sh\n\n  çµæœã¯æŒ‡å®šã—ãŸSAVE_PATHã«ä¿å­˜ã•ã‚Œã€ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨ãƒ“ãƒ‡ã‚ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå«ã¾ã‚Œã¾ã™ã€‚è©•ä¾¡ãƒ—ãƒ­ã‚»ã‚¹ä¸­ã«çµæœã‚’ç›£è¦–ã™ã‚‹ã«ã¯ã€æ¬¡ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„:",
  "status": "ok"
}