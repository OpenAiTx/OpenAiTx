{
  "id": 1,
  "origin": "<p align=\"center\">\n<h1 align=\"center\"><strong>TrackVLA: Embodied Visual Tracking in the Wild</strong></h1>\n  <p align=\"center\">\n    <!--   \t<strong>CoRL 2025</strong><br> -->\n    <a href='https://wsakobe.github.io/' target='_blank'>Shaoan Wang</a>&emsp;\n\t<a href='https://jzhzhang.github.io/' target='_blank'>Jiazhao Zhang</a>&emsp;\n    Minghan Li&emsp;\n    Jiahang Liu&emsp;\n    Anqi Li&emsp; <br>\n    Kui Wu&emsp;\n    <a href='https://fangweizhong.xyz/' target='_blank'>Fangwei Zhong</a>&emsp;\n    <a href='https://www.coe.pku.edu.cn/teaching/manufacturing/9993.html' target='_blank'>Junzhi Yu</a>&emsp;\n\t<a href='https://scholar.google.com/citations?user=X7M0I8kAAAAJ&hl=zh-CN' target='_blank'>Zhizheng Zhang</a>&emsp;\n  <a href='https://hughw19.github.io/' target='_blank'>He Wang</a>&emsp;\n    <br>\n    Peking University&emsp; \n    Galbot&emsp; <br>\n    Beihang University&emsp;\n    Beijing Normal University&emsp;\n    Beijing Academy of Artificial Intelligence&emsp;\n    <br>\n  </p>\n</p>\n\n<div id=\"top\" align=\"center\">\n\n[![Project](https://img.shields.io/badge/Project-%239c403d?style=flat&logoColor=%23FA7F6F\n)](https://pku-epic.github.io/TrackVLA-web/)\n[![arXiv](https://img.shields.io/badge/Arxiv-%233b6291?style=flat&logoColor=%23FA7F6F\n)](http://arxiv.org/abs/2505.23189)\n[![Video](https://img.shields.io/badge/Video-%23c97937?style=flat&logoColor=%23FA7F6F\n)](https://youtu.be/v51U3Nk-SK4?si=foz3zbYD8hLHSybC)\n\n</div>\n\n## ğŸ¡ About\n<strong><em>TrackVLA</em></strong> is a vision-language-action model capable of simultaneous object recognition and visual tracking, trained on a dataset of 1.7 million samples. It demonstrates robust tracking, long-horizon tracking, and cross-domain generalization across diverse challenging environments.\n<div style=\"text-align: center;\">\n    <img src=\"https://raw.githubusercontent.com/wsakobe/TrackVLA/main/assets/teaser.png\" alt=\"Dialogue_Teaser\" width=100% >\n</div>\n\n## ğŸ“¢ News\n\n* [25/07/02]: The EVT-Bench is now available.\n\n## ğŸ’¡ Installation\n1. **Preparing conda env**\n\n   First, you need to install [conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/). Once conda installed, create a new env:\n   ```bash\n   conda create -n evt_bench python=3.9 cmake=3.14.0\n   conda activate evt_bench\n   ```\n\n2. **Conda install habitat-sim**\n   \n   You need to install habitat-sim v0.3.1\n      ```\n      conda install habitat-sim==0.3.1 withbullet -c conda-forge -c aihabitat\n      ```\n\n3. **Install habitat-lab from source**\n      ```\n      cd habitat-lab\n      pip install -e habitat-lab\n      ```\n\n4. **Prepare datasets**\n\n    Download Habitat Matterport 3D (HM3D) dataset from [here](https://github.com/facebookresearch/habitat-sim/blob/main/DATASETS.md#habitat-matterport-3d-research-dataset-hm3d) and Matterport3D (MP3D) from [here](https://github.com/facebookresearch/habitat-sim/blob/main/DATASETS.md#matterport3d-mp3d-dataset).\n\n    Then move the dataset to `data/scene_datasets`. The structure of the dataset is outlined as follows:\n    ```\n    data/\n     â””â”€â”€ scene_datasets/\n        â”œâ”€â”€ hm3d/\n        â”‚ â”œâ”€â”€ train/\n        â”‚ â”‚   â””â”€â”€ ...\n        â”‚ â”œâ”€â”€ val/\n        â”‚ â”‚   â””â”€â”€ ...\n        â”‚ â””â”€â”€ minival\n        â”‚     â””â”€â”€ ...\n        â””â”€â”€ mp3d/\n          â”œâ”€â”€ 1LXtFkjw3qL\n          â”‚   â””â”€â”€ ...\n          â””â”€â”€ ...\n    ```\n\n    Next, run the following code to obtain data for the humanoid avatars:\n      ```\n      python download_humanoid_data.py\n      ```\n\n\n## ğŸ§ª Evaluation\n  Run the script with:\n\n    bash eval.sh\n\n  Results will be saved in the specified SAVE_PATH, which will include a log directory and a video directory. To monitor the results during the evaluation process, run:",
  "origin_sha": "chO/Ywe8xISNVTZxxh2xcDW/cZFvAWrpfNAJZKmMzTw=",
  "translate": "<p align=\"center\">\n<h1 align=\"center\"><strong>TrackVLA: Pelacakan Visual Berbadan di Alam Liar</strong></h1>\n  <p align=\"center\">\n    <!--   \t<strong>CoRL 2025</strong><br> -->\n    <a href='https://wsakobe.github.io/' target='_blank'>Shaoan Wang</a>&emsp;\n\t<a href='https://jzhzhang.github.io/' target='_blank'>Jiazhao Zhang</a>&emsp;\n    Minghan Li&emsp;\n    Jiahang Liu&emsp;\n    Anqi Li&emsp; <br>\n    Kui Wu&emsp;\n    <a href='https://fangweizhong.xyz/' target='_blank'>Fangwei Zhong</a>&emsp;\n    <a href='https://www.coe.pku.edu.cn/teaching/manufacturing/9993.html' target='_blank'>Junzhi Yu</a>&emsp;\n\t<a href='https://scholar.google.com/citations?user=X7M0I8kAAAAJ&hl=zh-CN' target='_blank'>Zhizheng Zhang</a>&emsp;\n  <a href='https://hughw19.github.io/' target='_blank'>He Wang</a>&emsp;\n    <br>\n    Universitas Peking&emsp; \n    Galbot&emsp; <br>\n    Universitas Beihang&emsp;\n    Universitas Normal Beijing&emsp;\n    Akademi Kecerdasan Buatan Beijing&emsp;\n    <br>\n  </p>\n</p>\n\n<div id=\"top\" align=\"center\">\n\n[![Project](https://img.shields.io/badge/Project-%239c403d?style=flat&logoColor=%23FA7F6F\n)](https://pku-epic.github.io/TrackVLA-web/)\n[![arXiv](https://img.shields.io/badge/Arxiv-%233b6291?style=flat&logoColor=%23FA7F6F\n)](http://arxiv.org/abs/2505.23189)\n[![Video](https://img.shields.io/badge/Video-%23c97937?style=flat&logoColor=%23FA7F6F\n)](https://youtu.be/v51U3Nk-SK4?si=foz3zbYD8hLHSybC)\n\n</div>\n\n## ğŸ¡ Tentang\n<strong><em>TrackVLA</em></strong> adalah model visi-bahasa-aksi yang mampu melakukan pengenalan objek dan pelacakan visual secara simultan, dilatih pada dataset sebanyak 1,7 juta sampel. Model ini menunjukkan pelacakan yang kuat, pelacakan jarak jauh, serta generalisasi lintas domain di berbagai lingkungan yang menantang.\n<div style=\"text-align: center;\">\n    <img src=\"https://raw.githubusercontent.com/wsakobe/TrackVLA/main/assets/teaser.png\" alt=\"Dialogue_Teaser\" width=100% >\n</div>\n\n## ğŸ“¢ Berita\n\n* [25/07/02]: EVT-Bench sekarang sudah tersedia.\n\n## ğŸ’¡ Instalasi\n1. **Menyiapkan lingkungan conda**\n\n   Pertama, Anda perlu menginstal [conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/). Setelah conda terpasang, buat lingkungan baru:\n   ```bash\n   conda create -n evt_bench python=3.9 cmake=3.14.0\n   conda activate evt_bench\n   ```\n\n2. **Instalasi habitat-sim melalui Conda**\n   \n   Anda perlu menginstal habitat-sim versi 0.3.1\n      ```\n      conda install habitat-sim==0.3.1 withbullet -c conda-forge -c aihabitat\n      ```\n\n3. **Instalasi habitat-lab dari source**\n      ```\n      cd habitat-lab\n      pip install -e habitat-lab\n      ```\n\n4. **Menyiapkan dataset**\n\n    Unduh dataset Habitat Matterport 3D (HM3D) dari [sini](https://github.com/facebookresearch/habitat-sim/blob/main/DATASETS.md#habitat-matterport-3d-research-dataset-hm3d) dan Matterport3D (MP3D) dari [sini](https://github.com/facebookresearch/habitat-sim/blob/main/DATASETS.md#matterport3d-mp3d-dataset).\n\n    Kemudian pindahkan dataset ke `data/scene_datasets`. Struktur dataset dijabarkan sebagai berikut:\n    ```\n    data/\n     â””â”€â”€ scene_datasets/\n        â”œâ”€â”€ hm3d/\n        â”‚ â”œâ”€â”€ train/\n        â”‚ â”‚   â””â”€â”€ ...\n        â”‚ â”œâ”€â”€ val/\n        â”‚ â”‚   â””â”€â”€ ...\n        â”‚ â””â”€â”€ minival\n        â”‚     â””â”€â”€ ...\n        â””â”€â”€ mp3d/\n          â”œâ”€â”€ 1LXtFkjw3qL\n          â”‚   â””â”€â”€ ...\n          â””â”€â”€ ...\n    ```\n\n    Selanjutnya, jalankan kode berikut untuk mendapatkan data avatar humanoid:\n      ```\n      python download_humanoid_data.py\n      ```\n\n\n## ğŸ§ª Evaluasi\n  Jalankan skrip dengan:\n\n    bash eval.sh\n\n  Hasil akan disimpan di SAVE_PATH yang telah ditentukan, yang akan mencakup direktori log dan direktori video. Untuk memantau hasil selama proses evaluasi, jalankan:",
  "status": "ok"
}