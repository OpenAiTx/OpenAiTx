# Les mod√®les de langage sont-ils r√©ellement utiles pour la pr√©vision des s√©ries temporelles ? 

**(NeurIPS 2024 Spotlight)** üåü [Lien vers l‚Äôarticle](https://arxiv.org/pdf/2406.16964) 

Dans ce travail, nous avons montr√© que malgr√© la popularit√© r√©cente des grands mod√®les de langage (LLM) dans la pr√©vision des s√©ries temporelles (TSF), ils ne semblent pas am√©liorer significativement les performances. Une simple m√©thode de base, "PAttn," a √©t√© propos√©e, qui a surpass√© la plupart des mod√®les TSF bas√©s sur les LLM. 

Auteurs : [Mingtian Tan](https://x.com/MTTan1203),[Mike A. Merrill](https://mikemerrill.io/),[Vinayak Gupta](https://gvinayak.github.io/),[Tim Althoff](https://homes.cs.washington.edu/~althoff/),[Thomas Hartvigsen](https://www.tomhartvigsen.com/)

### Les LLM autoregressifs ont un grand potentiel pour exploiter le contexte afin de raisonner (pr√©dire) les s√©ries temporelles futures. Les mod√®les de pr√©vision des s√©ries temporelles mentionn√©s dans cet article ne le font pas.

Pour raisonner (pr√©dire) les s√©ries temporelles via du texte, vous pouvez vous r√©f√©rer √† ce [travail sur le raisonnement (pr√©vision) des s√©ries temporelles](https://github.com/behavioral-data/TSandLanguage/tree/main/text_aid_forecast).

## Aper√ßu üíÅüèº
Les travaux r√©cents en analyse des s√©ries temporelles se sont de plus en plus concentr√©s sur l‚Äôadaptation des grands mod√®les de langage pr√©entra√Æn√©s (LLM) pour la **pr√©vision (TSF)**, la classification, et la d√©tection d‚Äôanomalies. Ces √©tudes sugg√®rent que les mod√®les de langage, con√ßus pour les d√©pendances s√©quentielles dans le texte, pourraient se g√©n√©raliser aux donn√©es de s√©ries temporelles. Bien que cette id√©e soit en accord avec la popularit√© des mod√®les de langage en apprentissage automatique, les liens directs entre mod√©lisation du langage et TSF restent flous. **Quelle est l‚Äôutilit√© des mod√®les de langage pour les t√¢ches traditionnelles de TSF ?**

√Ä travers une s√©rie d‚Äô√©tudes d‚Äôablation sur trois m√©thodes r√©centes **TSF bas√©es sur LLM**, nous avons constat√© que **supprimer la composante LLM ou la remplacer par une simple couche d‚Äôattention ne d√©gradait pas les r√©sultats ‚Äî dans de nombreux cas, cela conduisait m√™me √† des am√©liorations.** De plus, nous avons introduit PAttn, montrant que les structures de patching et d‚Äôattention peuvent √©galer les performances des meilleurs pr√©visionnistes bas√©s sur LLM.

![Ablations/PAttn](https://raw.githubusercontent.com/BennyTMT/LLMsForTimeSeries/main/pic/ablations.png)

## Jeu de donn√©es üìñ
Vous pouvez acc√©der aux jeux de donn√©es bien pr√©-trait√©s depuis [Google Drive](https://drive.google.com/file/d/1NF7VEefXCmXuWNbnNe858WvQAkJ_7wuP/view), puis placer les contenus t√©l√©charg√©s sous ./datasets

## Installation üîß
Trois m√©thodes populaires diff√©rentes bas√©es sur LLM pour la TSF ont √©t√© incluses dans notre approche d‚Äôablation. Vous pouvez suivre les d√©p√¥ts correspondants, [OneFitsAll](https://github.com/DAMO-DI-ML/NeurIPS2023-One-Fits-All), [Time-LLM](https://github.com/KimMeen/Time-LLM), et [CALF](https://github.com/Hank0626/CALF), pour configurer l‚Äôenvironnement respectivement. Pour la m√©thode **''PAttn''**, l‚Äôenvironnement de n‚Äôimporte lequel des d√©p√¥ts ci-dessus est compatible.


## PAttn ü§î
La principale diff√©rence entre **PAttn** et [PatchTST](https://github.com/yuqinie98/PatchTST) est que nous avons progressivement retir√© des parties du module Transformer qui peuvent ne pas √™tre aussi essentielles, ainsi que l‚ÄôEncodage de Position. Pour plus d‚Äôexplications, veuillez consulter [cette r√©ponse](https://github.com/BennyTMT/LLMsForTimeSeries/issues/7).

**Motivation** : Lorsque [DLinear](https://github.com/cure-lab/LTSF-Linear) a √©t√© surpass√© par de nombreuses nouvelles m√©thodes, nous visons √† fournir une m√©thode bas√©e sur le patching qui soit √† la fois simple et efficace, servant de r√©f√©rence basique.

     cd ./PAttn 

     bash ./scripts/ETTh.sh (pour ETTh1 & ETTh2)
     bash ./scripts/ETTm.sh (pour ETTm1 & ETTm2)
     bash ./scripts/weather.sh (pour Weather)
     
#### Pour les autres jeux de donn√©es, veuillez changer le nom du script dans la commande ci-dessus.

## Ablations
     
#### Ex√©cuter les ablations sur CALF (ETT) :
     
    cd ./CALF
    sh scripts/long_term_forecasting/ETTh_GPT2.sh
    sh scripts/long_term_forecasting/ETTm_GPT2.sh
    
    sh scripts/long_term_forecasting/traffic.sh 
    (Pour d‚Äôautres jeux de donn√©es, comme traffic)

#### Ex√©cuter les ablations sur OneFitsAll (ETT) :
     cd ./OFA
     bash ./script/ETTh_GPT2.sh   
     bash ./script/ETTm_GPT2.sh

     bash ./script/illness.sh 
     (Pour d‚Äôautres jeux de donn√©es, comme illness)

#### Ex√©cuter les ablations sur Time-LLM (ETT) 
     cd ./Time-LLM-exp
     bash ./scripts/train_script/TimeLLM_ETTh1.sh
     bash ./scripts/train_script/TimeLLM_ETTm1.sh 

     bash ./scripts/train_script/TimeLLM_Weather.sh
     (Pour d‚Äôautres jeux de donn√©es, comme Weather)

#### (Pour ex√©cuter des ablations sur d‚Äôautres jeux de donn√©es, veuillez changer le nom du jeu de donn√©es comme indiqu√© dans l‚Äôexemple.)

## Remerciements

Cette base de code est construite √† partir de la [Time-Series-Library](https://github.com/thuml/Time-Series-Library). Merci !


## Citation
Si vous trouvez notre travail utile, merci de bien vouloir citer notre travail comme suit :
```bibtex
@inproceedings{tan2024language,
    title={Les mod√®les de langage sont-ils r√©ellement utiles pour la pr√©vision des s√©ries temporelles ?},
    author={Tan, Mingtian et Merrill, Mike A et Gupta, Vinayak et Althoff, Tim et Hartvigsen, Thomas},
    booktitle={Neural Information Processing Systems (NeurIPS)},
    year={2024}
}

```

---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-29

---