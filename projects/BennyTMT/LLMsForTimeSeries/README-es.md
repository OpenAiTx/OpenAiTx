# 驴Son los Modelos de Lenguaje Realmente tiles para la Predicci贸n de Series Temporales?

**(NeurIPS 2024 Spotlight)**  [Enlace al art铆culo](https://arxiv.org/pdf/2406.16964)

En este trabajo mostramos que, a pesar de la reciente popularidad de los LLMs en la predicci贸n de series temporales (TSF), no parecen mejorar significativamente el rendimiento. Se propuso una l铆nea base simple, "PAttn," que super贸 a la mayor铆a de los modelos TSF basados en LLM.

Autores: [Mingtian Tan](https://x.com/MTTan1203),[Mike A. Merrill](https://mikemerrill.io/),[Vinayak Gupta](https://gvinayak.github.io/),[Tim Althoff](https://homes.cs.washington.edu/~althoff/),[Thomas Hartvigsen](https://www.tomhartvigsen.com/)

### Los LLMs autorregresivos tienen gran potencial para aprovechar el contexto y razonar (predecir) series temporales futuras. Los modelos de predicci贸n de series temporales mencionados en este art铆culo no.

Para razonar (predecir) series temporales mediante texto, puede consultar este [trabajo de razonamiento (predicci贸n) de series temporales](https://github.com/behavioral-data/TSandLanguage/tree/main/text_aid_forecast).

## Resumen 
El trabajo reciente en an谩lisis de series temporales se ha centrado cada vez m谩s en adaptar modelos de lenguaje grandes preentrenados (LLMs) para **predicci贸n (TSF)**, clasificaci贸n y detecci贸n de anomal铆as. Estos estudios sugieren que los modelos de lenguaje, dise帽ados para dependencias secuenciales en texto, podr铆an generalizar a datos de series temporales. Aunque esta idea est谩 alineada con la popularidad de los modelos de lenguaje en aprendizaje autom谩tico, las conexiones directas entre modelado de lenguaje y TSF siguen siendo poco claras. **驴Qu茅 tan beneficiosos son los modelos de lenguaje para tareas tradicionales de TSF?**

Mediante una serie de estudios de ablaci贸n en tres m茅todos recientes **TSF basados en LLM**, encontramos que **eliminar el componente LLM o reemplazarlo con una capa simple de atenci贸n no empeor贸 los resultados; en muchos casos, incluso condujo a mejoras.** Adem谩s, introdujimos PAttn, demostrando que las estructuras de parches y atenci贸n pueden rendir igual que los pronosticadores basados en LLM de 煤ltima generaci贸n.

![Ablations/PAttn](https://raw.githubusercontent.com/BennyTMT/LLMsForTimeSeries/main/pic/ablations.png)

## Conjunto de datos 
Puede acceder a los conjuntos de datos bien preprocesados desde [Google Drive](https://drive.google.com/file/d/1NF7VEefXCmXuWNbnNe858WvQAkJ_7wuP/view), luego coloque el contenido descargado bajo ./datasets

## Configuraci贸n 
Se incluyeron tres m茅todos TSF populares basados en LLM en nuestro enfoque de ablaci贸n. Puede seguir los repositorios correspondientes, [OneFitsAll](https://github.com/DAMO-DI-ML/NeurIPS2023-One-Fits-All), [Time-LLM](https://github.com/KimMeen/Time-LLM), y [CALF](https://github.com/Hank0626/CALF), para configurar el entorno respectivamente. Para el m茅todo **''PAttn''**, el entorno de cualquiera de los repositorios anteriores es compatible.

## PAttn 
La principal diferencia entre **PAttn** y [PatchTST](https://github.com/yuqinie98/PatchTST) es que eliminamos gradualmente partes del m贸dulo Transformer que pueden no ser tan esenciales, y el Embedding Posicional. Para m谩s explicaci贸n, consulte [esta respuesta](https://github.com/BennyTMT/LLMsForTimeSeries/issues/7).

**Motivaci贸n**: Cuando [DLinear](https://github.com/cure-lab/LTSF-Linear) fue superado por muchos m茅todos nuevos, nuestro objetivo es proporcionar un m茅todo basado en Patching que sea simple y efectivo, sirviendo como una l铆nea base sencilla.

     cd ./PAttn 

     bash ./scripts/ETTh.sh (para ETTh1 & ETTh2)
     bash ./scripts/ETTm.sh (para ETTm1 & ETTm2)
     bash ./scripts/weather.sh (para Weather)
     
#### Para otros conjuntos de datos, por favor cambie el nombre del script en el comando anterior.

## Ablaciones
     
#### Ejecutar Ablaciones en CALF (ETT):
     
    cd ./CALF
    sh scripts/long_term_forecasting/ETTh_GPT2.sh
    sh scripts/long_term_forecasting/ETTm_GPT2.sh
    
    sh scripts/long_term_forecasting/traffic.sh 
    (Para otros conjuntos de datos, como traffic)

#### Ejecutar Ablaciones en OneFitsAll (ETT):
     cd ./OFA
     bash ./script/ETTh_GPT2.sh   
     bash ./script/ETTm_GPT2.sh

     bash ./script/illness.sh 
     (Para otros conjuntos de datos, como illness)

#### Ejecutar Ablaciones en Time-LLM (ETT)
     cd ./Time-LLM-exp
     bash ./scripts/train_script/TimeLLM_ETTh1.sh
     bash ./scripts/train_script/TimeLLM_ETTm1.sh 

     bash ./scripts/train_script/TimeLLM_Weather.sh
     (Para otros conjuntos de datos, como Weather)

#### (Para ejecutar ablaciones en otros conjuntos de datos, por favor cambie el nombre del conjunto como se muestra en el ejemplo.)

## Agradecimientos

Esta base de c贸digo est谩 construida sobre la [Time-Series-Library](https://github.com/thuml/Time-Series-Library). 隆Gracias!

## Citaci贸n
Si encuentra nuestro trabajo 煤til, por favor cite nuestro trabajo de la siguiente manera:


```bibtex
@inproceedings{tan2024language,
    title={驴Son realmente 煤tiles los modelos de lenguaje para la predicci贸n de series temporales?},
    author={Tan, Mingtian y Merrill, Mike A y Gupta, Vinayak y Althoff, Tim y Hartvigsen, Thomas},
    booktitle={Sistemas de Procesamiento de Informaci贸n Neural (NeurIPS)},
    year={2024}
}

```

---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-29

---