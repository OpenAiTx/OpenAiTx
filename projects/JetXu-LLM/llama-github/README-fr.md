# llama-github

[Document d√©taill√©] https://deepwiki.com/JetXu-LLM/llama-github

[![Version PyPI](https://badge.fury.io/py/llama-github.svg)](https://badge.fury.io/py/llama-github)
[![T√©l√©chargements](https://static.pepy.tech/badge/Llama-github)](https://pepy.tech/project/Llama-github)
[![Licence](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)

Llama-github est un outil puissant qui vous aide √† r√©cup√©rer (bas√© sur Agentic RAG) les extraits de code, probl√®mes et informations de d√©p√¥t les plus pertinents de GitHub en fonction de vos requ√™tes, les transformant en un contexte de connaissance pr√©cieux. Il permet aux chatbots LLM, agents IA et agents Auto-dev de r√©soudre des t√¢ches de codage complexes. Que vous soyez un d√©veloppeur cherchant des solutions rapides ou un ing√©nieur impl√©mentant des agents Auto Dev IA avanc√©s, llama-github rend cela simple et efficace.

Si vous aimez ce projet ou pensez qu‚Äôil a du potentiel, merci de lui donner une ‚≠êÔ∏è. Votre soutien est notre plus grande motivation !

## Architecture
![Architecture de haut niveau](https://raw.githubusercontent.com/JetXu-LLM/llama-github/main/./docs/high_level_architecture.drawio.svg)

## Installation
```
pip install llama-github
```

## Utilisation

Voici un exemple simple de comment utiliser llama-github¬†:

```python
from llama_github import GithubRAG

# Initialize GithubRAG with your credentials
github_rag = GithubRAG(
    github_access_token="your_github_access_token", 
    openai_api_key="your_openai_api_key", # Optional in Simple Mode
    jina_api_key="your_jina_api_key" # Optional - unless you want high concurrency production deployment (s.jina.ai API will be used in llama-github)
)

# Retrieve context for a coding question (simple_mode is default set to False)
query = "How to create a NumPy array in Python?"
context = github_rag.retrieve_context(
    query, # In professional mode, one query will take nearly 1 min to generate final contexts. You could set log level to INFO to monitor the retrieval progress
    # simple_mode = True
)

print(context)
```
Pour un usage plus avanc√© et des exemples, veuillez consulter la [documentation](https://raw.githubusercontent.com/JetXu-LLM/llama-github/main/docs/usage.md).

## Fonctionnalit√©s cl√©s

- **üîç Recherche intelligente sur GitHub** : Exploitez la puissance de llama-github pour r√©cup√©rer des extraits de code, des issues et des informations sur les d√©p√¥ts hautement pertinents sur GitHub en fonction des requ√™tes des utilisateurs. Nos techniques avanc√©es de recherche garantissent que vous trouvez rapidement et efficacement les informations les plus pertinentes.

- **‚ö° Mise en cache du pool de d√©p√¥ts** : Llama-github dispose d‚Äôun m√©canisme innovant de mise en cache du pool de d√©p√¥ts. En mettant en cache les d√©p√¥ts (y compris les README, structures, codes et issues) √† travers les threads, llama-github acc√©l√®re significativement l‚Äôefficacit√© de la recherche sur GitHub et minimise la consommation des jetons API GitHub. D√©ployez llama-github dans des environnements de production multithread en toute confiance, sachant qu‚Äôil fonctionnera de mani√®re optimale et vous fera √©conomiser des ressources pr√©cieuses.

- **üß† Analyse des questions aliment√©e par LLM** : Profitez des mod√®les de langage de pointe pour analyser les questions des utilisateurs et g√©n√©rer des strat√©gies et crit√®res de recherche tr√®s efficaces. Llama-github d√©compose intelligemment les requ√™tes complexes, garantissant que vous r√©cup√©rez les informations les plus pertinentes du vaste r√©seau de d√©p√¥ts GitHub.

- **üìö G√©n√©ration de contexte compl√®te** : G√©n√©rez des r√©ponses riches et contextuellement pertinentes en combinant de mani√®re fluide les informations r√©cup√©r√©es sur GitHub avec les capacit√©s de raisonnement des mod√®les de langage avanc√©s. Llama-github excelle dans le traitement des questions les plus complexes et longues, fournissant des r√©ponses compl√®tes et perspicaces incluant un contexte √©tendu pour soutenir vos besoins en d√©veloppement.

- **üöÄ Excellence dans le traitement asynchrone** : Llama-github est con√ßu d√®s le d√©part pour exploiter tout le potentiel de la programmation asynchrone. Avec des m√©canismes asynchrones minutieusement impl√©ment√©s dans tout le code, llama-github peut g√©rer plusieurs requ√™tes simultan√©ment, augmentant significativement la performance globale. Exp√©rimentez la diff√©rence alors que llama-github g√®re efficacement des charges de travail √©lev√©es sans compromettre la rapidit√© ni la qualit√©.

- **üîß Int√©gration flexible des LLM** : Int√©grez facilement llama-github avec divers fournisseurs de LLM, mod√®les d‚Äôincorporation et mod√®les de reranking pour adapter les capacit√©s de la biblioth√®que √† vos besoins sp√©cifiques. Notre architecture extensible vous permet de personnaliser et d‚Äôam√©liorer les fonctionnalit√©s de llama-github, garantissant une adaptation fluide √† votre environnement de d√©veloppement unique.

- **üîí Options robustes d‚Äôauthentification** : Llama-github supporte √† la fois les jetons d‚Äôacc√®s personnel et l‚Äôauthentification via GitHub App, vous offrant la flexibilit√© de l‚Äôint√©grer dans diff√©rents environnements de d√©veloppement. Que vous soyez un d√©veloppeur individuel ou travailliez dans un contexte organisationnel, llama-github vous couvre avec des m√©canismes d‚Äôauthentification s√©curis√©s et fiables.

- **üõ†Ô∏è Journalisation et gestion des erreurs** : Nous comprenons l‚Äôimportance d‚Äôun fonctionnement fluide et d‚Äôun d√©pannage facile. C‚Äôest pourquoi llama-github est √©quip√© de m√©canismes complets de journalisation et de gestion des erreurs. Obtenez des informations approfondies sur le comportement de la biblioth√®que, diagnostiquez rapidement les probl√®mes et maintenez un flux de travail de d√©veloppement stable et fiable.

## ü§ñ Essayez notre assistant de revue de PR aliment√© par IA : LlamaPReview

Si vous trouvez llama-github utile, vous pourriez √©galement √™tre int√©ress√© par notre assistant de revue de PR GitHub aliment√© par IA, LlamaPReview. Il est con√ßu pour compl√©ter votre flux de d√©veloppement et am√©liorer encore la qualit√© du code.

### Fonctionnalit√©s cl√©s de LlamaPReview :
- üöÄ Installation en un clic, aucune configuration requise, fonctionnement enti√®rement automatique
- üíØ Actuellement gratuit - pas besoin de carte de cr√©dit ni d‚Äôinformations de paiement
- üß† Revues automatiques de PR aliment√©es par IA avec une compr√©hension profonde du code
- üåê Supporte plusieurs langages de programmation

**LlamaPReview utilise la r√©cup√©ration contextuelle avanc√©e de llama-github et l‚Äôanalyse aliment√©e par LLM** pour fournir des revues de code intelligentes et conscientes du contexte. C‚Äôest comme avoir un d√©veloppeur senior, arm√© du contexte complet de votre d√©p√¥t, qui r√©vise automatiquement chaque PR !

üëâ [Installer LlamaPReview Maintenant](https://github.com/marketplace/llamapreview/) (Gratuit)

En utilisant llama-github pour la r√©cup√©ration de contexte et LlamaPReview pour les revues de code, vous pouvez cr√©er un environnement de d√©veloppement puissant et enrichi par l‚ÄôIA.

## Vision et feuille de route

### Vision

Notre vision est de devenir un module cl√© dans l‚Äôavenir des solutions de d√©veloppement pilot√©es par IA, s‚Äôint√©grant parfaitement avec GitHub pour permettre aux LLM de r√©soudre automatiquement des t√¢ches de codage complexes.

![Vision Architecture](https://raw.githubusercontent.com/JetXu-LLM/llama-github/main/./docs/vision.drawio.svg)

### Feuille de route

Pour une vue d√©taill√©e de notre feuille de route projet, veuillez visiter notre [Feuille de route du projet](https://github.com/users/JetXu-LLM/projects/2).

## Remerciements

Nous souhaitons exprimer notre gratitude aux projets open source suivants pour leur soutien et leurs contributions :

- **[LangChain](https://github.com/langchain-ai/langchain)** : Pour avoir fourni le cadre fondamental qui alimente les capacit√©s de prompting et de traitement LLM dans llama-github.
- **[Jina.ai](https://github.com/jina-ai/reader)** : Pour avoir offert l‚ÄôAPI s.jina.ai ainsi que des mod√®les open source de reranking et d‚Äôincorporation qui am√©liorent la pr√©cision et la pertinence des contextes g√©n√©r√©s dans llama-github.

Leurs contributions ont √©t√© essentielles au d√©veloppement de llama-github, et nous recommandons vivement de consulter leurs projets pour plus de solutions innovantes.

## Contribution

Nous accueillons les contributions √† llama-github ! Veuillez consulter nos [directives de contribution](https://raw.githubusercontent.com/JetXu-LLM/llama-github/main/CONTRIBUTING.md) pour plus d‚Äôinformations.

## Licence

Ce projet est sous licence Apache 2.0. Voir le fichier [LICENSE](LICENSE) pour plus de d√©tails.

## Contact

Si vous avez des questions, suggestions ou retours, n‚Äôh√©sitez pas √† nous contacter √† [email de Jet Xu](https://raw.githubusercontent.com/JetXu-LLM/llama-github/main/mailto:Voldemort.xu@foxmail.com).

---

Merci d‚Äôavoir choisi llama-github ! Nous esp√©rons que cette biblioth√®que am√©liorera votre exp√©rience de d√©veloppement IA et vous aidera √† cr√©er des applications puissantes en toute simplicit√©.



---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-28

---