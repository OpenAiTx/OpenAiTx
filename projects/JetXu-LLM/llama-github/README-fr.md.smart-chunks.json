[
  {
    "Id": 1,
    "Content": "# llama-github\n\n[Detail Document] https://deepwiki.com/JetXu-LLM/llama-github\n\n[![PyPI version](https://badge.fury.io/py/llama-github.svg)](https://badge.fury.io/py/llama-github)\n[![Downloads](https://static.pepy.tech/badge/Llama-github)](https://pepy.tech/project/Llama-github)\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n\nLlama-github is a powerful tool that helps you retrieve(based on Agentic RAG) the most relevant code snippets, issues, and repository information from GitHub based on your queries, transforming them into valuable knowledge context. It empowers LLM Chatbots, AI Agents, and Auto-dev Agents to solve complex coding tasks. Whether you're a developer looking for quick solutions or an engineer implementing advanced Auto Dev AI Agents, llama-github makes it easy and efficient.\n\nIf you like this project or believe it has potential, please give it a ‚≠êÔ∏è. Your support is our greatest motivation!\n\n## Architecture\n![High Level Architecture](https://raw.githubusercontent.com/JetXu-LLM/llama-github/main/./docs/high_level_architecture.drawio.svg)\n\n## Installation",
    "ContentSha": "TZmxyaoSVzegm0TJMJvowqs2RLRV2vw3TmLRyiZUZyA=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "# llama-github\n\n[Document d√©taill√©] https://deepwiki.com/JetXu-LLM/llama-github\n\n[![Version PyPI](https://badge.fury.io/py/llama-github.svg)](https://badge.fury.io/py/llama-github)\n[![T√©l√©chargements](https://static.pepy.tech/badge/Llama-github)](https://pepy.tech/project/Llama-github)\n[![Licence](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n\nLlama-github est un outil puissant qui vous aide √† r√©cup√©rer (bas√© sur Agentic RAG) les extraits de code, probl√®mes et informations de d√©p√¥t les plus pertinents de GitHub en fonction de vos requ√™tes, les transformant en un contexte de connaissance pr√©cieux. Il permet aux chatbots LLM, agents IA et agents Auto-dev de r√©soudre des t√¢ches de codage complexes. Que vous soyez un d√©veloppeur cherchant des solutions rapides ou un ing√©nieur impl√©mentant des agents Auto Dev IA avanc√©s, llama-github rend cela simple et efficace.\n\nSi vous aimez ce projet ou pensez qu‚Äôil a du potentiel, merci de lui donner une ‚≠êÔ∏è. Votre soutien est notre plus grande motivation !\n\n## Architecture\n![Architecture de haut niveau](https://raw.githubusercontent.com/JetXu-LLM/llama-github/main/./docs/high_level_architecture.drawio.svg)\n\n## Installation",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "cAI8B2fw5xCydddeNcosOFY2wrQw8An1lrdomk97KBc=",
        "originContent": "# llama-github",
        "translatedContent": "# llama-github"
      },
      {
        "row": 2,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 3,
        "rowsha": "jSTPMiMb5qDjmRYlxqDSx6zbsKYKOv4c6HQQgjbDUqo=",
        "originContent": "[Detail Document] https://deepwiki.com/JetXu-LLM/llama-github",
        "translatedContent": "[Document d√©taill√©] https://deepwiki.com/JetXu-LLM/llama-github"
      },
      {
        "row": 4,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 5,
        "rowsha": "48lo1Cg6fyAcQl554s/7eKSc41UYvSn/CD6yxtLVgjY=",
        "originContent": "[![PyPI version](https://badge.fury.io/py/llama-github.svg)](https://badge.fury.io/py/llama-github)",
        "translatedContent": "[![Version PyPI](https://badge.fury.io/py/llama-github.svg)](https://badge.fury.io/py/llama-github)"
      },
      {
        "row": 6,
        "rowsha": "VG3AfiYLkBPipFZPAt/fJt5jj8aI773UKbmI1Z/kc2M=",
        "originContent": "[![Downloads](https://static.pepy.tech/badge/Llama-github)](https://pepy.tech/project/Llama-github)",
        "translatedContent": "[![T√©l√©chargements](https://static.pepy.tech/badge/Llama-github)](https://pepy.tech/project/Llama-github)"
      },
      {
        "row": 7,
        "rowsha": "U0vs77tGfFYY+r4ScaOhTOZDgt8vSKCUyj59tU13ZtE=",
        "originContent": "[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)",
        "translatedContent": "[![Licence](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)"
      },
      {
        "row": 8,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 9,
        "rowsha": "y7xsY1R0s8pmwDQukNS2PQ5JqK6CRs8AwuPrZkBKiT8=",
        "originContent": "Llama-github is a powerful tool that helps you retrieve(based on Agentic RAG) the most relevant code snippets, issues, and repository information from GitHub based on your queries, transforming them into valuable knowledge context. It empowers LLM Chatbots, AI Agents, and Auto-dev Agents to solve complex coding tasks. Whether you're a developer looking for quick solutions or an engineer implementing advanced Auto Dev AI Agents, llama-github makes it easy and efficient.",
        "translatedContent": "Llama-github est un outil puissant qui vous aide √† r√©cup√©rer (bas√© sur Agentic RAG) les extraits de code, probl√®mes et informations de d√©p√¥t les plus pertinents de GitHub en fonction de vos requ√™tes, les transformant en un contexte de connaissance pr√©cieux. Il permet aux chatbots LLM, agents IA et agents Auto-dev de r√©soudre des t√¢ches de codage complexes. Que vous soyez un d√©veloppeur cherchant des solutions rapides ou un ing√©nieur impl√©mentant des agents Auto Dev IA avanc√©s, llama-github rend cela simple et efficace."
      },
      {
        "row": 10,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 11,
        "rowsha": "6h9r25r7afHOT0YHlaj7xv1WU2JfjWqSqLUM7UgJNfQ=",
        "originContent": "If you like this project or believe it has potential, please give it a ‚≠êÔ∏è. Your support is our greatest motivation!",
        "translatedContent": "Si vous aimez ce projet ou pensez qu‚Äôil a du potentiel, merci de lui donner une ‚≠êÔ∏è. Votre soutien est notre plus grande motivation !"
      },
      {
        "row": 12,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 13,
        "rowsha": "p+VkcrEb08g4vIGZYB9aVScRKgA8afv5WwErwTVzoZM=",
        "originContent": "## Architecture",
        "translatedContent": "## Architecture"
      },
      {
        "row": 14,
        "rowsha": "TiSukfY/+ter5MJ39DMtNdHlqRXwhZFkP3uTdAfvA0c=",
        "originContent": "![High Level Architecture](https://raw.githubusercontent.com/JetXu-LLM/llama-github/main/./docs/high_level_architecture.drawio.svg)",
        "translatedContent": "![Architecture de haut niveau](https://raw.githubusercontent.com/JetXu-LLM/llama-github/main/./docs/high_level_architecture.drawio.svg)"
      },
      {
        "row": 15,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 16,
        "rowsha": "oV0SUDvwD2VN8Gi9nlr2JZ2xcDrASmE2W5kc5SVX5eo=",
        "originContent": "## Installation",
        "translatedContent": "## Installation"
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 2,
    "Content": "```\npip install llama-github\n```",
    "ContentSha": "A7Ioj19mKQ2poo2/SNNEl/PUNM9x+13/7/ncgppdcgY=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```\npip install llama-github\n```",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "8bkBhHOQsO1+N058HkZOwXtGpCfEh6WtbL0pBkBQg9U=",
        "originContent": "```",
        "translatedContent": "```"
      },
      {
        "row": 2,
        "rowsha": "HUzQ8Fuh/Wg8Hu8KR8bezJLbFVpPKMpf5Pa7xDY6vYM=",
        "originContent": "pip install llama-github",
        "translatedContent": "pip install llama-github"
      },
      {
        "row": 3,
        "rowsha": "8bkBhHOQsO1+N058HkZOwXtGpCfEh6WtbL0pBkBQg9U=",
        "originContent": "```",
        "translatedContent": "```"
      }
    ],
    "IsCodeBlock": true
  },
  {
    "Id": 3,
    "Content": "\n## Usage\n\nHere's a simple example of how to use llama-github:\n",
    "ContentSha": "oqzkRVGHtTMxKJniKYy+bJRBu/B3l8AYVVo5suNCmR4=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "\n## Utilisation\n\nVoici un exemple simple de comment utiliser llama-github¬†:\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 2,
        "rowsha": "Y7/WGgVhslWiyqCi463Qkcp+Mn8voGpAfLsv80G4WNg=",
        "originContent": "## Usage",
        "translatedContent": "## Utilisation"
      },
      {
        "row": 3,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 4,
        "rowsha": "4zRpXijfFggW+Fd/VZ8qaLkuJwnjePE96Hy1hPxOIRE=",
        "originContent": "Here's a simple example of how to use llama-github:",
        "translatedContent": "Voici un exemple simple de comment utiliser llama-github¬†:"
      },
      {
        "row": 5,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 4,
    "Content": "```python\nfrom llama_github import GithubRAG\n\n# Initialize GithubRAG with your credentials\ngithub_rag = GithubRAG(\n    github_access_token=\"your_github_access_token\", \n    openai_api_key=\"your_openai_api_key\", # Optional in Simple Mode\n    jina_api_key=\"your_jina_api_key\" # Optional - unless you want high concurrency production deployment (s.jina.ai API will be used in llama-github)\n)\n\n# Retrieve context for a coding question (simple_mode is default set to False)\nquery = \"How to create a NumPy array in Python?\"\ncontext = github_rag.retrieve_context(\n    query, # In professional mode, one query will take nearly 1 min to generate final contexts. You could set log level to INFO to monitor the retrieval progress\n    # simple_mode = True\n)\n\nprint(context)\n```",
    "ContentSha": "HXBtstD3V5wfKLk68/A2T/KFqRB/TtW5s0tq89CzhFU=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```python\nfrom llama_github import GithubRAG\n\n# Initialize GithubRAG with your credentials\ngithub_rag = GithubRAG(\n    github_access_token=\"your_github_access_token\", \n    openai_api_key=\"your_openai_api_key\", # Optional in Simple Mode\n    jina_api_key=\"your_jina_api_key\" # Optional - unless you want high concurrency production deployment (s.jina.ai API will be used in llama-github)\n)\n\n# Retrieve context for a coding question (simple_mode is default set to False)\nquery = \"How to create a NumPy array in Python?\"\ncontext = github_rag.retrieve_context(\n    query, # In professional mode, one query will take nearly 1 min to generate final contexts. You could set log level to INFO to monitor the retrieval progress\n    # simple_mode = True\n)\n\nprint(context)\n```",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "uDnRsrcDV2kZVI2wi9EA58m+F4ILdr1bvjhqNlB+wSc=",
        "originContent": "```python",
        "translatedContent": "```python"
      },
      {
        "row": 2,
        "rowsha": "hA5UQDttQHKAwB+ESl7GqyiAa1AotkDZYMGTRuo/AoY=",
        "originContent": "from llama_github import GithubRAG",
        "translatedContent": "from llama_github import GithubRAG"
      },
      {
        "row": 3,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 4,
        "rowsha": "/Xio12pWjWQxdHtqP9gkrQo8FvXup5Rnp8m6s9rWQoA=",
        "originContent": "# Initialize GithubRAG with your credentials",
        "translatedContent": "# Initialize GithubRAG with your credentials"
      },
      {
        "row": 5,
        "rowsha": "Z/GdSwf7Zfjb56iNk/lrSDiBY4vaKvdGc5F3qO5IgSQ=",
        "originContent": "github_rag = GithubRAG(",
        "translatedContent": "github_rag = GithubRAG("
      },
      {
        "row": 6,
        "rowsha": "DCUT+XvqWQMhTvTxgZeTkNrf6sB1egP0ataQmEq//Do=",
        "originContent": "    github_access_token=\"your_github_access_token\", ",
        "translatedContent": "    github_access_token=\"your_github_access_token\", "
      },
      {
        "row": 7,
        "rowsha": "7hDiOA+9nEfENa9R7NEJW8WSnmj71txD0Ndrw3HjngM=",
        "originContent": "    openai_api_key=\"your_openai_api_key\", # Optional in Simple Mode",
        "translatedContent": "    openai_api_key=\"your_openai_api_key\", # Optional in Simple Mode"
      },
      {
        "row": 8,
        "rowsha": "3gER4AqcTSuWLMk5yIWdhwT240c1f6O0Qk92225ZuVU=",
        "originContent": "    jina_api_key=\"your_jina_api_key\" # Optional - unless you want high concurrency production deployment (s.jina.ai API will be used in llama-github)",
        "translatedContent": "    jina_api_key=\"your_jina_api_key\" # Optional - unless you want high concurrency production deployment (s.jina.ai API will be used in llama-github)"
      },
      {
        "row": 9,
        "rowsha": "ul7FHQekrA6VFghwRDHVmgKyGk6VGswQUFqNxAfFAe4=",
        "originContent": ")",
        "translatedContent": ")"
      },
      {
        "row": 10,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 11,
        "rowsha": "U1SUJQpzS8NM2yh7MFPYJl5dQ6gq/MR813WqTX952X0=",
        "originContent": "# Retrieve context for a coding question (simple_mode is default set to False)",
        "translatedContent": "# Retrieve context for a coding question (simple_mode is default set to False)"
      },
      {
        "row": 12,
        "rowsha": "ZXVeMLKgPJjdKhiZ9CK5U4oP9OsdwnWdpg8zNqEI+58=",
        "originContent": "query = \"How to create a NumPy array in Python?\"",
        "translatedContent": "query = \"How to create a NumPy array in Python?\""
      },
      {
        "row": 13,
        "rowsha": "tMIfL+j+gxN0VAT5YvCwzTJ6i5aIIY3fcw3LxJWoTrQ=",
        "originContent": "context = github_rag.retrieve_context(",
        "translatedContent": "context = github_rag.retrieve_context("
      },
      {
        "row": 14,
        "rowsha": "Q1vTmUoyeud2ZNyE6iVCY7OriQi61Wg6NLrSyM9pVLE=",
        "originContent": "    query, # In professional mode, one query will take nearly 1 min to generate final contexts. You could set log level to INFO to monitor the retrieval progress",
        "translatedContent": "    query, # In professional mode, one query will take nearly 1 min to generate final contexts. You could set log level to INFO to monitor the retrieval progress"
      },
      {
        "row": 15,
        "rowsha": "uvb8HENvYITO6V3XF0E9tyqODM0Be+CdeIuXZsYj/CA=",
        "originContent": "    # simple_mode = True",
        "translatedContent": "    # simple_mode = True"
      },
      {
        "row": 16,
        "rowsha": "ul7FHQekrA6VFghwRDHVmgKyGk6VGswQUFqNxAfFAe4=",
        "originContent": ")",
        "translatedContent": ")"
      },
      {
        "row": 17,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 18,
        "rowsha": "ZZEcMA2fivnNWsWr7Vg5FUhhpwQQJ/ThJU+S148t13E=",
        "originContent": "print(context)",
        "translatedContent": "print(context)"
      },
      {
        "row": 19,
        "rowsha": "8bkBhHOQsO1+N058HkZOwXtGpCfEh6WtbL0pBkBQg9U=",
        "originContent": "```",
        "translatedContent": "```"
      }
    ],
    "IsCodeBlock": true
  },
  {
    "Id": 5,
    "Content": "\nFor more advanced usage and examples, please refer to the [documentation](https://raw.githubusercontent.com/JetXu-LLM/llama-github/main/docs/usage.md).\n\n## Key Features\n\n- **üîç Intelligent GitHub Retrieval**: Harness the power of llama-github to retrieve highly relevant code snippets, issues, and repository information from GitHub based on user queries. Our advanced retrieval techniques ensure you find the most pertinent information quickly and efficiently.\n\n- **‚ö° Repository Pool Caching**: Llama-github has an innovative repository pool caching mechanism. By caching repositories (including READMEs, structures, code, and issues) across threads, llama-github significantly accelerates GitHub search retrieval efficiency and minimizes the consumption of GitHub API tokens. Deploy llama-github in multi-threaded production environments with confidence, knowing that it will perform optimally and save you valuable resources.\n\n- **üß† LLM-Powered Question Analysis**: Leverage state-of-the-art language models to analyze user questions and generate highly effective search strategies and criteria. Llama-github intelligently breaks down complex queries, ensuring that you retrieve the most relevant information from GitHub's vast repository network.\n\n- **üìö Comprehensive Context Generation**: Generate rich, contextually relevant answers by seamlessly combining information retrieved from GitHub with the reasoning capabilities of advanced language models. Llama-github excels at handling even the most complex and lengthy questions, providing comprehensive and insightful responses that include extensive context to support your development needs.\n\n- **üöÄ Asynchronous Processing Excellence**: Llama-github is built from the ground up to leverage the full potential of asynchronous programming. With meticulously implemented asynchronous mechanisms woven throughout the codebase, llama-github can handle multiple requests concurrently, significantly boosting overall performance. Experience the difference as llama-github efficiently manages high-volume workloads without compromising on speed or quality.\n\n- **üîß Flexible LLM Integration**: Easily integrate llama-github with various LLM providers, embedding models, and reranking models to tailor the library's capabilities to your specific requirements. Our extensible architecture allows you to customize and enhance llama-github's functionality, ensuring that it adapts seamlessly to your unique development environment.\n\n- **üîí Robust Authentication Options**: Llama-github supports both personal access tokens and GitHub App authentication, providing you with the flexibility to integrate it into different development setups. Whether you're an individual developer or working within an organizational context, llama-github has you covered with secure and reliable authentication mechanisms.\n\n- **üõ†Ô∏è Logging and Error Handling**: We understand the importance of smooth operations and easy troubleshooting. That's why llama-github comes equipped with comprehensive logging and error handling mechanisms. Gain deep insights into the library's behavior, quickly diagnose issues, and maintain a stable and reliable development workflow.\n\n## ü§ñ Try Our AI-Powered PR Review Assistant: LlamaPReview\n\nIf you find llama-github useful, you might also be interested in our AI-powered GitHub PR review assistant, LlamaPReview. It's designed to complement your development workflow and further enhance code quality.\n\n### Key Features of LlamaPReview:\n- üöÄ One-click installation, zero configuration required, fully auto-run\n- üíØ Currently free to use - no credit card or payment info needed\n- üß† AI-powered, automatic PR reviews with deep code understanding\n- üåê Supports multiple programming languages\n\n**LlamaPReview utilizes llama-github's advanced context retrieval and LLM-powered analysis** to provide intelligent, context-aware code reviews. It's like having a senior developer, armed with the full context of your repository, review every PR automatically!\n\nüëâ [Install LlamaPReview Now](https://github.com/marketplace/llamapreview/) (Free)\n\nBy using llama-github for context retrieval and LlamaPReview for code reviews, you can create a powerful, AI-enhanced development environment.\n\n## Vision and Roadmap\n\n### Vision\n\nOur vision is to become a pivotal module in the future of AI-driven development solutions, seamlessly integrating with GitHub to empower LLMs in automatically resolving complex coding tasks.\n\n![Vision Architecture](https://raw.githubusercontent.com/JetXu-LLM/llama-github/main/./docs/vision.drawio.svg)\n\n### Roadmap\n\nFor a detailed view of our project roadmap, please visit our [Project Roadmap](https://github.com/users/JetXu-LLM/projects/2).\n\n## Acknowledgments\n\nWe would like to express our gratitude to the following open-source projects for their support and contributions:\n\n- **[LangChain](https://github.com/langchain-ai/langchain)**: For providing the foundational framework that empowers the LLM prompting and processing capabilities in llama-github.\n- **[Jina.ai](https://github.com/jina-ai/reader)**: For offering s.jina.ai API and open source reranker and embedding models that enhance the accuracy and relevance of the generated contexts in llama-github.\n\nTheir contributions have been instrumental in the development of llama-github, and we highly recommend checking out their projects for more innovative solutions.\n\n## Contributing\n\nWe welcome contributions to llama-github! Please see our [contributing guidelines](https://raw.githubusercontent.com/JetXu-LLM/llama-github/main/CONTRIBUTING.md) for more information.\n\n## License\n\nThis project is licensed under the terms of the Apache 2.0 license. See the [LICENSE](LICENSE) file for more details.\n\n## Contact\n\nIf you have any questions, suggestions, or feedback, please feel free to reach out to us at [Jet Xu's email](https://raw.githubusercontent.com/JetXu-LLM/llama-github/main/mailto:Voldemort.xu@foxmail.com).\n\n---\n\nThank you for choosing llama-github! We hope this library enhances your AI development experience and helps you build powerful applications with ease.\n",
    "ContentSha": "9uwuSakxESmyLicwqpb9te9XJtaLzK41aW7HGRKfVFM=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "Pour un usage plus avanc√© et des exemples, veuillez consulter la [documentation](https://raw.githubusercontent.com/JetXu-LLM/llama-github/main/docs/usage.md).\n\n## Fonctionnalit√©s cl√©s\n\n- **üîç Recherche intelligente sur GitHub** : Exploitez la puissance de llama-github pour r√©cup√©rer des extraits de code, des issues et des informations sur les d√©p√¥ts hautement pertinents sur GitHub en fonction des requ√™tes des utilisateurs. Nos techniques avanc√©es de recherche garantissent que vous trouvez rapidement et efficacement les informations les plus pertinentes.\n\n- **‚ö° Mise en cache du pool de d√©p√¥ts** : Llama-github dispose d‚Äôun m√©canisme innovant de mise en cache du pool de d√©p√¥ts. En mettant en cache les d√©p√¥ts (y compris les README, structures, codes et issues) √† travers les threads, llama-github acc√©l√®re significativement l‚Äôefficacit√© de la recherche sur GitHub et minimise la consommation des jetons API GitHub. D√©ployez llama-github dans des environnements de production multithread en toute confiance, sachant qu‚Äôil fonctionnera de mani√®re optimale et vous fera √©conomiser des ressources pr√©cieuses.\n\n- **üß† Analyse des questions aliment√©e par LLM** : Profitez des mod√®les de langage de pointe pour analyser les questions des utilisateurs et g√©n√©rer des strat√©gies et crit√®res de recherche tr√®s efficaces. Llama-github d√©compose intelligemment les requ√™tes complexes, garantissant que vous r√©cup√©rez les informations les plus pertinentes du vaste r√©seau de d√©p√¥ts GitHub.\n\n- **üìö G√©n√©ration de contexte compl√®te** : G√©n√©rez des r√©ponses riches et contextuellement pertinentes en combinant de mani√®re fluide les informations r√©cup√©r√©es sur GitHub avec les capacit√©s de raisonnement des mod√®les de langage avanc√©s. Llama-github excelle dans le traitement des questions les plus complexes et longues, fournissant des r√©ponses compl√®tes et perspicaces incluant un contexte √©tendu pour soutenir vos besoins en d√©veloppement.\n\n- **üöÄ Excellence dans le traitement asynchrone** : Llama-github est con√ßu d√®s le d√©part pour exploiter tout le potentiel de la programmation asynchrone. Avec des m√©canismes asynchrones minutieusement impl√©ment√©s dans tout le code, llama-github peut g√©rer plusieurs requ√™tes simultan√©ment, augmentant significativement la performance globale. Exp√©rimentez la diff√©rence alors que llama-github g√®re efficacement des charges de travail √©lev√©es sans compromettre la rapidit√© ni la qualit√©.\n\n- **üîß Int√©gration flexible des LLM** : Int√©grez facilement llama-github avec divers fournisseurs de LLM, mod√®les d‚Äôincorporation et mod√®les de reranking pour adapter les capacit√©s de la biblioth√®que √† vos besoins sp√©cifiques. Notre architecture extensible vous permet de personnaliser et d‚Äôam√©liorer les fonctionnalit√©s de llama-github, garantissant une adaptation fluide √† votre environnement de d√©veloppement unique.\n\n- **üîí Options robustes d‚Äôauthentification** : Llama-github supporte √† la fois les jetons d‚Äôacc√®s personnel et l‚Äôauthentification via GitHub App, vous offrant la flexibilit√© de l‚Äôint√©grer dans diff√©rents environnements de d√©veloppement. Que vous soyez un d√©veloppeur individuel ou travailliez dans un contexte organisationnel, llama-github vous couvre avec des m√©canismes d‚Äôauthentification s√©curis√©s et fiables.\n\n- **üõ†Ô∏è Journalisation et gestion des erreurs** : Nous comprenons l‚Äôimportance d‚Äôun fonctionnement fluide et d‚Äôun d√©pannage facile. C‚Äôest pourquoi llama-github est √©quip√© de m√©canismes complets de journalisation et de gestion des erreurs. Obtenez des informations approfondies sur le comportement de la biblioth√®que, diagnostiquez rapidement les probl√®mes et maintenez un flux de travail de d√©veloppement stable et fiable.\n\n## ü§ñ Essayez notre assistant de revue de PR aliment√© par IA : LlamaPReview\n\nSi vous trouvez llama-github utile, vous pourriez √©galement √™tre int√©ress√© par notre assistant de revue de PR GitHub aliment√© par IA, LlamaPReview. Il est con√ßu pour compl√©ter votre flux de d√©veloppement et am√©liorer encore la qualit√© du code.\n\n### Fonctionnalit√©s cl√©s de LlamaPReview :\n- üöÄ Installation en un clic, aucune configuration requise, fonctionnement enti√®rement automatique\n- üíØ Actuellement gratuit - pas besoin de carte de cr√©dit ni d‚Äôinformations de paiement\n- üß† Revues automatiques de PR aliment√©es par IA avec une compr√©hension profonde du code\n- üåê Supporte plusieurs langages de programmation\n\n**LlamaPReview utilise la r√©cup√©ration contextuelle avanc√©e de llama-github et l‚Äôanalyse aliment√©e par LLM** pour fournir des revues de code intelligentes et conscientes du contexte. C‚Äôest comme avoir un d√©veloppeur senior, arm√© du contexte complet de votre d√©p√¥t, qui r√©vise automatiquement chaque PR !\n\nüëâ [Installer LlamaPReview Maintenant](https://github.com/marketplace/llamapreview/) (Gratuit)\n\nEn utilisant llama-github pour la r√©cup√©ration de contexte et LlamaPReview pour les revues de code, vous pouvez cr√©er un environnement de d√©veloppement puissant et enrichi par l‚ÄôIA.\n\n## Vision et feuille de route\n\n### Vision\n\nNotre vision est de devenir un module cl√© dans l‚Äôavenir des solutions de d√©veloppement pilot√©es par IA, s‚Äôint√©grant parfaitement avec GitHub pour permettre aux LLM de r√©soudre automatiquement des t√¢ches de codage complexes.\n\n![Vision Architecture](https://raw.githubusercontent.com/JetXu-LLM/llama-github/main/./docs/vision.drawio.svg)\n\n### Feuille de route\n\nPour une vue d√©taill√©e de notre feuille de route projet, veuillez visiter notre [Feuille de route du projet](https://github.com/users/JetXu-LLM/projects/2).\n\n## Remerciements\n\nNous souhaitons exprimer notre gratitude aux projets open source suivants pour leur soutien et leurs contributions :\n\n- **[LangChain](https://github.com/langchain-ai/langchain)** : Pour avoir fourni le cadre fondamental qui alimente les capacit√©s de prompting et de traitement LLM dans llama-github.\n- **[Jina.ai](https://github.com/jina-ai/reader)** : Pour avoir offert l‚ÄôAPI s.jina.ai ainsi que des mod√®les open source de reranking et d‚Äôincorporation qui am√©liorent la pr√©cision et la pertinence des contextes g√©n√©r√©s dans llama-github.\n\nLeurs contributions ont √©t√© essentielles au d√©veloppement de llama-github, et nous recommandons vivement de consulter leurs projets pour plus de solutions innovantes.\n\n## Contribution\n\nNous accueillons les contributions √† llama-github ! Veuillez consulter nos [directives de contribution](https://raw.githubusercontent.com/JetXu-LLM/llama-github/main/CONTRIBUTING.md) pour plus d‚Äôinformations.\n\n## Licence\n\nCe projet est sous licence Apache 2.0. Voir le fichier [LICENSE](LICENSE) pour plus de d√©tails.\n\n## Contact\n\nSi vous avez des questions, suggestions ou retours, n‚Äôh√©sitez pas √† nous contacter √† [email de Jet Xu](https://raw.githubusercontent.com/JetXu-LLM/llama-github/main/mailto:Voldemort.xu@foxmail.com).\n\n---\n\nMerci d‚Äôavoir choisi llama-github ! Nous esp√©rons que cette biblioth√®que am√©liorera votre exp√©rience de d√©veloppement IA et vous aidera √† cr√©er des applications puissantes en toute simplicit√©.\n\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "Pour un usage plus avanc√© et des exemples, veuillez consulter la [documentation](https://raw.githubusercontent.com/JetXu-LLM/llama-github/main/docs/usage.md)."
      },
      {
        "row": 2,
        "rowsha": "zrMByz3uboJewll5VwM0OlM3JCc+dvchgXugvMGdZyA=",
        "originContent": "For more advanced usage and examples, please refer to the [documentation](https://raw.githubusercontent.com/JetXu-LLM/llama-github/main/docs/usage.md).",
        "translatedContent": ""
      },
      {
        "row": 3,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## Fonctionnalit√©s cl√©s"
      },
      {
        "row": 4,
        "rowsha": "khTPM/+Q4D8FMdf3qgrMDcjDejggkuMl1+JmWZMODFI=",
        "originContent": "## Key Features",
        "translatedContent": ""
      },
      {
        "row": 5,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "- **üîç Recherche intelligente sur GitHub** : Exploitez la puissance de llama-github pour r√©cup√©rer des extraits de code, des issues et des informations sur les d√©p√¥ts hautement pertinents sur GitHub en fonction des requ√™tes des utilisateurs. Nos techniques avanc√©es de recherche garantissent que vous trouvez rapidement et efficacement les informations les plus pertinentes."
      },
      {
        "row": 6,
        "rowsha": "DfE9NcmRSF011JyFgs3cD/a2VQqPhp+XXqaTnhAtBwc=",
        "originContent": "- **üîç Intelligent GitHub Retrieval**: Harness the power of llama-github to retrieve highly relevant code snippets, issues, and repository information from GitHub based on user queries. Our advanced retrieval techniques ensure you find the most pertinent information quickly and efficiently.",
        "translatedContent": ""
      },
      {
        "row": 7,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "- **‚ö° Mise en cache du pool de d√©p√¥ts** : Llama-github dispose d‚Äôun m√©canisme innovant de mise en cache du pool de d√©p√¥ts. En mettant en cache les d√©p√¥ts (y compris les README, structures, codes et issues) √† travers les threads, llama-github acc√©l√®re significativement l‚Äôefficacit√© de la recherche sur GitHub et minimise la consommation des jetons API GitHub. D√©ployez llama-github dans des environnements de production multithread en toute confiance, sachant qu‚Äôil fonctionnera de mani√®re optimale et vous fera √©conomiser des ressources pr√©cieuses."
      },
      {
        "row": 8,
        "rowsha": "OtNyTLgFLD0QDlCwfKWcttMbckHUBj2YJNKV9r+Uc54=",
        "originContent": "- **‚ö° Repository Pool Caching**: Llama-github has an innovative repository pool caching mechanism. By caching repositories (including READMEs, structures, code, and issues) across threads, llama-github significantly accelerates GitHub search retrieval efficiency and minimizes the consumption of GitHub API tokens. Deploy llama-github in multi-threaded production environments with confidence, knowing that it will perform optimally and save you valuable resources.",
        "translatedContent": ""
      },
      {
        "row": 9,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "- **üß† Analyse des questions aliment√©e par LLM** : Profitez des mod√®les de langage de pointe pour analyser les questions des utilisateurs et g√©n√©rer des strat√©gies et crit√®res de recherche tr√®s efficaces. Llama-github d√©compose intelligemment les requ√™tes complexes, garantissant que vous r√©cup√©rez les informations les plus pertinentes du vaste r√©seau de d√©p√¥ts GitHub."
      },
      {
        "row": 10,
        "rowsha": "JeXb8RZZhv2Wmw97j628XoyPreY+fMdIPsxoUQiEvzw=",
        "originContent": "- **üß† LLM-Powered Question Analysis**: Leverage state-of-the-art language models to analyze user questions and generate highly effective search strategies and criteria. Llama-github intelligently breaks down complex queries, ensuring that you retrieve the most relevant information from GitHub's vast repository network.",
        "translatedContent": ""
      },
      {
        "row": 11,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "- **üìö G√©n√©ration de contexte compl√®te** : G√©n√©rez des r√©ponses riches et contextuellement pertinentes en combinant de mani√®re fluide les informations r√©cup√©r√©es sur GitHub avec les capacit√©s de raisonnement des mod√®les de langage avanc√©s. Llama-github excelle dans le traitement des questions les plus complexes et longues, fournissant des r√©ponses compl√®tes et perspicaces incluant un contexte √©tendu pour soutenir vos besoins en d√©veloppement."
      },
      {
        "row": 12,
        "rowsha": "yA7pWfx0KVCXQ2Eo49pVkbLR+DABI/mKnMw/tj/sCL0=",
        "originContent": "- **üìö Comprehensive Context Generation**: Generate rich, contextually relevant answers by seamlessly combining information retrieved from GitHub with the reasoning capabilities of advanced language models. Llama-github excels at handling even the most complex and lengthy questions, providing comprehensive and insightful responses that include extensive context to support your development needs.",
        "translatedContent": ""
      },
      {
        "row": 13,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "- **üöÄ Excellence dans le traitement asynchrone** : Llama-github est con√ßu d√®s le d√©part pour exploiter tout le potentiel de la programmation asynchrone. Avec des m√©canismes asynchrones minutieusement impl√©ment√©s dans tout le code, llama-github peut g√©rer plusieurs requ√™tes simultan√©ment, augmentant significativement la performance globale. Exp√©rimentez la diff√©rence alors que llama-github g√®re efficacement des charges de travail √©lev√©es sans compromettre la rapidit√© ni la qualit√©."
      },
      {
        "row": 14,
        "rowsha": "qku0KFnzN0JDJsFtvSnr1U47mAfoIMLCUgoTAF0FV2Q=",
        "originContent": "- **üöÄ Asynchronous Processing Excellence**: Llama-github is built from the ground up to leverage the full potential of asynchronous programming. With meticulously implemented asynchronous mechanisms woven throughout the codebase, llama-github can handle multiple requests concurrently, significantly boosting overall performance. Experience the difference as llama-github efficiently manages high-volume workloads without compromising on speed or quality.",
        "translatedContent": ""
      },
      {
        "row": 15,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "- **üîß Int√©gration flexible des LLM** : Int√©grez facilement llama-github avec divers fournisseurs de LLM, mod√®les d‚Äôincorporation et mod√®les de reranking pour adapter les capacit√©s de la biblioth√®que √† vos besoins sp√©cifiques. Notre architecture extensible vous permet de personnaliser et d‚Äôam√©liorer les fonctionnalit√©s de llama-github, garantissant une adaptation fluide √† votre environnement de d√©veloppement unique."
      },
      {
        "row": 16,
        "rowsha": "0g6jkIpIeIIz2Gt8w6I2VAjPUz51+RDJs7fxVchaN/k=",
        "originContent": "- **üîß Flexible LLM Integration**: Easily integrate llama-github with various LLM providers, embedding models, and reranking models to tailor the library's capabilities to your specific requirements. Our extensible architecture allows you to customize and enhance llama-github's functionality, ensuring that it adapts seamlessly to your unique development environment.",
        "translatedContent": ""
      },
      {
        "row": 17,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "- **üîí Options robustes d‚Äôauthentification** : Llama-github supporte √† la fois les jetons d‚Äôacc√®s personnel et l‚Äôauthentification via GitHub App, vous offrant la flexibilit√© de l‚Äôint√©grer dans diff√©rents environnements de d√©veloppement. Que vous soyez un d√©veloppeur individuel ou travailliez dans un contexte organisationnel, llama-github vous couvre avec des m√©canismes d‚Äôauthentification s√©curis√©s et fiables."
      },
      {
        "row": 18,
        "rowsha": "WH0+zgaxAP/W/KtIygVDuxydKTKp6imCYyXpqvXpjaM=",
        "originContent": "- **üîí Robust Authentication Options**: Llama-github supports both personal access tokens and GitHub App authentication, providing you with the flexibility to integrate it into different development setups. Whether you're an individual developer or working within an organizational context, llama-github has you covered with secure and reliable authentication mechanisms.",
        "translatedContent": ""
      },
      {
        "row": 19,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "- **üõ†Ô∏è Journalisation et gestion des erreurs** : Nous comprenons l‚Äôimportance d‚Äôun fonctionnement fluide et d‚Äôun d√©pannage facile. C‚Äôest pourquoi llama-github est √©quip√© de m√©canismes complets de journalisation et de gestion des erreurs. Obtenez des informations approfondies sur le comportement de la biblioth√®que, diagnostiquez rapidement les probl√®mes et maintenez un flux de travail de d√©veloppement stable et fiable."
      },
      {
        "row": 20,
        "rowsha": "IYhTxjMvy72M7T9kEBfrXUNMz08qwsXzETVoE3WSNyQ=",
        "originContent": "- **üõ†Ô∏è Logging and Error Handling**: We understand the importance of smooth operations and easy troubleshooting. That's why llama-github comes equipped with comprehensive logging and error handling mechanisms. Gain deep insights into the library's behavior, quickly diagnose issues, and maintain a stable and reliable development workflow.",
        "translatedContent": ""
      },
      {
        "row": 21,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## ü§ñ Essayez notre assistant de revue de PR aliment√© par IA : LlamaPReview"
      },
      {
        "row": 22,
        "rowsha": "QDeqAvaQEIQHPYjwq2KF5Bw3a76EiPq7ebbsGNKcRI8=",
        "originContent": "## ü§ñ Try Our AI-Powered PR Review Assistant: LlamaPReview",
        "translatedContent": ""
      },
      {
        "row": 23,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "Si vous trouvez llama-github utile, vous pourriez √©galement √™tre int√©ress√© par notre assistant de revue de PR GitHub aliment√© par IA, LlamaPReview. Il est con√ßu pour compl√©ter votre flux de d√©veloppement et am√©liorer encore la qualit√© du code."
      },
      {
        "row": 24,
        "rowsha": "laWUe+J402emMP2V1JeMqFpYefA8HTKWoI+lq7NfoyE=",
        "originContent": "If you find llama-github useful, you might also be interested in our AI-powered GitHub PR review assistant, LlamaPReview. It's designed to complement your development workflow and further enhance code quality.",
        "translatedContent": ""
      },
      {
        "row": 25,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "### Fonctionnalit√©s cl√©s de LlamaPReview :"
      },
      {
        "row": 26,
        "rowsha": "J3M87xxuLQISS/m0fiv7phjnw7mYaznst6rel6n7MyY=",
        "originContent": "### Key Features of LlamaPReview:",
        "translatedContent": "- üöÄ Installation en un clic, aucune configuration requise, fonctionnement enti√®rement automatique"
      },
      {
        "row": 27,
        "rowsha": "dGBd2SxNdlW1UATPSu01Iq95yCVz8fXbl7xxHqDYI7E=",
        "originContent": "- üöÄ One-click installation, zero configuration required, fully auto-run",
        "translatedContent": "- üíØ Actuellement gratuit - pas besoin de carte de cr√©dit ni d‚Äôinformations de paiement"
      },
      {
        "row": 28,
        "rowsha": "EP9hYUs4aYVAzXcYQyCGf3bjwo64OjAsqQHIN75oW60=",
        "originContent": "- üíØ Currently free to use - no credit card or payment info needed",
        "translatedContent": "- üß† Revues automatiques de PR aliment√©es par IA avec une compr√©hension profonde du code"
      },
      {
        "row": 29,
        "rowsha": "jjWF/iUqSzPriHJ5iiFIq0Q6Pzx+gzPlnLu0Z4wmsSg=",
        "originContent": "- üß† AI-powered, automatic PR reviews with deep code understanding",
        "translatedContent": "- üåê Supporte plusieurs langages de programmation"
      },
      {
        "row": 30,
        "rowsha": "Dsl+HsDM5NGI480TDo6eaFqgDcaofr3TDhLbUghSXhU=",
        "originContent": "- üåê Supports multiple programming languages",
        "translatedContent": ""
      },
      {
        "row": 31,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "**LlamaPReview utilise la r√©cup√©ration contextuelle avanc√©e de llama-github et l‚Äôanalyse aliment√©e par LLM** pour fournir des revues de code intelligentes et conscientes du contexte. C‚Äôest comme avoir un d√©veloppeur senior, arm√© du contexte complet de votre d√©p√¥t, qui r√©vise automatiquement chaque PR !"
      },
      {
        "row": 32,
        "rowsha": "GrzJg4j2iz94pupBQwJnFffhmsJ5IO9OkhqYvj0QEmo=",
        "originContent": "**LlamaPReview utilizes llama-github's advanced context retrieval and LLM-powered analysis** to provide intelligent, context-aware code reviews. It's like having a senior developer, armed with the full context of your repository, review every PR automatically!",
        "translatedContent": ""
      },
      {
        "row": 33,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "üëâ [Installer LlamaPReview Maintenant](https://github.com/marketplace/llamapreview/) (Gratuit)"
      },
      {
        "row": 34,
        "rowsha": "4qGUTknerL/UTVikcUjFHvKzZlWho9opuf58fSIypyk=",
        "originContent": "üëâ [Install LlamaPReview Now](https://github.com/marketplace/llamapreview/) (Free)",
        "translatedContent": ""
      },
      {
        "row": 35,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "En utilisant llama-github pour la r√©cup√©ration de contexte et LlamaPReview pour les revues de code, vous pouvez cr√©er un environnement de d√©veloppement puissant et enrichi par l‚ÄôIA."
      },
      {
        "row": 36,
        "rowsha": "luTTMbXm2ikxAkHhFfh2iG9uPEwMYBRd2lolUoOY+3o=",
        "originContent": "By using llama-github for context retrieval and LlamaPReview for code reviews, you can create a powerful, AI-enhanced development environment.",
        "translatedContent": ""
      },
      {
        "row": 37,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## Vision et feuille de route"
      },
      {
        "row": 38,
        "rowsha": "W2b655gztUgECPmTnBIkK/WbBo7d68AbNAjMtkHS5xY=",
        "originContent": "## Vision and Roadmap",
        "translatedContent": ""
      },
      {
        "row": 39,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "### Vision"
      },
      {
        "row": 40,
        "rowsha": "Ewakmoa/Yb8fcDlChA12w7kAXVMFh8OrCLwVCTwIl84=",
        "originContent": "### Vision",
        "translatedContent": ""
      },
      {
        "row": 41,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "Notre vision est de devenir un module cl√© dans l‚Äôavenir des solutions de d√©veloppement pilot√©es par IA, s‚Äôint√©grant parfaitement avec GitHub pour permettre aux LLM de r√©soudre automatiquement des t√¢ches de codage complexes."
      },
      {
        "row": 42,
        "rowsha": "0e6GYQC4W2Y8Ky5Q9UNeMwDc+ebEjDIUb9h4gSLQJaI=",
        "originContent": "Our vision is to become a pivotal module in the future of AI-driven development solutions, seamlessly integrating with GitHub to empower LLMs in automatically resolving complex coding tasks.",
        "translatedContent": ""
      },
      {
        "row": 43,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "![Vision Architecture](https://raw.githubusercontent.com/JetXu-LLM/llama-github/main/./docs/vision.drawio.svg)"
      },
      {
        "row": 44,
        "rowsha": "e3eGMrQR9LxN0lPlezIImRzi4tUu6n/GFfZsq5SBP4I=",
        "originContent": "![Vision Architecture](https://raw.githubusercontent.com/JetXu-LLM/llama-github/main/./docs/vision.drawio.svg)",
        "translatedContent": ""
      },
      {
        "row": 45,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "### Feuille de route"
      },
      {
        "row": 46,
        "rowsha": "Q0D6oxy93TTXOZDvIiuco/ghaytG064tWvZdnLoKnOw=",
        "originContent": "### Roadmap",
        "translatedContent": ""
      },
      {
        "row": 47,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "Pour une vue d√©taill√©e de notre feuille de route projet, veuillez visiter notre [Feuille de route du projet](https://github.com/users/JetXu-LLM/projects/2)."
      },
      {
        "row": 48,
        "rowsha": "UHmL4/lEZk2jitjRxUVuHtD8JilL4OTzFLbU8R72Ymc=",
        "originContent": "For a detailed view of our project roadmap, please visit our [Project Roadmap](https://github.com/users/JetXu-LLM/projects/2).",
        "translatedContent": ""
      },
      {
        "row": 49,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## Remerciements"
      },
      {
        "row": 50,
        "rowsha": "N66ywbuGccMYNTyswaMSnFOlWqX+iaYP/myGJqr1tHo=",
        "originContent": "## Acknowledgments",
        "translatedContent": ""
      },
      {
        "row": 51,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "Nous souhaitons exprimer notre gratitude aux projets open source suivants pour leur soutien et leurs contributions :"
      },
      {
        "row": 52,
        "rowsha": "WodCIp8BqxoCo7bxE7dxagf/Dvpvw7nWeECf0qG6FGI=",
        "originContent": "We would like to express our gratitude to the following open-source projects for their support and contributions:",
        "translatedContent": ""
      },
      {
        "row": 53,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "- **[LangChain](https://github.com/langchain-ai/langchain)** : Pour avoir fourni le cadre fondamental qui alimente les capacit√©s de prompting et de traitement LLM dans llama-github."
      },
      {
        "row": 54,
        "rowsha": "HBIRZKkbsYOXsJY2Xepu1VxTAAdh869qSnsGbJ0cIg0=",
        "originContent": "- **[LangChain](https://github.com/langchain-ai/langchain)**: For providing the foundational framework that empowers the LLM prompting and processing capabilities in llama-github.",
        "translatedContent": "- **[Jina.ai](https://github.com/jina-ai/reader)** : Pour avoir offert l‚ÄôAPI s.jina.ai ainsi que des mod√®les open source de reranking et d‚Äôincorporation qui am√©liorent la pr√©cision et la pertinence des contextes g√©n√©r√©s dans llama-github."
      },
      {
        "row": 55,
        "rowsha": "l7sjWwbh0OUJwk4+yJE2ZbVGypyaPrzI1YYMP7Uw9X0=",
        "originContent": "- **[Jina.ai](https://github.com/jina-ai/reader)**: For offering s.jina.ai API and open source reranker and embedding models that enhance the accuracy and relevance of the generated contexts in llama-github.",
        "translatedContent": ""
      },
      {
        "row": 56,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "Leurs contributions ont √©t√© essentielles au d√©veloppement de llama-github, et nous recommandons vivement de consulter leurs projets pour plus de solutions innovantes."
      },
      {
        "row": 57,
        "rowsha": "hrChp1vDZFs3UZia39niD4Kx9hT0wKUMxSSiiyYyi7A=",
        "originContent": "Their contributions have been instrumental in the development of llama-github, and we highly recommend checking out their projects for more innovative solutions.",
        "translatedContent": ""
      },
      {
        "row": 58,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## Contribution"
      },
      {
        "row": 59,
        "rowsha": "R5ZPLZ4vkE9tjX5qe8QB7AkTfWZsuNTGFLFKMp2KUzM=",
        "originContent": "## Contributing",
        "translatedContent": ""
      },
      {
        "row": 60,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "Nous accueillons les contributions √† llama-github ! Veuillez consulter nos [directives de contribution](https://raw.githubusercontent.com/JetXu-LLM/llama-github/main/CONTRIBUTING.md) pour plus d‚Äôinformations."
      },
      {
        "row": 61,
        "rowsha": "JUWgAoIn9VhOEzmNL5dlAlHwcmo7N2Rm2GNx0g5Rijs=",
        "originContent": "We welcome contributions to llama-github! Please see our [contributing guidelines](https://raw.githubusercontent.com/JetXu-LLM/llama-github/main/CONTRIBUTING.md) for more information.",
        "translatedContent": ""
      },
      {
        "row": 62,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## Licence"
      },
      {
        "row": 63,
        "rowsha": "bFSaVtsB4CHySNjaeCiaMZfT24b+DTbTM4HQ38cR6Lw=",
        "originContent": "## License",
        "translatedContent": ""
      },
      {
        "row": 64,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "Ce projet est sous licence Apache 2.0. Voir le fichier [LICENSE](LICENSE) pour plus de d√©tails."
      },
      {
        "row": 65,
        "rowsha": "hUzQdbczna0Cd3FyH+bhS5SWBDzmVQyA+nCi/UZO6VI=",
        "originContent": "This project is licensed under the terms of the Apache 2.0 license. See the [LICENSE](LICENSE) file for more details.",
        "translatedContent": ""
      },
      {
        "row": 66,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## Contact"
      },
      {
        "row": 67,
        "rowsha": "ZX9nbduzXgvmAaK8yTd+tNgwR7cV79HXV/+xbvL8suw=",
        "originContent": "## Contact",
        "translatedContent": ""
      },
      {
        "row": 68,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "Si vous avez des questions, suggestions ou retours, n‚Äôh√©sitez pas √† nous contacter √† [email de Jet Xu](https://raw.githubusercontent.com/JetXu-LLM/llama-github/main/mailto:Voldemort.xu@foxmail.com)."
      },
      {
        "row": 69,
        "rowsha": "sZud9u8DDdJRsIyc+T0tdusx1FWC0pdv0Yn2hhndP9o=",
        "originContent": "If you have any questions, suggestions, or feedback, please feel free to reach out to us at [Jet Xu's email](https://raw.githubusercontent.com/JetXu-LLM/llama-github/main/mailto:Voldemort.xu@foxmail.com).",
        "translatedContent": ""
      },
      {
        "row": 70,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "---"
      },
      {
        "row": 71,
        "rowsha": "yz+R1U7uMOU+NbK5mQX3Dxae1Un9eJCdPawt78ntjTs=",
        "originContent": "---",
        "translatedContent": ""
      },
      {
        "row": 72,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "Merci d‚Äôavoir choisi llama-github ! Nous esp√©rons que cette biblioth√®que am√©liorera votre exp√©rience de d√©veloppement IA et vous aidera √† cr√©er des applications puissantes en toute simplicit√©."
      },
      {
        "row": 73,
        "rowsha": "98t5imS5RZt8kUxGAXqZcPmlZMcru27Gl/g31hb3g/c=",
        "originContent": "Thank you for choosing llama-github! We hope this library enhances your AI development experience and helps you build powerful applications with ease.",
        "translatedContent": ""
      },
      {
        "row": 74,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  }
]