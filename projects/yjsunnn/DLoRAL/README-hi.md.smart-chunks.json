[
  {
    "Id": 1,
    "Content": "<div align=\"center\">\n<h2>One-Step Diffusion for Detail-Rich and Temporally Consistent Video Super-Resolution</h2>\n\n[Yujing Sun](https://yjsunnn.github.io/)<sup>1,2, *</sup> | \n[Lingchen Sun](https://scholar.google.com/citations?hl=zh-CN&tzom=-480&user=ZCDjTn8AAAAJ)<sup>1,2, *</sup> | \n[Shuaizheng Liu](https://scholar.google.com/citations?user=wzdCc-QAAAAJ&hl=en)<sup>1,2</sup> | \n[Rongyuan Wu](https://scholar.google.com/citations?user=A-U8zE8AAAAJ&hl=zh-CN)<sup>1,2</sup> | \n[Zhengqiang Zhang](https://scholar.google.com.tw/citations?user=UX26wSMAAAAJ&hl=en)<sup>1,2</sup> | \n[Lei Zhang](https://www4.comp.polyu.edu.hk/~cslzhang)<sup>1,2</sup>\n\n<sup>1</sup>The Hong Kong Polytechnic University, <sup>2</sup>OPPO Research Institute\n</div>\n\n<div>\n    <h4 align=\"center\">\n        <a href=\"https://yjsunnn.github.io/DLoRAL-project/\" target='_blank'>\n        <img src=\"https://img.shields.io/badge/ЁЯТб-Project%20Page-gold\">\n        </a>\n        <a href=\"https://arxiv.org/pdf/2506.15591\" target='_blank'>\n        <img src=\"https://img.shields.io/badge/arXiv-2312.06640-b31b1b.svg\">\n        </a>\n        <a href=\"https://www.youtube.com/embed/Jsk8zSE3U-w?si=jz1Isdzxt_NqqDFL&vq=hd1080\" target='_blank'>\n        <img src=\"https://img.shields.io/badge/Demo%20Video-%23FF0000.svg?logo=YouTube&logoColor=white\">\n        </a>\n        <a href=\"https://www.youtube.com/embed/xzZL8X10_KU?si=vOB3chIa7Zo0l54v\" target=\"_blank\">\n        <img src=\"https://img.shields.io/badge/2--Min%20Explainer-brightgreen?logo=YouTube&logoColor=white\">\n        </a>\n        </a>\n        <a href=\"https://github.com/yjsunnn/Awesome-video-super-resolution-diffusion\" target=\"_blank\">\n        <img src=\"https://img.shields.io/badge/GitHub-Awesome--VSR--Diffusion-181717.svg?logo=github&logoColor=white\">\n        </a>\n<!--         <a href=\"https://www.youtube.com/embed/Jsk8zSE3U-w?si=jz1Isdzxt_NqqDFL&vq=hd1080\" target='_blank'>\n        <img src=\"https://img.shields.io/badge/1--Min%20Algorithm%20Explainer-%23FF0000.svg?logo=YouTube&logoColor=white\">\n        </a> -->\n        <a href=\"https://github.com/yjsunnn/DLoRAL\" target='_blank' style=\"text-decoration: none;\"><img src=\"https://visitor-badge.laobi.icu/badge?page_id=yjsunnn/DLoRAL\"></a>\n    </h4>\n</div>\n\n<p align=\"center\">\n\n<img src=\"https://raw.githubusercontent.com/yjsunnn/DLoRAL/main/assets/visual_results.svg\" alt=\"Visual Results\">\n\n</p>\n",
    "ContentSha": "bVvVxNHO5h3vkATLAmVTtzDCag2JgKRyJH13Z0uT2Dw=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "<div align=\"center\">\n<h2>рдбрд┐рдЯреЗрд▓-рд░рд┐рдЪ рдФрд░ рдЯреЗрдореНрдкреЛрд░рд▓реА рдХрдВрд╕рд┐рд╕реНрдЯреЗрдВрдЯ рд╡реАрдбрд┐рдпреЛ рд╕реБрдкрд░-рд░рд┐рдЬрд╝реЙрд▓реНрдпреВрд╢рди рдХреЗ рд▓рд┐рдП рд╡рди-рд╕реНрдЯреЗрдк рдбрд┐рдлреНрдпреВрдЬрди</h2>\n\n[Yujing Sun](https://yjsunnn.github.io/)<sup>1,2, *</sup> | \n[Lingchen Sun](https://scholar.google.com/citations?hl=zh-CN&tzom=-480&user=ZCDjTn8AAAAJ)<sup>1,2, *</sup> | \n[Shuaizheng Liu](https://scholar.google.com/citations?user=wzdCc-QAAAAJ&hl=en)<sup>1,2</sup> | \n[Rongyuan Wu](https://scholar.google.com/citations?user=A-U8zE8AAAAJ&hl=zh-CN)<sup>1,2</sup> | \n[Zhengqiang Zhang](https://scholar.google.com.tw/citations?user=UX26wSMAAAAJ&hl=en)<sup>1,2</sup> | \n[Lei Zhang](https://www4.comp.polyu.edu.hk/~cslzhang)<sup>1,2</sup>\n\n<sup>1</sup>рдж рд╣реЙрдиреНрдЧ рдХреЙрдиреНрдЧ рдкреЙрд▓рд┐рдЯреЗрдХреНрдирд┐рдХ рдпреВрдирд┐рд╡рд░реНрд╕рд┐рдЯреА, <sup>2</sup>рдУрдкреНрдкреЛ рд░рд┐рд╕рд░реНрдЪ рдЗрдВрд╕реНрдЯреАрдЯреНрдпреВрдЯ\n</div>\n\n<div>\n    <h4 align=\"center\">\n        <a href=\"https://yjsunnn.github.io/DLoRAL-project/\" target='_blank'>\n        <img src=\"https://img.shields.io/badge/ЁЯТб-Project%20Page-gold\">\n        </a>\n        <a href=\"https://arxiv.org/pdf/2506.15591\" target='_blank'>\n        <img src=\"https://img.shields.io/badge/arXiv-2312.06640-b31b1b.svg\">\n        </a>\n        <a href=\"https://www.youtube.com/embed/Jsk8zSE3U-w?si=jz1Isdzxt_NqqDFL&vq=hd1080\" target='_blank'>\n        <img src=\"https://img.shields.io/badge/Demo%20Video-%23FF0000.svg?logo=YouTube&logoColor=white\">\n        </a>\n        <a href=\"https://www.youtube.com/embed/xzZL8X10_KU?si=vOB3chIa7Zo0l54v\" target=\"_blank\">\n        <img src=\"https://img.shields.io/badge/2--Min%20Explainer-brightgreen?logo=YouTube&logoColor=white\">\n        </a>\n        </a>\n        <a href=\"https://github.com/yjsunnn/Awesome-video-super-resolution-diffusion\" target=\"_blank\">\n        <img src=\"https://img.shields.io/badge/GitHub-Awesome--VSR--Diffusion-181717.svg?logo=github&logoColor=white\">\n        </a>\n<!--         <a href=\"https://www.youtube.com/embed/Jsk8zSE3U-w?si=jz1Isdzxt_NqqDFL&vq=hd1080\" target='_blank'>\n        <img src=\"https://img.shields.io/badge/1--Min%20Algorithm%20Explainer-%23FF0000.svg?logo=YouTube&logoColor=white\">\n        </a> -->\n        <a href=\"https://github.com/yjsunnn/DLoRAL\" target='_blank' style=\"text-decoration: none;\"><img src=\"https://visitor-badge.laobi.icu/badge?page_id=yjsunnn/DLoRAL\"></a>\n    </h4>\n</div>\n\n<p align=\"center\">\n\n<img src=\"https://raw.githubusercontent.com/yjsunnn/DLoRAL/main/assets/visual_results.svg\" alt=\"рд╡рд┐рдЬрд╝реБрдЕрд▓ рд░рд┐рдЬрд▓реНрдЯреНрд╕\">\n\n</p>",
    "Status": "ok"
  },
  {
    "Id": 2,
    "Content": "## тП░ Update\n\n- **2025.07.08**: The inference code and pretrained weights are available.\n- **2025.06.24**: The project page is available, including a brief 2-minute explanation video, more visual results and relevant researches.\n- **2025.06.17**: The repo is released.\n\n:star: If DLoRAL is helpful to your videos or projects, please help star this repo. Thanks! :hugs:\n\nЁЯШК You may also want to check our relevant works:\n\n1. **OSEDiff (NIPS2024)** [Paper](https://arxiv.org/abs/2406.08177) | [Code](https://github.com/cswry/OSEDiff/)  \n\n   Real-time Image SR algorithm that has been applied to the OPPO Find X8 series.\n\n2. **PiSA-SR (CVPR2025)** [Paper](https://arxiv.org/pdf/2412.03017) | [Code](https://github.com/csslc/PiSA-SR) \n\n   Pioneering exploration of Dual-LoRA paradigm in Image SR.\n\n3. **Awesome Diffusion Models for Video Super-Resolution** [Repo](https://github.com/yjsunnn/Awesome-video-super-resolution-diffusion)\n\n   A curated list of resources for Video Super-Resolution (VSR) using Diffusion Models.\n",
    "ContentSha": "dfxVaVa+I0eZ9HnE6Yp/PsWOOtEK1Z+QwdmjxrtNNfg=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## тП░ рдЕрдкрдбреЗрдЯ\n\n- **2025.07.08**: рдЗрдирдлреЗрд░реЗрдВрд╕ рдХреЛрдб рдФрд░ рдкреНрд░реА-рдЯреНрд░реЗрдВрдб рд╡реЗрдЯреНрд╕ рдЙрдкрд▓рдмреНрдз рд╣реИрдВред\n- **2025.06.24**: рдкреНрд░реЛрдЬреЗрдХреНрдЯ рдкреЗрдЬ рдЙрдкрд▓рдмреНрдз рд╣реИ, рдЬрд┐рд╕рдореЗрдВ рдПрдХ рд╕рдВрдХреНрд╖рд┐рдкреНрдд 2-рдорд┐рдирдЯ рдХрд╛ рд╡реНрдпрд╛рдЦреНрдпрд╛рддреНрдордХ рд╡реАрдбрд┐рдпреЛ, рдЕрдзрд┐рдХ рд╡рд┐рдЬреБрдЕрд▓ рдкрд░рд┐рдгрд╛рдо рдФрд░ рд╕рдВрдмрдВрдзрд┐рдд рд╢реЛрдз рд╢рд╛рдорд┐рд▓ рд╣реИрдВред\n- **2025.06.17**: рд░рд┐рдкреЙрдЬрд┐рдЯрд░реА рдЬрд╛рд░реА рдХрд░ рджреА рдЧрдИ рд╣реИред\n\n:star: рдпрджрд┐ DLoRAL рдЖрдкрдХреЗ рд╡реАрдбрд┐рдпреЛ рдпрд╛ рдкреНрд░реЛрдЬреЗрдХреНрдЯреНрд╕ рдХреЗ рд▓рд┐рдП рдЙрдкрдпреЛрдЧреА рд╣реИ, рддреЛ рдХреГрдкрдпрд╛ рдЗрд╕ рд░рд┐рдкреЛ рдХреЛ рд╕реНрдЯрд╛рд░ рдХрд░реЗрдВред рдзрдиреНрдпрд╡рд╛рдж! :hugs:\n\nЁЯШК рдЖрдк рд╣рдорд╛рд░реЗ рд╕рдВрдмрдВрдзрд┐рдд рдХрд╛рд░реНрдп рднреА рджреЗрдЦ рд╕рдХрддреЗ рд╣реИрдВ:\n\n1. **OSEDiff (NIPS2024)** [рдкреЗрдкрд░](https://arxiv.org/abs/2406.08177) | [рдХреЛрдб](https://github.com/cswry/OSEDiff/)  \n\n   рд░рд┐рдпрд▓-рдЯрд╛рдЗрдо рдЗрдореЗрдЬ рдПрд╕рдЖрд░ рдПрд▓реНрдЧреЛрд░рд┐рджрдо рдЬрд┐рд╕реЗ OPPO Find X8 рд╕реАрд░реАрдЬ рдореЗрдВ рд▓рд╛рдЧреВ рдХрд┐рдпрд╛ рдЧрдпрд╛ рд╣реИред\n\n2. **PiSA-SR (CVPR2025)** [рдкреЗрдкрд░](https://arxiv.org/pdf/2412.03017) | [рдХреЛрдб](https://github.com/csslc/PiSA-SR) \n\n   рдЗрдореЗрдЬ рдПрд╕рдЖрд░ рдореЗрдВ рдбреНрдпреВрд▓-рд▓реЛрд░рд╛ рдкреИрд░реЗрдбрд╛рдЗрдо рдХреА рдЕрдЧреНрд░рдгреА рдЦреЛрдЬред\n\n3. **Awesome Diffusion Models for Video Super-Resolution** [рд░рд┐рдкреЛ](https://github.com/yjsunnn/Awesome-video-super-resolution-diffusion)\n\n   рдбрд┐рдлреНрдпреВрдЬрди рдореЙрдбрд▓реНрд╕ рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рдХреЗ рд╡реАрдбрд┐рдпреЛ рд╕реБрдкрд░-рд░рд┐рдЬрд╝реЙрд▓реНрдпреВрд╢рди (VSR) рдХреЗ рд▓рд┐рдП рд╕рдВрд╕рд╛рдзрдиреЛрдВ рдХреА рдПрдХ рдХреНрдпреВрд░реЗрдЯреЗрдб рд╕реВрдЪреАред",
    "Status": "ok"
  },
  {
    "Id": 3,
    "Content": "## ЁЯСА TODO\n- [x] Release inference code.\n- [ ] Colab and Huggingface UI for convenient test (Soon!).\n- [ ] Release training code.\n- [ ] Release training data.\n\n\n## ЁЯМЯ Overview Framework\n\n<p align=\"center\">\n\n<img src=\"https://raw.githubusercontent.com/yjsunnn/DLoRAL/main/assets/pipeline.svg\" alt=\"DLoRAL Framework\">\n\n</p>\n\n**Training**: A dynamic dual-stage training scheme alternates between optimizing temporal coherence (consistency stage) and refining high-frequency spatial details (enhancement stage) with smooth loss interpolation to ensure stability.\n\n**Inference**: During inference, both C-LoRA and D-LoRA are merged into the frozen diffusion UNet, enabling one-step enhancement of low-quality inputs into high-quality outputs.\n\n",
    "ContentSha": "ZGelK9ipNT81XA+Ow1at5yuSzYjWCpL6tnO6FOf9+7I=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## ЁЯСА TODO\n- [x] рдЗрдирдлреЗрд░реЗрдВрд╕ рдХреЛрдб рдЬрд╛рд░реА рдХрд░реЗрдВред\n- [ ] рд╕реБрд╡рд┐рдзрд╛рдЬрдирдХ рдкрд░реАрдХреНрд╖рдг рдХреЗ рд▓рд┐рдП рдХреЛрд▓реИрдм рдФрд░ рд╣рдЧрд┐рдВрдЧрдлреЗрд╕ UI (рдЬрд▓реНрдж рдЖ рд░рд╣рд╛ рд╣реИ!).\n- [ ] рдкреНрд░рд╢рд┐рдХреНрд╖рдг рдХреЛрдб рдЬрд╛рд░реА рдХрд░реЗрдВред\n- [ ] рдкреНрд░рд╢рд┐рдХреНрд╖рдг рдбреЗрдЯрд╛ рдЬрд╛рд░реА рдХрд░реЗрдВред\n\n\n## ЁЯМЯ рдЕрд╡рд▓реЛрдХрди рдлреНрд░реЗрдорд╡рд░реНрдХ\n\n<p align=\"center\">\n\n<img src=\"https://raw.githubusercontent.com/yjsunnn/DLoRAL/main/assets/pipeline.svg\" alt=\"DLoRAL Framework\">\n\n</p>\n\n**рдкреНрд░рд╢рд┐рдХреНрд╖рдг**: рдПрдХ рдбрд╛рдпрдиреЗрдорд┐рдХ рдбреНрдпреВрд▓-рд╕реНрдЯреЗрдЬ рдкреНрд░рд╢рд┐рдХреНрд╖рдг рдпреЛрдЬрдирд╛, рдЯреЗрдореНрдкреЛрд░рд▓ рдХреЛрд╣реЗрд░реЗрдВрд╕ (рдХрдВрд╕рд┐рд╕реНрдЯреЗрдВрд╕реА рд╕реНрдЯреЗрдЬ) рдХреЛ рдСрдкреНрдЯрд┐рдорд╛рдЗрдЬрд╝ рдХрд░рдиреЗ рдФрд░ рдЙрдЪреНрдЪ-рдЖрд╡реГрддреНрддрд┐ рд╕реНрдкреИрдЯрд┐рдпрд▓ рдбрд┐рдЯреЗрд▓реНрд╕ (рдПрдирд╣рд╛рдВрд╕рдореЗрдВрдЯ рд╕реНрдЯреЗрдЬ) рдХреЛ рдкрд░рд┐рд╖реНрдХреГрдд рдХрд░рдиреЗ рдХреЗ рдмреАрдЪ рдмрд╛рд░реА-рдмрд╛рд░реА рд╕реЗ рдХрд╛рдо рдХрд░рддреА рд╣реИ, рдЬрд┐рд╕рд╕реЗ рд╕реНрдерд┐рд░рддрд╛ рд╕реБрдирд┐рд╢реНрдЪрд┐рдд рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рд╕реНрдореВрде рд▓реЙрд╕ рдЗрдВрдЯрд░рдкреЛрд▓реЗрд╢рди рд╣реЛрддрд╛ рд╣реИред\n\n**рдЗрдирдлреЗрд░реЗрдВрд╕**: рдЗрдирдлреЗрд░реЗрдВрд╕ рдХреЗ рджреМрд░рд╛рди, C-LoRA рдФрд░ D-LoRA рджреЛрдиреЛрдВ рдХреЛ рдлреНрд░реАрдЬрд╝ рдХрд┐рдП рдЧрдП рдбрд┐рдлреНрдпреВрдЬрди UNet рдореЗрдВ рдорд░реНрдЬ рдХрд░ рджрд┐рдпрд╛ рдЬрд╛рддрд╛ рд╣реИ, рдЬрд┐рд╕рд╕реЗ рдХрдо рдЧреБрдгрд╡рддреНрддрд╛ рд╡рд╛рд▓реЗ рдЗрдирдкреБрдЯреНрд╕ рдХреЛ рдЙрдЪреНрдЪ рдЧреБрдгрд╡рддреНрддрд╛ рд╡рд╛рд▓реЗ рдЖрдЙрдЯрдкреБрдЯреНрд╕ рдореЗрдВ рдПрдХ рд╣реА рдЪрд░рдг рдореЗрдВ рд╕реБрдзрд╛рд░рд╛ рдЬрд╛ рд╕рдХрддрд╛ рд╣реИред",
    "Status": "ok"
  },
  {
    "Id": 4,
    "Content": "## ЁЯФз Dependencies and Installation\n\n1. Clone repo\n    ```bash\n    git clone https://github.com/yjsunnn/DLoRAL.git\n    cd DLoRAL\n    ```\n\n2. Install dependent packages\n    ```bash\n    conda create -n DLoRAL python=3.10 -y\n    conda activate DLoRAL\n    pip install -r requirements.txt\n    ```\n\n3. Download Models \n#### Dependent Models\n* [SD21 Base](https://huggingface.co/stabilityai/stable-diffusion-2-1-base) --> put into **/path/to/DLoRAL/preset_models/stable-diffusion-2-1-base**\n* [Bert-Base](https://huggingface.co/google-bert/bert-base-uncased) --> put into **/path/to/DLoRAL/preset_models/bert-base-uncased**\n* [RAM](https://huggingface.co/spaces/xinyu1205/recognize-anything/blob/main/ram_swin_large_14m.pth) --> put into **/path/to/DLoRAL/preset/models/ram_swin_large_14m.pth**\n* [DAPE](https://drive.google.com/file/d/1KIV6VewwO2eDC9g4Gcvgm-a0LDI7Lmwm/view?usp=drive_link) --> put into **/path/to/DLoRAL/preset/models/DAPE.pth**\n* [Pretrained Weights](https://drive.google.com/file/d/1vpcaySpRx_K-tXq2D2EBqFZ-03Foky8G/view?usp=sharing) --> put into **/path/to/DLoRAL/preset/models/checkpoints/model.pkl**\n\nEach path can be modified according to its own requirements, and the corresponding changes should also be applied to the command line and the code.\n",
    "ContentSha": "vvwBfkJn4SJeRBMzLa2/Uod2dvEBbBZ+Waj2PNmgQek=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## ЁЯФз рдбрд┐рдкреЗрдВрдбреЗрдВрд╕реАрдЬрд╝ рдФрд░ рдЗрдВрд╕реНрдЯреЙрд▓реЗрд╢рди\n\n1. рд░реЗрдкреЛ рдХреНрд▓реЛрди рдХрд░реЗрдВ\n    ```bash\n    git clone https://github.com/yjsunnn/DLoRAL.git\n    cd DLoRAL\n    ```\n\n2. рдбрд┐рдкреЗрдВрдбреЗрдВрдЯ рдкреИрдХреЗрдЬ рдЗрдВрд╕реНрдЯреЙрд▓ рдХрд░реЗрдВ\n    ```bash\n    conda create -n DLoRAL python=3.10 -y\n    conda activate DLoRAL\n    pip install -r requirements.txt\n    ```\n\n3. рдореЙрдбрд▓ рдбрд╛рдЙрдирд▓реЛрдб рдХрд░реЗрдВ \n#### рдбрд┐рдкреЗрдВрдбреЗрдВрдЯ рдореЙрдбрд▓реНрд╕\n* [SD21 Base](https://huggingface.co/stabilityai/stable-diffusion-2-1-base) --> **/path/to/DLoRAL/preset_models/stable-diffusion-2-1-base** рдореЗрдВ рд░рдЦреЗрдВ\n* [Bert-Base](https://huggingface.co/google-bert/bert-base-uncased) --> **/path/to/DLoRAL/preset_models/bert-base-uncased** рдореЗрдВ рд░рдЦреЗрдВ\n* [RAM](https://huggingface.co/spaces/xinyu1205/recognize-anything/blob/main/ram_swin_large_14m.pth) --> **/path/to/DLoRAL/preset/models/ram_swin_large_14m.pth** рдореЗрдВ рд░рдЦреЗрдВ\n* [DAPE](https://drive.google.com/file/d/1KIV6VewwO2eDC9g4Gcvgm-a0LDI7Lmwm/view?usp=drive_link) --> **/path/to/DLoRAL/preset/models/DAPE.pth** рдореЗрдВ рд░рдЦреЗрдВ\n* [Pretrained Weights](https://drive.google.com/file/d/1vpcaySpRx_K-tXq2D2EBqFZ-03Foky8G/view?usp=sharing) --> **/path/to/DLoRAL/preset/models/checkpoints/model.pkl** рдореЗрдВ рд░рдЦреЗрдВ\n\nрдкреНрд░рддреНрдпреЗрдХ рдкрде рдХреЛ рдЕрдкрдиреА рдЖрд╡рд╢реНрдпрдХрддрд╛рдУрдВ рдХреЗ рдЕрдиреБрд╕рд╛рд░ рдмрджрд▓рд╛ рдЬрд╛ рд╕рдХрддрд╛ рд╣реИ, рдФрд░ рдХрдорд╛рдВрдб рд▓рд╛рдЗрди рддрдерд╛ рдХреЛрдб рдореЗрдВ рднреА рдЙрдкрдпреБрдХреНрдд рдкрд░рд┐рд╡рд░реНрддрди рдХрд┐рдП рдЬрд╛рдиреЗ рдЪрд╛рд╣рд┐рдПред",
    "Status": "ok"
  },
  {
    "Id": 5,
    "Content": "## ЁЯЦ╝я╕П Quick Inference\nFor Real-World Video Super-Resolution:\n\n```\npython src/test_DLoRAL.py     \\\n--pretrained_model_path /path/to/stable-diffusion-2-1-base     \\\n--ram_ft_path /path/to/DAPE.pth     \\\n--ram_path '/path/to/ram_swin_large_14m.pth'     \\\n--merge_and_unload_lora False     \\\n--process_size 512     \\\n--pretrained_model_name_or_path '/path/to/stable-diffusion-2-1-base'     \\\n--vae_encoder_tiled_size 4096     \\\n--load_cfr     \\\n--pretrained_path /path/to/model_checkpoint.pkl     \\\n--stages 1     \\\n-i /path/to/input_videos/     \\\n-o /path/to/results\n```\n\n\n",
    "ContentSha": "ktj+podNwub3lDyiQLZjTmjqDb9M6mdrGxjfY/EiWXU=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## ЁЯЦ╝я╕П рддреНрд╡рд░рд┐рдд рдЕрдиреБрдХрд░рдг\nрд╡рд╛рд╕реНрддрд╡рд┐рдХ рджреБрдирд┐рдпрд╛ рдХреЗ рд╡реАрдбрд┐рдпреЛ рд╕реБрдкрд░-рд░рд┐рдЬрд╝реЙрд▓реНрдпреВрд╢рди рдХреЗ рд▓рд┐рдП:\n\n```\npython src/test_DLoRAL.py     \\\n--pretrained_model_path /path/to/stable-diffusion-2-1-base     \\\n--ram_ft_path /path/to/DAPE.pth     \\\n--ram_path '/path/to/ram_swin_large_14m.pth'     \\\n--merge_and_unload_lora False     \\\n--process_size 512     \\\n--pretrained_model_name_or_path '/path/to/stable-diffusion-2-1-base'     \\\n--vae_encoder_tiled_size 4096     \\\n--load_cfr     \\\n--pretrained_path /path/to/model_checkpoint.pkl     \\\n--stages 1     \\\n-i /path/to/input_videos/     \\\n-o /path/to/results\n```",
    "Status": "ok"
  },
  {
    "Id": 6,
    "Content": "### Citations\nIf our code helps your research or work, please consider citing our paper.\nThe following are BibTeX references:\n\n```\n@misc{sun2025onestepdiffusiondetailrichtemporally,\n      title={One-Step Diffusion for Detail-Rich and Temporally Consistent Video Super-Resolution}, \n      author={Yujing Sun and Lingchen Sun and Shuaizheng Liu and Rongyuan Wu and Zhengqiang Zhang and Lei Zhang},\n      year={2025},\n      eprint={2506.15591},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2506.15591}, \n}\n",
    "ContentSha": "8oig3poiN8He5uh3Lud+BRL7mNg6+g0kMUL4Om73m68=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### рдЙрджреНрдзрд░рдг\nрдпрджрд┐ рд╣рдорд╛рд░рд╛ рдХреЛрдб рдЖрдкрдХреЗ рд╢реЛрдз рдпрд╛ рдХрд╛рд░реНрдп рдореЗрдВ рд╕рд╣рд╛рдпрдХ рд╣реИ, рддреЛ рдХреГрдкрдпрд╛ рд╣рдорд╛рд░реЗ рдкреЗрдкрд░ рдХрд╛ рдЙрд▓реНрд▓реЗрдЦ рдХрд░реЗрдВред\nрдирд┐рдореНрдирд▓рд┐рдЦрд┐рдд BibTeX рд╕рдВрджрд░реНрдн рд╣реИрдВ:\n\n```\n@misc{sun2025onestepdiffusiondetailrichtemporally,\n      title={One-Step Diffusion for Detail-Rich and Temporally Consistent Video Super-Resolution}, \n      author={Yujing Sun and Lingchen Sun and Shuaizheng Liu and Rongyuan Wu and Zhengqiang Zhang and Lei Zhang},\n      year={2025},\n      eprint={2506.15591},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2506.15591}, \n}\n```",
    "Status": "ok"
  }
]