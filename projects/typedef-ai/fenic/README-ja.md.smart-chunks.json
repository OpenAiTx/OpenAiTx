[
  {
    "Id": 1,
    "Content": "<!-- markdownlint-disable MD041 MD033 -->\n<div align=\"center\">\n    <picture>\n        <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/typedef-fenic-logo-dark.png\">\n        <img src=\"https://raw.githubusercontent.com/typedef-ai/fenic/main/docs/images/typedef-fenic-logo.png\" alt=\"fenic, by typedef\" width=\"90%\">\n    </picture>\n</div>\n\n# fenic: the dataframe (re)built for LLM inference\n\n[![PyPI version](https://img.shields.io/pypi/v/fenic.svg)](https://pypi.org/project/fenic/)\n[![Python versions](https://img.shields.io/pypi/pyversions/fenic.svg)](https://pypi.org/project/fenic/)\n[![License](https://img.shields.io/github/license/typedef-ai/fenic.svg)](https://github.com/typedef-ai/fenic/blob/main/LICENSE)\n[![Discord](https://img.shields.io/discord/1381706122322513952?label=Discord&logo=discord)](https://discord.gg/GdqF3J7huR)\n\n---\n\n## **Documentation**: [docs.fenic.ai](https://docs.fenic.ai/)\n\nfenic is an opinionated, PySpark-inspired DataFrame framework from typedef.ai for building AI and agentic applications. Transform unstructured and structured data into insights using familiar DataFrame operations enhanced with semantic intelligence. With first-class support for markdown, transcripts, and semantic operators, plus efficient batch inference across any model provider.\n",
    "ContentSha": "tmub1AxUb0Y4Al5Ltz67crbP453GMVUntyWUISmU7NQ=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "",
    "Status": "error"
  },
  {
    "Id": 2,
    "Content": "## Install\n\nfenic supports Python `[3.10, 3.11, 3.12]`\n\n```bash\npip install fenic\n```\n\n### LLM Provider Setup\n\nfenic requires an API key from at least one LLM provider. Set the appropriate environment variable for your chosen provider:\n\n```bash\n# For OpenAI\nexport OPENAI_API_KEY=\"your-openai-api-key\"\n\n# For Anthropic\nexport ANTHROPIC_API_KEY=\"your-anthropic-api-key\"\n\n# For Google",
    "ContentSha": "QT2GWoHGnyfwnJxlfZAk0I5rpIsEfvGEhJD38oz6lCY=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## インストール\n\nfenic は Python `[3.10, 3.11, 3.12]` をサポートしています\n\n```bash\npip install fenic\n```\n\n### LLM プロバイダーのセットアップ\n\nfenic では、少なくとも 1 つの LLM プロバイダーの API キーが必要です。選択したプロバイダーに応じて、適切な環境変数を設定してください:\n\n```bash\n# OpenAI の場合\nexport OPENAI_API_KEY=\"your-openai-api-key\"\n\n# Anthropic の場合\nexport ANTHROPIC_API_KEY=\"your-anthropic-api-key\"\n\n# Google の場合",
    "Status": "ok"
  },
  {
    "Id": 3,
    "Content": "export GEMINI_API_KEY=\"your-google-api-key\"\n```\n\n## Quickstart\n\nThe fastest way to learn about fenic is by checking the examples.\n\nBelow is a quick list of the examples in this repo:\n\n| Example                                                                 | Description                                                                                                                         |\n| ----------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------- |\n| [Hello World!](examples/hello_world)                                    | Introduction to semantic extraction and classification using fenic's core operators through error log analysis.                     |\n| [Enrichment](examples/enrichment)                                       | Multi-stage DataFrames with template-based text extraction, joins, and LLM-powered transformations demonstrated via log enrichment. |\n| [Meeting Transcript Processing](examples/meeting_transcript_processing) | Native transcript parsing, Pydantic schema integration, and complex aggregations shown through meeting analysis.                    |\n| [News Analysis](examples/news_analysis)                                 | Analyze and extract insights from news articles using semantic operators and structured data processing.                            |\n| [Podcast Summarization](examples/podcast_summarization)                 | Process and summarize podcast transcripts with speaker-aware analysis and key point extraction.                                     |\n| [Semantic Join](examples/semantic_joins)                                | Instead of simple fuzzy matching, use fenic's powerful semantic join functionality to match data across tables.                     |\n| [Named Entity Recognition](examples/named_entity_recognition)           | Extract and classify named entities from text using semantic extraction and classification.                                         |\n| [Markdown Processing](examples/markdown_processing)                     | Process and transform markdown documents with structured data extraction and formatting.                                            |\n| [JSON Processing](examples/json_processing)                             | Handle complex JSON data structures with semantic operations and schema validation.                                                 |\n| [Feedback Clustering](examples/feedback_clustering)                     | Group and analyze feedback using semantic similarity and clustering operations.                                                     |\n| [Document Extraction](examples/document_extraction)                     | Extract structured information from various document formats using semantic operators.                                              |\n\n(Feel free to click any example above to jump right to its folder.)\n",
    "ContentSha": "RBvJ+qAaDZODC8rvu1sFp9QTE4SUhSKTTu/91HAxBRc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```\nexport GEMINI_API_KEY=\"your-google-api-key\"\n```\n\n## クイックスタート\n\nfenicを素早く学ぶ最良の方法は、サンプル例を確認することです。\n\n以下は、このリポジトリに含まれているサンプル例の一覧です。\n\n| サンプル例                                                                  | 説明                                                                                                                              |\n| --------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------- |\n| [Hello World!](examples/hello_world)                                        | エラーログ分析を通じて、fenicのコアオペレーターによるセマンティック抽出および分類の導入。                                        |\n| [Enrichment](examples/enrichment)                                           | テンプレートベースのテキスト抽出、結合、LLMによる変換を用いた多段階DataFrameの実例をログエンリッチメントで紹介。                   |\n| [Meeting Transcript Processing](examples/meeting_transcript_processing)     | ネイティブなトランスクリプト解析、Pydanticスキーマ連携、複雑な集計処理を会議分析を通じて紹介。                                    |\n| [News Analysis](examples/news_analysis)                                     | セマンティックオペレーターや構造化データ処理を用いてニュース記事からインサイトを分析・抽出。                                       |\n| [Podcast Summarization](examples/podcast_summarization)                     | 話者情報を考慮した分析や主要ポイント抽出を通じて、ポッドキャストのトランスクリプトを処理・要約。                                   |\n| [Semantic Join](examples/semantic_joins)                                    | 単純な曖昧一致の代わりに、fenicの強力なセマンティック結合機能を使ってテーブル間データをマッチング。                                |\n| [Named Entity Recognition](examples/named_entity_recognition)               | セマンティック抽出と分類を用いてテキストから固有表現を抽出・分類。                                                               |\n| [Markdown Processing](examples/markdown_processing)                         | 構造化データ抽出とフォーマット変換により、Markdownドキュメントを処理・変換。                                                     |\n| [JSON Processing](examples/json_processing)                                 | セマンティック操作とスキーマ検証により、複雑なJSONデータ構造を処理。                                                             |\n| [Feedback Clustering](examples/feedback_clustering)                         | セマンティック類似度とクラスタリング操作を利用してフィードバックをグループ化・分析。                                               |\n| [Document Extraction](examples/document_extraction)                         | セマンティックオペレーターを用いて様々なドキュメント形式から構造化情報を抽出。                                                   |\n\n（上記のいずれかのサンプル例をクリックすると、該当フォルダへジャンプします。）\n",
    "Status": "ok"
  },
  {
    "Id": 4,
    "Content": "## Why use fenic?\n\nfenic is an opinionated, PySpark-inspired DataFrame framework for building production AI and agentic applications.\n\nUnlike traditional data tools retrofitted for LLMs, fenic's query engine is built from the ground up with inference in mind.\n\nTransform structured and unstructured data into insights using familiar DataFrame operations enhanced with semantic intelligence. With first-class support for markdown, transcripts, and semantic operators, plus efficient batch inference across any model provider.\n\nfenic brings the reliability of traditional data pipelines to AI workloads.\n\n### Key Features\n\n#### Purpose-Built for LLM Inference\n\n- Query engine designed from scratch for AI workloads, not retrofitted\n- Automatic batch optimization for API calls\n- Built-in retry logic and rate limiting\n- Token counting and cost tracking\n\n#### Semantic Operators as First-Class Citizens",
    "ContentSha": "POPTpr2d3zFT3V0X0/NRjmMCUTjlIPBHtMTVHIRsyfM=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## なぜfenicを使うのか？\n\nfenicは、意見を持った、PySparkにインスパイアされたDataFrameフレームワークであり、プロダクションAIおよびエージェントアプリケーションの構築のために設計されています。\n\n従来のLLM向けに後付けされたデータツールとは異なり、fenicのクエリエンジンは推論を念頭にゼロから構築されています。\n\n構造化データと非構造化データを、意味的インテリジェンスで強化された使い慣れたDataFrame操作を用いてインサイトに変換できます。Markdown、トランスクリプト、セマンティックオペレータへのファーストクラスサポート、さらにあらゆるモデルプロバイダーを横断した効率的なバッチ推論が可能です。\n\nfenicは、従来のデータパイプラインの信頼性をAIワークロードにもたらします。\n\n### 主な機能\n\n#### LLM推論のための専用設計\n\n- AIワークロード向けにゼロから設計されたクエリエンジン（後付けではありません）\n- APIコールの自動バッチ最適化\n- 組み込みのリトライロジックとレート制限\n- トークンカウントとコストトラッキング\n\n#### セマンティックオペレータをファーストクラス市民として",
    "Status": "ok"
  },
  {
    "Id": 5,
    "Content": "\n- `semantic.analyze_sentiment` - Built-in sentiment analysis\n- `semantic.classify` - Categorize text with few-shot examples\n- `semantic.extract` - Transform unstructured text into structured data with schemas\n- `semantic.group_by` - Group data by semantic similarity\n- `semantic.join` - Join DataFrames on meaning, not just values\n- `semantic.map` - Apply natural language transformations\n- `semantic.predicate` - Create predicates using natural language to filter rows\n- `semantic.reduce` - Aggregate grouped data with LLM operations\n\n#### Native Unstructured Data Support\n\nGoes beyond typical multimodal data types (audio, images) by creating specialized types for text-heavy workloads:\n\n- Markdown parsing and extraction as a first-class data type\n- Transcript processing (SRT, generic formats) with speaker and timestamp awareness\n- JSON manipulation with JQ expressions for nested data\n- Automatic text chunking with configurable overlap for long documents\n\n#### Production-Ready Infrastructure",
    "ContentSha": "NE5dPjdhTPxhAD1E+gcEg2tw/wEtQVkEuw1AGA1YwCQ=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "- `semantic.analyze_sentiment` - 組み込みの感情分析\n- `semantic.classify` - 少数の例でテキストを分類\n- `semantic.extract` - 非構造化テキストをスキーマで構造化データに変換\n- `semantic.group_by` - セマンティックな類似性でデータをグループ化\n- `semantic.join` - 値だけでなく意味でDataFrameを結合\n- `semantic.map` - 自然言語変換を適用\n- `semantic.predicate` - 自然言語で述語を作成し、行をフィルタリング\n- `semantic.reduce` - LLM操作でグループ化したデータを集計\n\n#### ネイティブな非構造化データサポート\n\n一般的なマルチモーダルデータ型（音声、画像）を超え、テキスト中心のワークロード向けに特化した型を作成：\n\n- Markdownのパースと抽出を第一級データ型としてサポート\n- スピーカーとタイムスタンプに対応したトランスクリプト処理（SRT、汎用フォーマット）\n- ネストされたデータ向けのJQ式によるJSON操作\n- 長文ドキュメントに対する設定可能なオーバーラップ付き自動テキスト分割\n\n#### 本番環境対応インフラストラクチャ",
    "Status": "ok"
  },
  {
    "Id": 6,
    "Content": "\n- Multi-provider support (OpenAI, Anthropic, Gemini)\n- Local and cloud execution backends\n- Comprehensive error handling and logging\n- Pydantic integration for type safety\n\n#### Familiar DataFrame API\n\n- PySpark-compatible operations\n- Lazy evaluation and query optimization\n- SQL support for complex queries\n- Seamless integration with existing data pipelines\n\n### Why DataFrames for LLM and Agentic Applications?\n\nAI and agentic applications are fundamentally pipelines and workflows - exactly what DataFrame APIs were designed to handle. Rather than reinventing patterns for data transformation, filtering, and aggregation, fenic leverages decades of proven engineering practices.\n\n#### Decoupled Architecture for Better Agents\n\nfenic creates a clear separation between heavy inference tasks and real-time agent interactions. By moving batch processing out of the agent runtime, you get:\n\n- More predictable and responsive agents\n- Better resource utilization with batched LLM calls\n- Cleaner separation between planning/orchestration and execution\n",
    "ContentSha": "mT0TcKmDXUG4vMCvQ5Zt2Hov+kI1MOWz6tDxxgD3BGY=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "- 複数プロバイダー対応（OpenAI、Anthropic、Gemini）\n- ローカルおよびクラウド実行バックエンド\n- 包括的なエラー処理とロギング\n- 型安全性のためのPydantic統合\n\n#### なじみのあるDataFrame API\n\n- PySpark互換の操作\n- 遅延評価とクエリ最適化\n- 複雑なクエリのためのSQLサポート\n- 既存のデータパイプラインとのシームレスな統合\n\n### なぜLLMやエージェントアプリケーションにDataFrameを使うのか？\n\nAIやエージェントアプリケーションは、本質的にパイプラインやワークフローです—まさにDataFrame APIが処理するために設計されたものです。データ変換、フィルタリング、集約のためのパターンを再発明するのではなく、fenicは長年にわたる実証済みのエンジニアリング手法を活用します。\n\n#### より良いエージェントのための分離アーキテクチャ\n\nfenicは、重い推論タスクとリアルタイムのエージェントインタラクションの間に明確な分離を作ります。バッチ処理をエージェントランタイムから分離することで、次のようなメリットが得られます：\n\n- より予測可能で応答性の高いエージェント\n- バッチ化されたLLM呼び出しによるリソースの有効活用\n- 計画／オーケストレーションと実行の明確な分離",
    "Status": "ok"
  },
  {
    "Id": 7,
    "Content": "#### Built for All Engineers\n\nDataFrames aren't just for data practitioners. The fluent, composable API design makes it accessible to any engineer:\n\n- Chain operations naturally: `df.filter(...).semantic.group_by(...)`\n- Mix imperative and declarative styles seamlessly\n- Get started quickly with familiar patterns from pandas/PySpark or SQL\n\n## Support\n\nJoin our community on [Discord](https://discord.gg/GdqF3J7huR) where you can connect with other users, ask questions, and get help with your fenic projects. Our community is always happy to welcome newcomers!\n\nIf you find fenic useful, consider giving us a ⭐ at the top of this repository. Your support helps us grow and improve the framework for everyone!\n\n## Contributing\n\nWe welcome contributions of all kinds! Whether you're interested in writing code, improving documentation, testing features, or proposing new ideas, your help is valuable to us.\n\nFor developers planning to submit code changes, we encourage you to first open an issue to discuss your ideas before creating a Pull Request. This helps ensure alignment with the project's direction and prevents duplicate efforts.\n\nPlease refer to our [contribution guidelines](https://raw.githubusercontent.com/typedef-ai/fenic/main/CONTRIBUTING.md) for detailed information about the development process and project setup.\n",
    "ContentSha": "uskg5roWGwsGUjyK072Ea16WzdZZykGudJVeAy5e46I=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "#### すべてのエンジニアのために作られた\n\nDataFrameはデータの専門家だけのものではありません。流暢で合成可能なAPIデザインにより、あらゆるエンジニアが利用できます。\n\n- 操作を自然にチェーンできる： `df.filter(...).semantic.group_by(...)`\n- 命令型と宣言型のスタイルをシームレスに混在可能\n- pandas/PySparkやSQLで馴染みのあるパターンですぐに始められる\n\n## サポート\n\n[Discord](https://discord.gg/GdqF3J7huR) のコミュニティに参加して、他のユーザーとつながったり、質問したり、fenicプロジェクトについてサポートを受けたりできます。私たちのコミュニティは新しい参加者をいつでも歓迎しています！\n\nfenicが役立つと感じたら、このリポジトリのトップで ⭐ を付けていただけると嬉しいです。皆さんのサポートが、フレームワークの成長と改善につながります！\n\n## コントリビューション\n\nあらゆる種類の貢献を歓迎します！ コードの執筆、ドキュメントの改善、機能のテスト、新しいアイデアの提案など、どんな形でもあなたの協力は私たちにとって価値があります。\n\nコード変更を提出しようと考えている開発者の方は、まずissueを作成してアイデアを議論してからPull Requestを作成することを推奨します。これにより、プロジェクトの方向性との整合性が保たれ、重複作業を防ぐことができます。\n\n開発プロセスやプロジェクトのセットアップについて詳しくは、[コントリビューションガイドライン](https://raw.githubusercontent.com/typedef-ai/fenic/main/CONTRIBUTING.md) をご参照ください。",
    "Status": "ok"
  }
]