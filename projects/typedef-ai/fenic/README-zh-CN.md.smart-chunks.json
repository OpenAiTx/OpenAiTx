[
  {
    "Id": 1,
    "Content": "<!-- markdownlint-disable MD041 MD033 -->\n<div align=\"center\">\n    <picture>\n        <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/typedef-fenic-logo-dark.png\">\n        <img src=\"https://raw.githubusercontent.com/typedef-ai/fenic/main/docs/images/typedef-fenic-logo.png\" alt=\"fenic, by typedef\" width=\"90%\">\n    </picture>\n</div>\n\n# fenic: the dataframe (re)built for LLM inference\n\n[![PyPI version](https://img.shields.io/pypi/v/fenic.svg)](https://pypi.org/project/fenic/)\n[![Python versions](https://img.shields.io/pypi/pyversions/fenic.svg)](https://pypi.org/project/fenic/)\n[![License](https://img.shields.io/github/license/typedef-ai/fenic.svg)](https://github.com/typedef-ai/fenic/blob/main/LICENSE)\n[![Discord](https://img.shields.io/discord/1381706122322513952?label=Discord&logo=discord)](https://discord.gg/GdqF3J7huR)\n\n---\n\n## **Documentation**: [docs.fenic.ai](https://docs.fenic.ai/)\n\nfenic is an opinionated, PySpark-inspired DataFrame framework from typedef.ai for building AI and agentic applications. Transform unstructured and structured data into insights using familiar DataFrame operations enhanced with semantic intelligence. With first-class support for markdown, transcripts, and semantic operators, plus efficient batch inference across any model provider.\n",
    "ContentSha": "tmub1AxUb0Y4Al5Ltz67crbP453GMVUntyWUISmU7NQ=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "<!-- markdownlint-disable MD041 MD033 -->\n<div align=\"center\">\n    <picture>\n        <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/typedef-fenic-logo-dark.png\">\n        <img src=\"https://raw.githubusercontent.com/typedef-ai/fenic/main/docs/images/typedef-fenic-logo.png\" alt=\"fenic, by typedef\" width=\"90%\">\n    </picture>\n</div>\n\n# fenic：为LLM推理（重）构的数据框架\n\n[![PyPI version](https://img.shields.io/pypi/v/fenic.svg)](https://pypi.org/project/fenic/)\n[![Python versions](https://img.shields.io/pypi/pyversions/fenic.svg)](https://pypi.org/project/fenic/)\n[![License](https://img.shields.io/github/license/typedef-ai/fenic.svg)](https://github.com/typedef-ai/fenic/blob/main/LICENSE)\n[![Discord](https://img.shields.io/discord/1381706122322513952?label=Discord&logo=discord)](https://discord.gg/GdqF3J7huR)\n\n---\n\n## **文档**: [docs.fenic.ai](https://docs.fenic.ai/)\n\nfenic 是 typedef.ai 推出的一个具有主见、受 PySpark 启发的数据框架，用于构建 AI 和智能体应用。通过增强了语义智能的熟悉 DataFrame 操作，将非结构化和结构化数据转化为洞见。原生支持 markdown、转录文本和语义操作符，并可在任意模型提供商上高效批量推理。",
    "Status": "ok"
  },
  {
    "Id": 2,
    "Content": "## Install\n\nfenic supports Python `[3.10, 3.11, 3.12]`\n\n```bash\npip install fenic\n```\n\n### LLM Provider Setup\n\nfenic requires an API key from at least one LLM provider. Set the appropriate environment variable for your chosen provider:\n\n```bash\n# For OpenAI\nexport OPENAI_API_KEY=\"your-openai-api-key\"\n\n# For Anthropic\nexport ANTHROPIC_API_KEY=\"your-anthropic-api-key\"\n\n# For Google",
    "ContentSha": "QT2GWoHGnyfwnJxlfZAk0I5rpIsEfvGEhJD38oz6lCY=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## 安装\n\nfenic 支持 Python `[3.10, 3.11, 3.12]`\n\n```bash\npip install fenic\n```\n\n### LLM 提供商设置\n\nfenic 需要至少一个 LLM 提供商的 API 密钥。请为你选择的提供商设置相应的环境变量：\n\n```bash\n# 对于 OpenAI\nexport OPENAI_API_KEY=\"your-openai-api-key\"\n\n# 对于 Anthropic\nexport ANTHROPIC_API_KEY=\"your-anthropic-api-key\"\n\n# 对于 Google",
    "Status": "ok"
  },
  {
    "Id": 3,
    "Content": "export GEMINI_API_KEY=\"your-google-api-key\"\n```\n\n## Quickstart\n\nThe fastest way to learn about fenic is by checking the examples.\n\nBelow is a quick list of the examples in this repo:\n\n| Example                                                                 | Description                                                                                                                         |\n| ----------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------- |\n| [Hello World!](examples/hello_world)                                    | Introduction to semantic extraction and classification using fenic's core operators through error log analysis.                     |\n| [Enrichment](examples/enrichment)                                       | Multi-stage DataFrames with template-based text extraction, joins, and LLM-powered transformations demonstrated via log enrichment. |\n| [Meeting Transcript Processing](examples/meeting_transcript_processing) | Native transcript parsing, Pydantic schema integration, and complex aggregations shown through meeting analysis.                    |\n| [News Analysis](examples/news_analysis)                                 | Analyze and extract insights from news articles using semantic operators and structured data processing.                            |\n| [Podcast Summarization](examples/podcast_summarization)                 | Process and summarize podcast transcripts with speaker-aware analysis and key point extraction.                                     |\n| [Semantic Join](examples/semantic_joins)                                | Instead of simple fuzzy matching, use fenic's powerful semantic join functionality to match data across tables.                     |\n| [Named Entity Recognition](examples/named_entity_recognition)           | Extract and classify named entities from text using semantic extraction and classification.                                         |\n| [Markdown Processing](examples/markdown_processing)                     | Process and transform markdown documents with structured data extraction and formatting.                                            |\n| [JSON Processing](examples/json_processing)                             | Handle complex JSON data structures with semantic operations and schema validation.                                                 |\n| [Feedback Clustering](examples/feedback_clustering)                     | Group and analyze feedback using semantic similarity and clustering operations.                                                     |\n| [Document Extraction](examples/document_extraction)                     | Extract structured information from various document formats using semantic operators.                                              |\n\n(Feel free to click any example above to jump right to its folder.)\n",
    "ContentSha": "RBvJ+qAaDZODC8rvu1sFp9QTE4SUhSKTTu/91HAxBRc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "export GEMINI_API_KEY=\"your-google-api-key\"\n```\n\n## 快速入门\n\n了解 fenic 的最快方式是查看示例。\n\n以下是本仓库中的示例快速列表：\n\n| 示例                                                                      | 描述                                                                                                                           |\n| ------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------ |\n| [Hello World!](examples/hello_world)                                      | 通过错误日志分析，介绍如何使用 fenic 的核心操作符进行语义抽取与分类。                                                          |\n| [Enrichment](examples/enrichment)                                         | 通过日志丰富演示基于模板的文本抽取、多阶段 DataFrame、连接和 LLM 驱动的转换。                                                  |\n| [会议记录处理](examples/meeting_transcript_processing)                    | 通过会议分析展示原生转录解析、Pydantic 模式集成和复杂聚合操作。                                                                |\n| [新闻分析](examples/news_analysis)                                        | 使用语义操作符和结构化数据处理，对新闻文章进行分析并提取见解。                                                                  |\n| [播客摘要](examples/podcast_summarization)                                | 处理和总结播客转录，进行基于说话人的分析和要点提取。                                                                            |\n| [语义连接](examples/semantic_joins)                                       | 不仅仅是简单的模糊匹配，使用 fenic 强大的语义连接功能实现跨表数据匹配。                                                        |\n| [命名实体识别](examples/named_entity_recognition)                         | 通过语义抽取与分类从文本中提取和识别命名实体。                                                                                 |\n| [Markdown 处理](examples/markdown_processing)                             | 通过结构化数据抽取与格式转换，处理和转换 Markdown 文档。                                                                       |\n| [JSON 处理](examples/json_processing)                                     | 通过语义操作和模式校验处理复杂的 JSON 数据结构。                                                                                |\n| [反馈聚类](examples/feedback_clustering)                                  | 利用语义相似性和聚类操作对反馈进行分组与分析。                                                                                 |\n| [文档抽取](examples/document_extraction)                                  | 使用语义操作符从各种文档格式中提取结构化信息。                                                                                 |\n\n（欢迎点击上方任意示例，直接跳转到对应文件夹。）",
    "Status": "ok"
  },
  {
    "Id": 4,
    "Content": "## Why use fenic?\n\nfenic is an opinionated, PySpark-inspired DataFrame framework for building production AI and agentic applications.\n\nUnlike traditional data tools retrofitted for LLMs, fenic's query engine is built from the ground up with inference in mind.\n\nTransform structured and unstructured data into insights using familiar DataFrame operations enhanced with semantic intelligence. With first-class support for markdown, transcripts, and semantic operators, plus efficient batch inference across any model provider.\n\nfenic brings the reliability of traditional data pipelines to AI workloads.\n\n### Key Features\n\n#### Purpose-Built for LLM Inference\n\n- Query engine designed from scratch for AI workloads, not retrofitted\n- Automatic batch optimization for API calls\n- Built-in retry logic and rate limiting\n- Token counting and cost tracking\n\n#### Semantic Operators as First-Class Citizens",
    "ContentSha": "POPTpr2d3zFT3V0X0/NRjmMCUTjlIPBHtMTVHIRsyfM=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## 为什么使用 fenic？\n\nfenic 是一个具有主观性的、受 PySpark 启发的数据框架，专为构建生产级 AI 和智能体应用而设计。\n\n与为 LLMs 改造的传统数据工具不同，fenic 的查询引擎自底向上为推理场景而构建。\n\n使用增强了语义智能的熟悉 DataFrame 操作，将结构化和非结构化数据转化为洞察。对 markdown、转录文本和语义操作符提供一流支持，并能高效地跨任意模型提供商进行批量推理。\n\nfenic 为 AI 工作负载带来了传统数据管道的可靠性。\n\n### 主要特性\n\n#### 为 LLM 推理专门构建\n\n- 查询引擎从零开始为 AI 工作负载设计，而非后期改造\n- API 调用的自动批量优化\n- 内置重试逻辑和速率限制\n- Token 计数和成本追踪\n\n#### 语义操作符作为一等公民",
    "Status": "ok"
  },
  {
    "Id": 5,
    "Content": "\n- `semantic.analyze_sentiment` - Built-in sentiment analysis\n- `semantic.classify` - Categorize text with few-shot examples\n- `semantic.extract` - Transform unstructured text into structured data with schemas\n- `semantic.group_by` - Group data by semantic similarity\n- `semantic.join` - Join DataFrames on meaning, not just values\n- `semantic.map` - Apply natural language transformations\n- `semantic.predicate` - Create predicates using natural language to filter rows\n- `semantic.reduce` - Aggregate grouped data with LLM operations\n\n#### Native Unstructured Data Support\n\nGoes beyond typical multimodal data types (audio, images) by creating specialized types for text-heavy workloads:\n\n- Markdown parsing and extraction as a first-class data type\n- Transcript processing (SRT, generic formats) with speaker and timestamp awareness\n- JSON manipulation with JQ expressions for nested data\n- Automatic text chunking with configurable overlap for long documents\n\n#### Production-Ready Infrastructure",
    "ContentSha": "NE5dPjdhTPxhAD1E+gcEg2tw/wEtQVkEuw1AGA1YwCQ=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "- `semantic.analyze_sentiment` - 内置情感分析\n- `semantic.classify` - 使用少量示例对文本进行分类\n- `semantic.extract` - 通过模式将非结构化文本转化为结构化数据\n- `semantic.group_by` - 按语义相似性对数据进行分组\n- `semantic.join` - 基于语义（不仅仅是数值）进行 DataFrame 连接\n- `semantic.map` - 应用自然语言转换\n- `semantic.predicate` - 使用自然语言创建谓词以过滤行\n- `semantic.reduce` - 使用 LLM 操作对分组数据进行聚合\n\n#### 原生非结构化数据支持\n\n不仅仅支持常见的多模态数据类型（音频、图像），还为以文本为主的工作负载创建了专用类型：\n\n- 作为一等数据类型的 Markdown 解析与提取\n- 支持说话人和时间戳识别的转录处理（SRT、通用格式）\n- 使用 JQ 表达式对嵌套数据进行 JSON 操作\n- 针对长文档的自动文本分块，并可配置重叠部分\n\n#### 生产级基础设施",
    "Status": "ok"
  },
  {
    "Id": 6,
    "Content": "\n- Multi-provider support (OpenAI, Anthropic, Gemini)\n- Local and cloud execution backends\n- Comprehensive error handling and logging\n- Pydantic integration for type safety\n\n#### Familiar DataFrame API\n\n- PySpark-compatible operations\n- Lazy evaluation and query optimization\n- SQL support for complex queries\n- Seamless integration with existing data pipelines\n\n### Why DataFrames for LLM and Agentic Applications?\n\nAI and agentic applications are fundamentally pipelines and workflows - exactly what DataFrame APIs were designed to handle. Rather than reinventing patterns for data transformation, filtering, and aggregation, fenic leverages decades of proven engineering practices.\n\n#### Decoupled Architecture for Better Agents\n\nfenic creates a clear separation between heavy inference tasks and real-time agent interactions. By moving batch processing out of the agent runtime, you get:\n\n- More predictable and responsive agents\n- Better resource utilization with batched LLM calls\n- Cleaner separation between planning/orchestration and execution\n",
    "ContentSha": "mT0TcKmDXUG4vMCvQ5Zt2Hov+kI1MOWz6tDxxgD3BGY=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "- 多提供商支持（OpenAI、Anthropic、Gemini）\n- 本地和云端执行后端\n- 全面的错误处理和日志记录\n- 集成 Pydantic 实现类型安全\n\n#### 熟悉的 DataFrame API\n\n- 兼容 PySpark 的操作\n- 惰性求值与查询优化\n- 支持复杂查询的 SQL\n- 与现有数据管道的无缝集成\n\n### 为什么为 LLM 和 Agentic 应用选择 DataFrame？\n\nAI 和 Agentic 应用本质上是管道和工作流——这正是 DataFrame API 的设计初衷。fenic 没有重新发明数据转换、过滤和聚合的模式，而是利用了几十年来经过验证的工程实践。\n\n#### 为更优代理提供解耦架构\n\nfenic 在重型推理任务与实时代理交互之间建立了清晰的分离。通过将批处理移出代理运行时，您将获得：\n\n- 更可预测和响应迅速的代理\n- 通过批量 LLM 调用实现更优的资源利用\n- 规划/编排与执行之间更清晰的分离",
    "Status": "ok"
  },
  {
    "Id": 7,
    "Content": "#### Built for All Engineers\n\nDataFrames aren't just for data practitioners. The fluent, composable API design makes it accessible to any engineer:\n\n- Chain operations naturally: `df.filter(...).semantic.group_by(...)`\n- Mix imperative and declarative styles seamlessly\n- Get started quickly with familiar patterns from pandas/PySpark or SQL\n\n## Support\n\nJoin our community on [Discord](https://discord.gg/GdqF3J7huR) where you can connect with other users, ask questions, and get help with your fenic projects. Our community is always happy to welcome newcomers!\n\nIf you find fenic useful, consider giving us a ⭐ at the top of this repository. Your support helps us grow and improve the framework for everyone!\n\n## Contributing\n\nWe welcome contributions of all kinds! Whether you're interested in writing code, improving documentation, testing features, or proposing new ideas, your help is valuable to us.\n\nFor developers planning to submit code changes, we encourage you to first open an issue to discuss your ideas before creating a Pull Request. This helps ensure alignment with the project's direction and prevents duplicate efforts.\n\nPlease refer to our [contribution guidelines](https://raw.githubusercontent.com/typedef-ai/fenic/main/CONTRIBUTING.md) for detailed information about the development process and project setup.\n",
    "ContentSha": "uskg5roWGwsGUjyK072Ea16WzdZZykGudJVeAy5e46I=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "#### 为所有工程师而打造\n\nDataFrame 不仅仅适用于数据从业者。流畅、可组合的 API 设计让任何工程师都能轻松上手：\n\n- 自然地链式操作：`df.filter(...).semantic.group_by(...)`\n- 无缝混合命令式和声明式风格\n- 借助 pandas/PySpark 或 SQL 的熟悉模式快速上手\n\n## 支持\n\n欢迎加入我们的 [Discord 社区](https://discord.gg/GdqF3J7huR)，在这里你可以与其他用户交流、提问，并获得 fenic 项目的帮助。我们的社区始终欢迎新成员的加入！\n\n如果你觉得 fenic 有用，请考虑在本仓库顶部为我们点一个 ⭐。你的支持有助于我们不断发展和完善这个框架，惠及更多人！\n\n## 贡献\n\n我们欢迎各种形式的贡献！无论你是想编写代码、完善文档、测试功能，还是提出新想法，你的帮助对我们都非常宝贵。\n\n对于计划提交代码更改的开发者，我们建议你先提交 issue 讨论你的想法，再创建 Pull Request。这有助于确保与项目方向一致，并避免重复劳动。\n\n请参阅我们的[贡献指南](https://raw.githubusercontent.com/typedef-ai/fenic/main/CONTRIBUTING.md)，了解开发流程和项目配置的详细信息。",
    "Status": "ok"
  }
]