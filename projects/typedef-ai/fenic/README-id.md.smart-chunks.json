[
  {
    "Id": 1,
    "Content": "<!-- markdownlint-disable MD041 MD033 -->\n<div align=\"center\">\n    <picture>\n        <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/typedef-fenic-logo-dark.png\">\n        <img src=\"https://raw.githubusercontent.com/typedef-ai/fenic/main/docs/images/typedef-fenic-logo.png\" alt=\"fenic, by typedef\" width=\"90%\">\n    </picture>\n</div>\n\n# fenic: the dataframe (re)built for LLM inference\n\n[![PyPI version](https://img.shields.io/pypi/v/fenic.svg)](https://pypi.org/project/fenic/)\n[![Python versions](https://img.shields.io/pypi/pyversions/fenic.svg)](https://pypi.org/project/fenic/)\n[![License](https://img.shields.io/github/license/typedef-ai/fenic.svg)](https://github.com/typedef-ai/fenic/blob/main/LICENSE)\n[![Discord](https://img.shields.io/discord/1381706122322513952?label=Discord&logo=discord)](https://discord.gg/GdqF3J7huR)\n\n---\n\n## **Documentation**: [docs.fenic.ai](https://docs.fenic.ai/)\n\nfenic is an opinionated, PySpark-inspired DataFrame framework from typedef.ai for building AI and agentic applications. Transform unstructured and structured data into insights using familiar DataFrame operations enhanced with semantic intelligence. With first-class support for markdown, transcripts, and semantic operators, plus efficient batch inference across any model provider.\n",
    "ContentSha": "tmub1AxUb0Y4Al5Ltz67crbP453GMVUntyWUISmU7NQ=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "<!-- markdownlint-disable MD041 MD033 -->\n<div align=\"center\">\n    <picture>\n        <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/typedef-fenic-logo-dark.png\">\n        <img src=\"https://raw.githubusercontent.com/typedef-ai/fenic/main/docs/images/typedef-fenic-logo.png\" alt=\"fenic, by typedef\" width=\"90%\">\n    </picture>\n</div>\n\n# fenic: dataframe (dibangun ulang) untuk inferensi LLM\n\n[![Versi PyPI](https://img.shields.io/pypi/v/fenic.svg)](https://pypi.org/project/fenic/)\n[![Versi Python](https://img.shields.io/pypi/pyversions/fenic.svg)](https://pypi.org/project/fenic/)\n[![Lisensi](https://img.shields.io/github/license/typedef-ai/fenic.svg)](https://github.com/typedef-ai/fenic/blob/main/LICENSE)\n[![Discord](https://img.shields.io/discord/1381706122322513952?label=Discord&logo=discord)](https://discord.gg/GdqF3J7huR)\n\n---\n\n## **Dokumentasi**: [docs.fenic.ai](https://docs.fenic.ai/)\n\nfenic adalah framework DataFrame dari typedef.ai yang terinspirasi oleh PySpark, dirancang khusus untuk membangun aplikasi AI dan agen. Transformasikan data terstruktur maupun tidak terstruktur menjadi insight menggunakan operasi DataFrame yang sudah familiar, diperkuat dengan kecerdasan semantik. Dengan dukungan utama untuk markdown, transkrip, dan operator semantik, serta inferensi batch yang efisien di berbagai penyedia model.",
    "Status": "ok"
  },
  {
    "Id": 2,
    "Content": "## Install\n\nfenic supports Python `[3.10, 3.11, 3.12]`\n\n```bash\npip install fenic\n```\n\n### LLM Provider Setup\n\nfenic requires an API key from at least one LLM provider. Set the appropriate environment variable for your chosen provider:\n\n```bash\n# For OpenAI\nexport OPENAI_API_KEY=\"your-openai-api-key\"\n\n# For Anthropic\nexport ANTHROPIC_API_KEY=\"your-anthropic-api-key\"\n\n# For Google",
    "ContentSha": "QT2GWoHGnyfwnJxlfZAk0I5rpIsEfvGEhJD38oz6lCY=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## Instalasi\n\nfenic mendukung Python `[3.10, 3.11, 3.12]`\n\n```bash\npip install fenic\n```\n\n### Pengaturan Penyedia LLM\n\nfenic memerlukan kunci API dari setidaknya satu penyedia LLM. Atur variabel lingkungan yang sesuai untuk penyedia pilihan Anda:\n\n```bash\n# Untuk OpenAI\nexport OPENAI_API_KEY=\"your-openai-api-key\"\n\n# Untuk Anthropic\nexport ANTHROPIC_API_KEY=\"your-anthropic-api-key\"\n\n# Untuk Google",
    "Status": "ok"
  },
  {
    "Id": 3,
    "Content": "export GEMINI_API_KEY=\"your-google-api-key\"\n```\n\n## Quickstart\n\nThe fastest way to learn about fenic is by checking the examples.\n\nBelow is a quick list of the examples in this repo:\n\n| Example                                                                 | Description                                                                                                                         |\n| ----------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------- |\n| [Hello World!](examples/hello_world)                                    | Introduction to semantic extraction and classification using fenic's core operators through error log analysis.                     |\n| [Enrichment](examples/enrichment)                                       | Multi-stage DataFrames with template-based text extraction, joins, and LLM-powered transformations demonstrated via log enrichment. |\n| [Meeting Transcript Processing](examples/meeting_transcript_processing) | Native transcript parsing, Pydantic schema integration, and complex aggregations shown through meeting analysis.                    |\n| [News Analysis](examples/news_analysis)                                 | Analyze and extract insights from news articles using semantic operators and structured data processing.                            |\n| [Podcast Summarization](examples/podcast_summarization)                 | Process and summarize podcast transcripts with speaker-aware analysis and key point extraction.                                     |\n| [Semantic Join](examples/semantic_joins)                                | Instead of simple fuzzy matching, use fenic's powerful semantic join functionality to match data across tables.                     |\n| [Named Entity Recognition](examples/named_entity_recognition)           | Extract and classify named entities from text using semantic extraction and classification.                                         |\n| [Markdown Processing](examples/markdown_processing)                     | Process and transform markdown documents with structured data extraction and formatting.                                            |\n| [JSON Processing](examples/json_processing)                             | Handle complex JSON data structures with semantic operations and schema validation.                                                 |\n| [Feedback Clustering](examples/feedback_clustering)                     | Group and analyze feedback using semantic similarity and clustering operations.                                                     |\n| [Document Extraction](examples/document_extraction)                     | Extract structured information from various document formats using semantic operators.                                              |\n\n(Feel free to click any example above to jump right to its folder.)\n",
    "ContentSha": "RBvJ+qAaDZODC8rvu1sFp9QTE4SUhSKTTu/91HAxBRc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "export GEMINI_API_KEY=\"your-google-api-key\"\n```\n\n## Quickstart\n\nCara tercepat untuk mempelajari fenic adalah dengan melihat contoh-contohnya.\n\nDi bawah ini adalah daftar singkat contoh yang ada di repositori ini:\n\n| Contoh                                                                  | Deskripsi                                                                                                                          |\n| ----------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------- |\n| [Hello World!](examples/hello_world)                                    | Pengantar ekstraksi dan klasifikasi semantik menggunakan operator inti fenic melalui analisis log error.                           |\n| [Enrichment](examples/enrichment)                                       | DataFrame multi-tahap dengan ekstraksi teks berbasis template, join, dan transformasi bertenaga LLM yang didemonstrasikan melalui enrichment log. |\n| [Meeting Transcript Processing](examples/meeting_transcript_processing) | Parsing transkrip secara native, integrasi skema Pydantic, dan agregasi kompleks yang diperlihatkan melalui analisis rapat.        |\n| [News Analysis](examples/news_analysis)                                 | Menganalisis dan mengekstrak wawasan dari artikel berita menggunakan operator semantik dan pemrosesan data terstruktur.            |\n| [Podcast Summarization](examples/podcast_summarization)                 | Memproses dan merangkum transkrip podcast dengan analisis berbasis pembicara dan ekstraksi poin-poin penting.                     |\n| [Semantic Join](examples/semantic_joins)                                | Alih-alih pencocokan fuzzy sederhana, gunakan fungsi semantic join fenic yang kuat untuk mencocokkan data antar tabel.             |\n| [Named Entity Recognition](examples/named_entity_recognition)           | Mengekstrak dan mengklasifikasikan entitas bernama dari teks menggunakan ekstraksi dan klasifikasi semantik.                       |\n| [Markdown Processing](examples/markdown_processing)                     | Memproses dan mentransformasi dokumen markdown dengan ekstraksi data terstruktur dan pemformatan.                                  |\n| [JSON Processing](examples/json_processing)                             | Menangani struktur data JSON kompleks dengan operasi semantik dan validasi skema.                                                  |\n| [Feedback Clustering](examples/feedback_clustering)                     | Mengelompokkan dan menganalisis umpan balik menggunakan kemiripan semantik dan operasi klasterisasi.                               |\n| [Document Extraction](examples/document_extraction)                     | Mengekstrak informasi terstruktur dari berbagai format dokumen menggunakan operator semantik.                                      |\n\n(Silakan klik contoh apa pun di atas untuk langsung menuju ke foldernya.)",
    "Status": "ok"
  },
  {
    "Id": 4,
    "Content": "## Why use fenic?\n\nfenic is an opinionated, PySpark-inspired DataFrame framework for building production AI and agentic applications.\n\nUnlike traditional data tools retrofitted for LLMs, fenic's query engine is built from the ground up with inference in mind.\n\nTransform structured and unstructured data into insights using familiar DataFrame operations enhanced with semantic intelligence. With first-class support for markdown, transcripts, and semantic operators, plus efficient batch inference across any model provider.\n\nfenic brings the reliability of traditional data pipelines to AI workloads.\n\n### Key Features\n\n#### Purpose-Built for LLM Inference\n\n- Query engine designed from scratch for AI workloads, not retrofitted\n- Automatic batch optimization for API calls\n- Built-in retry logic and rate limiting\n- Token counting and cost tracking\n\n#### Semantic Operators as First-Class Citizens",
    "ContentSha": "POPTpr2d3zFT3V0X0/NRjmMCUTjlIPBHtMTVHIRsyfM=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## Mengapa menggunakan fenic?\n\nfenic adalah kerangka kerja DataFrame yang opiniatif dan terinspirasi dari PySpark untuk membangun aplikasi AI produksi dan aplikasi agentik.\n\nBerbeda dengan alat data tradisional yang dimodifikasi untuk LLM, mesin query fenic dibangun dari awal dengan mempertimbangkan kebutuhan inferensi.\n\nUbah data terstruktur dan tidak terstruktur menjadi wawasan menggunakan operasi DataFrame yang sudah dikenal, ditingkatkan dengan kecerdasan semantik. Dengan dukungan kelas satu untuk markdown, transkrip, dan operator semantik, serta inferensi batch yang efisien di berbagai penyedia model.\n\nfenic menghadirkan keandalan pipeline data tradisional ke dalam beban kerja AI.\n\n### Fitur Utama\n\n#### Dirancang Khusus untuk Inferensi LLM\n\n- Mesin query yang dirancang dari awal untuk beban kerja AI, bukan hasil modifikasi\n- Optimasi batch otomatis untuk panggilan API\n- Logika retry bawaan dan pembatasan laju\n- Penghitungan token dan pelacakan biaya\n\n#### Operator Semantik sebagai Fitur Utama",
    "Status": "ok"
  },
  {
    "Id": 5,
    "Content": "\n- `semantic.analyze_sentiment` - Built-in sentiment analysis\n- `semantic.classify` - Categorize text with few-shot examples\n- `semantic.extract` - Transform unstructured text into structured data with schemas\n- `semantic.group_by` - Group data by semantic similarity\n- `semantic.join` - Join DataFrames on meaning, not just values\n- `semantic.map` - Apply natural language transformations\n- `semantic.predicate` - Create predicates using natural language to filter rows\n- `semantic.reduce` - Aggregate grouped data with LLM operations\n\n#### Native Unstructured Data Support\n\nGoes beyond typical multimodal data types (audio, images) by creating specialized types for text-heavy workloads:\n\n- Markdown parsing and extraction as a first-class data type\n- Transcript processing (SRT, generic formats) with speaker and timestamp awareness\n- JSON manipulation with JQ expressions for nested data\n- Automatic text chunking with configurable overlap for long documents\n\n#### Production-Ready Infrastructure",
    "ContentSha": "NE5dPjdhTPxhAD1E+gcEg2tw/wEtQVkEuw1AGA1YwCQ=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "- `semantic.analyze_sentiment` - Analisis sentimen bawaan\n- `semantic.classify` - Mengkategorikan teks dengan contoh few-shot\n- `semantic.extract` - Mengubah teks tidak terstruktur menjadi data terstruktur dengan skema\n- `semantic.group_by` - Mengelompokkan data berdasarkan kemiripan semantik\n- `semantic.join` - Menggabungkan DataFrame berdasarkan makna, bukan hanya nilai\n- `semantic.map` - Menerapkan transformasi bahasa alami\n- `semantic.predicate` - Membuat predikat menggunakan bahasa alami untuk memfilter baris\n- `semantic.reduce` - Mengagregasi data yang dikelompokkan dengan operasi LLM\n\n#### Dukungan Data Tidak Terstruktur Secara Native\n\nMelampaui tipe data multimodal tipikal (audio, gambar) dengan menciptakan tipe khusus untuk beban kerja yang didominasi teks:\n\n- Parsing dan ekstraksi Markdown sebagai tipe data utama\n- Pemrosesan transkrip (SRT, format generik) dengan kesadaran pembicara dan cap waktu\n- Manipulasi JSON dengan ekspresi JQ untuk data bersarang\n- Pemotongan teks otomatis dengan overlap yang dapat dikonfigurasi untuk dokumen panjang\n\n#### Infrastruktur Siap Produksi",
    "Status": "ok"
  },
  {
    "Id": 6,
    "Content": "\n- Multi-provider support (OpenAI, Anthropic, Gemini)\n- Local and cloud execution backends\n- Comprehensive error handling and logging\n- Pydantic integration for type safety\n\n#### Familiar DataFrame API\n\n- PySpark-compatible operations\n- Lazy evaluation and query optimization\n- SQL support for complex queries\n- Seamless integration with existing data pipelines\n\n### Why DataFrames for LLM and Agentic Applications?\n\nAI and agentic applications are fundamentally pipelines and workflows - exactly what DataFrame APIs were designed to handle. Rather than reinventing patterns for data transformation, filtering, and aggregation, fenic leverages decades of proven engineering practices.\n\n#### Decoupled Architecture for Better Agents\n\nfenic creates a clear separation between heavy inference tasks and real-time agent interactions. By moving batch processing out of the agent runtime, you get:\n\n- More predictable and responsive agents\n- Better resource utilization with batched LLM calls\n- Cleaner separation between planning/orchestration and execution\n",
    "ContentSha": "mT0TcKmDXUG4vMCvQ5Zt2Hov+kI1MOWz6tDxxgD3BGY=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "- Dukungan multi-penyedia (OpenAI, Anthropic, Gemini)\n- Backend eksekusi lokal dan cloud\n- Penanganan kesalahan dan pencatatan yang komprehensif\n- Integrasi Pydantic untuk keamanan tipe data\n\n#### API DataFrame yang Familiar\n\n- Operasi yang kompatibel dengan PySpark\n- Evaluasi malas dan optimisasi kueri\n- Dukungan SQL untuk kueri kompleks\n- Integrasi mulus dengan pipeline data yang sudah ada\n\n### Mengapa DataFrame untuk Aplikasi LLM dan Agentik?\n\nAplikasi AI dan agentik pada dasarnya adalah pipeline dan alur kerja—persis seperti yang dirancang untuk ditangani oleh API DataFrame. Alih-alih menciptakan kembali pola untuk transformasi data, pemfilteran, dan agregasi, fenic memanfaatkan praktik rekayasa yang telah terbukti selama puluhan tahun.\n\n#### Arsitektur Terpisah untuk Agen yang Lebih Baik\n\nfenic menciptakan pemisahan yang jelas antara tugas inferensi berat dan interaksi agen waktu nyata. Dengan memindahkan pemrosesan batch keluar dari runtime agen, Anda mendapatkan:\n\n- Agen yang lebih dapat diprediksi dan responsif\n- Pemanfaatan sumber daya yang lebih baik dengan panggilan LLM secara batch\n- Pemisahan yang lebih bersih antara perencanaan/orchestrasi dan eksekusi",
    "Status": "ok"
  },
  {
    "Id": 7,
    "Content": "#### Built for All Engineers\n\nDataFrames aren't just for data practitioners. The fluent, composable API design makes it accessible to any engineer:\n\n- Chain operations naturally: `df.filter(...).semantic.group_by(...)`\n- Mix imperative and declarative styles seamlessly\n- Get started quickly with familiar patterns from pandas/PySpark or SQL\n\n## Support\n\nJoin our community on [Discord](https://discord.gg/GdqF3J7huR) where you can connect with other users, ask questions, and get help with your fenic projects. Our community is always happy to welcome newcomers!\n\nIf you find fenic useful, consider giving us a ⭐ at the top of this repository. Your support helps us grow and improve the framework for everyone!\n\n## Contributing\n\nWe welcome contributions of all kinds! Whether you're interested in writing code, improving documentation, testing features, or proposing new ideas, your help is valuable to us.\n\nFor developers planning to submit code changes, we encourage you to first open an issue to discuss your ideas before creating a Pull Request. This helps ensure alignment with the project's direction and prevents duplicate efforts.\n\nPlease refer to our [contribution guidelines](https://raw.githubusercontent.com/typedef-ai/fenic/main/CONTRIBUTING.md) for detailed information about the development process and project setup.\n",
    "ContentSha": "uskg5roWGwsGUjyK072Ea16WzdZZykGudJVeAy5e46I=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "#### Dibuat untuk Semua Insinyur\n\nDataFrame tidak hanya untuk praktisi data. Desain API yang lancar dan dapat dikombinasikan membuatnya mudah diakses oleh semua insinyur:\n\n- Rangkaikan operasi secara alami: `df.filter(...).semantic.group_by(...)`\n- Gabungkan gaya imperatif dan deklaratif dengan mulus\n- Mulai dengan cepat menggunakan pola yang sudah familiar dari pandas/PySpark atau SQL\n\n## Dukungan\n\nBergabunglah dengan komunitas kami di [Discord](https://discord.gg/GdqF3J7huR) di mana Anda dapat terhubung dengan pengguna lain, mengajukan pertanyaan, dan mendapatkan bantuan untuk proyek fenic Anda. Komunitas kami selalu senang menyambut anggota baru!\n\nJika Anda merasa fenic bermanfaat, pertimbangkan untuk memberi kami ⭐ di bagian atas repositori ini. Dukungan Anda membantu kami tumbuh dan meningkatkan framework ini untuk semua orang!\n\n## Kontribusi\n\nKami menerima segala bentuk kontribusi! Baik Anda tertarik menulis kode, memperbaiki dokumentasi, menguji fitur, atau mengusulkan ide baru, bantuan Anda sangat berharga bagi kami.\n\nBagi pengembang yang berencana mengirimkan perubahan kode, kami mendorong Anda untuk terlebih dahulu membuka issue guna mendiskusikan ide Anda sebelum membuat Pull Request. Ini membantu memastikan keselarasan dengan arah proyek dan mencegah upaya yang duplikat.\n\nSilakan lihat [panduan kontribusi](https://raw.githubusercontent.com/typedef-ai/fenic/main/CONTRIBUTING.md) kami untuk informasi detail mengenai proses pengembangan dan penyiapan proyek.",
    "Status": "ok"
  }
]