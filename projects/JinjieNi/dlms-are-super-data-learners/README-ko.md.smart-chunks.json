[
  {
    "Id": 1,
    "Content": "<div align=\"center\">\n\n<!-- TITLE -->\n# **Diffusion Language Models are Super Data Learners**\n\n[![Static Badge](https://img.shields.io/badge/Blog-2025--08--09-darkcyan)](https://jinjieni.notion.site/Diffusion-Language-Models-are-Super-Data-Learners-239d8f03a866800ab196e49928c019ac)\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/cloudposse.svg?style=social&label=tweet)](https://x.com/NiJinjie/status/1954177095435014533)\n</div>\n\n# Highlights\n- We pre-trained DLMs and AR models from scratch for up to **8B parameters** and **480B tokens**. DLMs demonstrate > **3x** greater data potential compared to autoregressive (AR) models. Notably, a 1B-parameter masked diffusion model achieves > **56%** accuracy on HellaSwag and > **33%** on MMLU using only **1B** tokens, without any special tricks, just by repeating standard pre-training data. Note that more repetitions could further improve its performance, as **no signs of diminishing returns** were observed.\n- DLMs are super-dense models that consume more FLOPs than dense AR models. Training DLMs to fully leverage the data typically demands at least **two orders of magnitude** more FLOPs. During inference, generating sequences ranging from 16 to 4096 tokens incurs a **16× to 4700×** increase in FLOPs compared to AR baselines. In addition, the more expressive bidirectional attention enabled by the diffusion objective allows **bidirectional modeling of the language data**, which is not fully causal, to fully squeeze its value.\n- Our concurrent work, “Diffusion Beats Autoregressive in Data-Constrained Settings”, contains critical methodological issues potentially leading to problematic conclusions, including **problematic diffusion loss formulation, invalid metrics for comparison, unfair settings for AR models, and problematic scaling law formulation.** All of which might lead to questionable results and conclusions.\n\n<br>\n\n# The Crossover\n<p align=\"center\" width=\"100%\">\n<img src=\"https://raw.githubusercontent.com/JinjieNi/dlms-are-super-data-learners/main/resources/imgs/1.jpg\"  width=\"80%\" height=\"100%\">\n</p>\n\n*Figure A of the blog: The performance comparison of autoregressive (AR) and masked diffusion models (Diffusion) when repeating on a limited portion of data. All models are trained on 96B total tokens (including repetition), varying the unique tokens from 0.5B to 96B. Diffusion models exploit the data better through more repetition on limited unique data. More unique tokens requires more repetition to see the crossover, where the high unique token runs postpone the crossover beyond our 96B token observation scope.*\n\n<br>\n\n# Citation",
    "ContentSha": "AbkcvqZbnMFTBUzM+rIPencKuTJbi+d6hxVXHqq6nhU=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "<div align=\"center\">\n\n<!-- TITLE -->\n# **확산 언어 모델은 초대용량 데이터 학습자입니다**\n\n[![Static Badge](https://img.shields.io/badge/Blog-2025--08--09-darkcyan)](https://jinjieni.notion.site/Diffusion-Language-Models-are-Super-Data-Learners-239d8f03a866800ab196e49928c019ac)\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/cloudposse.svg?style=social&label=tweet)](https://x.com/NiJinjie/status/1954177095435014533)\n</div>\n\n# 하이라이트\n- 우리는 **80억 매개변수** 및 **4800억 토큰**까지 DLM과 AR 모델을 처음부터 사전학습했습니다. DLM은 자기회귀(AR) 모델에 비해 > **3배** 이상의 데이터 잠재력을 보여줍니다. 특히, 10억 매개변수 마스크 확산 모델은 특수한 기법 없이 표준 사전학습 데이터를 반복 학습하는 것만으로 HellaSwag에서 > **56%**, MMLU에서 > **33%** 정확도를 10억 토큰만으로 달성합니다. 더 많은 반복은 성능을 더욱 향상시킬 수 있는데, **수익 감소 현상은 관찰되지 않았기 때문입니다.**\n- DLM은 조밀한 AR 모델보다 더 많은 FLOPs를 소모하는 초고밀도 모델입니다. 데이터를 완전히 활용하기 위해 DLM을 학습하려면 일반적으로 최소 **두 자릿수 배** 이상의 FLOPs가 필요합니다. 추론 시, 16에서 4096 토큰 범위의 시퀀스 생성은 AR 기준 모델 대비 **16배에서 4700배**의 FLOPs 증가를 초래합니다. 또한, 확산 목적함수로 가능해진 더 표현력 높은 양방향 어텐션은 **완전 인과적이지 않은 언어 데이터를 양방향으로 모델링**하여 그 가치를 최대한 끌어냅니다.\n- 동시 진행된 연구 “Diffusion Beats Autoregressive in Data-Constrained Settings”는 **문제 있는 확산 손실 공식화, 비교에 부적합한 지표, AR 모델에 대한 불공정한 설정, 문제 있는 스케일링 법칙 공식화** 등 중대한 방법론적 문제를 포함하여 문제적인 결론에 이를 수 있습니다. 이 모든 요소는 의심스러운 결과와 결론을 초래할 수 있습니다.\n\n<br>\n\n# 크로스오버\n<p align=\"center\" width=\"100%\">\n<img src=\"https://raw.githubusercontent.com/JinjieNi/dlms-are-super-data-learners/main/resources/imgs/1.jpg\"  width=\"80%\" height=\"100%\">\n</p>\n\n*블로그의 도표 A: 제한된 데이터 일부를 반복할 때 자기회귀(AR) 모델과 마스크 확산 모델(확산)의 성능 비교. 모든 모델은 총 960억 토큰(반복 포함)으로 학습하며, 고유 토큰 수는 5억에서 960억까지 변동. 확산 모델은 제한된 고유 데이터에서 더 많은 반복을 통해 데이터를 더 잘 활용함. 더 많은 고유 토큰은 크로스오버를 보기 위해 더 많은 반복이 필요하며, 고유 토큰 수가 높은 경우 크로스오버가 960억 토큰 관찰 범위를 넘어서 지연됨.*\n\n<br>\n\n# 인용문",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "94MDjHJY1ZLwHNTLIEUIfk7TMc9cq1L/1FmwhqBTe/k=",
        "originContent": "<div align=\"center\">",
        "translatedContent": "<div align=\"center\">"
      },
      {
        "row": 2,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 3,
        "rowsha": "yqrhiiULVYPt0o9P3o3i0CYqR+vRwS6/lvbCZ6tXurA=",
        "originContent": "<!-- TITLE -->",
        "translatedContent": "<!-- TITLE -->"
      },
      {
        "row": 4,
        "rowsha": "vFTSvkAxOvC/d+yaGJ7ukECfjYc4fxm6pNMHJIjh8eQ=",
        "originContent": "# **Diffusion Language Models are Super Data Learners**",
        "translatedContent": "# **확산 언어 모델은 초대용량 데이터 학습자입니다**"
      },
      {
        "row": 5,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 6,
        "rowsha": "GhY4rmY66z9d6GwRxHY1vOtYyOQlAHZWP2qYoOXqZOA=",
        "originContent": "[![Static Badge](https://img.shields.io/badge/Blog-2025--08--09-darkcyan)](https://jinjieni.notion.site/Diffusion-Language-Models-are-Super-Data-Learners-239d8f03a866800ab196e49928c019ac)",
        "translatedContent": "[![Static Badge](https://img.shields.io/badge/Blog-2025--08--09-darkcyan)](https://jinjieni.notion.site/Diffusion-Language-Models-are-Super-Data-Learners-239d8f03a866800ab196e49928c019ac)"
      },
      {
        "row": 7,
        "rowsha": "rUXTK000SkPSwFZfzvLyisXa61ReAt6EUVtLEbPoH0s=",
        "originContent": "[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/cloudposse.svg?style=social&label=tweet)](https://x.com/NiJinjie/status/1954177095435014533)",
        "translatedContent": "[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/cloudposse.svg?style=social&label=tweet)](https://x.com/NiJinjie/status/1954177095435014533)"
      },
      {
        "row": 8,
        "rowsha": "qsMmUbEPVnxGG5tPJV1vsfpoWbU2jYvZpRr5IKshzyM=",
        "originContent": "</div>",
        "translatedContent": "</div>"
      },
      {
        "row": 9,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 10,
        "rowsha": "vtWsRi5B9ZEfVQ3Z4br5JbmAUbq8B3rp6oGV80sFMQ0=",
        "originContent": "# Highlights",
        "translatedContent": "# 하이라이트"
      },
      {
        "row": 11,
        "rowsha": "c2N4w+eKTfn6KvoI/tcoaVRSdi5NndU8BhC4XV/gzJg=",
        "originContent": "- We pre-trained DLMs and AR models from scratch for up to **8B parameters** and **480B tokens**. DLMs demonstrate > **3x** greater data potential compared to autoregressive (AR) models. Notably, a 1B-parameter masked diffusion model achieves > **56%** accuracy on HellaSwag and > **33%** on MMLU using only **1B** tokens, without any special tricks, just by repeating standard pre-training data. Note that more repetitions could further improve its performance, as **no signs of diminishing returns** were observed.",
        "translatedContent": "- 우리는 **80억 매개변수** 및 **4800억 토큰**까지 DLM과 AR 모델을 처음부터 사전학습했습니다. DLM은 자기회귀(AR) 모델에 비해 > **3배** 이상의 데이터 잠재력을 보여줍니다. 특히, 10억 매개변수 마스크 확산 모델은 특수한 기법 없이 표준 사전학습 데이터를 반복 학습하는 것만으로 HellaSwag에서 > **56%**, MMLU에서 > **33%** 정확도를 10억 토큰만으로 달성합니다. 더 많은 반복은 성능을 더욱 향상시킬 수 있는데, **수익 감소 현상은 관찰되지 않았기 때문입니다.**"
      },
      {
        "row": 12,
        "rowsha": "Wrn8teZUQ+P3l7wk0edkwbANAQwZLdCmuf6SQ6Al9Jg=",
        "originContent": "- DLMs are super-dense models that consume more FLOPs than dense AR models. Training DLMs to fully leverage the data typically demands at least **two orders of magnitude** more FLOPs. During inference, generating sequences ranging from 16 to 4096 tokens incurs a **16× to 4700×** increase in FLOPs compared to AR baselines. In addition, the more expressive bidirectional attention enabled by the diffusion objective allows **bidirectional modeling of the language data**, which is not fully causal, to fully squeeze its value.",
        "translatedContent": "- DLM은 조밀한 AR 모델보다 더 많은 FLOPs를 소모하는 초고밀도 모델입니다. 데이터를 완전히 활용하기 위해 DLM을 학습하려면 일반적으로 최소 **두 자릿수 배** 이상의 FLOPs가 필요합니다. 추론 시, 16에서 4096 토큰 범위의 시퀀스 생성은 AR 기준 모델 대비 **16배에서 4700배**의 FLOPs 증가를 초래합니다. 또한, 확산 목적함수로 가능해진 더 표현력 높은 양방향 어텐션은 **완전 인과적이지 않은 언어 데이터를 양방향으로 모델링**하여 그 가치를 최대한 끌어냅니다."
      },
      {
        "row": 13,
        "rowsha": "wuptl/I/AU8SrJOx9oJIzj5pJZeC1U20Uhe1W/eCL6s=",
        "originContent": "- Our concurrent work, “Diffusion Beats Autoregressive in Data-Constrained Settings”, contains critical methodological issues potentially leading to problematic conclusions, including **problematic diffusion loss formulation, invalid metrics for comparison, unfair settings for AR models, and problematic scaling law formulation.** All of which might lead to questionable results and conclusions.",
        "translatedContent": "- 동시 진행된 연구 “Diffusion Beats Autoregressive in Data-Constrained Settings”는 **문제 있는 확산 손실 공식화, 비교에 부적합한 지표, AR 모델에 대한 불공정한 설정, 문제 있는 스케일링 법칙 공식화** 등 중대한 방법론적 문제를 포함하여 문제적인 결론에 이를 수 있습니다. 이 모든 요소는 의심스러운 결과와 결론을 초래할 수 있습니다."
      },
      {
        "row": 14,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 15,
        "rowsha": "Za7xrbqGcqX+eTLZ+fDcNcJDWJP2URKsrGKNLcF6C88=",
        "originContent": "<br>",
        "translatedContent": "<br>"
      },
      {
        "row": 16,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 17,
        "rowsha": "c7OxBJKVlKctkr804O9z2fZEiJApr/XnU0pWdFQY3xg=",
        "originContent": "# The Crossover",
        "translatedContent": "# 크로스오버"
      },
      {
        "row": 18,
        "rowsha": "xw0e+DkxEZlrdnSE9ePetYAONFa0gGij2ZqfpU4cetA=",
        "originContent": "<p align=\"center\" width=\"100%\">",
        "translatedContent": "<p align=\"center\" width=\"100%\">"
      },
      {
        "row": 19,
        "rowsha": "nU8JfcxW0W+z6VSyEvV7so/VX5Cr7BZBI58weSTDtKc=",
        "originContent": "<img src=\"https://raw.githubusercontent.com/JinjieNi/dlms-are-super-data-learners/main/resources/imgs/1.jpg\"  width=\"80%\" height=\"100%\">",
        "translatedContent": "<img src=\"https://raw.githubusercontent.com/JinjieNi/dlms-are-super-data-learners/main/resources/imgs/1.jpg\"  width=\"80%\" height=\"100%\">"
      },
      {
        "row": 20,
        "rowsha": "dSdvPNAZSmR86FDDSF6tkQUCVfI9qmACHOR5tThOetY=",
        "originContent": "</p>",
        "translatedContent": "</p>"
      },
      {
        "row": 21,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 22,
        "rowsha": "Dd6SzeiB0npn2/n+VD0ojAyzWsTMo94OuzLqoDXRXjE=",
        "originContent": "*Figure A of the blog: The performance comparison of autoregressive (AR) and masked diffusion models (Diffusion) when repeating on a limited portion of data. All models are trained on 96B total tokens (including repetition), varying the unique tokens from 0.5B to 96B. Diffusion models exploit the data better through more repetition on limited unique data. More unique tokens requires more repetition to see the crossover, where the high unique token runs postpone the crossover beyond our 96B token observation scope.*",
        "translatedContent": "*블로그의 도표 A: 제한된 데이터 일부를 반복할 때 자기회귀(AR) 모델과 마스크 확산 모델(확산)의 성능 비교. 모든 모델은 총 960억 토큰(반복 포함)으로 학습하며, 고유 토큰 수는 5억에서 960억까지 변동. 확산 모델은 제한된 고유 데이터에서 더 많은 반복을 통해 데이터를 더 잘 활용함. 더 많은 고유 토큰은 크로스오버를 보기 위해 더 많은 반복이 필요하며, 고유 토큰 수가 높은 경우 크로스오버가 960억 토큰 관찰 범위를 넘어서 지연됨.*"
      },
      {
        "row": 23,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 24,
        "rowsha": "Za7xrbqGcqX+eTLZ+fDcNcJDWJP2URKsrGKNLcF6C88=",
        "originContent": "<br>",
        "translatedContent": "<br>"
      },
      {
        "row": 25,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 26,
        "rowsha": "IYveXwQyASnTxk975argwU5ZV1Mu8XpPLdj2YZocajs=",
        "originContent": "# Citation",
        "translatedContent": "# 인용문"
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 2,
    "Content": "```\n@misc{ni2025difflm,\ntitle={Diffusion Language Models are Super Data Learners},\nauthor={Jinjie Ni and the team},\nyear={2025},\nhowpublished={\\url{https://jinjieni.notion.site/Diffusion-Language-Models-are-Super-Data-Learners-239d8f03a866800ab196e49928c019ac}},\nnote={Notion Blog},\n}\n```",
    "ContentSha": "oZFbcPAz8gb0bUHaLaSOXW8/ratxBZV0IpPqYBmeTts=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```\n@misc{ni2025difflm,\ntitle={Diffusion Language Models are Super Data Learners},\nauthor={Jinjie Ni and the team},\nyear={2025},\nhowpublished={\\url{https://jinjieni.notion.site/Diffusion-Language-Models-are-Super-Data-Learners-239d8f03a866800ab196e49928c019ac}},\nnote={Notion Blog},\n}\n```",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "8bkBhHOQsO1+N058HkZOwXtGpCfEh6WtbL0pBkBQg9U=",
        "originContent": "```",
        "translatedContent": "```"
      },
      {
        "row": 2,
        "rowsha": "E2Z6E1KZsUDEDbabBAfSHKzcRjWG/AP2E6IzyeCPhco=",
        "originContent": "@misc{ni2025difflm,",
        "translatedContent": "@misc{ni2025difflm,"
      },
      {
        "row": 3,
        "rowsha": "RsGgPtoYg/mU0CwWXTp1FHJWZkUmjO9J2URb4mQPJlk=",
        "originContent": "title={Diffusion Language Models are Super Data Learners},",
        "translatedContent": "title={Diffusion Language Models are Super Data Learners},"
      },
      {
        "row": 4,
        "rowsha": "sK12vSNSAncCqdHmwVv1KV1yYWFJzK4xIMKHKSd/u+Q=",
        "originContent": "author={Jinjie Ni and the team},",
        "translatedContent": "author={Jinjie Ni and the team},"
      },
      {
        "row": 5,
        "rowsha": "ovfFG+BkK4tuLUmR03xZ3o2U3TWAR5hbgzw/VaRZbkA=",
        "originContent": "year={2025},",
        "translatedContent": "year={2025},"
      },
      {
        "row": 6,
        "rowsha": "4gflS9YBrEuCwdeUoslckOi0+lYITt8vHL93uDX0xHc=",
        "originContent": "howpublished={\\url{https://jinjieni.notion.site/Diffusion-Language-Models-are-Super-Data-Learners-239d8f03a866800ab196e49928c019ac}},",
        "translatedContent": "howpublished={\\url{https://jinjieni.notion.site/Diffusion-Language-Models-are-Super-Data-Learners-239d8f03a866800ab196e49928c019ac}},"
      },
      {
        "row": 7,
        "rowsha": "MKUVE8wZ4mMUiCklw16Upf5MiSWizWAhPoM04oqTO4E=",
        "originContent": "note={Notion Blog},",
        "translatedContent": "note={Notion Blog},"
      },
      {
        "row": 8,
        "rowsha": "0Qs2qnSlm89KiBhYN/ZYr682Ru/yuxbDko0OkzXpRdI=",
        "originContent": "}",
        "translatedContent": "}"
      },
      {
        "row": 9,
        "rowsha": "8bkBhHOQsO1+N058HkZOwXtGpCfEh6WtbL0pBkBQg9U=",
        "originContent": "```",
        "translatedContent": "```"
      }
    ],
    "IsCodeBlock": true
  },
  {
    "Id": 3,
    "Content": "",
    "ContentSha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  }
]