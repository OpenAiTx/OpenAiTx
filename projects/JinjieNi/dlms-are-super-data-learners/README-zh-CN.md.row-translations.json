[
  {
    "row": 1,
    "rowsha": "94MDjHJY1ZLwHNTLIEUIfk7TMc9cq1L/1FmwhqBTe/k=",
    "originContent": "<div align=\"center\">",
    "translatedContent": "<div align=\"center\">"
  },
  {
    "row": 2,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 3,
    "rowsha": "yqrhiiULVYPt0o9P3o3i0CYqR+vRwS6/lvbCZ6tXurA=",
    "originContent": "<!-- TITLE -->",
    "translatedContent": "<!-- TITLE -->"
  },
  {
    "row": 4,
    "rowsha": "vFTSvkAxOvC/d+yaGJ7ukECfjYc4fxm6pNMHJIjh8eQ=",
    "originContent": "# **Diffusion Language Models are Super Data Learners**",
    "translatedContent": "# **扩散语言模型是超级数据学习者**"
  },
  {
    "row": 5,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 6,
    "rowsha": "GhY4rmY66z9d6GwRxHY1vOtYyOQlAHZWP2qYoOXqZOA=",
    "originContent": "[![Static Badge](https://img.shields.io/badge/Blog-2025--08--09-darkcyan)](https://jinjieni.notion.site/Diffusion-Language-Models-are-Super-Data-Learners-239d8f03a866800ab196e49928c019ac)",
    "translatedContent": "[![Static Badge](https://img.shields.io/badge/Blog-2025--08--09-darkcyan)](https://jinjieni.notion.site/Diffusion-Language-Models-are-Super-Data-Learners-239d8f03a866800ab196e49928c019ac)"
  },
  {
    "row": 7,
    "rowsha": "rUXTK000SkPSwFZfzvLyisXa61ReAt6EUVtLEbPoH0s=",
    "originContent": "[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/cloudposse.svg?style=social&label=tweet)](https://x.com/NiJinjie/status/1954177095435014533)",
    "translatedContent": "[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/cloudposse.svg?style=social&label=tweet)](https://x.com/NiJinjie/status/1954177095435014533)"
  },
  {
    "row": 8,
    "rowsha": "qsMmUbEPVnxGG5tPJV1vsfpoWbU2jYvZpRr5IKshzyM=",
    "originContent": "</div>",
    "translatedContent": "</div>"
  },
  {
    "row": 9,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 10,
    "rowsha": "vtWsRi5B9ZEfVQ3Z4br5JbmAUbq8B3rp6oGV80sFMQ0=",
    "originContent": "# Highlights",
    "translatedContent": "# 亮点"
  },
  {
    "row": 11,
    "rowsha": "c2N4w+eKTfn6KvoI/tcoaVRSdi5NndU8BhC4XV/gzJg=",
    "originContent": "- We pre-trained DLMs and AR models from scratch for up to **8B parameters** and **480B tokens**. DLMs demonstrate > **3x** greater data potential compared to autoregressive (AR) models. Notably, a 1B-parameter masked diffusion model achieves > **56%** accuracy on HellaSwag and > **33%** on MMLU using only **1B** tokens, without any special tricks, just by repeating standard pre-training data. Note that more repetitions could further improve its performance, as **no signs of diminishing returns** were observed.",
    "translatedContent": "- 我们从零开始预训练了多达**80亿参数**和**4800亿标记**的DLM和AR模型。DLM表现出比自回归（AR）模型高出**3倍以上**的数据潜力。值得注意的是，一个10亿参数的掩码扩散模型仅使用**10亿**标记，在HellaSwag上实现了超过**56%**的准确率，在MMLU上超过**33%**，无需任何特殊技巧，仅通过重复标准预训练数据。需要注意的是，更多的重复可能进一步提升其性能，因为**未观察到收益递减的迹象**。"
  },
  {
    "row": 12,
    "rowsha": "Wrn8teZUQ+P3l7wk0edkwbANAQwZLdCmuf6SQ6Al9Jg=",
    "originContent": "- DLMs are super-dense models that consume more FLOPs than dense AR models. Training DLMs to fully leverage the data typically demands at least **two orders of magnitude** more FLOPs. During inference, generating sequences ranging from 16 to 4096 tokens incurs a **16× to 4700×** increase in FLOPs compared to AR baselines. In addition, the more expressive bidirectional attention enabled by the diffusion objective allows **bidirectional modeling of the language data**, which is not fully causal, to fully squeeze its value.",
    "translatedContent": "- DLM是超密集模型，消耗的FLOPs超过密集的AR模型。训练DLM以充分利用数据通常需要至少**两个数量级**以上的FLOPs。在推理阶段，生成长度从16到4096标记的序列，FLOPs相较于AR基线增加了**16倍到4700倍**。此外，扩散目标启用的更具表现力的双向注意力允许对语言数据进行**双向建模**，语言数据并非完全因果，因此能够充分挤压其价值。"
  },
  {
    "row": 13,
    "rowsha": "wuptl/I/AU8SrJOx9oJIzj5pJZeC1U20Uhe1W/eCL6s=",
    "originContent": "- Our concurrent work, “Diffusion Beats Autoregressive in Data-Constrained Settings”, contains critical methodological issues potentially leading to problematic conclusions, including **problematic diffusion loss formulation, invalid metrics for comparison, unfair settings for AR models, and problematic scaling law formulation.** All of which might lead to questionable results and conclusions.",
    "translatedContent": "- 我们的同期工作“扩散在数据受限环境中超越自回归”存在关键方法论问题，可能导致有问题的结论，包括**有问题的扩散损失公式、无效的比较指标、不公平的AR模型设置及有问题的规模规律公式。**这些都可能导致结果和结论的可疑性。"
  },
  {
    "row": 14,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 15,
    "rowsha": "Za7xrbqGcqX+eTLZ+fDcNcJDWJP2URKsrGKNLcF6C88=",
    "originContent": "<br>",
    "translatedContent": "<br>"
  },
  {
    "row": 16,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 17,
    "rowsha": "c7OxBJKVlKctkr804O9z2fZEiJApr/XnU0pWdFQY3xg=",
    "originContent": "# The Crossover",
    "translatedContent": "# 交叉点"
  },
  {
    "row": 18,
    "rowsha": "xw0e+DkxEZlrdnSE9ePetYAONFa0gGij2ZqfpU4cetA=",
    "originContent": "<p align=\"center\" width=\"100%\">",
    "translatedContent": "<p align=\"center\" width=\"100%\">"
  },
  {
    "row": 19,
    "rowsha": "nU8JfcxW0W+z6VSyEvV7so/VX5Cr7BZBI58weSTDtKc=",
    "originContent": "<img src=\"https://raw.githubusercontent.com/JinjieNi/dlms-are-super-data-learners/main/resources/imgs/1.jpg\"  width=\"80%\" height=\"100%\">",
    "translatedContent": "<img src=\"https://raw.githubusercontent.com/JinjieNi/dlms-are-super-data-learners/main/resources/imgs/1.jpg\"  width=\"80%\" height=\"100%\">"
  },
  {
    "row": 20,
    "rowsha": "dSdvPNAZSmR86FDDSF6tkQUCVfI9qmACHOR5tThOetY=",
    "originContent": "</p>",
    "translatedContent": "</p>"
  },
  {
    "row": 21,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 22,
    "rowsha": "Dd6SzeiB0npn2/n+VD0ojAyzWsTMo94OuzLqoDXRXjE=",
    "originContent": "*Figure A of the blog: The performance comparison of autoregressive (AR) and masked diffusion models (Diffusion) when repeating on a limited portion of data. All models are trained on 96B total tokens (including repetition), varying the unique tokens from 0.5B to 96B. Diffusion models exploit the data better through more repetition on limited unique data. More unique tokens requires more repetition to see the crossover, where the high unique token runs postpone the crossover beyond our 96B token observation scope.*",
    "translatedContent": "*博客图A：在有限数据部分重复训练时，自回归（AR）模型与掩码扩散模型（Diffusion）的性能比较。所有模型均在960亿总标记（包括重复）上训练，唯一标记数量从5亿变化到960亿。扩散模型通过在有限唯一数据上更多重复更好地利用数据。更多唯一标记需要更多重复才能看到交叉点，高唯一标记运行将交叉点推迟到超出我们960亿标记的观察范围。*"
  },
  {
    "row": 23,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 24,
    "rowsha": "Za7xrbqGcqX+eTLZ+fDcNcJDWJP2URKsrGKNLcF6C88=",
    "originContent": "<br>",
    "translatedContent": "<br>"
  },
  {
    "row": 25,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 26,
    "rowsha": "IYveXwQyASnTxk975argwU5ZV1Mu8XpPLdj2YZocajs=",
    "originContent": "# Citation",
    "translatedContent": "# 引用"
  },
  {
    "row": 27,
    "rowsha": "8bkBhHOQsO1+N058HkZOwXtGpCfEh6WtbL0pBkBQg9U=",
    "originContent": "```",
    "translatedContent": "```"
  },
  {
    "row": 28,
    "rowsha": "E2Z6E1KZsUDEDbabBAfSHKzcRjWG/AP2E6IzyeCPhco=",
    "originContent": "@misc{ni2025difflm,",
    "translatedContent": "@misc{ni2025difflm,"
  },
  {
    "row": 29,
    "rowsha": "RsGgPtoYg/mU0CwWXTp1FHJWZkUmjO9J2URb4mQPJlk=",
    "originContent": "title={Diffusion Language Models are Super Data Learners},",
    "translatedContent": "title={Diffusion Language Models are Super Data Learners},"
  },
  {
    "row": 30,
    "rowsha": "sK12vSNSAncCqdHmwVv1KV1yYWFJzK4xIMKHKSd/u+Q=",
    "originContent": "author={Jinjie Ni and the team},",
    "translatedContent": "author={Jinjie Ni and the team},"
  },
  {
    "row": 31,
    "rowsha": "ovfFG+BkK4tuLUmR03xZ3o2U3TWAR5hbgzw/VaRZbkA=",
    "originContent": "year={2025},",
    "translatedContent": "year={2025},"
  },
  {
    "row": 32,
    "rowsha": "4gflS9YBrEuCwdeUoslckOi0+lYITt8vHL93uDX0xHc=",
    "originContent": "howpublished={\\url{https://jinjieni.notion.site/Diffusion-Language-Models-are-Super-Data-Learners-239d8f03a866800ab196e49928c019ac}},",
    "translatedContent": "howpublished={\\url{https://jinjieni.notion.site/Diffusion-Language-Models-are-Super-Data-Learners-239d8f03a866800ab196e49928c019ac}},"
  },
  {
    "row": 33,
    "rowsha": "MKUVE8wZ4mMUiCklw16Upf5MiSWizWAhPoM04oqTO4E=",
    "originContent": "note={Notion Blog},",
    "translatedContent": "note={Notion Blog},"
  },
  {
    "row": 34,
    "rowsha": "0Qs2qnSlm89KiBhYN/ZYr682Ru/yuxbDko0OkzXpRdI=",
    "originContent": "}",
    "translatedContent": "}"
  },
  {
    "row": 35,
    "rowsha": "8bkBhHOQsO1+N058HkZOwXtGpCfEh6WtbL0pBkBQg9U=",
    "originContent": "```",
    "translatedContent": "```"
  },
  {
    "row": 36,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  }
]