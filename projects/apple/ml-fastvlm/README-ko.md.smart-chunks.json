[
  {
    "Id": 1,
    "Content": "# FastVLM: Efficient Vision Encoding for Vision Language Models\n\nThis is the official repository of\n**[FastVLM: Efficient Vision Encoding for Vision Language Models](https://www.arxiv.org/abs/2412.13303). (CVPR 2025)**\n\n[//]: # (![FastViTHD Performance]&#40;docs/acc_vs_latency_qwen-2.png&#41;)\n<p align=\"center\">\n<img src=\"https://raw.githubusercontent.com/apple/ml-fastvlm/main/docs/acc_vs_latency_qwen-2.png\" alt=\"Accuracy vs latency figure.\" width=\"400\"/>\n</p>\n\n### Highlights\n* We introduce FastViTHD, a novel hybrid vision encoder designed to output fewer tokens and significantly reduce encoding time for high-resolution images.  \n* Our smallest variant outperforms LLaVA-OneVision-0.5B with 85x faster Time-to-First-Token (TTFT) and 3.4x smaller vision encoder.\n* Our larger variants using Qwen2-7B LLM outperform recent works like Cambrian-1-8B while using a single image encoder with a 7.9x faster TTFT.\n* Demo iOS app to demonstrate the performance of our model on a mobile device.\n\n<table>\n<tr>\n    <td><img src=\"https://raw.githubusercontent.com/apple/ml-fastvlm/main/docs/fastvlm-counting.gif\" alt=\"FastVLM - Counting\"></td>\n    <td><img src=\"https://raw.githubusercontent.com/apple/ml-fastvlm/main/docs/fastvlm-handwriting.gif\" alt=\"FastVLM - Handwriting\"></td>\n    <td><img src=\"https://raw.githubusercontent.com/apple/ml-fastvlm/main/docs/fastvlm-emoji.gif\" alt=\"FastVLM - Emoji\"></td>\n</tr>\n</table>\n\n## Getting Started\nWe use LLaVA codebase to train FastVLM variants. In order to train or finetune your own variants, \nplease follow instructions provided in [LLaVA](https://github.com/haotian-liu/LLaVA) codebase. \nWe provide instructions for running inference with our models.   \n\n### Setup",
    "ContentSha": "wsrzx0qlxCPsMjB+1lvktAho8SXdn5iyJDVW8zmREeg=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "<translate-content># FastVLM: 비전 언어 모델을 위한 효율적인 비전 인코딩\n\n이곳은\n**[FastVLM: 비전 언어 모델을 위한 효율적인 비전 인코딩](https://www.arxiv.org/abs/2412.13303). (CVPR 2025)**의 공식 저장소입니다.\n\n[//]: # (![FastViTHD 성능]&#40;docs/acc_vs_latency_qwen-2.png&#41;)\n<p align=\"center\">\n<img src=\"https://raw.githubusercontent.com/apple/ml-fastvlm/main/docs/acc_vs_latency_qwen-2.png\" alt=\"정확도 대 지연 시간 그래프.\" width=\"400\"/>\n</p>\n\n### 주요 내용\n* 우리는 고해상도 이미지에 대해 토큰 수를 줄이고 인코딩 시간을 크게 단축하는 새로운 하이브리드 비전 인코더인 FastViTHD를 소개합니다.  \n* 가장 작은 변형은 LLaVA-OneVision-0.5B보다 85배 빠른 첫 토큰 생성 시간(TTFT)과 3.4배 작은 비전 인코더 크기로 우수한 성능을 보입니다.\n* Qwen2-7B LLM을 사용하는 더 큰 변형들은 Cambrian-1-8B 같은 최신 작업들을 단일 이미지 인코더로 7.9배 빠른 TTFT와 함께 능가합니다.\n* 모바일 기기에서 모델 성능을 시연하는 데모 iOS 앱을 제공합니다.\n\n<table>\n<tr>\n    <td><img src=\"https://raw.githubusercontent.com/apple/ml-fastvlm/main/docs/fastvlm-counting.gif\" alt=\"FastVLM - 카운팅\"></td>\n    <td><img src=\"https://raw.githubusercontent.com/apple/ml-fastvlm/main/docs/fastvlm-handwriting.gif\" alt=\"FastVLM - 손글씨\"></td>\n    <td><img src=\"https://raw.githubusercontent.com/apple/ml-fastvlm/main/docs/fastvlm-emoji.gif\" alt=\"FastVLM - 이모지\"></td>\n</tr>\n</table>\n\n## 시작하기\n우리는 FastVLM 변형을 학습하기 위해 LLaVA 코드베이스를 사용합니다. 자체 변형을 학습하거나 미세 조정하려면,\n[LLaVA](https://github.com/haotian-liu/LLaVA) 코드베이스에서 제공하는 지침을 따라 주십시오.\n우리 모델로 추론을 실행하는 방법도 안내합니다.   \n\n### 설정</translate-content>",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "re25b1/AsVvkGcjPmitUvdEQjXbD3M1O4pzFBGIu6Gk=",
        "originContent": "# FastVLM: Efficient Vision Encoding for Vision Language Models",
        "translatedContent": "<translate-content># FastVLM: 비전 언어 모델을 위한 효율적인 비전 인코딩"
      },
      {
        "row": 2,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 3,
        "rowsha": "g2344bef599vjW/bUbjaxuux4Z1PJZBH3XIqUI9oCEE=",
        "originContent": "This is the official repository of",
        "translatedContent": "이곳은"
      },
      {
        "row": 4,
        "rowsha": "K71zo6Tiv5ua2mrdvDXM95P+lAzeSQcl1i7M49zJsdY=",
        "originContent": "**[FastVLM: Efficient Vision Encoding for Vision Language Models](https://www.arxiv.org/abs/2412.13303). (CVPR 2025)**",
        "translatedContent": "**[FastVLM: 비전 언어 모델을 위한 효율적인 비전 인코딩](https://www.arxiv.org/abs/2412.13303). (CVPR 2025)**의 공식 저장소입니다."
      },
      {
        "row": 5,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 6,
        "rowsha": "mDPE1ZCMkbjTAn1dDxuhruAQmZ+lxOPvH7Z0xwAaZEM=",
        "originContent": "[//]: # (![FastViTHD Performance]&#40;docs/acc_vs_latency_qwen-2.png&#41;)",
        "translatedContent": "[//]: # (![FastViTHD 성능]&#40;docs/acc_vs_latency_qwen-2.png&#41;)"
      },
      {
        "row": 7,
        "rowsha": "+/a9XmPwQixGFroME/GMEOLpReZZV4ARosR9orAplJY=",
        "originContent": "<p align=\"center\">",
        "translatedContent": "<p align=\"center\">"
      },
      {
        "row": 8,
        "rowsha": "SKvwjmW9xaytX3oHDq5Dg4sD4JuHSXJbqucBWd84C+8=",
        "originContent": "<img src=\"https://raw.githubusercontent.com/apple/ml-fastvlm/main/docs/acc_vs_latency_qwen-2.png\" alt=\"Accuracy vs latency figure.\" width=\"400\"/>",
        "translatedContent": "<img src=\"https://raw.githubusercontent.com/apple/ml-fastvlm/main/docs/acc_vs_latency_qwen-2.png\" alt=\"정확도 대 지연 시간 그래프.\" width=\"400\"/>"
      },
      {
        "row": 9,
        "rowsha": "dSdvPNAZSmR86FDDSF6tkQUCVfI9qmACHOR5tThOetY=",
        "originContent": "</p>",
        "translatedContent": "</p>"
      },
      {
        "row": 10,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 11,
        "rowsha": "cxAIGfEA3El60/qMS6EjCVQ8S9ibfnDqCalf5nlplPo=",
        "originContent": "### Highlights",
        "translatedContent": "### 주요 내용"
      },
      {
        "row": 12,
        "rowsha": "Qbjy7m9z961IGQTLEjGRiq+qIL4qsOZ2Np9RDk1Or3k=",
        "originContent": "* We introduce FastViTHD, a novel hybrid vision encoder designed to output fewer tokens and significantly reduce encoding time for high-resolution images.  ",
        "translatedContent": "* 우리는 고해상도 이미지에 대해 토큰 수를 줄이고 인코딩 시간을 크게 단축하는 새로운 하이브리드 비전 인코더인 FastViTHD를 소개합니다.  "
      },
      {
        "row": 13,
        "rowsha": "tnIqJOspZu3EY7LtIeWupio5uTzaKW2HZLZJSPH26PQ=",
        "originContent": "* Our smallest variant outperforms LLaVA-OneVision-0.5B with 85x faster Time-to-First-Token (TTFT) and 3.4x smaller vision encoder.",
        "translatedContent": "* 가장 작은 변형은 LLaVA-OneVision-0.5B보다 85배 빠른 첫 토큰 생성 시간(TTFT)과 3.4배 작은 비전 인코더 크기로 우수한 성능을 보입니다."
      },
      {
        "row": 14,
        "rowsha": "3LbcpEdVpjhUHayEQCb2uNiAORybizFDqCociFBcvDg=",
        "originContent": "* Our larger variants using Qwen2-7B LLM outperform recent works like Cambrian-1-8B while using a single image encoder with a 7.9x faster TTFT.",
        "translatedContent": "* Qwen2-7B LLM을 사용하는 더 큰 변형들은 Cambrian-1-8B 같은 최신 작업들을 단일 이미지 인코더로 7.9배 빠른 TTFT와 함께 능가합니다."
      },
      {
        "row": 15,
        "rowsha": "GYySOSL4l7fvEzVxm+pbPTq+oZHONw2OYeWH0ZRbmoQ=",
        "originContent": "* Demo iOS app to demonstrate the performance of our model on a mobile device.",
        "translatedContent": "* 모바일 기기에서 모델 성능을 시연하는 데모 iOS 앱을 제공합니다."
      },
      {
        "row": 16,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 17,
        "rowsha": "kq0DFTS69SZCm3Odp9SwOmLuj08yYNkZvbhcRxtdMkQ=",
        "originContent": "<table>",
        "translatedContent": "<table>"
      },
      {
        "row": 18,
        "rowsha": "m4TGFLRzvx0YLmLMue/0JQ+Zd9Xal1VIBS1erVxEKBU=",
        "originContent": "<tr>",
        "translatedContent": "<tr>"
      },
      {
        "row": 19,
        "rowsha": "AWO5iLlAetK3Nmw3rahwZC52gF+8V4NXryHosnjw0as=",
        "originContent": "    <td><img src=\"https://raw.githubusercontent.com/apple/ml-fastvlm/main/docs/fastvlm-counting.gif\" alt=\"FastVLM - Counting\"></td>",
        "translatedContent": "    <td><img src=\"https://raw.githubusercontent.com/apple/ml-fastvlm/main/docs/fastvlm-counting.gif\" alt=\"FastVLM - 카운팅\"></td>"
      },
      {
        "row": 20,
        "rowsha": "t2vGd7sN0ktBcHjog+r0mrZQ6j58MpHFhIsxkTxMKb8=",
        "originContent": "    <td><img src=\"https://raw.githubusercontent.com/apple/ml-fastvlm/main/docs/fastvlm-handwriting.gif\" alt=\"FastVLM - Handwriting\"></td>",
        "translatedContent": "    <td><img src=\"https://raw.githubusercontent.com/apple/ml-fastvlm/main/docs/fastvlm-handwriting.gif\" alt=\"FastVLM - 손글씨\"></td>"
      },
      {
        "row": 21,
        "rowsha": "UX8W4w2W19v7FCBTtFmRpQ6G0eJwYjHFCAAY9TyNDyM=",
        "originContent": "    <td><img src=\"https://raw.githubusercontent.com/apple/ml-fastvlm/main/docs/fastvlm-emoji.gif\" alt=\"FastVLM - Emoji\"></td>",
        "translatedContent": "    <td><img src=\"https://raw.githubusercontent.com/apple/ml-fastvlm/main/docs/fastvlm-emoji.gif\" alt=\"FastVLM - 이모지\"></td>"
      },
      {
        "row": 22,
        "rowsha": "PtdEDOB1aSgnb7wNnHW1mo4VL3Eh6O3aBzC76+S4YE0=",
        "originContent": "</tr>",
        "translatedContent": "</tr>"
      },
      {
        "row": 23,
        "rowsha": "H+dtb55ry3VN2CLvAetudgE9ICnYQdUralLHuIqMdZM=",
        "originContent": "</table>",
        "translatedContent": "</table>"
      },
      {
        "row": 24,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 25,
        "rowsha": "xhRBNsX93gDAZr7QGoGuyvjyOQNvnsq8Qy5ThtW3pJo=",
        "originContent": "## Getting Started",
        "translatedContent": "## 시작하기"
      },
      {
        "row": 26,
        "rowsha": "UZ1F0QmkQoxf5S8hzS+qlRRTzbJgr+1whEHuq3/FbQQ=",
        "originContent": "We use LLaVA codebase to train FastVLM variants. In order to train or finetune your own variants, ",
        "translatedContent": "우리는 FastVLM 변형을 학습하기 위해 LLaVA 코드베이스를 사용합니다. 자체 변형을 학습하거나 미세 조정하려면,"
      },
      {
        "row": 27,
        "rowsha": "vXpfCE1cQ3SrbfrjVKdhNXcRnuNIAuC1ODYjmFfvEAU=",
        "originContent": "please follow instructions provided in [LLaVA](https://github.com/haotian-liu/LLaVA) codebase. ",
        "translatedContent": "[LLaVA](https://github.com/haotian-liu/LLaVA) 코드베이스에서 제공하는 지침을 따라 주십시오."
      },
      {
        "row": 28,
        "rowsha": "lInJjWomOVM0sJWaujawC9DYrHOJ3dZ00dRFm0rFLVw=",
        "originContent": "We provide instructions for running inference with our models.   ",
        "translatedContent": "우리 모델로 추론을 실행하는 방법도 안내합니다.   "
      },
      {
        "row": 29,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 30,
        "rowsha": "iJa409tTUC9P1gTULbIw6Kod+KAUdLl5kgZl7whoChE=",
        "originContent": "### Setup",
        "translatedContent": "### 설정</translate-content>"
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 2,
    "Content": "```bash\nconda create -n fastvlm python=3.10\nconda activate fastvlm\npip install -e .\n```",
    "ContentSha": "T3dliF50X9Nt+qx0S9Qem9C6tFumV7uUEcvPZ0r7hO0=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bash\nconda create -n fastvlm python=3.10\nconda activate fastvlm\npip install -e .\n```",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "rTMjNc+qNIxb1xXDR5hEYiJw1fc4muBwuBsTXVIaIho=",
        "originContent": "```bash",
        "translatedContent": "```bash"
      },
      {
        "row": 2,
        "rowsha": "CsFYljSuqOAMOoSwDtAMwiwovcx2tjD/nemFG7SA8zA=",
        "originContent": "conda create -n fastvlm python=3.10",
        "translatedContent": "conda create -n fastvlm python=3.10"
      },
      {
        "row": 3,
        "rowsha": "uXK2931BK8Xf8HOVjoQ3QWVdxUDSP91w+UuHGRiUvXo=",
        "originContent": "conda activate fastvlm",
        "translatedContent": "conda activate fastvlm"
      },
      {
        "row": 4,
        "rowsha": "knVRIKwsU4emj9biFUgJoBjbMP5EER6U5AGxS0Ix1+Y=",
        "originContent": "pip install -e .",
        "translatedContent": "pip install -e ."
      },
      {
        "row": 5,
        "rowsha": "8bkBhHOQsO1+N058HkZOwXtGpCfEh6WtbL0pBkBQg9U=",
        "originContent": "```",
        "translatedContent": "```"
      }
    ],
    "IsCodeBlock": true
  },
  {
    "Id": 3,
    "Content": "\n### Model Zoo\nFor detailed information on various evaluations, please refer to our [paper](https://www.arxiv.org/abs/2412.13303).\n\n| Model        | Stage |                                            Pytorch Checkpoint (url)                                             |\n|:-------------|:-----:|:---------------------------------------------------------------------------------------------------------------:|\n| FastVLM-0.5B |   2   | [fastvlm_0.5b_stage2](https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_0.5b_stage2.zip) |\n|              |   3   | [fastvlm_0.5b_stage3](https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_0.5b_stage3.zip) |\n| FastVLM-1.5B |   2   | [fastvlm_1.5b_stage2](https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_1.5b_stage2.zip) |\n|              |   3   | [fastvlm_1.5b_stage3](https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_1.5b_stage3.zip)  |\n| FastVLM-7B   |   2   | [fastvlm_7b_stage2](https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_7b_stage2.zip)  |\n|              |   3   | [fastvlm_7b_stage3](https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_7b_stage3.zip)  |\n\nTo download all the pretrained checkpoints run the command below (note that this might take some time depending on your connection so might be good to grab ☕️ while you wait).\n",
    "ContentSha": "SOYyeTJSeV3tyVGGMFuYakkvewSGfMC0U4L2iufKo6s=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### 모델 동물원\n다양한 평가에 대한 자세한 정보는 저희 [논문](https://www.arxiv.org/abs/2412.13303)을 참조하세요.\n\n| 모델          | 단계  |                                            파이토치 체크포인트 (url)                                             |\n|:-------------|:-----:|:---------------------------------------------------------------------------------------------------------------:|\n| FastVLM-0.5B |   2   | [fastvlm_0.5b_stage2](https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_0.5b_stage2.zip) |\n|              |   3   | [fastvlm_0.5b_stage3](https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_0.5b_stage3.zip) |\n| FastVLM-1.5B |   2   | [fastvlm_1.5b_stage2](https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_1.5b_stage2.zip) |\n|              |   3   | [fastvlm_1.5b_stage3](https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_1.5b_stage3.zip)  |\n| FastVLM-7B   |   2   | [fastvlm_7b_stage2](https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_7b_stage2.zip)  |\n|              |   3   | [fastvlm_7b_stage3](https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_7b_stage3.zip)  |\n\n모든 사전 학습된 체크포인트를 다운로드하려면 아래 명령어를 실행하세요 (연결 상태에 따라 시간이 걸릴 수 있으니 ☕️를 준비해두는 것이 좋습니다).\n\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "### 모델 동물원"
      },
      {
        "row": 2,
        "rowsha": "AokGAShXuFoP04pgNTbDQs5ciOX7z/y7ej0I58mh7AQ=",
        "originContent": "### Model Zoo",
        "translatedContent": "다양한 평가에 대한 자세한 정보는 저희 [논문](https://www.arxiv.org/abs/2412.13303)을 참조하세요."
      },
      {
        "row": 3,
        "rowsha": "uwk92fgLGMykcUcd5kDa6goQiOvzCdLchBj+LPa5/QU=",
        "originContent": "For detailed information on various evaluations, please refer to our [paper](https://www.arxiv.org/abs/2412.13303).",
        "translatedContent": ""
      },
      {
        "row": 4,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "| 모델          | 단계  |                                            파이토치 체크포인트 (url)                                             |"
      },
      {
        "row": 5,
        "rowsha": "DR6HZSMrk0OE7uAclXxCnKuYwZMSJFR4DT6FQSQ4Kko=",
        "originContent": "| Model        | Stage |                                            Pytorch Checkpoint (url)                                             |",
        "translatedContent": "|:-------------|:-----:|:---------------------------------------------------------------------------------------------------------------:|"
      },
      {
        "row": 6,
        "rowsha": "zJtCBnmPbITmWDOpHm4ezWeHgMj4Dtv1bd1qZfUtkU4=",
        "originContent": "|:-------------|:-----:|:---------------------------------------------------------------------------------------------------------------:|",
        "translatedContent": "| FastVLM-0.5B |   2   | [fastvlm_0.5b_stage2](https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_0.5b_stage2.zip) |"
      },
      {
        "row": 7,
        "rowsha": "JRTQjuGPaigBfFOg0/qoT1Q4EvxI+72rF9fgTMnEGxM=",
        "originContent": "| FastVLM-0.5B |   2   | [fastvlm_0.5b_stage2](https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_0.5b_stage2.zip) |",
        "translatedContent": "|              |   3   | [fastvlm_0.5b_stage3](https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_0.5b_stage3.zip) |"
      },
      {
        "row": 8,
        "rowsha": "gV4nJ6znWZjfHJTMVQbvZ/QkswvsJAFVTbJf0eKontE=",
        "originContent": "|              |   3   | [fastvlm_0.5b_stage3](https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_0.5b_stage3.zip) |",
        "translatedContent": "| FastVLM-1.5B |   2   | [fastvlm_1.5b_stage2](https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_1.5b_stage2.zip) |"
      },
      {
        "row": 9,
        "rowsha": "Hc8XA1uORzx/pJ15qJ2d9IXn9Lzlkqd8cDXN8V96JNI=",
        "originContent": "| FastVLM-1.5B |   2   | [fastvlm_1.5b_stage2](https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_1.5b_stage2.zip) |",
        "translatedContent": "|              |   3   | [fastvlm_1.5b_stage3](https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_1.5b_stage3.zip)  |"
      },
      {
        "row": 10,
        "rowsha": "jiyV8Ah5kdBZdLaW+ILbKd6jbNFuimpXFzHnBFMZrw0=",
        "originContent": "|              |   3   | [fastvlm_1.5b_stage3](https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_1.5b_stage3.zip)  |",
        "translatedContent": "| FastVLM-7B   |   2   | [fastvlm_7b_stage2](https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_7b_stage2.zip)  |"
      },
      {
        "row": 11,
        "rowsha": "tf+47x25kBNqna36a5PtOQTPr1AER36VLiUOCtPG1qA=",
        "originContent": "| FastVLM-7B   |   2   | [fastvlm_7b_stage2](https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_7b_stage2.zip)  |",
        "translatedContent": "|              |   3   | [fastvlm_7b_stage3](https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_7b_stage3.zip)  |"
      },
      {
        "row": 12,
        "rowsha": "Qeue66CqMjKJnPUqJJGTtv0Ta/MACAP3EUXnIbKjZ9w=",
        "originContent": "|              |   3   | [fastvlm_7b_stage3](https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_7b_stage3.zip)  |",
        "translatedContent": ""
      },
      {
        "row": 13,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "모든 사전 학습된 체크포인트를 다운로드하려면 아래 명령어를 실행하세요 (연결 상태에 따라 시간이 걸릴 수 있으니 ☕️를 준비해두는 것이 좋습니다)."
      },
      {
        "row": 14,
        "rowsha": "k6ffSX+Nds0X2lXPlofrwas27Z3d51ct+GLCk9FYik0=",
        "originContent": "To download all the pretrained checkpoints run the command below (note that this might take some time depending on your connection so might be good to grab ☕️ while you wait).",
        "translatedContent": ""
      },
      {
        "row": 15,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 4,
    "Content": "```bash\nbash get_models.sh   # Files will be downloaded to `checkpoints` directory.\n```",
    "ContentSha": "wq6Hvv1Wmv/NNPVjoLg89dkODyIp4n7an5RFvX/98+I=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bash\nbash get_models.sh   # Files will be downloaded to `checkpoints` directory.\n```",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "rTMjNc+qNIxb1xXDR5hEYiJw1fc4muBwuBsTXVIaIho=",
        "originContent": "```bash",
        "translatedContent": "```bash"
      },
      {
        "row": 2,
        "rowsha": "yk9h1Ruku5QmQQGZU6ywdPUVw1B+CaOe8gsdlnzVwIs=",
        "originContent": "bash get_models.sh   # Files will be downloaded to `checkpoints` directory.",
        "translatedContent": "bash get_models.sh   # Files will be downloaded to `checkpoints` directory."
      },
      {
        "row": 3,
        "rowsha": "8bkBhHOQsO1+N058HkZOwXtGpCfEh6WtbL0pBkBQg9U=",
        "originContent": "```",
        "translatedContent": "```"
      }
    ],
    "IsCodeBlock": true
  },
  {
    "Id": 5,
    "Content": "\n### Usage Example\nTo run inference of PyTorch checkpoint, follow the instruction below",
    "ContentSha": "d8WgBdyUYsRoNoLsh7b2NX8+hwPCst7b5AkcDsyhtt8=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### 사용 예시  \nPyTorch 체크포인트의 추론을 실행하려면 아래 지침을 따르세요.\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "### 사용 예시  "
      },
      {
        "row": 2,
        "rowsha": "VL2pmH7xD+uBhvgpZPXwHbQ7euVDY/RuQBxlpsRCABU=",
        "originContent": "### Usage Example",
        "translatedContent": "PyTorch 체크포인트의 추론을 실행하려면 아래 지침을 따르세요."
      },
      {
        "row": 3,
        "rowsha": "6P2D/aZp3l5rnwQFu/X9PvnjletR47x2ynoDLVsJm+4=",
        "originContent": "To run inference of PyTorch checkpoint, follow the instruction below",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 6,
    "Content": "```bash\npython predict.py --model-path /path/to/checkpoint-dir \\\n                  --image-file /path/to/image.png \\\n                  --prompt \"Describe the image.\"\n```",
    "ContentSha": "azshNSSHOWT2np4N696p+W34+hbaOJsqoJ8qHV3ZEG4=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bash\npython predict.py --model-path /path/to/checkpoint-dir \\\n                  --image-file /path/to/image.png \\\n                  --prompt \"Describe the image.\"\n```",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "rTMjNc+qNIxb1xXDR5hEYiJw1fc4muBwuBsTXVIaIho=",
        "originContent": "```bash",
        "translatedContent": "```bash"
      },
      {
        "row": 2,
        "rowsha": "8bwtPdi5Yqezu6w3Yu7fl8ywcXJBnH85aS8WBrC2zfE=",
        "originContent": "python predict.py --model-path /path/to/checkpoint-dir \\",
        "translatedContent": "python predict.py --model-path /path/to/checkpoint-dir \\"
      },
      {
        "row": 3,
        "rowsha": "81x/XUTKjINZI0XteFbhg5+P9gJLhiiz+Keu2NgCiaE=",
        "originContent": "                  --image-file /path/to/image.png \\",
        "translatedContent": "                  --image-file /path/to/image.png \\"
      },
      {
        "row": 4,
        "rowsha": "6l8MkyWqk1EnxXDjC3BiVh5uyxPpuGs54mcf3kvZ7ws=",
        "originContent": "                  --prompt \"Describe the image.\"",
        "translatedContent": "                  --prompt \"Describe the image.\""
      },
      {
        "row": 5,
        "rowsha": "8bkBhHOQsO1+N058HkZOwXtGpCfEh6WtbL0pBkBQg9U=",
        "originContent": "```",
        "translatedContent": "```"
      }
    ],
    "IsCodeBlock": true
  },
  {
    "Id": 7,
    "Content": "\n### Inference on Apple Silicon\nTo run inference on Apple Silicon, pytorch checkpoints have to be exported to format \nsuitable for running on Apple Silicon, detailed instructions and code can be found [`model_export`](model_export/) subfolder.\nPlease see the README there for more details.\n\nFor convenience, we provide 3 models that are in Apple Silicon compatible format: [fastvlm_0.5b_stage3](https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_0.5b_stage3_llm.fp16.zip), \n[fastvlm_1.5b_stage3](https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_1.5b_stage3_llm.int8.zip), \n[fastvlm_7b_stage3](https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_7b_stage3_llm.int4.zip). \nWe encourage developers to export the model of their choice with the appropriate quantization levels following \nthe instructions in [`model_export`](model_export/).\n\n### Inference on Apple Devices\nTo run inference on Apple devices like iPhone, iPad or Mac, see [`app`](app/) subfolder for more details.\n\n## Citation\nIf you found this code useful, please cite the following paper:",
    "ContentSha": "l+rw94czofFmx8Iev6xIM/gmT3Nu8eH7y/iIGFP+bxA=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### Apple Silicon에서 추론하기  \nApple Silicon에서 추론을 실행하려면 pytorch 체크포인트를 Apple Silicon에서 실행할 수 있는 형식으로 내보내야 하며, 자세한 지침과 코드는 [`model_export`](model_export/) 하위 폴더에서 확인할 수 있습니다.  \n자세한 내용은 해당 README를 참조하십시오.  \n\n편의를 위해 Apple Silicon 호환 형식의 3가지 모델을 제공합니다: [fastvlm_0.5b_stage3](https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_0.5b_stage3_llm.fp16.zip),  \n[fastvlm_1.5b_stage3](https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_1.5b_stage3_llm.int8.zip),  \n[fastvlm_7b_stage3](https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_7b_stage3_llm.int4.zip).  \n개발자분들은 [`model_export`](model_export/)의 지침에 따라 적절한 양자화 수준으로 원하는 모델을 내보내는 것을 권장합니다.  \n\n### Apple 기기에서 추론하기  \niPhone, iPad 또는 Mac과 같은 Apple 기기에서 추론을 실행하려면 [`app`](app/) 하위 폴더를 참조하십시오.  \n\n## 인용  \n이 코드를 유용하게 사용하셨다면, 다음 논문을 인용해 주십시오:\n\n\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "### Apple Silicon에서 추론하기  "
      },
      {
        "row": 2,
        "rowsha": "NXa++l2NDMJL0bGTnvmmwTraFGTC1we15uahI5MTsa4=",
        "originContent": "### Inference on Apple Silicon",
        "translatedContent": "Apple Silicon에서 추론을 실행하려면 pytorch 체크포인트를 Apple Silicon에서 실행할 수 있는 형식으로 내보내야 하며, 자세한 지침과 코드는 [`model_export`](model_export/) 하위 폴더에서 확인할 수 있습니다.  "
      },
      {
        "row": 3,
        "rowsha": "UdRodf8F0dw+XJHY8/jS68PtY0p5MyLWD3nZzAbD+Ts=",
        "originContent": "To run inference on Apple Silicon, pytorch checkpoints have to be exported to format ",
        "translatedContent": "자세한 내용은 해당 README를 참조하십시오.  "
      },
      {
        "row": 4,
        "rowsha": "v7gTQhg9cNGz5e28hvwaQy3A13t56wEmY64NhIyTq9I=",
        "originContent": "suitable for running on Apple Silicon, detailed instructions and code can be found [`model_export`](model_export/) subfolder.",
        "translatedContent": ""
      },
      {
        "row": 5,
        "rowsha": "RNkN4dwSh5GjEvk1L1Icnt4FR6ZnHeukNimIPGh4kSI=",
        "originContent": "Please see the README there for more details.",
        "translatedContent": "편의를 위해 Apple Silicon 호환 형식의 3가지 모델을 제공합니다: [fastvlm_0.5b_stage3](https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_0.5b_stage3_llm.fp16.zip),  "
      },
      {
        "row": 6,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "[fastvlm_1.5b_stage3](https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_1.5b_stage3_llm.int8.zip),  "
      },
      {
        "row": 7,
        "rowsha": "pUoTkqMVYHXTjqfSLPPDvSmtyNmXX2FF6n0qi8AKkRg=",
        "originContent": "For convenience, we provide 3 models that are in Apple Silicon compatible format: [fastvlm_0.5b_stage3](https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_0.5b_stage3_llm.fp16.zip), ",
        "translatedContent": "[fastvlm_7b_stage3](https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_7b_stage3_llm.int4.zip).  "
      },
      {
        "row": 8,
        "rowsha": "6irmAf7U+H5zk4YPqiiF4OUQ6gFcheOlYtKFcTiAeoI=",
        "originContent": "[fastvlm_1.5b_stage3](https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_1.5b_stage3_llm.int8.zip), ",
        "translatedContent": "개발자분들은 [`model_export`](model_export/)의 지침에 따라 적절한 양자화 수준으로 원하는 모델을 내보내는 것을 권장합니다.  "
      },
      {
        "row": 9,
        "rowsha": "CXIKRsCIo8caPfEvC3afUo8GblRYobfw6kJq+Tl5vm4=",
        "originContent": "[fastvlm_7b_stage3](https://ml-site.cdn-apple.com/datasets/fastvlm/llava-fastvithd_7b_stage3_llm.int4.zip). ",
        "translatedContent": ""
      },
      {
        "row": 10,
        "rowsha": "64H2/wWYBaNOTsYAYpyBiAffep//2RUKJXNz/kk5weg=",
        "originContent": "We encourage developers to export the model of their choice with the appropriate quantization levels following ",
        "translatedContent": "### Apple 기기에서 추론하기  "
      },
      {
        "row": 11,
        "rowsha": "e/Y/2hGlJSZ+bWvzHbDJXap2BEpKLTiKSDNegATyt+o=",
        "originContent": "the instructions in [`model_export`](model_export/).",
        "translatedContent": "iPhone, iPad 또는 Mac과 같은 Apple 기기에서 추론을 실행하려면 [`app`](app/) 하위 폴더를 참조하십시오.  "
      },
      {
        "row": 12,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 13,
        "rowsha": "9+g0tQT/UWocVywJcp84359W/nnFHw6+VD0RsxFPU0o=",
        "originContent": "### Inference on Apple Devices",
        "translatedContent": "## 인용  "
      },
      {
        "row": 14,
        "rowsha": "rSoAI+6XgZgckIbxQItfXdm1ZsSyjeBToTOTgma/Ey4=",
        "originContent": "To run inference on Apple devices like iPhone, iPad or Mac, see [`app`](app/) subfolder for more details.",
        "translatedContent": "이 코드를 유용하게 사용하셨다면, 다음 논문을 인용해 주십시오:"
      },
      {
        "row": 15,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 16,
        "rowsha": "ZwTp5ajUmpHTJefyHhIKzXcG2wnB1jv8iv8cvmdcb/g=",
        "originContent": "## Citation",
        "translatedContent": ""
      },
      {
        "row": 17,
        "rowsha": "D241nVniXLP9c8MNkhjVIWtN79G45BL9pkEXHf5ROkE=",
        "originContent": "If you found this code useful, please cite the following paper:",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 8,
    "Content": "```\n@InProceedings{fastvlm2025,\n  author = {Pavan Kumar Anasosalu Vasu, Fartash Faghri, Chun-Liang Li, Cem Koc, Nate True, Albert Antony, Gokul Santhanam, James Gabriel, Peter Grasch, Oncel Tuzel, Hadi Pouransari},\n  title = {FastVLM: Efficient Vision Encoding for Vision Language Models},\n  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n  month = {June},\n  year = {2025},\n}\n```",
    "ContentSha": "wktJnJsNX2UAMFMXYCpdbBjqC4bOogQCLoWSAv05pPc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```\n@InProceedings{fastvlm2025,\n  author = {Pavan Kumar Anasosalu Vasu, Fartash Faghri, Chun-Liang Li, Cem Koc, Nate True, Albert Antony, Gokul Santhanam, James Gabriel, Peter Grasch, Oncel Tuzel, Hadi Pouransari},\n  title = {FastVLM: Efficient Vision Encoding for Vision Language Models},\n  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n  month = {June},\n  year = {2025},\n}\n```",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "8bkBhHOQsO1+N058HkZOwXtGpCfEh6WtbL0pBkBQg9U=",
        "originContent": "```",
        "translatedContent": "```"
      },
      {
        "row": 2,
        "rowsha": "PJ+Dbzx1scBgzlQ/eMIJeIoUZHibH7qdlpq1YiJXLIo=",
        "originContent": "@InProceedings{fastvlm2025,",
        "translatedContent": "@InProceedings{fastvlm2025,"
      },
      {
        "row": 3,
        "rowsha": "bxsxnQ41EAsvGU7G989ZnfY3L7+J2SzSL1Ib724RvMI=",
        "originContent": "  author = {Pavan Kumar Anasosalu Vasu, Fartash Faghri, Chun-Liang Li, Cem Koc, Nate True, Albert Antony, Gokul Santhanam, James Gabriel, Peter Grasch, Oncel Tuzel, Hadi Pouransari},",
        "translatedContent": "  author = {Pavan Kumar Anasosalu Vasu, Fartash Faghri, Chun-Liang Li, Cem Koc, Nate True, Albert Antony, Gokul Santhanam, James Gabriel, Peter Grasch, Oncel Tuzel, Hadi Pouransari},"
      },
      {
        "row": 4,
        "rowsha": "GqdE3lse1CZavRo9ckXarVhfHGIasKwi8iILuoFynTg=",
        "originContent": "  title = {FastVLM: Efficient Vision Encoding for Vision Language Models},",
        "translatedContent": "  title = {FastVLM: Efficient Vision Encoding for Vision Language Models},"
      },
      {
        "row": 5,
        "rowsha": "0cefqeJxOVmeOvCIZkK7RTs/2p5+7jncPiDhs6G0p7M=",
        "originContent": "  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},",
        "translatedContent": "  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},"
      },
      {
        "row": 6,
        "rowsha": "U1Izw4geK+wX4RkAbLnK9NT0MBNGq5VLS9uL46bPZVs=",
        "originContent": "  month = {June},",
        "translatedContent": "  month = {June},"
      },
      {
        "row": 7,
        "rowsha": "IbKbfF34UsZcnQSCU7je3bJsvgw8HBWO8MxHAycUz7M=",
        "originContent": "  year = {2025},",
        "translatedContent": "  year = {2025},"
      },
      {
        "row": 8,
        "rowsha": "0Qs2qnSlm89KiBhYN/ZYr682Ru/yuxbDko0OkzXpRdI=",
        "originContent": "}",
        "translatedContent": "}"
      },
      {
        "row": 9,
        "rowsha": "8bkBhHOQsO1+N058HkZOwXtGpCfEh6WtbL0pBkBQg9U=",
        "originContent": "```",
        "translatedContent": "```"
      }
    ],
    "IsCodeBlock": true
  },
  {
    "Id": 9,
    "Content": "\n## Acknowledgements\nOur codebase is built using multiple opensource contributions, please see [ACKNOWLEDGEMENTS](ACKNOWLEDGEMENTS) for more details. \n\n## License\nPlease check out the repository [LICENSE](LICENSE) before using the provided code and\n[LICENSE_MODEL](LICENSE_MODEL) for the released models.\n",
    "ContentSha": "9l39I30jCrjEBn9TZjuj43Rk1zD0FREHZ/k3RZXs2sc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## 감사의 글\n우리 코드베이스는 여러 오픈소스 기여로 구축되었습니다. 자세한 내용은 [ACKNOWLEDGEMENTS](ACKNOWLEDGEMENTS)를 참조하십시오.\n\n## 라이선스\n제공된 코드를 사용하기 전에 저장소의 [LICENSE](LICENSE)를 확인하시고\n출시된 모델에 대해서는 [LICENSE_MODEL](LICENSE_MODEL)을 확인하십시오.\n\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## 감사의 글"
      },
      {
        "row": 2,
        "rowsha": "HvkwNudYOlwL8j/t4djBVF3hUJwHWa2r5QjmSxgq3AA=",
        "originContent": "## Acknowledgements",
        "translatedContent": "우리 코드베이스는 여러 오픈소스 기여로 구축되었습니다. 자세한 내용은 [ACKNOWLEDGEMENTS](ACKNOWLEDGEMENTS)를 참조하십시오."
      },
      {
        "row": 3,
        "rowsha": "RJXd3com8bsu7kR/bmE4eqzaQpI0iRS5V+1VRag/Leo=",
        "originContent": "Our codebase is built using multiple opensource contributions, please see [ACKNOWLEDGEMENTS](ACKNOWLEDGEMENTS) for more details. ",
        "translatedContent": ""
      },
      {
        "row": 4,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## 라이선스"
      },
      {
        "row": 5,
        "rowsha": "bFSaVtsB4CHySNjaeCiaMZfT24b+DTbTM4HQ38cR6Lw=",
        "originContent": "## License",
        "translatedContent": "제공된 코드를 사용하기 전에 저장소의 [LICENSE](LICENSE)를 확인하시고"
      },
      {
        "row": 6,
        "rowsha": "H+kY5xnGhZR4AP8HXEBrI5pJl2u2T6PRw9DgjCUq7pc=",
        "originContent": "Please check out the repository [LICENSE](LICENSE) before using the provided code and",
        "translatedContent": "출시된 모델에 대해서는 [LICENSE_MODEL](LICENSE_MODEL)을 확인하십시오."
      },
      {
        "row": 7,
        "rowsha": "zuOf0YpuoKYLTi5hsnbW2+hldgBXg2bI7qG+fu8b8Pc=",
        "originContent": "[LICENSE_MODEL](LICENSE_MODEL) for the released models.",
        "translatedContent": ""
      },
      {
        "row": 8,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  }
]