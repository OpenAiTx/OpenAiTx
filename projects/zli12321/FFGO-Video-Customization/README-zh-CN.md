# FFGO: è§†é¢‘å†…å®¹å®šåˆ¶çš„é¦–é€‰èµ·ç‚¹

**â€œè§†é¢‘å†…å®¹å®šåˆ¶çš„é¦–é€‰èµ·ç‚¹â€å®˜æ–¹ä»“åº“**

**English:**  
[[ç½‘ç«™](http://firstframego.github.io)] | [[è®ºæ–‡](https://arxiv.org/abs/2511.15700)] | [[ğŸ”´ YouTubeï¼šéå®˜æ–¹ç¤¾åŒºå±•ç¤º](https://www.youtube.com/watch?v=Dks3q5w7sdw)] | [[ğŸ”´ çœŸå®ç”¨æˆ·æ¼”ç¤º](https://github.com/kijai/ComfyUI-WanVideoWrapper/issues/1676)]

**ä¸­æ–‡:**  
[[æ–°æ™ºå…ƒ](https://mp.weixin.qq.com/s/XQGmskJqqFdKx4vCc45tDA)] | [[bilibili](https://www.bilibili.com/video/BV1DQSzB9Eo7/)]


![teaser.gif](https://raw.githubusercontent.com/zli12321/FFGO-Video-Customization/main/./asset/git.gif)



### å³å°†æ¨å‡º  
- æ·»åŠ å®˜æ–¹ ComfyUI å·¥ä½œæµæ”¯æŒï¼Œä½¿ç”¨æˆ‘ä»¬è®­ç»ƒçš„ LoRAï¼Œæ‰€æœ‰å‚æ•°è®¾ç½®ä¸æ¨ç†ä»£ç ä¿æŒä¸€è‡´ã€‚  
- å¾…åŠäº‹é¡¹ï¼šå‘å¸ƒé€‚ç”¨äºæ›´å°è§„æ¨¡åŸºç¡€æ¨¡å‹çš„ LoRA - æ··å…ƒ 1.5 8B æˆ– Wan2.2 5B  


**ğŸ¤— Huggingface ä¸Šçš„ Lora é€‚é…å™¨ï¼š**  
- [FFGO-Lora-Adapter](https://huggingface.co/Video-Customization/FFGO-Lora-Adapter)


#### è®­ç»ƒæ•°æ®ç¤ºä¾‹  
- **è¯·æ³¨æ„ï¼Œæˆ‘ä»¬ç›®å‰åªæä¾›äº† 50 ä¸ªè®­ç»ƒè§†é¢‘çš„ä¸€ä¸ªå­é›†ä»¥æ¼”ç¤ºæ•°æ®æ ¼å¼ã€‚**

- è¯·æŸ¥çœ‹ ```/Data/train/``` æ–‡ä»¶å¤¹

### å®‰è£…é…ç½®  
- åˆ›å»ºç¯å¢ƒ
```
conda create -n ffgo python=3.11
conda activate ffgo
```
- å…‹éš†ä»“åº“å¹¶è®¾ç½®

```
git clone https://github.com/zli12321/FFGO-Video-Customization.git
cd FFGO-Video-Customization
bash setup.sh
```


### Test data
- Test data is available in [Data](https://github.com/zli12321/FFGO-Video-Customization/tree/main/Data/combined_first_frames) folder. All test data involving personal portrait rights has been removed. [0-data.csv](https://github.com/zli12321/FFGO-Video-Customization/blob/main/Data/combined_first_frames/0-data.csv) has the input image path and the caption to generate the video.
- Test data materials are available in [data_materials](https://github.com/zli12321/FFGO-Video-Customization/tree/main/Data/data_materials) folder. These are materials that can form the final input image for video generations.
- Get your own test data: find any images online and segment out the elements as RGBA layer, then combine it with a background using our [combine script]().


### Running Inference

- **When running on your own data, make sure to append our learned transition phrase, "ad23r2 the camera view suddenly changes. ", to your text prompt to ensure the model behaves correctly.**

- **All video results in the paper are generated at 1280â€¯Ã—â€¯720 resolution with 81 frames, which requires an H200 GPU for inference unless memory-saving techniques are applied. For lower resource usage, 640â€¯Ã—â€¯480 resolution videos can be generated without H200. However outputs at this lower resolution can differ significantly in content from the 1280â€¯Ã—â€¯720 results as we shown in the paper.**

- **We are using H200 (141GB RAM) to run inference. If you are using A100 or H100, the memory saving such as cpu offload features need to be turned on.**

1. Download [Wan2.2-I2V-14B](https://huggingface.co/Wan-AI/Wan2.2-I2V-A14B) from huggingface or modelscope and download our Lora adapters. 

```bash
bash download.sh
```


2. Run fun demo video inference

```
bash ./example_single_inference.sh
```
3. åœ¨æˆ‘ä»¬çš„ç¤ºä¾‹æµ‹è¯•æ•°æ®é›†ä¸Šè¿è¡Œè¿ç»­æ¨ç†

```
bash example_inference.sh
```

<!-- 
4. Run 4 Step Lora speedup (Will cause quality degrade and inconsistency.)

```
bash ./example_4_step_lora_inference.sh
``` -->


### å¼•ç”¨
```
@article{chen2025first,
  title={First Frame Is the Place to Go for Video Content Customization},
  author={Chen, Jingxi and Li, Zongxia and Liu, Zhichao and Shi, Guangyao and Wu, Xiyang and Liu, Fuxiao and Fermuller, Cornelia and Feng, Brandon Y and Aloimonos, Yiannis},
  journal={arXiv preprint arXiv:2511.15700},
  year={2025}
}
```

---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2026-02-08

---