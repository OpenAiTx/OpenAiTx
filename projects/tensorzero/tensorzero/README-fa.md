<img src="https://github.com/user-attachments/assets/47d67430-386d-4675-82ad-d4734d3262d9" width=128 height=128>

# TensorZero

**تنسورزیرو یک حلقه بازخورد برای بهینه‌سازی برنامه‌های LLM ایجاد می‌کند — داده‌های تولید را به مدل‌های هوشمندتر، سریع‌تر و ارزان‌تر تبدیل می‌کند.**

1. درگاه مدل ما را ادغام کنید
2. متریک‌ها یا بازخورد ارسال کنید
3. پرامپت‌ها، مدل‌ها و استراتژی‌های استنتاج را بهینه کنید
4. پیشرفت LLMهای خود را در طول زمان مشاهده کنید

این پلتفرم یک **چرخه داده و یادگیری برای LLMها** فراهم می‌کند با یکپارچه‌سازی:

- [x] **استنتاج:** یک API برای همه LLMها، با کمتر از ۱ میلی‌ثانیه سربار P99
- [x] **قابلیت مشاهده:** استنتاج و بازخورد → پایگاه داده شما
- [x] **بهینه‌سازی:** از پرامپت تا فاین‌تیونینگ و RL
- [x] **ارزیابی:** مقایسه پرامپت‌ها، مدل‌ها، استراتژی‌های استنتاج
- [x] **آزمایش:** تست A/B داخلی، مسیریابی، جایگزینی

---

<p align="center">
  <b><a href="https://www.tensorzero.com/" target="_blank">وبسایت</a></b>
  ·
  <b><a href="https://www.tensorzero.com/docs" target="_blank">مستندات</a></b>
  ·
  <b><a href="https://www.x.com/tensorzero" target="_blank">توییتر</a></b>
  ·
  <b><a href="https://www.tensorzero.com/slack" target="_blank">اسلک</a></b>
  ·
  <b><a href="https://www.tensorzero.com/discord" target="_blank">دیسکورد</a></b>
  <br>
  <br>
  <b><a href="https://www.tensorzero.com/docs/quickstart" target="_blank">شروع سریع (۵ دقیقه‌ای)</a></b>
  ·
  <b><a href="https://www.tensorzero.com/docs/gateway/tutorial" target="_blank">آموزش جامع</a></b>
  ·
  <b><a href="https://www.tensorzero.com/docs/gateway/deployment" target="_blank">راهنمای استقرار</a></b>
  ·
  <b><a href="https://www.tensorzero.com/docs/gateway/api-reference" target="_blank">مستندات API</a></b>
  ·
  <b><a href="https://www.tensorzero.com/docs/gateway/deployment" target="_blank">مستندات پیکربندی</a></b>
</p>

---

<table>
  <tr>
    <td width="30%" valign="top"><b>تنسورزیرو چیست؟</b></td>
    <td width="70%" valign="top">تنسورزیرو یک چارچوب متن‌باز برای ساخت برنامه‌های LLM در سطح تولید است. این پلتفرم یک درگاه LLM، قابلیت مشاهده، بهینه‌سازی، ارزیابی و آزمایش را یکپارچه می‌کند.</td>
  </tr>
  <tr>
    <td width="30%" valign="top"><b>تفاوت تنسورزیرو با سایر چارچوب‌های LLM چیست؟</b></td>
    <td width="70%" valign="top">
      1. تنسورزیرو به شما امکان می‌دهد برنامه‌های پیچیده LLM را بر اساس متریک‌های تولید و بازخورد انسانی بهینه کنید.<br>
      2. تنسورزیرو نیازهای برنامه‌های LLM در مقیاس صنعتی را پشتیبانی می‌کند: تأخیر کم، توان عملیاتی بالا، ایمنی نوع، خودمیزبان، GitOps، قابلیت سفارشی‌سازی و غیره.<br>
      3. تنسورزیرو کل پشته LLMOps را یکپارچه می‌کند و مزایای ترکیبی ایجاد می‌کند. به عنوان مثال، ارزیابی‌های LLM می‌توانند برای فاین‌تیونینگ مدل‌ها در کنار داوران هوش مصنوعی استفاده شوند.
    </td>
  </tr>
  <tr>
    <td width="30%" valign="top"><b>آیا می‌توانم تنسورزیرو را با ___ استفاده کنم؟</b></td>
    <td width="70%" valign="top">بله. هر زبان برنامه‌نویسی اصلی پشتیبانی می‌شود. شما می‌توانید از تنسورزیرو با کلاینت پایتون ما، هر SDK متناظر با OpenAI، یا API مبتنی بر HTTP ما استفاده کنید.</td>
  </tr>
  <tr>
    <td width="30%" valign="top"><b>آیا تنسورزیرو آماده استفاده در تولید است؟</b></td>
    <td width="70%" valign="top">بله. یک مطالعه موردی: <b><a href="https://www.tensorzero.com/blog/case-study-automating-code-changelogs-at-a-large-bank-with-llms">اتوماسیون تغییرات کد در یک بانک بزرگ با LLMها</a></b></td>
  </tr>
  <tr>
    <td width="30%" valign="top"><b>هزینه تنسورزیرو چقدر است؟</b></td>
    <td width="70%" valign="top">هیچ هزینه‌ای ندارد. تنسورزیرو ۱۰۰٪ خودمیزبان و متن‌باز است. هیچ ویژگی پولی وجود ندارد.</td>
  </tr>
  <tr>
    <td width="30%" valign="top"><b>چه کسانی تنسورزیرو را می‌سازند؟</b></td>
    <td width="70%" valign="top">تیم فنی ما شامل نگهدارنده سابق کامپایلر Rust، پژوهشگران یادگیری ماشین (استنفورد، CMU، آکسفورد، کلمبیا) با هزاران ارجاع و مدیر محصول ارشد یک استارتاپ دکاکورن است. ما توسط همان سرمایه‌گذاران پروژه‌های متن‌باز پیشرو (مانند ClickHouse، CockroachDB) و آزمایشگاه‌های هوش مصنوعی (مانند OpenAI، Anthropic) حمایت می‌شویم.</td>
  </tr>
  <tr>
    <td width="30%" valign="top"><b>چطور شروع کنم؟</b></td>
    <td width="70%" valign="top">می‌توانید تنسورزیرو را به صورت تدریجی به کار بگیرید. <b><a href="https://www.tensorzero.com/docs/quickstart">شروع سریع</a></b> ما از یک پوشش ساده OpenAI به یک برنامه LLM آماده تولید با قابلیت مشاهده و فاین‌تیونینگ تنها در ۵ دقیقه می‌رسد.</td>
  </tr>
</table>

---

## ویژگی‌ها

### 🌐 درگاه LLM

> **یک بار با تنسورزیرو ادغام شوید و به همه ارائه‌دهندگان اصلی LLM دسترسی داشته باشید.**

<table>
  <tr></tr> <!-- flip highlight order -->
  <tr>
    <td width="50%" align="center" valign="middle"><b>ارائه‌دهندگان مدل</b></td>
    <td width="50%" align="center" valign="middle"><b>ویژگی‌ها</b></td>
  </tr>
  <tr>
    <td width="50%" align="left" valign="top">
      <p>
        درگاه تنسورزیرو به طور بومی از موارد زیر پشتیبانی می‌کند:
      </p>
      <ul>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/anthropic">Anthropic</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/aws-bedrock">AWS Bedrock</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/aws-sagemaker">AWS SageMaker</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/azure">Azure OpenAI Service</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/deepseek">DeepSeek</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/fireworks">Fireworks</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/gcp-vertex-ai-anthropic">GCP Vertex AI Anthropic</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/gcp-vertex-ai-gemini">GCP Vertex AI Gemini</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/google-ai-studio-gemini">Google AI Studio (Gemini API)</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/hyperbolic">Hyperbolic</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/mistral">Mistral</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/openai">OpenAI</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/together">Together</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/vllm">vLLM</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/xai">xAI</a></b></li>
      </ul>
      <p>
        <em>
          مورد دیگری نیاز دارید؟
          ارائه‌دهنده شما به احتمال زیاد پشتیبانی می‌شود، زیرا تنسورزیرو با <b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/openai-compatible">هر API سازگار با OpenAI (مانند Ollama)</a></b> ادغام می‌شود.
          </em>
      </p>
    </td>
    <td width="50%" align="left" valign="top">
      <p>
        درگاه تنسورزیرو از ویژگی‌های پیشرفته‌ای مانند موارد زیر پشتیبانی می‌کند:
      </p>
      <ul>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/retries-fallbacks">تلاش مجدد و جایگزینی</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations">بهینه‌سازی زمان استنتاج</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/prompt-templates-schemas">الگوها و اسکیماهای پرامپت</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/tutorial#experimentation">آزمایش (A/B Testing)</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/configuration-reference">پیکربندی به عنوان کد (GitOps)</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/batch-inference">استنتاج دسته‌ای</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/multimodal-inference">استنتاج چندوجهی (VLMs)</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/inference-caching">ذخیره‌سازی استنتاج</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/metrics-feedback">متریک‌ها و بازخورد</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/episodes">گردش‌های کاری چندمرحله‌ای LLM (Episodes)</a></b></li>
        <li><em>و بسیاری دیگر...</em></li>
      </ul>
      <p>
        درگاه تنسورزیرو با زبان Rust 🦀 و با تمرکز بر <b>عملکرد</b> نوشته شده است (کمتر از ۱ میلی‌ثانیه سربار تأخیر p99 در ۱۰٬۰۰۰ QPS).
        مشاهده <b><a href="https://www.tensorzero.com/docs/gateway/benchmarks">بنچمارک‌ها</a></b>.<br>
      </p>
      <p>
        می‌توانید از <b>کلاینت تنسورزیرو</b> (توصیه‌شده)، <b>کلاینت OpenAI</b> یا <b>HTTP API</b> برای استنتاج استفاده کنید.
      </p>
    </td>
  </tr>
</table>

<br>

<details open>
<summary><b>نحوه استفاده: پایتون — کلاینت TensorZero (توصیه‌شده)</b></summary>

شما می‌توانید به هر ارائه‌دهنده‌ای با استفاده از کلاینت پایتون تنسورزیرو دسترسی داشته باشید.

1. `pip install tensorzero`
2. اختیاری: پیکربندی تنسورزیرو را تنظیم کنید.
3. استنتاج را اجرا کنید:

```python
from tensorzero import TensorZeroGateway  # یا AsyncTensorZeroGateway


with TensorZeroGateway.build_embedded(clickhouse_url="...", config_file="...") as client:
    response = client.inference(
        model_name="openai::gpt-4o-mini",
        # به‌راحتی سایر ارائه‌دهندگان را امتحان کنید: "anthropic::claude-3-7-sonnet-20250219"
        input={
            "messages": [
                {
                    "role": "user",
                    "content": "یک هایکو درباره هوش مصنوعی بنویس.",
                }
            ]
        },
    )
```

برای اطلاعات بیشتر به **[شروع سریع](https://www.tensorzero.com/docs/quickstart)** مراجعه کنید.

</details>

<details>
<summary><b>نحوه استفاده: پایتون — کلاینت OpenAI</b></summary>

شما می‌توانید به هر ارائه‌دهنده‌ای با استفاده از کلاینت پایتون OpenAI و تنسورزیرو دسترسی داشته باشید.

1. `pip install tensorzero`
2. اختیاری: پیکربندی تنسورزیرو را تنظیم کنید.
3. استنتاج را اجرا کنید:

```python
from openai import OpenAI  # یا AsyncOpenAI
from tensorzero import patch_openai_client

client = OpenAI()
```python
patch_openai_client(
    client,
    clickhouse_url="http://chuser:chpassword@localhost:8123/tensorzero",
    config_file="config/tensorzero.toml",
    async_setup=False,
)

response = client.chat.completions.create(
    model="tensorzero::model_name::openai::gpt-4o-mini",
    # به راحتی ارائه‌دهندگان دیگر را امتحان کنید: "tensorzero::model_name::anthropic::claude-3-7-sonnet-20250219"
    messages=[
        {
            "role": "user",
            "content": "یک هایکو درباره هوش مصنوعی بنویس.",
        }
    ],
)
```

برای اطلاعات بیشتر به **[شروع سریع](https://www.tensorzero.com/docs/quickstart)** مراجعه کنید.

</details>

<details>
<summary><b>نحوه استفاده: JavaScript / TypeScript (Node) &mdash; کلاینت OpenAI</b></summary>

شما می‌توانید به هر ارائه‌دهنده‌ای با استفاده از کلاینت OpenAI Node به همراه TensorZero دسترسی داشته باشید.

1. `tensorzero/gateway` را با Docker اجرا کنید.
   **[راهنمای کامل →](https://www.tensorzero.com/docs/gateway/deployment)**
2. پیکربندی TensorZero را انجام دهید.
3. استنتاج را اجرا کنید:

```ts
import OpenAI from "openai";

const client = new OpenAI({
  baseURL: "http://localhost:3000/openai/v1",
});

const response = await client.chat.completions.create({
  model: "tensorzero::model_name::openai::gpt-4o-mini",
  // به راحتی ارائه‌دهندگان دیگر را امتحان کنید: "tensorzero::model_name::anthropic::claude-3-7-sonnet-20250219"
  messages: [
    {
      role: "user",
      content: "یک هایکو درباره هوش مصنوعی بنویس.",
    },
  ],
});
```

برای اطلاعات بیشتر به **[شروع سریع](https://www.tensorzero.com/docs/quickstart)** مراجعه کنید.

</details>

<details>
<summary><b>نحوه استفاده: زبان‌ها و پلتفرم‌های دیگر &mdash; API مبتنی بر HTTP</b></summary>

TensorZero تقریباً از هر زبان برنامه‌نویسی یا پلتفرمی از طریق API مبتنی بر HTTP خود پشتیبانی می‌کند.

1. `tensorzero/gateway` را با Docker اجرا کنید.
   **[راهنمای کامل →](https://www.tensorzero.com/docs/gateway/deployment)**
2. اختیاری: پیکربندی TensorZero را انجام دهید.
3. استنتاج را اجرا کنید:

```bash
curl -X POST "http://localhost:3000/inference" \
  -H "Content-Type: application/json" \
  -d '{
    "model_name": "openai::gpt-4o-mini",
    "input": {
      "messages": [
        {
          "role": "user",
          "content": "یک هایکو درباره هوش مصنوعی بنویس."
        }
      ]
    }
  }'
```

برای اطلاعات بیشتر به **[شروع سریع](https://www.tensorzero.com/docs/quickstart)** مراجعه کنید.

</details>

<br>

### 📈 بهینه‌سازی LLM

> **برای بهینه‌سازی آسان پرامپت‌ها، مدل‌ها و استراتژی‌های استنتاج خود متریک‌های تولید و بازخورد انسانی را ارسال کنید — با استفاده از رابط کاربری یا به صورت برنامه‌نویسی.**

#### بهینه‌سازی مدل

مدل‌های متن‌باز و متن‌بسته را با استفاده از یادگیری نظارت‌شده (SFT) و یادگیری مبتنی بر ترجیح (DPO) بهینه کنید.

<table>
  <tr></tr> <!-- flip highlight order -->
  <tr>
    <td width="50%" align="center" valign="middle"><b>یادگیری نظارت‌شده &mdash; رابط کاربری</b></td>
    <td width="50%" align="center" valign="middle"><b>یادگیری ترجیحی (DPO) &mdash; نوت‌بوک Jupyter</b></td>
  </tr>
  <tr>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/cf7acf66-732b-43b3-af2a-5eba1ce40f6f"></td>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/a67a0634-04a7-42b0-b934-9130cb7cdf51"></td>
  </tr>
</table>

#### بهینه‌سازی هنگام استنتاج

عملکرد را با به‌روزرسانی پویا پرامپت‌ها با مثال‌های مرتبط، ترکیب پاسخ‌ها از چندین استنتاج و موارد دیگر افزایش دهید.

<table>
  <tr></tr> <!-- flip highlight order -->
  <tr>
    <td width="50%" align="center" valign="middle"><b><a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#best-of-n-sampling">نمونه‌گیری بهترین از N</a></b></td>
    <td width="50%" align="center" valign="middle"><b><a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#mixture-of-n-sampling">نمونه‌گیری ترکیبی از N</a></b></td>
  </tr>
  <tr>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/c0edfa4c-713c-4996-9964-50c0d26e6970"></td>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/75b5bf05-4c1f-43c4-b158-d69d1b8d05be"></td>
  </tr>
  <tr>
    <td width="50%" align="center" valign="middle"><b><a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#dynamic-in-context-learning-dicl">یادگیری پویا در بستر (DICL)</a></b></td>
    <td width="50%" align="center" valign="middle"><b><a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#chain-of-thought-cot">زنجیره تفکر (CoT)</a></b></td>
  </tr>
  <tr>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/d8489e92-ce93-46ac-9aab-289ce19bb67d"></td>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/ea13d73c-76a4-4e0c-a35b-0c648f898311" height="320"></td>
  </tr>
</table>

_موارد بیشتر به‌زودی..._

<br>

#### بهینه‌سازی پرامپت

پرامپت‌های خود را به صورت برنامه‌نویسی با استفاده از تکنیک‌های بهینه‌سازی مبتنی بر پژوهش بهینه کنید.

<table>
  <tr></tr> <!-- flip highlight order -->
  <tr>
    <td width="50%" align="center" valign="middle"><b><a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#best-of-n-sampling">MIPROv2</a></b></td>
    <td width="50%" align="center" valign="middle"><b><a href="https://github.com/tensorzero/tensorzero/tree/main/examples/gsm8k-custom-recipe-dspy">ادغام DSPy</a></b></td>
  </tr>
  <tr>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/d81a7c37-382f-4c46-840f-e6c2593301db" alt="MIPROv2 diagram"></td>
    <td width="50%" align="center" valign="middle">
      TensorZero با چندین دستورالعمل بهینه‌سازی ارائه می‌شود، اما همچنین می‌توانید به راحتی دستورالعمل‌های خود را بسازید.
      این مثال نشان می‌دهد که چگونه یک تابع TensorZero را با استفاده از یک ابزار دلخواه — در اینجا DSPy، یک کتابخانه محبوب برای مهندسی خودکار پرامپت — بهینه کنید.
    </td>
  </tr>
</table>

_موارد بیشتر به‌زودی..._

<br>

### 🔍 مشاهده‌پذیری LLM

> **برای اشکال‌زدایی فراخوانی‌های منفرد API بزرگ‌نمایی کنید یا برای پایش متریک‌ها در مدل‌ها و پرامپت‌ها در طول زمان کوچک‌نمایی کنید — همه با رابط کاربری متن‌باز TensorZero.**

<table>
  <tr></tr> <!-- flip highlight order -->
  <tr>
    <td width="50%" align="center" valign="middle"><b>مشاهده‌پذیری » استنتاج</b></td>
    <td width="50%" align="center" valign="middle"><b>مشاهده‌پذیری » تابع</b></td>
  </tr>
  <tr>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/2cc3cc9a-f33f-4e94-b8de-07522326f80a"></td>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/00ae6605-8fa0-4efd-8238-ae8ea589860f"></td>
  </tr>
</table>

<br>

### 📊 ارزیابی‌های LLM

> **پرامپت‌ها، مدل‌ها و استراتژی‌های استنتاج را با استفاده از ارزیابی‌های TensorZero مقایسه کنید — با پشتیبانی از اکتشافی‌ها و داورهای LLM.**

<table>
  <tr></tr> <!-- flip highlight order -->
  <tr>
    <td width="50%" align="center" valign="middle"><b>ارزیابی » رابط کاربری</b></td>
    <td width="50%" align="center" valign="middle"><b>ارزیابی » CLI</b></td>
  </tr>
  <tr>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/f4bf54e3-1b63-46c8-be12-2eaabf615699"></td>
    <td width="50%" align="left" valign="middle">
<pre><code class="language-bash">docker compose run --rm evaluations \
  --evaluation-name extract_data \
  --dataset-name hard_test_cases \
  --variant-name gpt_4o \
  --concurrency 5</code></pre>
<pre><code class="language-bash">Run ID: 01961de9-c8a4-7c60-ab8d-15491a9708e4
Number of datapoints: 100
██████████████████████████████████████ 100/100
exact_match: 0.83 ± 0.03
```
semantic_match: ۰.۹۸ ± ۰.۰۱  
item_count: ۷.۱۵ ± ۰.۳۹</code></pre>
    </td>
  </tr>
</table>

## دمو

> **مشاهده پیشرفت LLMها در استخراج داده به صورت زنده با TensorZero!**
>
> **[یادگیری پویا در زمینه (DICL)](https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#dynamic-in-context-learning-dicl)** یک بهینه‌سازی قدرتمند در زمان استنتاج است که به طور پیش‌فرض با TensorZero در دسترس است.
> این قابلیت عملکرد LLM را با وارد کردن خودکار نمونه‌های تاریخی مرتبط به پرامپت، بدون نیاز به فاین‌تیون کردن مدل، بهبود می‌بخشد.

https://github.com/user-attachments/assets/4df1022e-886e-48c2-8f79-6af3cdad79cb

## مهندسی LLM با TensorZero

<br>
<p align="center" >
  <a href="https://www.tensorzero.com/docs">
    <picture>
      <source media="(prefers-color-scheme: light)" srcset="https://github.com/user-attachments/assets/34a92c18-242e-4d76-a99c-861283de68a6">
      <source media="(prefers-color-scheme: dark)" srcset="https://github.com/user-attachments/assets/e8bc699b-6378-4c2a-9cc1-6d189025e270">
      <img alt="TensorZero Flywheel" src="https://github.com/user-attachments/assets/34a92c18-242e-4d76-a99c-861283de68a6" width=720>
    </picture>
  </a>
</p>
<br>

1. **[درگاه TensorZero](https://www.tensorzero.com/docs/gateway/)** یک درگاه مدل با کارایی بالا و نوشته‌شده به زبان Rust 🦀 است که یک رابط API یکپارچه برای همه ارائه‌دهندگان اصلی LLM فراهم می‌کند و امکان یکپارچه‌سازی و جایگزینی متقابل را به آسانی می‌دهد.
2. این درگاه، استنتاج مبتنی بر طرح‌واره ساختاریافته را با تاخیر کمتر از ۱ میلی‌ثانیه در P99 (به **[بنچمارک‌ها](https://www.tensorzero.com/docs/gateway/benchmarks)** مراجعه کنید) و قابلیت مشاهده‌پذیری، آزمایش‌پذیری و **[بهینه‌سازی‌های زمان استنتاج](https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations)** داخلی ارائه می‌کند.
3. همچنین معیارها و بازخوردهای پایین‌دستی مرتبط با این استنتاج‌ها را جمع‌آوری می‌کند، با پشتیبانی درجه یک برای سیستم‌های LLM چندمرحله‌ای.
4. همه چیز در یک دیتابیس ClickHouse ذخیره می‌شود که تحت کنترل شماست و امکان تحلیل‌های بلادرنگ، مقیاس‌پذیر و مناسب توسعه‌دهندگان را فراهم می‌کند.
5. در طول زمان، **[دستورالعمل‌های TensorZero](https://www.tensorzero.com/docs/recipes)** از این داده ساختارمند برای بهینه‌سازی پرامپت‌ها و مدل‌های شما استفاده می‌کند: دستورالعمل‌های آماده برای فرآیندهای متداول مانند فاین‌تیون اجرا کنید یا با هر زبان و پلتفرمی، دستورالعمل اختصاصی خود را بسازید.
6. در نهایت، امکانات آزمایش و ارکستراسیون GitOps در درگاه به شما امکان می‌دهد با اطمینان، چه یک LLM و چه هزاران LLM را تکرار و مستقر کنید.

هدف ما کمک به مهندسان برای ساخت، مدیریت و بهینه‌سازی نسل بعدی اپلیکیشن‌های LLM است: سیستم‌هایی که از تجربه دنیای واقعی یاد می‌گیرند.  
برای اطلاعات بیشتر، **[چشم‌انداز و نقشه راه](https://www.tensorzero.com/docs/vision-roadmap/)** ما را بخوانید.

## شروع کنید

**امروز شروع به ساخت کنید.**  
**[راهنمای شروع سریع](https://www.tensorzero.com/docs/quickstart)** نشان می‌دهد راه‌اندازی یک اپلیکیشن LLM با TensorZero چقدر آسان است.  
اگر می‌خواهید عمیق‌تر شوید، **[آموزش](https://www.tensorzero.com/docs/gateway/tutorial)** نحوه ساخت یک چت‌بات ساده، یک دستیار ایمیل، یک سیستم RAG هواشناسی، و یک خط لوله استخراج داده ساختاریافته را آموزش می‌دهد.

**سوالی دارید؟**  
در **[Slack](https://www.tensorzero.com/slack)** یا **[Discord](https://www.tensorzero.com/discord)** از ما بپرسید.

**از TensorZero در محل کار استفاده می‌کنید؟**  
به ما ایمیل بزنید: **[hello@tensorzero.com](mailto:hello@tensorzero.com)** تا یک کانال Slack یا Teams رایگان برای تیم شما راه‌اندازی کنیم.

**با ما کار کنید.**  
ما **[در نیویورک استخدام می‌کنیم](https://www.tensorzero.com/jobs)**.  
همچنین از **[مشارکت‌های متن‌باز](https://github.com/tensorzero/tensorzero/blob/main/CONTRIBUTING.md)** استقبال می‌کنیم!

## مثال‌ها

ما در حال کار بر روی مجموعه‌ای از **مثال‌های کامل و قابل اجرا** برای نشان دادن چرخه داده و یادگیری TensorZero هستیم.

> **[بهینه‌سازی استخراج داده (NER) با TensorZero](https://github.com/tensorzero/tensorzero/tree/main/examples/data-extraction-ner)**
>
> این مثال نشان می‌دهد چگونه با TensorZero یک خط لوله استخراج داده را بهینه کنید.
> تکنیک‌هایی مانند فاین‌تیون و یادگیری پویا در زمینه (DICL) را به نمایش می‌گذاریم.
> در نهایت، یک مدل بهینه‌شده GPT-4o Mini، عملکرد بهتری نسبت به GPT-4o در این وظیفه دارد — با کسری از هزینه و تاخیر — و با مقدار کمی داده آموزشی.

> **[Agentic RAG — پاسخ‌گویی چندمرحله‌ای به سوالات با LLMها](https://github.com/tensorzero/tensorzero/tree/main/examples/rag-retrieval-augmented-generation/simple-agentic-rag/)**
>
> این مثال نشان می‌دهد چگونه یک عامل بازیابی چندمرحله‌ای با استفاده از TensorZero بسازید.
> عامل به طور تکراری در ویکی‌پدیا جستجو می‌کند تا اطلاعات جمع‌آوری کند و تصمیم می‌گیرد چه زمانی زمینه کافی برای پاسخ به یک سوال پیچیده دارد.

> **[سرودن هایکو مطابق سلیقه یک داور با ترجیحات پنهان](https://github.com/tensorzero/tensorzero/tree/main/examples/haiku-hidden-preferences)**
>
> این مثال با فاین‌تیون کردن GPT-4o Mini، هایکوهایی مطابق یک سلیقه خاص تولید می‌کند.
> "چرخه داده TensorZero در یک جعبه" را عملی خواهید دید: نسخه‌های بهتر به داده بهتر منجر می‌شوند، و داده بهتر به نسخه‌های بهتر.
> با چندین بار فاین‌تیون مدل، پیشرفت را مشاهده خواهید کرد.

> **[افزایش توانایی شطرنج LLM با نمونه‌گیری Best-of-N](https://github.com/tensorzero/tensorzero/tree/main/examples/chess-puzzles/)**
>
> این مثال نشان می‌دهد چگونه نمونه‌گیری Best-of-N می‌تواند توانایی شطرنج بازی کردن یک LLM را با انتخاب بهترین حرکت از بین گزینه‌های تولید شده، به طور قابل توجهی افزایش دهد.

> **[بهبود استدلال ریاضی با دستورالعمل سفارشی مهندسی پرامپت خودکار (DSPy)](https://github.com/tensorzero/tensorzero/tree/main/examples/gsm8k-custom-recipe-dspy)**
>
> TensorZero مجموعه‌ای از دستورالعمل‌های بهینه‌سازی آماده برای فرآیندهای متداول مهندسی LLM ارائه می‌دهد.
> اما شما همچنین می‌توانید به راحتی دستورالعمل‌ها و فرآیندهای اختصاصی خود را بسازید!
> این مثال نشان می‌دهد چگونه یک تابع TensorZero را با هر ابزاری بهینه کنید — اینجا، DSPy.

_و بسیاری دیگر در راه‌اند!_

---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-06-09

---