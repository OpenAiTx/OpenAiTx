<img src="https://github.com/user-attachments/assets/47d67430-386d-4675-82ad-d4734d3262d9" width=128 height=128>

# TensorZero

**TensorZero cr√©e une boucle de r√©troaction pour optimiser les applications LLM ‚Äî transformant les donn√©es de production en mod√®les plus intelligents, plus rapides et moins co√ªteux.**

1. Int√©grez notre passerelle de mod√®les
2. Envoyez des m√©triques ou des retours d‚Äôexp√©rience
3. Optimisez les prompts, mod√®les et strat√©gies d‚Äôinf√©rence
4. Observez l‚Äôam√©lioration de vos LLMs au fil du temps

Il fournit une **boucle de donn√©es & d‚Äôapprentissage pour les LLMs** en unifiant¬†:

- [x] **Inf√©rence¬†:** une API pour tous les LLMs, avec une surcharge P99 <1ms
- [x] **Observabilit√©¬†:** inf√©rence & retours ‚Üí votre base de donn√©es
- [x] **Optimisation¬†:** des prompts au fine-tuning et RL
- [x] **√âvaluations¬†:** comparer prompts, mod√®les, strat√©gies d‚Äôinf√©rence
- [x] **Exp√©rimentation¬†:** tests A/B int√©gr√©s, routage, fallbacks

---

<p align="center">
  <b><a href="https://www.tensorzero.com/" target="_blank">Site Web</a></b>
  ¬∑
  <b><a href="https://www.tensorzero.com/docs" target="_blank">Docs</a></b>
  ¬∑
  <b><a href="https://www.x.com/tensorzero" target="_blank">Twitter</a></b>
  ¬∑
  <b><a href="https://www.tensorzero.com/slack" target="_blank">Slack</a></b>
  ¬∑
  <b><a href="https://www.tensorzero.com/discord" target="_blank">Discord</a></b>
  <br>
  <br>
  <b><a href="https://www.tensorzero.com/docs/quickstart" target="_blank">D√©marrage rapide (5min)</a></b>
  ¬∑
  <b><a href="https://www.tensorzero.com/docs/gateway/tutorial" target="_blank">Tutoriel complet</a></b>
  ¬∑
  <b><a href="https://www.tensorzero.com/docs/gateway/deployment" target="_blank">Guide de d√©ploiement</a></b>
  ¬∑
  <b><a href="https://www.tensorzero.com/docs/gateway/api-reference" target="_blank">R√©f√©rence API</a></b>
  ¬∑
  <b><a href="https://www.tensorzero.com/docs/gateway/deployment" target="_blank">R√©f√©rence de configuration</a></b>
</p>

---

<table>
  <tr>
    <td width="30%" valign="top"><b>Qu‚Äôest-ce que TensorZero¬†?</b></td>
    <td width="70%" valign="top">TensorZero est un framework open-source pour construire des applications LLM de niveau production. Il unifie une passerelle LLM, l‚Äôobservabilit√©, l‚Äôoptimisation, les √©valuations et l‚Äôexp√©rimentation.</td>
  </tr>
  <tr>
    <td width="30%" valign="top"><b>En quoi TensorZero est-il diff√©rent des autres frameworks LLM¬†?</b></td>
    <td width="70%" valign="top">
      1. TensorZero vous permet d‚Äôoptimiser des applications LLM complexes sur la base de m√©triques de production et de retours humains.<br>
      2. TensorZero r√©pond aux besoins des applications LLM √† l‚Äô√©chelle industrielle¬†: faible latence, haut d√©bit, s√©curit√© des types, auto-h√©berg√©, GitOps, personnalisation, etc.<br>
      3. TensorZero unifie toute la stack LLMOps, cr√©ant des b√©n√©fices cumulatifs. Par exemple, les √©valuations LLM peuvent √™tre utilis√©es pour affiner les mod√®les en parall√®le des juges IA.
    </td>
  </tr>
  <tr>
    <td width="30%" valign="top"><b>Puis-je utiliser TensorZero avec ___¬†?</b></td>
    <td width="70%" valign="top">Oui. Tous les principaux langages de programmation sont pris en charge. Vous pouvez utiliser TensorZero avec notre client Python, n‚Äôimporte quel SDK OpenAI ou notre API HTTP.</td>
  </tr>
  <tr>
    <td width="30%" valign="top"><b>TensorZero est-il pr√™t pour la production¬†?</b></td>
    <td width="70%" valign="top">Oui. Voici une √©tude de cas¬†: <b><a href="https://www.tensorzero.com/blog/case-study-automating-code-changelogs-at-a-large-bank-with-llms">Automatisation des changelogs de code dans une grande banque avec des LLMs</a></b></td>
  </tr>
  <tr>
    <td width="30%" valign="top"><b>Combien co√ªte TensorZero¬†?</b></td>
    <td width="70%" valign="top">Rien. TensorZero est 100% auto-h√©berg√© et open-source. Il n‚Äôy a aucune fonctionnalit√© payante.</td>
  </tr>
  <tr>
    <td width="30%" valign="top"><b>Qui construit TensorZero¬†?</b></td>
    <td width="70%" valign="top">Notre √©quipe technique comprend un ancien mainteneur du compilateur Rust, des chercheurs en machine learning (Stanford, CMU, Oxford, Columbia) avec des milliers de citations, et le chief product officer d‚Äôune startup decacorn. Nous sommes soutenus par les m√™mes investisseurs que des projets open-source majeurs (par ex. ClickHouse, CockroachDB) et des laboratoires d‚ÄôIA (par ex. OpenAI, Anthropic).</td>
  </tr>
  <tr>
    <td width="30%" valign="top"><b>Comment d√©marrer¬†?</b></td>
    <td width="70%" valign="top">Vous pouvez adopter TensorZero de fa√ßon incr√©mentale. Notre <b><a href="https://www.tensorzero.com/docs/quickstart">D√©marrage rapide</a></b> passe d‚Äôun simple wrapper OpenAI √† une application LLM pr√™te pour la production avec observabilit√© et fine-tuning en seulement 5 minutes.</td>
  </tr>
</table>

---

## Fonctionnalit√©s

### üåê Passerelle LLM

> **Int√©grez-vous une seule fois √† TensorZero et acc√©dez √† tous les principaux fournisseurs LLM.**

<table>
  <tr></tr> <!-- flip highlight order -->
  <tr>
    <td width="50%" align="center" valign="middle"><b>Fournisseurs de mod√®les</b></td>
    <td width="50%" align="center" valign="middle"><b>Fonctionnalit√©s</b></td>
  </tr>
  <tr>
    <td width="50%" align="left" valign="top">
      <p>
        La passerelle TensorZero prend en charge nativement¬†:
      </p>
      <ul>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/anthropic">Anthropic</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/aws-bedrock">AWS Bedrock</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/aws-sagemaker">AWS SageMaker</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/azure">Azure OpenAI Service</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/deepseek">DeepSeek</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/fireworks">Fireworks</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/gcp-vertex-ai-anthropic">GCP Vertex AI Anthropic</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/gcp-vertex-ai-gemini">GCP Vertex AI Gemini</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/google-ai-studio-gemini">Google AI Studio (Gemini API)</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/hyperbolic">Hyperbolic</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/mistral">Mistral</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/openai">OpenAI</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/together">Together</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/vllm">vLLM</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/xai">xAI</a></b></li>
      </ul>
      <p>
        <em>
          Besoin d‚Äôautre chose¬†?
          Votre fournisseur est tr√®s probablement pris en charge, car TensorZero s‚Äôint√®gre avec <b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/openai-compatible">n‚Äôimporte quelle API compatible OpenAI (par ex. Ollama)</a></b>.
        </em>
      </p>
    </td>
    <td width="50%" align="left" valign="top">
      <p>
        La passerelle TensorZero prend en charge des fonctionnalit√©s avanc√©es comme¬†:
      </p>
      <ul>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/retries-fallbacks">Reprises & Fallbacks</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations">Optimisations √† l‚Äôinf√©rence</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/prompt-templates-schemas">Templates & Sch√©mas de prompts</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/tutorial#experimentation">Exp√©rimentation (Tests A/B)</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/configuration-reference">Configuration-as-Code (GitOps)</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/batch-inference">Inf√©rence par lot</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/multimodal-inference">Inf√©rence multimodale (VLMs)</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/inference-caching">Cache d‚Äôinf√©rence</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/metrics-feedback">M√©triques & Retours</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/episodes">Workflows LLM multi-√©tapes (√âpisodes)</a></b></li>
        <li><em>& bien d‚Äôautres‚Ä¶</em></li>
      </ul>
      <p>
        La passerelle TensorZero est √©crite en Rust ü¶Ä pour la <b>performance</b> (&lt;1ms de latence P99 @ 10k QPS).
        Voir <b><a href="https://www.tensorzero.com/docs/gateway/benchmarks">Benchmarks</a></b>.<br>
      </p>
      <p>
        Vous pouvez effectuer l‚Äôinf√©rence avec le <b>client TensorZero</b> (recommand√©), le <b>client OpenAI</b> ou l‚Äô<b>API HTTP</b>.
      </p>
    </td>
  </tr>
</table>

<br>

<details open>
<summary><b>Utilisation¬†: Python &mdash; Client TensorZero (Recommand√©)</b></summary>

Vous pouvez acc√©der √† n‚Äôimporte quel fournisseur avec le client Python TensorZero.

1. `pip install tensorzero`
2. Optionnel¬†: configurez TensorZero.
3. Lancez une inf√©rence¬†:

```python
from tensorzero import TensorZeroGateway  # ou AsyncTensorZeroGateway


with TensorZeroGateway.build_embedded(clickhouse_url="...", config_file="...") as client:
    response = client.inference(
        model_name="openai::gpt-4o-mini",
        # Essayez facilement d'autres fournisseurs¬†: "anthropic::claude-3-7-sonnet-20250219"
        input={
            "messages": [
                {
                    "role": "user",
                    "content": "√âcris un ha√Øku sur l‚Äôintelligence artificielle.",
                }
            ]
        },
    )
```

Voir **[D√©marrage rapide](https://www.tensorzero.com/docs/quickstart)** pour plus d‚Äôinformations.

</details>

<details>
<summary><b>Utilisation¬†: Python &mdash; Client OpenAI</b></summary>

Vous pouvez acc√©der √† n‚Äôimporte quel fournisseur avec le client Python OpenAI via TensorZero.

1. `pip install tensorzero`
2. Optionnel¬†: configurez TensorZero.
3. Lancez une inf√©rence¬†:

```python
from openai import OpenAI  # ou AsyncOpenAI
from tensorzero import patch_openai_client

client = OpenAI()
patch_openai_client(
    client,
    clickhouse_url="http://chuser:chpassword@localhost:8123/tensorzero",
    config_file="config/tensorzero.toml",
    async_setup=False,
)

response = client.chat.completions.create(
    model="tensorzero::model_name::openai::gpt-4o-mini",
    # Essayez d'autres fournisseurs facilement : "tensorzero::model_name::anthropic::claude-3-7-sonnet-20250219"
    messages=[
        {
            "role": "user",
            "content": "√âcris un ha√Øku sur l'intelligence artificielle.",
        }
    ],
)
```

Voir **[D√©marrage rapide](https://www.tensorzero.com/docs/quickstart)** pour plus d'informations.

</details>

<details>
<summary><b>Utilisation : JavaScript / TypeScript (Node) &mdash; Client OpenAI</b></summary>

Vous pouvez acc√©der √† n'importe quel fournisseur en utilisant le client OpenAI Node avec TensorZero.

1. D√©ployez `tensorzero/gateway` √† l'aide de Docker.
   **[Instructions d√©taill√©es ‚Üí](https://www.tensorzero.com/docs/gateway/deployment)**
2. Configurez la configuration TensorZero.
3. Lancez l'inf√©rence :

```ts
import OpenAI from "openai";

const client = new OpenAI({
  baseURL: "http://localhost:3000/openai/v1",
});

const response = await client.chat.completions.create({
  model: "tensorzero::model_name::openai::gpt-4o-mini",
  // Essayez d'autres fournisseurs facilement : "tensorzero::model_name::anthropic::claude-3-7-sonnet-20250219"
  messages: [
    {
      role: "user",
      content: "√âcris un ha√Øku sur l'intelligence artificielle.",
    },
  ],
});
```

Voir **[D√©marrage rapide](https://www.tensorzero.com/docs/quickstart)** pour plus d'informations.

</details>

<details>
<summary><b>Utilisation : Autres Langages & Plateformes &mdash; API HTTP</b></summary>

TensorZero prend en charge pratiquement tous les langages de programmation ou plateformes via son API HTTP.

1. D√©ployez `tensorzero/gateway` √† l'aide de Docker.
   **[Instructions d√©taill√©es ‚Üí](https://www.tensorzero.com/docs/gateway/deployment)**
2. Optionnel : Configurez TensorZero.
3. Lancez l'inf√©rence :

```bash
curl -X POST "http://localhost:3000/inference" \
  -H "Content-Type: application/json" \
  -d '{
    "model_name": "openai::gpt-4o-mini",
    "input": {
      "messages": [
        {
          "role": "user",
          "content": "√âcris un ha√Øku sur l'intelligence artificielle."
        }
      ]
    }
  }'
```

Voir **[D√©marrage rapide](https://www.tensorzero.com/docs/quickstart)** pour plus d'informations.

</details>

<br>

### üìà Optimisation LLM

> **Envoyez des m√©triques de production et des retours humains pour optimiser facilement vos prompts, mod√®les et strat√©gies d'inf√©rence &mdash; via l'interface utilisateur ou par programmation.**

#### Optimisation de mod√®le

Optimisez les mod√®les propri√©taires et open-source en utilisant l'ajustement supervis√© (SFT) et l'ajustement par pr√©f√©rence (DPO).

<table>
  <tr></tr> <!-- inverser l'ordre de surbrillance -->
  <tr>
    <td width="50%" align="center" valign="middle"><b>Ajustement supervis√© &mdash; UI</b></td>
    <td width="50%" align="center" valign="middle"><b>Ajustement par pr√©f√©rence (DPO) &mdash; Jupyter Notebook</b></td>
  </tr>
  <tr>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/cf7acf66-732b-43b3-af2a-5eba1ce40f6f"></td>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/a67a0634-04a7-42b0-b934-9130cb7cdf51"></td>
  </tr>
</table>

#### Optimisation √† l'inf√©rence

Am√©liorez les performances en mettant √† jour dynamiquement vos prompts avec des exemples pertinents, en combinant les r√©ponses de plusieurs inf√©rences, et plus encore.

<table>
  <tr></tr> <!-- inverser l'ordre de surbrillance -->
  <tr>
    <td width="50%" align="center" valign="middle"><b><a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#best-of-n-sampling">Best-of-N Sampling</a></b></td>
    <td width="50%" align="center" valign="middle"><b><a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#mixture-of-n-sampling">Mixture-of-N Sampling</a></b></td>
  </tr>
  <tr>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/c0edfa4c-713c-4996-9964-50c0d26e6970"></td>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/75b5bf05-4c1f-43c4-b158-d69d1b8d05be"></td>
  </tr>
  <tr>
    <td width="50%" align="center" valign="middle"><b><a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#dynamic-in-context-learning-dicl">Dynamic In-Context Learning (DICL)</a></b></td>
    <td width="50%" align="center" valign="middle"><b><a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#chain-of-thought-cot">Chain-of-Thought (CoT)</a></b></td>
  </tr>
  <tr>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/d8489e92-ce93-46ac-9aab-289ce19bb67d"></td>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/ea13d73c-76a4-4e0c-a35b-0c648f898311" height="320"></td>
  </tr>
</table>

_D'autres arrivent bient√¥t..._

<br>

#### Optimisation de prompt

Optimisez vos prompts par programmation √† l'aide de techniques d'optimisation issues de la recherche.

<table>
  <tr></tr> <!-- inverser l'ordre de surbrillance -->
  <tr>
    <td width="50%" align="center" valign="middle"><b><a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#best-of-n-sampling">MIPROv2</a></b></td>
    <td width="50%" align="center" valign="middle"><b><a href="https://github.com/tensorzero/tensorzero/tree/main/examples/gsm8k-custom-recipe-dspy">Int√©gration DSPy</a></b></td>
  </tr>
  <tr>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/d81a7c37-382f-4c46-840f-e6c2593301db" alt="MIPROv2 diagram"></td>
    <td width="50%" align="center" valign="middle">
      TensorZero est fourni avec plusieurs recettes d'optimisation, mais vous pouvez √©galement cr√©er facilement les v√¥tres.
      Cet exemple montre comment optimiser une fonction TensorZero en utilisant un outil arbitraire ‚Äî ici, DSPy, une biblioth√®que populaire pour l'ing√©nierie de prompts automatis√©e.
    </td>
  </tr>
</table>

_D'autres arrivent bient√¥t..._

<br>

### üîç Observabilit√© LLM

> **Zoomez pour d√©boguer les appels API individuels, ou d√©zoomez pour surveiller les m√©triques √† travers les mod√®les et prompts dans le temps &mdash; tout cela gr√¢ce √† l'interface open-source de TensorZero.**

<table>
  <tr></tr> <!-- inverser l'ordre de surbrillance -->
  <tr>
    <td width="50%" align="center" valign="middle"><b>Observabilit√© ¬ª Inf√©rence</b></td>
    <td width="50%" align="center" valign="middle"><b>Observabilit√© ¬ª Fonction</b></td>
  </tr>
  <tr>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/2cc3cc9a-f33f-4e94-b8de-07522326f80a"></td>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/00ae6605-8fa0-4efd-8238-ae8ea589860f"></td>
  </tr>
</table>

<br>

### üìä √âvaluations LLM

> **Comparez les prompts, mod√®les et strat√©gies d'inf√©rence √† l'aide des √©valuations TensorZero &mdash; avec prise en charge des heuristiques et juges LLM.**

<table>
  <tr></tr> <!-- inverser l'ordre de surbrillance -->
  <tr>
    <td width="50%" align="center" valign="middle"><b>√âvaluation ¬ª UI</b></td>
    <td width="50%" align="center" valign="middle"><b>√âvaluation ¬ª CLI</b></td>
  </tr>
  <tr>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/f4bf54e3-1b63-46c8-be12-2eaabf615699"></td>
    <td width="50%" align="left" valign="middle">
<pre><code class="language-bash">docker compose run --rm evaluations \
  --evaluation-name extract_data \
  --dataset-name hard_test_cases \
  --variant-name gpt_4o \
  --concurrency 5</code></pre>
<pre><code class="language-bash">Run ID: 01961de9-c8a4-7c60-ab8d-15491a9708e4
Number of datapoints: 100
‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100/100
exact_match: 0.83 ¬± 0.03
semantic_match : 0,98 ¬± 0,01  
item_count : 7,15 ¬± 0,39</code></pre>
    </td>
  </tr>
</table>

## D√©mo

> **Regardez les LLMs s'am√©liorer en extraction de donn√©es en temps r√©el avec TensorZero !**
>
> **[Dynamic in-context learning (DICL)](https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#dynamic-in-context-learning-dicl)** est une puissante optimisation au moment de l'inf√©rence, disponible imm√©diatement avec TensorZero.
> Cela am√©liore la performance des LLM en int√©grant automatiquement des exemples historiques pertinents dans le prompt, sans avoir besoin d'un ajustement du mod√®le.

https://github.com/user-attachments/assets/4df1022e-886e-48c2-8f79-6af3cdad79cb

## Ing√©nierie LLM avec TensorZero

<br>
<p align="center" >
  <a href="https://www.tensorzero.com/docs">
    <picture>
      <source media="(prefers-color-scheme: light)" srcset="https://github.com/user-attachments/assets/34a92c18-242e-4d76-a99c-861283de68a6">
      <source media="(prefers-color-scheme: dark)" srcset="https://github.com/user-attachments/assets/e8bc699b-6378-4c2a-9cc1-6d189025e270">
      <img alt="TensorZero Flywheel" src="https://github.com/user-attachments/assets/34a92c18-242e-4d76-a99c-861283de68a6" width=720>
    </picture>
  </a>
</p>
<br>

1. La **[TensorZero Gateway](https://www.tensorzero.com/docs/gateway/)** est une passerelle de mod√®les haute performance √©crite en Rust ü¶Ä qui offre une interface API unifi√©e pour tous les principaux fournisseurs de LLM, permettant une int√©gration et des solutions de repli multiplateformes transparentes.
2. Elle g√®re l'inf√©rence structur√©e bas√©e sur des sch√©mas avec une latence P99 &lt;1ms (voir **[Benchmarks](https://www.tensorzero.com/docs/gateway/benchmarks)**), l'observabilit√© int√©gr√©e, l'exp√©rimentation et les **[optimisations au moment de l'inf√©rence](https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations)**.
3. Elle collecte √©galement des m√©triques et des retours associ√©s √† ces inf√©rences, avec une prise en charge native des syst√®mes LLM multi-√©tapes.
4. Toutes les donn√©es sont stock√©es dans un entrep√¥t de donn√©es ClickHouse que vous contr√¥lez, pour une analytique en temps r√©el, √©volutive et conviviale pour les d√©veloppeurs.
5. Au fil du temps, les **[Recettes TensorZero](https://www.tensorzero.com/docs/recipes)** exploitent cet ensemble de donn√©es structur√© pour optimiser vos prompts et mod√®les : ex√©cutez des recettes pr√©-construites pour des workflows courants comme le fine-tuning, ou cr√©ez les v√¥tres avec une flexibilit√© totale, en utilisant n'importe quel langage et plateforme.
6. Enfin, les fonctionnalit√©s d'exp√©rimentation et l'orchestration GitOps de la passerelle vous permettent d'it√©rer et de d√©ployer en toute confiance, qu'il s'agisse d'un seul LLM ou de milliers de LLMs.

Notre objectif est d'aider les ing√©nieurs √† construire, g√©rer et optimiser la prochaine g√©n√©ration d'applications LLM : des syst√®mes qui apprennent de l'exp√©rience r√©elle.
En savoir plus sur notre **[Vision & Feuille de route](https://www.tensorzero.com/docs/vision-roadmap/)**.

## Commencer

**Commencez √† construire d√®s aujourd'hui.**
Le **[D√©marrage rapide](https://www.tensorzero.com/docs/quickstart)** montre qu'il est facile de mettre en place une application LLM avec TensorZero.
Si vous souhaitez aller plus loin, le **[Tutoriel](https://www.tensorzero.com/docs/gateway/tutorial)** vous apprend √† construire un chatbot simple, un copilote email, un syst√®me m√©t√©o RAG, et un pipeline d'extraction de donn√©es structur√©es.

**Des questions ?**
Rejoignez-nous sur **[Slack](https://www.tensorzero.com/slack)** ou **[Discord](https://www.tensorzero.com/discord)**.

**Vous utilisez TensorZero au travail ?**
√âcrivez-nous √† **[hello@tensorzero.com](mailto:hello@tensorzero.com)** pour cr√©er un canal Slack ou Teams avec votre √©quipe (gratuitement).

**Rejoignez-nous.**
Nous **[recrutons √† New York](https://www.tensorzero.com/jobs)**.
Nous accueillons √©galement les **[contributions open-source](https://github.com/tensorzero/tensorzero/blob/main/CONTRIBUTING.md)** !

## Exemples

Nous pr√©parons une s√©rie de **cas d'usage complets et ex√©cutables** illustrant le flywheel de donn√©es & d'apprentissage de TensorZero.

> **[Optimisation de l'extraction de donn√©es (NER) avec TensorZero](https://github.com/tensorzero/tensorzero/tree/main/examples/data-extraction-ner)**
>
> Cet exemple montre comment utiliser TensorZero pour optimiser un pipeline d'extraction de donn√©es.
> Nous d√©montrons des techniques comme le fine-tuning et l'apprentissage dynamique en contexte (DICL).
> Au final, un mod√®le GPT-4o Mini optimis√© surpasse GPT-4o sur cette t√¢che &mdash; pour une fraction du co√ªt et de la latence &mdash; en utilisant une petite quantit√© de donn√©es d'entra√Ænement.

> **[Agentic RAG ‚Äî Questionnement multi-sauts avec LLMs](https://github.com/tensorzero/tensorzero/tree/main/examples/rag-retrieval-augmented-generation/simple-agentic-rag/)**
>
> Cet exemple montre comment construire un agent de recherche multi-sauts avec TensorZero.
> L'agent recherche de mani√®re it√©rative sur Wikip√©dia pour rassembler des informations, et d√©cide quand il dispose de suffisamment de contexte pour r√©pondre √† une question complexe.

> **[G√©n√©rer des ha√Økus pour satisfaire un juge aux pr√©f√©rences cach√©es](https://github.com/tensorzero/tensorzero/tree/main/examples/haiku-hidden-preferences)**
>
> Cet exemple ajuste GPT-4o Mini pour g√©n√©rer des ha√Økus adapt√©s √† un go√ªt particulier.
> Vous verrez le "data flywheel in a box" de TensorZero en action : de meilleures variantes produisent de meilleures donn√©es, et de meilleures donn√©es m√®nent √† de meilleures variantes.
> Vous verrez la progression en r√©-entra√Ænant le LLM plusieurs fois.

> **[Am√©liorer les capacit√©s aux √©checs d'un LLM avec le Best-of-N Sampling](https://github.com/tensorzero/tensorzero/tree/main/examples/chess-puzzles/)**
>
> Cet exemple montre comment le best-of-N sampling peut consid√©rablement am√©liorer les performances d'un LLM aux √©checs en s√©lectionnant les coups les plus prometteurs parmi plusieurs options g√©n√©r√©es.

> **[Am√©liorer le raisonnement math√©matique avec une recette personnalis√©e pour l‚Äôing√©nierie automatis√©e des prompts (DSPy)](https://github.com/tensorzero/tensorzero/tree/main/examples/gsm8k-custom-recipe-dspy)**
>
> TensorZero propose un certain nombre de recettes d'optimisation pr√™tes √† l'emploi pour les workflows classiques d'ing√©nierie LLM.
> Mais vous pouvez aussi facilement cr√©er vos propres recettes et workflows !
> Cet exemple montre comment optimiser une fonction TensorZero √† l'aide d'un outil arbitraire ‚Äî ici, DSPy.

_& bien d'autres √† venir !_

---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-06-09

---