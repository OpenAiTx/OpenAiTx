# Ax, DSPy f√ºr Typescript

Die Arbeit mit LLMs ist komplex ‚Äì sie tun nicht immer das, was man m√∂chte. DSPy macht es einfacher, beeindruckende Anwendungen mit LLMs zu entwickeln. Definieren Sie einfach Ihre Eingaben und Ausgaben (Signatur), und ein effizienter Prompt wird automatisch generiert und verwendet. Verbinden Sie verschiedene Signaturen, um komplexe Systeme und Workflows mit LLMs zu bauen.

Und damit Sie Ax wirklich produktiv einsetzen k√∂nnen, liefern wir alles Weitere mit, was Sie ben√∂tigen: Observability, Streaming, Unterst√ºtzung f√ºr weitere Modalit√§ten (Bilder, Audio, etc.), Fehlerkorrektur, Multi-Step-Funktionsaufrufe, MCP, RAG und vieles mehr.

[![NPM Package](https://img.shields.io/npm/v/@ax-llm/ax?style=for-the-badge&color=green)](https://www.npmjs.com/package/@ax-llm/ax)
[![Discord Chat](https://dcbadge.vercel.app/api/server/DSHg3dU7dW?style=for-the-badge)](https://discord.gg/DSHg3dU7dW)
[![Twitter](https://img.shields.io/twitter/follow/dosco?style=for-the-badge&color=red)](https://twitter.com/dosco)

<!-- header -->

## Warum Ax verwenden?

- Standardisierte Schnittstelle f√ºr alle f√ºhrenden LLMs
- Prompts aus einfachen Signaturen kompiliert
- Volles, natives End-to-End-Streaming
- Unterst√ºtzung f√ºr "Thinking Budget" und "Thought Tokens"
- Baue Agenten, die andere Agenten aufrufen k√∂nnen
- Eingebaute MCP (Model Context Protocol) Unterst√ºtzung
- Dokumente beliebigen Formats in Text umwandeln
- RAG, intelligentes Chunking, Embedding, Abfragen
- Funktioniert mit Vercel AI SDK
- Ausgabevalidierung w√§hrend des Streamings
- Multi-modales DSPy unterst√ºtzt
- Automatisches Prompt-Tuning mit Optimierern
- OpenTelemetry Tracing / Observability
- Produktionsreifer Typescript-Code
- Leichtgewichtig, keine Abh√§ngigkeiten

## Produktionsreif

- Keine Breaking Changes (Minor Versions)
- Hohe Testabdeckung
- Eingebaute Open Telemetry `gen_ai` Unterst√ºtzung
- Wird produktiv von vielen Startups genutzt

## Was ist eine Prompt-Signatur?

<img width="860" alt="shapes at 24-03-31 00 05 55" src="https://raw.githubusercontent.com/ax-llm/ax/main/github.com/dosco/llm-client/assets/832235/0f0306ea-1812-4a0a-9ed5-76cd908cd26b">

Effiziente, typsichere Prompts werden aus einer einfachen Signatur automatisch generiert. Eine Prompt-Signatur besteht aus
`"task description" inputField:type "field description" -> "outputField:type`.
Die Idee der Prompt-Signaturen basiert auf Arbeiten aus dem Paper
"Demonstrate-Search-Predict".

Es sind mehrere Eingabe- und Ausgabefelder m√∂glich, jedes Feld kann die Typen `string`, `number`, `boolean`, `date`, `datetime`, `class "class1, class2"`, `JSON` oder ein Array davon, z.B. `string[]`, haben. Ist kein Typ definiert, wird `string` als Standard verwendet. Das Suffix `?` macht das Feld optional (Standard: erforderlich), `!` macht das Feld intern, was z. B. f√ºr Reasoning n√ºtzlich ist.

## Ausgabefeld-Typen

| Typ                      | Beschreibung                         | Verwendung                  | Beispielausgabe                                     |
| ------------------------ | ------------------------------------ | --------------------------- | --------------------------------------------------- |
| `string`                 | Eine Zeichenkette                    | `fullName:string`           | `"example"`                                         |
| `number`                 | Ein numerischer Wert                 | `price:number`              | `42`                                                |
| `boolean`                | Wahrheitswert                        | `isEvent:boolean`           | `true`, `false`                                     |
| `date`                   | Ein Datum                            | `startDate:date`            | `"2023-10-01"`                                      |
| `datetime`               | Datum und Uhrzeit                    | `createdAt:datetime`        | `"2023-10-01T12:00:00Z"`                            |
| `class "class1,class2"`  | Klassifizierung von Items            | `category:class`            | `["class1", "class2", "class3"]`                    |
| `string[]`               | Array von Zeichenketten              | `tags:string[]`             | `["example1", "example2"]`                          |
| `number[]`               | Array von Zahlen                     | `scores:number[]`           | `[1, 2, 3]`                                         |
| `boolean[]`              | Array von Wahrheitswerten            | `permissions:boolean[]`     | `[true, false, true]`                               |
| `date[]`                 | Array von Daten                      | `holidayDates:date[]`       | `["2023-10-01", "2023-10-02"]`                      |
| `datetime[]`             | Array von Datum und Uhrzeit          | `logTimestamps:datetime[]`  | `["2023-10-01T12:00:00Z", "2023-10-02T12:00:00Z"]`  |
| `class[] "class1,class2"`| Mehrere Klassen                      | `categories:class[]`        | `["class1", "class2", "class3"]`                    |
| `code "language"`        | Codeblock in einer Sprache           | `code:code "python"`        | `print('Hello, world!')`                            |

## Unterst√ºtzte LLMs

`Google Gemini`, `OpenAI`, `Azure OpenAI`, `Anthropic`, `X Grok`, `TogetherAI`, `Cohere`, `Mistral`, `Groq`, `DeepSeek`, `Ollama`, `Reka`, `Hugging Face`

## Installation

```bash
npm install @ax-llm/ax
# oder
yarn add @ax-llm/ax
```

## Beispiel: Chain-of-Thought zum Zusammenfassen von Text

```typescript
import { AxAI, AxChainOfThought } from '@ax-llm/ax'

const textToSummarize = `
The technological singularity‚Äîor simply the singularity[1]‚Äîis a hypothetical future point in time at which technological growth becomes uncontrollable and irreversible, resulting in unforeseeable changes to human civilization.[2][3] ...`

const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY as string,
})

const gen = new AxChainOfThought(
  `textToSummarize -> textType:class "note, email, reminder", shortSummary "summarize in 5 to 10 words"`
)

const res = await gen.forward(ai, { textToSummarize })

console.log('>', res)
```

## Beispiel: Einen Agenten bauen

Nutzen Sie das Agent-Prompt-Framework, um Agenten zu bauen, die mit anderen Agenten zusammenarbeiten, um Aufgaben zu erledigen. Agenten sind mit Prompt-Signaturen einfach zu erstellen. Probieren Sie das Agenten-Beispiel aus.

```typescript
# npm run tsx ./src/examples/agent.ts

const researcher = new AxAgent({
  name: 'researcher',
  description: 'Researcher agent',
  signature: `physicsQuestion "physics questions" -> answer "reply in bullet points"`
});

const summarizer = new AxAgent({
  name: 'summarizer',
  description: 'Summarizer agent',
  signature: `text "text so summarize" -> shortSummary "summarize in 5 to 10 words"`
});

const agent = new AxAgent({
  name: 'agent',
  description: 'A an agent to research complex topics',
  signature: `question -> answer`,
  agents: [researcher, summarizer]
});

agent.forward(ai, { questions: "How many atoms are there in the universe" })
```

## Unterst√ºtzung f√ºr Thinking Models

Ax bietet native Unterst√ºtzung f√ºr Modelle mit "Thinking"-F√§higkeiten, sodass Sie das Token-Budget f√ºr das Denken steuern und auf die √úberlegungen des Modells zugreifen k√∂nnen. Diese Funktion hilft dabei, den Denkprozess des Modells nachzuvollziehen und den Token-Verbrauch zu optimieren.

```typescript
const ai = new AxAI({
  name: 'google-gemini',
  apiKey: process.env.GOOGLE_APIKEY as string,
  config: {
    model: AxAIGoogleGeminiModel.Gemini25Flash,
    thinking: { includeThoughts: true },
  },
})

// Oder Thinking-Budget pro Anfrage steuern
const gen = new AxChainOfThought(`question -> answer`)
const res = await gen.forward(
  ai,
  { question: 'What is quantum entanglement?' },
  { thinkingTokenBudget: 'medium' } // 'minimal', 'low', 'medium', oder 'high'
)

// Gedanken aus der Antwort auslesen
console.log(res.thoughts) // Zeigt den Denkprozess des Modells
```

## Unterst√ºtzte Vektor-Datenbanken

Vektor-Datenbanken sind entscheidend f√ºr LLM-Workflows. Wir bieten saubere Abstraktionen √ºber popul√§re Vektor-Datenbanken und eine schnelle In-Memory-Variante.

| Anbieter    | Getestet |
| ----------- | -------- |
| In Memory   | üü¢ 100%  |
| Weaviate    | üü¢ 100%  |
| Cloudflare  | üü° 50%   |
| Pinecone    | üü° 50%   |

```typescript
// Erstellen von Embeddings aus Text mit einem LLM
const ret = await this.ai.embed({ texts: 'hello world' })

// In-Memory-Vektor-DB erstellen
const db = new axDB('memory')

// In die Vektor-DB einf√ºgen
await this.db.upsert({
  id: 'abc',
  table: 'products',
  values: ret.embeddings[0],
})

// √Ñhnliche Eintr√§ge mit Embeddings abfragen
const matches = await this.db.query({
  table: 'products',
  values: embeddings[0],
})
```

Alternativ k√∂nnen Sie den `AxDBManager` nutzen, der intelligentes Chunking, Embedding und Abfrage f√ºr Sie √ºbernimmt.

```typescript
const manager = new AxDBManager({ ai, db })
await manager.insert(text)

const matches = await manager.query(
  'John von Neumann on human intelligence and singularity.'
)
console.log(matches)
```

## RAG-Dokumente

Dokumente wie PDF, DOCX, PPT, XLS, etc. mit LLMs zu nutzen ist m√ºhsam. Wir machen es mit Apache Tika, einer Open-Source-Dokumenten-Engine, einfach.

Apache Tika starten:

```shell
docker run -p 9998:9998 apache/tika
```

Dokumente zu Text konvertieren und mit dem `AxDBManager` f√ºr Retrieval einbetten. Es sind auch ein Reranker und Query-Rewriter integriert (`AxDefaultResultReranker` und `AxDefaultQueryRewriter`).

```typescript
const tika = new AxApacheTika()
const text = await tika.convert('/path/to/document.pdf')

const manager = new AxDBManager({ ai, db })
await manager.insert(text)

const matches = await manager.query('Find some text')
console.log(matches)
```

## Multi-modal DSPy

Bei Modellen wie `GPT-4o` und `Gemini`, die Multi-Modal Prompts unterst√ºtzen, k√∂nnen Bildfelder verwendet werden ‚Äì funktioniert mit der gesamten DSP-Pipeline.

```typescript
const image = fs
  .readFileSync('./src/examples/assets/kitten.jpeg')
  .toString('base64')

const gen = new AxChainOfThought(`question, animalImage:image -> answer`)

const res = await gen.forward(ai, {
  question: 'What family does this animal belong to?',
  animalImage: { mimeType: 'image/jpeg', data: image },
})
```

Mit Modellen wie `gpt-4o-audio-preview`, die Audio unterst√ºtzen, sind auch Audiofelder m√∂glich:

```typescript
const audio = fs
  .readFileSync('./src/examples/assets/comment.wav')
  .toString('base64')

const gen = new AxGen(`question, commentAudio:audio -> answer`)

const res = await gen.forward(ai, {
  question: 'What family does this animal belong to?',
  commentAudio: { format: 'wav', data: audio },
})
```

## DSPy Chat API

Inspiriert von DSPys Demonstration Weaving bietet Ax `AxMessage` f√ºr nahtloses Management des Gespr√§chsverlaufs. So k√∂nnen Sie Chatbots bauen, die den Kontext √ºber mehrere Turns behalten und die volle Power der Prompt-Signaturen nutzen. Details siehe Beispiel.

```shell
GOOGLE_APIKEY=api-key npm run tsx ./src/examples/chat.ts
```

```typescript
const chatBot = new AxGen<
  { message: string } | ReadonlyArray<ChatMessage>,
  { reply: string }
>(
  `message:string "A casual message from the user" -> reply:string "A friendly, casual response"`
)

await chatBot.forward(ai, [
  {
    role: 'user',
    values: { message: 'Hi! How are you doing today?' },
  },
  {
    role: 'assistant',
    values: { reply: 'I am doing great! How about you?' },
  },
  {
    role: 'user',
    values: { message: 'Thats great!' },
  },
])
```

Die Gespr√§chshistorie wird automatisch in den Prompt eingebunden, wodurch das Modell den Kontext beh√§lt und zusammenh√§ngende Antworten gibt. Funktioniert mit allen Ax-Features wie Streaming, Funktionsaufrufen, Chain-of-Thought.

## Streaming

### Assertions

Wir unterst√ºtzen das Parsen von Ausgabefeldern und Funktionsausf√ºhrung w√§hrend des Streamings. Das erlaubt Fail-Fast und Fehlerkorrektur, spart Tokens, Kosten und reduziert Latenz. Assertions sind eine m√§chtige Methode, um die Einhaltung Ihrer Anforderungen sicherzustellen ‚Äì auch beim Streaming.

```typescript
// Prompt-Programm aufsetzen
const gen = new AxChainOfThought(
  ai,
  `startNumber:number -> next10Numbers:number[]`
)

// Assertion hinzuf√ºgen, dass die Zahl 5 nicht enthalten sein darf
gen.addAssert(({ next10Numbers }: Readonly<{ next10Numbers: number[] }>) => {
  return next10Numbers ? !next10Numbers.includes(5) : undefined
}, 'Numbers 5 is not allowed')

// Programm mit aktiviertem Streaming ausf√ºhren
const res = await gen.forward({ startNumber: 1 }, { stream: true })

// Oder End-to-End-Streaming
const generator = await gen.streamingForward(
  { startNumber: 1 },
  {
    stream: true,
  }
)
for await (const res of generator) {
}
```

Sie k√∂nnen ganze Ausgabefelder w√§hrend des Streamings validieren. Die Validierung wird ausgel√∂st, sobald das Feld komplett ist. F√ºr echte Validierung schon w√§hrend des Streamings siehe folgendes Beispiel ‚Äì das steigert Performance und spart Tokens in der Produktion.

```typescript
// Assertion f√ºr jede Zeile: muss mit Zahl und Punkt beginnen
gen.addStreamingAssert(
  'answerInPoints',
  (value: string) => {
    const re = /^\d+\./

    // Zeilen splitten, trimmen, leere Zeilen filtern, alle auf Regex pr√ºfen
    return value
      .split('\n')
      .map((x) => x.trim())
      .filter((x) => x.length > 0)
      .every((x) => re.test(x))
  },
  'Lines must start with a number and a dot. Eg: 1. This is a line.'
)

// Programm mit aktiviertem Streaming ausf√ºhren
const res = await gen.forward(
  {
    question: 'Provide a list of optimizations to speedup LLM inference.',
  },
  { stream: true, debug: true }
)
```

### Field Processors

Field Processors verarbeiten Felder in einem Prompt vor der √úbergabe an das LLM.

```typescript
const gen = new AxChainOfThought(
  ai,
  `startNumber:number -> next10Numbers:number[]`
)

const streamValue = false

const processorFunction = (value) => {
  return value.map((x) => x + 1)
}

// Feld-Processor zum Programm hinzuf√ºgen
const processor = new AxFieldProcessor(
  gen,
  'next10Numbers',
  processorFunction,
  streamValue
)

const res = await gen.forward({ startNumber: 1 })
```

## Model Context Protocol (MCP)

Ax integriert nahtlos das Model Context Protocol (MCP), sodass Ihre Agenten externe Tools und Ressourcen √ºber eine standardisierte Schnittstelle erreichen.

### Verwendung von AxMCPClient

Mit `AxMCPClient` k√∂nnen Sie sich mit jedem MCP-kompatiblen Server verbinden und dessen Funktionen in Ax-Agenten nutzen:

```typescript
import { AxMCPClient, AxMCPStdioTransport } from '@ax-llm/ax'

// MCP-Client mit Transport initialisieren
const transport = new AxMCPStdioTransport({
  command: 'npx',
  args: ['-y', '@modelcontextprotocol/server-memory'],
})

// Client mit optionalem Debug-Modus erstellen
const client = new AxMCPClient(transport, { debug: true })

// Verbindung initialisieren
await client.init()

// Funktionen des Clients im Agenten verwenden
const memoryAgent = new AxAgent({
  name: 'MemoryAssistant',
  description: 'An assistant with persistent memory',
  signature: 'input, userId -> response',
  functions: [client], // Client als Funktionsanbieter angeben
})

// Oder Client mit AxGen verwenden
const memoryGen = new AxGen('input, userId -> response', {
  functions: [client],
})
```

### AxMCPClient mit Remote-Server verwenden

Einen Remote-MCP-Server mit Ax anzusprechen ist unkompliziert. Beispiel: DeepWiki MCP-Server f√ºr Fragen zu √∂ffentlichen GitHub-Repos (`https://mcp.deepwiki.com/mcp`).

```typescript
import {
  AxAgent,
  AxAI,
  AxAIOpenAIModel,
  AxMCPClient,
  AxMCPStreambleHTTPTransport,
} from '@ax-llm/ax'

// 1. MCP-Transport zum DeepWiki-Server initialisieren
const transport = new AxMCPStreambleHTTPTransport(
  'https://mcp.deepwiki.com/mcp'
)

// 2. MCP-Client erstellen
const mcpClient = new AxMCPClient(transport, { debug: false })
await mcpClient.init() // Verbindung initialisieren

// 3. AI-Modell initialisieren (z.B. OpenAI)
const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY as string,
})

// 4. AxAgent, der den MCP-Client nutzt
const deepwikiAgent = new AxAgent<
  {
    // Input-Typen passend zur DeepWiki-Funktion
    questionAboutRepo: string
    githubRepositoryUrl: string
  },
  {
    answer: string
  }
>({
  name: 'DeepWikiQueryAgent',
  description: 'Agent to query public GitHub repositories via DeepWiki MCP.',
  signature: 'questionAboutRepo, githubRepositoryUrl -> answer',
  functions: [mcpClient], // MCP-Client als Funktion liefern
})

// 5. Frage formulieren und Agenten aufrufen
const result = await deepwikiAgent.forward(ai, {
  questionAboutRepo: 'What is the main purpose of this library?',
  githubRepositoryUrl: 'https://github.com/dosco/ax',
})
console.log('DeepWiki Answer:', result.answer)
```

Dieses Beispiel zeigt, wie Sie einen √∂ffentlichen MCP-Server mit einem Ax-Agenten nutzen. Die Signatur (`questionAboutRepo, githubRepositoryUrl -> answer`) ist ein Beispiel. Die verf√ºgbaren Funktionen und Signaturen erfahren Sie vom MCP-Server selbst (z.B. via `mcp.getFunctions` oder Dokumentation).

F√ºr komplexere Beispiele mit Authentifizierung und benutzerdefinierten Headern siehe die Datei `src/examples/mcp-client-pipedream.ts` in diesem Repository.

## AI Routing und Load Balancing

Ax bietet zwei leistungsstarke Methoden zur Nutzung mehrerer AI-Services: einen Load Balancer f√ºr hohe Verf√ºgbarkeit und einen Router f√ºr modell-spezifisches Routing.

### Load Balancer

Der Load Balancer verteilt Anfragen automatisch auf mehrere AI-Services basierend auf Performance und Verf√ºgbarkeit. F√§llt ein Service aus, wird automatisch zum n√§chsten gewechselt.

```typescript
import { AxAI, AxBalancer } from '@ax-llm/ax'

// Mehrere AI-Services aufsetzen
const openai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY,
})

const ollama = new AxAI({
  name: 'ollama',
  config: { model: 'nous-hermes2' },
})

const gemini = new AxAI({
  name: 'google-gemini',
  apiKey: process.env.GOOGLE_APIKEY,
})

// Load Balancer mit allen Services erstellen
const balancer = new AxBalancer([openai, ollama, gemini])

// Wie einen normalen AI-Service verwenden
const response = await balancer.chat({
  chatPrompt: [{ role: 'user', content: 'Hello!' }],
})

// Oder mit AxGen verwenden
const gen = new AxGen(`question -> answer`)
const res = await gen.forward(balancer, { question: 'Hello!' })
```

### Multi-Service Router

Der Router erm√∂glicht die Nutzung mehrerer AI-Services √ºber eine Schnittstelle und routet Anfragen modellbasiert.

```typescript
import { AxAI, AxAIOpenAIModel, AxMultiServiceRouter } from '@ax-llm/ax'

// OpenAI mit Modellen
const openai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY,
  models: [
    {
      key: 'basic',
      model: AxAIOpenAIModel.GPT4OMini,
      description:
        'Model for very simple tasks such as answering quick short questions',
    },
    {
      key: 'medium',
      model: AxAIOpenAIModel.GPT4O,
      description:
        'Model for semi-complex tasks such as summarizing text, writing code, and more',
    },
  ],
})

// Gemini mit Modellen
const gemini = new AxAI({
  name: 'google-gemini',
  apiKey: process.env.GOOGLE_APIKEY,
  models: [
    {
      key: 'deep-thinker',
      model: 'gemini-2.0-flash-thinking',
      description:
        'Model that can think deeply about a task, best for tasks that require planning',
    },
    {
      key: 'expert',
      model: 'gemini-2.0-pro',
      description:
        'Model that is the best for very complex tasks such as writing large essays, complex coding, and more',
    },
  ],
})

const ollama = new AxAI({
  name: 'ollama',
  config: { model: 'nous-hermes2' },
})

const secretService = {
  key: 'sensitive-secret',
  service: ollama,
  description: 'Model for sensitive secrets tasks',
}

// Router mit allen Services erstellen
const router = new AxMultiServiceRouter([openai, gemini, secretService])

// Routing zu OpenAIs Expert-Modell
const openaiResponse = await router.chat({
  chatPrompt: [{ role: 'user', content: 'Hello!' }],
  model: 'expert',
})

// Oder mit AxGen verwenden
const gen = new AxGen(`question -> answer`)
const res = await gen.forward(router, { question: 'Hello!' })
```

Der Load Balancer ist ideal f√ºr Hochverf√ºgbarkeit, der Router f√ºr modell-spezifische Aufgaben. Beide lassen sich mit allen Ax-Features wie Streaming, Funktionsaufrufen und Chain-of-Thought kombinieren.

Balancer und Router k√∂nnen auch zusammen eingesetzt werden: Mehrere Balancer mit dem Router, oder umgekehrt.

## OpenTelemetry-Unterst√ºtzung

Tracing und Observability Ihres LLM-Workflows sind zentral f√ºr den Betrieb in der Produktion. OpenTelemetry ist Standard und wir unterst√ºtzen das `gen_ai`-Attribut-Namespace. Siehe `src/examples/telemetry.ts` f√ºr mehr Informationen.

```typescript
import { trace } from '@opentelemetry/api'
import {
  BasicTracerProvider,
  ConsoleSpanExporter,
  SimpleSpanProcessor,
} from '@opentelemetry/sdk-trace-base'

const provider = new BasicTracerProvider()
provider.addSpanProcessor(new SimpleSpanProcessor(new ConsoleSpanExporter()))
trace.setGlobalTracerProvider(provider)

const tracer = trace.getTracer('test')

const ai = new AxAI({
  name: 'ollama',
  config: { model: 'nous-hermes2' },
  options: { tracer },
})

const gen = new AxChainOfThought(
  ai,
  `text -> shortSummary "summarize in 5 to 10 words"`
)

const res = await gen.forward({ text })
```

```json
{
  "traceId": "ddc7405e9848c8c884e53b823e120845",
  "name": "Chat Request",
  "id": "d376daad21da7a3c",
  "kind": "SERVER",
  "timestamp": 1716622997025000,
  "duration": 14190456.542,
  "attributes": {
    "gen_ai.system": "Ollama",
    "gen_ai.request.model": "nous-hermes2",
    "gen_ai.request.max_tokens": 500,
    "gen_ai.request.temperature": 0.1,
    "gen_ai.request.top_p": 0.9,
    "gen_ai.request.frequency_penalty": 0.5,
    "gen_ai.request.llm_is_streaming": false,
    "http.request.method": "POST",
    "url.full": "http://localhost:11434/v1/chat/completions",
    "gen_ai.usage.completion_tokens": 160,
    "gen_ai.usage.prompt_tokens": 290
  }
}
```

## Prompt-Tuning (Basic)

Optimieren Sie Ihre Prompts mit einem gr√∂√üeren Modell f√ºr effizientere und bessere Ergebnisse. Dies geschieht mit einem Optimierer wie `AxBootstrapFewShot` und Beispielen, z.B. aus dem `HotPotQA`-Datensatz. Der Optimierer generiert Demonstrationen (`demos`), die die Effizienz der Prompts steigern.

```typescript
// HotPotQA-Datensatz von Huggingface laden
const hf = new AxHFDataLoader({
  dataset: 'hotpot_qa',
  split: 'train',
})

const examples = await hf.getData<{ question: string; answer: string }>({
  count: 100,
  fields: ['question', 'answer'],
})

const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY as string,
})

// Programm zum Tuning aufsetzen
const program = new AxChainOfThought<{ question: string }, { answer: string }>(
  ai,
  `question -> answer "in short 2 or 3 words"`
)

// Bootstrap Few Shot Optimierer zum Tuning
const optimize = new AxBootstrapFewShot<
  { question: string },
  { answer: string }
>({
  program,
  examples,
})

// Evaluations-Metrik em, f1 Score ist beliebt f√ºr Retrieval-Performance
const metricFn: AxMetricFn = ({ prediction, example }) =>
  emScore(prediction.answer as string, example.answer as string)

// Optimierer ausf√ºhren und Ergebnis speichern
const result = await optimize.compile(metricFn);

// Demos in Datei speichern
// import fs from 'fs';
fs.writeFileSync('bootstrap-demos.json', JSON.stringify(result.demos, null, 2));
console.log('Demos saved to bootstrap-demos.json');
```

<img width="853" alt="tune-prompt" src="https://raw.githubusercontent.com/ax-llm/ax/main/github.com/dosco/llm-client/assets/832235/f924baa7-8922-424c-9c2c-f8b2018d8d74">
```

## Prompt-Tuning (Fortgeschritten, Mipro v2)

MiPRO v2 ist ein fortgeschrittenes Prompt-Optimierungs-Framework, das Bayes'sche Optimierung nutzt, um automatisch die besten Instruktionen, Demonstrationen und Beispiele f√ºr Ihre LLM-Programme zu finden. Durch systematisches Testen verschiedener Prompt-Konfigurationen maximiert MiPRO v2 die Modell-Performance ohne manuelles Tuning. 

### Hauptmerkmale

- **Instruktionsoptimierung**: Generiert und testet automatisch mehrere Instruktionskandidaten
- **Few-Shot-Beispielauswahl**: Findet optimale Demonstrationen aus Ihrem Datensatz
- **Bayes'sche Optimierung**: Nutzt UCB-Strategie f√ºr effiziente Exploration
- **Early Stopping**: Stoppt Optimierung, wenn Verbesserungen stagnieren
- **Programm- und datenbewusst**: Ber√ºcksichtigt Programmstruktur und Datensatz

### Funktionsweise

1. Generiert verschiedene Instruktionskandidaten
2. Bootstrapped Few-Shot-Beispiele aus Ihren Daten
3. W√§hlt gelabelte Beispiele direkt aus dem Datensatz
4. Findet mit Bayes'scher Optimierung die optimale Kombination
5. Setzt die beste Konfiguration im Programm ein

### Grundlegende Nutzung

```typescript
import { AxAI, AxChainOfThought, AxMiPRO } from '@ax-llm/ax'

// 1. AI-Service aufsetzen
const ai = new AxAI({
  name: 'google-gemini',
  apiKey: process.env.GOOGLE_APIKEY,
})

// 2. Programm erstellen
const program = new AxChainOfThought(`input -> output`)

// 3. Optimierer konfigurieren
const optimizer = new AxMiPRO({
  ai,
  program,
  examples: trainingData,
  options: {
    numTrials: 20, // Anzahl Konfigurationen
    auto: 'medium',
  },
})

// 4. Evaluationsmetrik definieren
const metricFn = ({ prediction, example }) => {
  return prediction.output === example.output
}

// 5. Optimierung ausf√ºhren
const optimizedProgram = await optimizer.compile(metricFn, {
  valset: validationData,
})

// 6. Optimiertes Programm nutzen
const result = await optimizedProgram.forward(ai, { input: 'test input' })
```

### Konfigurationsoptionen

MiPRO v2 bietet umfangreiche Konfigurationsm√∂glichkeiten:

| Option                    | Beschreibung                                    | Standard |
| ------------------------- | ----------------------------------------------- | -------- |
| `numCandidates`           | Anzahl Instruktionskandidaten                   | 5        |
| `numTrials`               | Optimierungsdurchl√§ufe                          | 30       |
| `maxBootstrappedDemos`    | Max. Anzahl bootstrapped Demos                  | 3        |
| `maxLabeledDemos`         | Max. Anzahl gelabelter Beispiele                | 4        |
| `minibatch`               | Minibatching f√ºr schnellere Auswertung          | true     |
| `minibatchSize`           | Gr√∂√üe der Minibatches                           | 25       |
| `earlyStoppingTrials`     | Stoppt nach N Durchl√§ufen ohne Verbesserung     | 5        |
| `minImprovementThreshold` | Minimale Verbesserung f√ºr Score                 | 0.01     |
| `programAwareProposer`    | Programmstruktur f√ºr bessere Vorschl√§ge nutzen  | true     |
| `dataAwareProposer`       | Ber√ºcksichtigt Datensatz-Charakteristika        | true     |
| `verbose`                 | Zeigt detaillierten Fortschritt                 | false    |
| abort-patterns.ts | Beispiel, wie Requests abgebrochen werden k√∂nnen      |

### Optimierungslevel

Mit dem `auto`-Parameter l√§sst sich die Optimierungsintensit√§t einstellen:

```typescript
// Light (schneller, weniger gr√ºndlich)
const optimizedProgram = await optimizer.compile(metricFn, { auto: 'light' })

// Medium (ausgewogen)
const optimizedProgram = await optimizer.compile(metricFn, { auto: 'medium' })

// Heavy (langsamer, sehr gr√ºndlich)
const optimizedProgram = await optimizer.compile(metricFn, { auto: 'heavy' })
```

### Fortgeschrittenes Beispiel: Sentimentanalyse

```typescript
// Sentimentanalyse-Programm erstellen
const classifyProgram = new AxChainOfThought<
  { productReview: string },
  { label: string }
>(`productReview -> label:string "positive" or "negative"`)

// Optimierer mit erweiterten Einstellungen
const optimizer = new AxMiPRO({
  ai,
  program: classifyProgram,
  examples: trainingData,
  options: {
    numCandidates: 3,
    numTrials: 10,
    maxBootstrappedDemos: 2,
    maxLabeledDemos: 3,
    earlyStoppingTrials: 3,
    programAwareProposer: true,
    dataAwareProposer: true,
    verbose: true,
  },
})

// Optimierung ausf√ºhren und Ergebnis speichern
const optimizedProgram = await optimizer.compile(metricFn, {
  valset: validationData,
})

// Konfiguration f√ºr sp√§ter speichern
const programConfig = JSON.stringify(optimizedProgram, null, 2);
await fs.promises.writeFile("./optimized-config.json", programConfig);
console.log('> Done. Optimized program config saved to optimized-config.json');
```

## Nutzung der getunten Prompts

Sowohl der Bootstrap Few Shot Optimierer als auch der MiPRO v2 Optimierer erzeugen **Demos** (Demonstrationen), die die Leistung Ihres Programms deutlich verbessern. Diese Demos zeigen dem LLM, wie es √§hnliche Aufgaben korrekt l√∂st.

### Was sind Demos?

Demos sind Input-Output-Beispiele, die automatisch in Ihren Prompts eingebunden werden, um dem LLM das erwartete Verhalten f√ºr Ihre Aufgabe zu zeigen.

### Laden und Nutzen von Demos

Unabh√§ngig davon, ob Sie Bootstrap Few Shot oder MiPRO v2 verwendet haben, ist die Nutzung gleich:

```typescript
import fs from 'fs'
import { AxAI, AxGen, AxChainOfThought } from '@ax-llm/ax'

// 1. AI-Service aufsetzen
const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY,
})

// 2. Programm mit derselben Signatur wie beim Tuning erstellen
const program = new AxChainOfThought(`question -> answer "in short 2 or 3 words"`)

// 3. Demos aus gespeicherter Datei laden
const demos = JSON.parse(fs.readFileSync('bootstrap-demos.json', 'utf8'))

// 4. Demos auf das Programm anwenden
program.setDemos(demos)

// 5. Programm nutzen
const result = await program.forward(ai, {
  question: 'What castle did David Gregory inherit?'
})

console.log(result) // Dank Beispielen bessere Leistung
```

### Einfaches Beispiel: Textklassifikation

Komplettes Beispiel, wie Demos die Klassifikation verbessern:

```typescript
// Klassifikationsprogramm erstellen
const classifier = new AxGen(`text -> category:class "positive, negative, neutral"`)

// Demos laden (Bootstrap oder MiPRO)
const savedDemos = JSON.parse(fs.readFileSync('classification-demos.json', 'utf8'))
classifier.setDemos(savedDemos)

// Klassifikator nutzt erlernte Beispiele f√ºr bessere Ergebnisse
const result = await classifier.forward(ai, {
  text: "This product exceeded my expectations!"
})

console.log(result.category) // Genauere Klassifikation
```

### Vorteile der Nutzung von Demos

- **Bessere Genauigkeit**: Programme liefern mit Beispielen deutlich bessere Ergebnisse
- **Konsistente Ausgaben**: Demos sichern konsistente Ausgabeformate
- **Weniger Halluzinationen**: Beispiele lenken das Modell auf erwartetes Verhalten
- **Kosteneffizient**: Bessere Resultate ohne gr√∂√üere/teurere Modelle

### Best Practices

1. **Demos speichern**: Immer in Dateien sichern f√ºr Wiederverwendung
2. **Signaturen abgleichen**: Exakt gleiche Signatur beim Laden verwenden
3. **Versionieren**: Demo-Dateien im Versionskontrollsystem halten
4. **Regelm√§√üig aktualisieren**: Mit neuen Daten regelm√§√üig neu tunen

Sowohl Bootstrap Few Shot als auch MiPRO v2 liefern Demos im gleichen Format, sodass das Laden immer gleich funktioniert.

## Eingebaute Funktionen

| Funktion            | Name               | Beschreibung                                   |
| ------------------- | ------------------ | ---------------------------------------------- |
| JS Interpreter      | AxJSInterpreter    | JS-Code in einer Sandbox ausf√ºhren             |
| Docker Sandbox      | AxDockerSession    | Kommandos in einer Docker-Umgebung ausf√ºhren   |
| Embeddings Adapter  | AxEmbeddingAdapter | Embedding abrufen und an Funktionen √ºbergeben  |

## Alle Beispiele ansehen

Mit dem `tsx`-Kommando Beispiele ausf√ºhren. Damit kann Node direkt TypeScript ausf√ºhren. `.env`-Datei f√ºr AI-API-Keys wird unterst√ºtzt.

```shell
OPENAI_APIKEY=api-key npm run tsx ./src/examples/marketing.ts
```

| Beispiel                 | Beschreibung                                             |
| ------------------------ | -------------------------------------------------------- |
| customer-support.ts      | Extrahiert Details aus Kundennachrichten                 |
| function.ts              | Einfaches Beispiel f√ºr Funktionsaufruf                   |
| food-search.ts           | Multi-Step, Multi-Funktionsaufruf                        |
| marketing.ts             | Kurze, effektive Marketing-SMS generieren                |
| vectordb.ts              | Text chunken, embedden, durchsuchen                      |
| fibonacci.ts             | JS-Code-Interpreter f√ºr Fibonacci                        |
| summarize.ts             | Kurzzusammenfassung eines gro√üen Textblocks              |
| chain-of-thought.ts      | Chain-of-Thought zum Beantworten von Fragen              |
| rag.ts                   | Multi-Hop-Retrieval f√ºr Fragenbeantwortung               |
| rag-docs.ts              | PDF in Text umwandeln und f√ºr Rag-Suche embedden         |
| react.ts                 | Funktionsaufruf und Reasoning f√ºr Antworten              |
| agent.ts                 | Agenten-Framework, Agenten nutzen andere Agenten, Tools  |
| streaming1.ts            | Validierung von Ausgabefeldern beim Streaming            |
| streaming2.ts            | Validierung pro Ausgabefeld beim Streaming               |
| streaming3.ts            | End-to-End Streaming Beispiel `streamingForward()`       |
| smart-hone.ts            | Agent sucht Hund im Smart Home                           |
| multi-modal.ts           | Bildinput plus weitere Textinputs                        |
| balancer.ts              | Balanciert verschiedene LLMs nach Kosten etc.            |
| docker.ts                | Docker-Sandbox sucht Dateien nach Beschreibung           |
| prime.ts                 | Field Processor f√ºr Promptfeld                           |
| simple-classify.ts       | Einfache Klassifikation                                  |
| mcp-client-memory.ts     | MCP-Server f√ºr Memory mit Ax                             |
| mcp-client-blender.ts    | MCP-Server f√ºr Blender mit Ax                            |
| mcp-client-pipedream.ts  | Integration mit Remote-MCP                               |
| tune-bootstrap.ts        | Bootstrap-Optimizer f√ºr Prompt-Effizienz                 |
| tune-mipro.ts            | MiPRO v2 Optimizer f√ºr Prompt-Effizienz                  |
| tune-usage.ts            | Optimierte Prompts nutzen                                |
| telemetry.ts             | Tracing und Jaeger-Integration                           |
| openai-responses.ts      | Nutzung der neuen OpenAI Responses API                   |
| use-examples.ts          | Verwendung von 'examples' zur Lenkung des LLM            |

## Unser Ziel

Gro√üe Sprachmodelle (LLMs) werden immer leistungsf√§higer und sind mittlerweile in der Lage, das Backend Ihres gesamten Produkts zu bilden. Dennoch bleibt viel Komplexit√§t zu bew√§ltigen: richtige Prompts, Modelle, Streaming, Funktionsaufrufe, Fehlerkorrektur usw. Unser Ziel ist es, all diese Komplexit√§t in einer gepflegten, einfach nutzbaren Bibliothek zu b√ºndeln, die mit allen modernen LLMs funktioniert. Au√üerdem bringen wir neue Features wie DSPy aus der Forschung direkt in die Bibliothek.

## Wie benutzt man diese Bibliothek?

### 1. W√§hlen Sie eine KI

```ts
// LLM w√§hlen
const ai = new AxOpenAI({ apiKey: process.env.OPENAI_APIKEY } as AxOpenAIArgs)
```

### 2. Prompt-Signatur je nach Anwendungsfall erstellen

```ts
// Die Signatur definiert Eingaben und Ausgaben des Prompt-Programms
const cot = new ChainOfThought(ai, `question:string -> answer:string`, { mem })
```

### 3. Prompt-Programm ausf√ºhren

```ts
// Die in der Signatur definierten Eingabefelder √ºbergeben
const res = await cot.forward({ question: 'Are we in a simulation?' })
```

### 4. Oder direkt das LLM nutzen

```ts
const res = await ai.chat([
  { role: "system", content: "Help the customer with his questions" }
  { role: "user", content: "I'm looking for a Macbook Pro M2 With 96GB RAM?" }
]);
```

## Wie nutzt man Funktionsaufrufe?

### 1. Funktionen definieren

```ts
// Eine oder mehrere Funktionen und einen Handler definieren
const functions = [
  {
    name: 'getCurrentWeather',
    description: 'get the current weather for a location',
    parameters: {
      type: 'object',
      properties: {
        location: {
          type: 'string',
          description: 'location to get weather for',
        },
        units: {
          type: 'string',
          enum: ['imperial', 'metric'],
          default: 'imperial',
          description: 'units to use',
        },
      },
      required: ['location'],
    },
    func: async (args: Readonly<{ location: string; units: string }>) => {
      return `The weather in ${args.location} is 72 degrees`
    },
  },
]
```

### 2. Funktionen an einen Prompt √ºbergeben

```ts
const cot = new AxGen(ai, `question:string -> answer:string`, { functions })
```

## Debug-Logs aktivieren

```ts
const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY,
} as AxOpenAIArgs)
ai.setOptions({ debug: true })
```

## Kontakt

Wir helfen gerne weiter ‚Äì bei Fragen einfach melden oder auf Discord joinen:  
[twitter/dosco](https://twitter.com/dosco)

## FAQ

### 1. Das LLM findet nicht die richtige Funktion

Verbessern Sie die Funktionsnamen und Beschreibungen. Seien Sie sehr klar, was die Funktion tut. Auch die Parameter sollten gut beschrieben sein ‚Äì kurz, aber pr√§zise.

### 2. Wie √§ndere ich die Konfiguration des verwendeten LLMs?

Sie k√∂nnen beim Erstellen eines neuen LLM-Objekts ein Konfigurationsobjekt als zweiten Parameter √ºbergeben.

```ts
const apiKey = process.env.OPENAI_APIKEY
const conf = AxOpenAIBestConfig()
const ai = new AxOpenAI({ apiKey, conf } as AxOpenAIArgs)
```

## 3. Mein Prompt ist zu lang / Kann ich die max Tokens √§ndern?

```ts
const conf = axOpenAIDefaultConfig() // oder OpenAIBestOptions()
conf.maxTokens = 2000
```

## 4. Wie √§ndere ich das Modell? (z.B. GPT4 nutzen)

```ts
const conf = axOpenAIDefaultConfig() // oder OpenAIBestOptions()
conf.model = OpenAIModel.GPT4Turbo
```

## Monorepo-Tipps & Tricks

F√ºhren Sie `npm install` nur im Root-Verzeichnis aus. So verhindern Sie verschachtelte `package-lock.json` und nicht deduplizierte `node_modules`.

Neue Abh√§ngigkeiten in Packages mit  
`npm install lodash --workspace=ax`  
hinzuf√ºgen (oder `package.json` anpassen und `npm install` im Root ausf√ºhren).

---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-06-07

---