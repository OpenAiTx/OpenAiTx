# Axï¼ŒTypeScript çš„ DSPy

ä¸ LLM åä½œå¾ˆå¤æ‚â€”â€”å®ƒä»¬å¹¶ä¸æ€»æ˜¯æŒ‰ä½ çš„æ„æ„¿è¡Œäº‹ã€‚DSPy è®©åŸºäº LLM çš„åˆ›æ–°å˜å¾—æ›´ç®€å•ã€‚ä½ åªéœ€å®šä¹‰è¾“å…¥å’Œè¾“å‡ºï¼ˆç­¾åï¼‰ï¼Œå³å¯è‡ªåŠ¨ç”Ÿæˆé«˜æ•ˆæç¤ºå¹¶ä½¿ç”¨ã€‚å°†ä¸åŒç­¾åè¿æ¥èµ·æ¥ï¼Œæ­å»ºåŸºäº LLM çš„å¤æ‚ç³»ç»Ÿå’Œå·¥ä½œæµã€‚

ä¸ºäº†è®©ä½ çœŸæ­£èƒ½åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ç”¨å¥½è¿™äº›ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†è§‚æµ‹æ€§ã€æµå¼å¤„ç†ã€æ”¯æŒå¤šæ¨¡æ€ï¼ˆå›¾åƒã€éŸ³é¢‘ç­‰ï¼‰ã€é”™è¯¯ä¿®æ­£ã€å¤šæ­¥å‡½æ•°è°ƒç”¨ã€MCPã€RAG ç­‰å®Œæ•´èƒ½åŠ›ã€‚

[![NPM Package](https://img.shields.io/npm/v/@ax-llm/ax?style=for-the-badge&color=green)](https://www.npmjs.com/package/@ax-llm/ax)
[![Discord Chat](https://dcbadge.vercel.app/api/server/DSHg3dU7dW?style=for-the-badge)](https://discord.gg/DSHg3dU7dW)
[![Twitter](https://img.shields.io/twitter/follow/dosco?style=for-the-badge&color=red)](https://twitter.com/dosco)

<!-- header -->

## ä¸ºä»€ä¹ˆé€‰æ‹© Axï¼Ÿ

- æ‰€æœ‰ä¸»æµ LLM çš„æ ‡å‡†æ¥å£
- ç”±ç®€å•ç­¾åè‡ªåŠ¨ç”Ÿæˆæç¤ºè¯
- å®Œå…¨åŸç”Ÿçš„ç«¯åˆ°ç«¯æµå¼å¤„ç†
- æ”¯æŒæ€è€ƒé¢„ç®—å’Œæ€è€ƒ Token
- æ„å»ºå¯è°ƒç”¨å…¶ä»– Agent çš„æ™ºèƒ½ä½“
- å†…ç½® MCPï¼ˆæ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼‰æ”¯æŒ
- ä»»æ„æ ¼å¼æ–‡æ¡£è½¬æ–‡æœ¬
- RAGã€æ™ºèƒ½åˆ†å—ã€åµŒå…¥ã€æŸ¥è¯¢
- å¯ä¸ Vercel AI SDK é…åˆä½¿ç”¨
- æµå¼è¾“å‡ºæ ¡éªŒ
- æ”¯æŒå¤šæ¨¡æ€ DSPy
- è‡ªåŠ¨ä½¿ç”¨ä¼˜åŒ–å™¨è°ƒæ•´æç¤º
- OpenTelemetry è¿½è¸ª/è§‚æµ‹æ€§
- ç”Ÿäº§çº§ TypeScript ä»£ç 
- è½»é‡ã€é›¶ä¾èµ–

## ç”Ÿäº§å¯ç”¨

- å°ç‰ˆæœ¬æ— ç ´åæ€§å˜æ›´
- å¤§é‡æµ‹è¯•è¦†ç›–
- å†…ç½® Open Telemetry `gen_ai` æ”¯æŒ
- å·²è¢«åˆ›ä¸šå…¬å¸å¹¿æ³›ç”¨äºç”Ÿäº§ç¯å¢ƒ

## ä»€ä¹ˆæ˜¯æç¤ºç­¾åï¼Ÿ

<img width="860" alt="shapes at 24-03-31 00 05 55" src="https://raw.githubusercontent.com/ax-llm/ax/main/https://github.com/dosco/llm-client/assets/832235/0f0306ea-1812-4a0a-9ed5-76cd908cd26b">

é«˜æ•ˆä¸”ç±»å‹å®‰å…¨çš„æç¤ºè¯ç”±ç®€å•ç­¾åè‡ªåŠ¨ç”Ÿæˆã€‚æç¤ºç­¾åç”± `"ä»»åŠ¡æè¿°" è¾“å…¥å­—æ®µ:ç±»å‹ "å­—æ®µæè¿°" -> è¾“å‡ºå­—æ®µ:ç±»å‹` ç»„æˆã€‚æç¤ºç­¾åçš„ç†å¿µæºè‡ª â€œDemonstrate-Search-Predictâ€ è®ºæ–‡ã€‚

ä½ å¯ä»¥æœ‰å¤šä¸ªè¾“å…¥å’Œè¾“å‡ºå­—æ®µï¼Œæ¯ä¸ªå­—æ®µç±»å‹å¯ä»¥ä¸º `string`ã€`number`ã€`boolean`ã€`date`ã€`datetime`ã€`class "class1, class2"`ã€`JSON` æˆ–è¿™äº›ç±»å‹çš„æ•°ç»„ï¼ˆå¦‚ `string[]`ï¼‰ã€‚æœªå®šä¹‰ç±»å‹æ—¶é»˜è®¤æ˜¯ `string`ã€‚åç¼€ `?` è¡¨ç¤ºå­—æ®µå¯é€‰ï¼ˆé»˜è®¤å¿…å¡«ï¼‰ï¼Œ`!` è¡¨ç¤ºå†…éƒ¨å­—æ®µï¼Œé€‚ç”¨äºæ¨ç†ç­‰ç”¨é€”ã€‚

## è¾“å‡ºå­—æ®µç±»å‹

| ç±»å‹                      | æè¿°                              | ç”¨æ³•                      | è¾“å‡ºç¤ºä¾‹                                               |
| ------------------------- | --------------------------------- | ------------------------- | ------------------------------------------------------ |
| `string`                  | å­—ç¬¦ä¸²åºåˆ—                        | `fullName:string`         | `"example"`                                            |
| `number`                  | æ•°å€¼                              | `price:number`            | `42`                                                   |
| `boolean`                 | å¸ƒå°”å€¼                            | `isEvent:boolean`         | `true`, `false`                                        |
| `date`                    | æ—¥æœŸ                              | `startDate:date`          | `"2023-10-01"`                                         |
| `datetime`                | æ—¥æœŸå’Œæ—¶é—´                        | `createdAt:datetime`      | `"2023-10-01T12:00:00Z"`                               |
| `class "class1,class2"`   | ç±»åˆ«åˆ†ç±»                          | `category:class`          | `["class1", "class2", "class3"]`                       |
| `string[]`                | å­—ç¬¦ä¸²æ•°ç»„                        | `tags:string[]`           | `["example1", "example2"]`                             |
| `number[]`                | æ•°å­—æ•°ç»„                          | `scores:number[]`         | `[1, 2, 3]`                                            |
| `boolean[]`               | å¸ƒå°”å€¼æ•°ç»„                        | `permissions:boolean[]`   | `[true, false, true]`                                  |
| `date[]`                  | æ—¥æœŸæ•°ç»„                          | `holidayDates:date[]`     | `["2023-10-01", "2023-10-02"]`                         |
| `datetime[]`              | æ—¥æœŸæ—¶é—´æ•°ç»„                      | `logTimestamps:datetime[]`| `["2023-10-01T12:00:00Z", "2023-10-02T12:00:00Z"]`     |
| `class[] "class1,class2"` | å¤šç±»åˆ«                            | `categories:class[]`      | `["class1", "class2", "class3"]`                       |
| `code "language"`         | æŒ‡å®šè¯­è¨€ä»£ç å—                    | `code:code "python"`      | `print('Hello, world!')`                               |

## æ”¯æŒçš„ LLM

`Google Gemini`ã€`OpenAI`ã€`Azure OpenAI`ã€`Anthropic`ã€`X Grok`ã€`TogetherAI`ã€`Cohere`ã€`Mistral`ã€`Groq`ã€`DeepSeek`ã€`Ollama`ã€`Reka`ã€`Hugging Face`

## å®‰è£…

```bash
npm install @ax-llm/ax
# æˆ–
yarn add @ax-llm/ax
```

## ç¤ºä¾‹ï¼šç”¨ chain-of-thought æ‘˜è¦æ–‡æœ¬

```typescript
import { AxAI, AxChainOfThought } from '@ax-llm/ax'

const textToSummarize = `
The technological singularityâ€”or simply the singularity[1]â€”is a hypothetical future point in time at which technological growth becomes uncontrollable and irreversible, resulting in unforeseeable changes to human civilization.[2][3] ...`

const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY as string,
})

const gen = new AxChainOfThought(
  `textToSummarize -> textType:class "note, email, reminder", shortSummary "summarize in 5 to 10 words"`
)

const res = await gen.forward(ai, { textToSummarize })

console.log('>', res)
```

## ç¤ºä¾‹ï¼šæ„å»º Agent

é€šè¿‡ Agent æç¤ºï¼ˆæ¡†æ¶ï¼‰åˆ›å»ºå¯ä¸å…¶ä»– Agent åä½œå®Œæˆä»»åŠ¡çš„ Agentã€‚ä½¿ç”¨æç¤ºç­¾ååˆ›å»º Agent ååˆ†ç®€å•ã€‚è¯•è¯•ä¸‹é¢çš„ä¾‹å­ã€‚

```typescript
# npm run tsx ./src/examples/agent.ts

const researcher = new AxAgent({
  name: 'researcher',
  description: 'Researcher agent',
  signature: `physicsQuestion "physics questions" -> answer "reply in bullet points"`
});

const summarizer = new AxAgent({
  name: 'summarizer',
  description: 'Summarizer agent',
  signature: `text "text so summarize" -> shortSummary "summarize in 5 to 10 words"`
});

const agent = new AxAgent({
  name: 'agent',
  description: 'A an agent to research complex topics',
  signature: `question -> answer`,
  agents: [researcher, summarizer]
});

agent.forward(ai, { questions: "How many atoms are there in the universe" })
```

## æ€è€ƒæ¨¡å‹æ”¯æŒ

Ax åŸç”Ÿæ”¯æŒå…·å¤‡æ€è€ƒèƒ½åŠ›çš„æ¨¡å‹ï¼Œå¯æ§åˆ¶æ€è€ƒ Token é¢„ç®—å¹¶è®¿é—®æ¨¡å‹æ€è€ƒè¿‡ç¨‹ï¼Œæœ‰åŠ©äºç†è§£æ¨¡å‹æ¨ç†å¹¶ä¼˜åŒ– Token ä½¿ç”¨ã€‚

```typescript
const ai = new AxAI({
  name: 'google-gemini',
  apiKey: process.env.GOOGLE_APIKEY as string,
  config: {
    model: AxAIGoogleGeminiModel.Gemini25Flash,
    thinking: { includeThoughts: true },
  },
})

// æˆ–æŒ‰è¯·æ±‚æ§åˆ¶æ€è€ƒé¢„ç®—
const gen = new AxChainOfThought(`question -> answer`)
const res = await gen.forward(
  ai,
  { question: 'What is quantum entanglement?' },
  { thinkingTokenBudget: 'medium' } // 'minimal', 'low', 'medium', 'high'
)

// åœ¨å“åº”ä¸­è®¿é—®æ€è€ƒè¿‡ç¨‹
console.log(res.thoughts) // æ˜¾ç¤ºæ¨¡å‹æ¨ç†è¿‡ç¨‹
```

## æ”¯æŒçš„å‘é‡æ•°æ®åº“

å‘é‡æ•°æ®åº“å¯¹æ­å»º LLM å·¥ä½œæµè‡³å…³é‡è¦ã€‚Ax æä¾›å¯¹ä¸»æµå‘é‡æ•°æ®åº“çš„æŠ½è±¡ï¼Œä»¥åŠè‡ªæœ‰çš„å†…å­˜å‹å‘é‡æ•°æ®åº“ã€‚

| æä¾›å•†      | æµ‹è¯•è¦†ç›– |
| ----------- | -------- |
| å†…å­˜        | ğŸŸ¢ 100%  |
| Weaviate    | ğŸŸ¢ 100%  |
| Cloudflare  | ğŸŸ¡ 50%   |
| Pinecone    | ğŸŸ¡ 50%   |

```typescript
// ä½¿ç”¨ LLM ä»æ–‡æœ¬ç”Ÿæˆ embedding
const ret = await this.ai.embed({ texts: 'hello world' })

// åˆ›å»ºå†…å­˜å‘é‡æ•°æ®åº“
const db = new axDB('memory')

// æ’å…¥å‘é‡æ•°æ®åº“
await this.db.upsert({
  id: 'abc',
  table: 'products',
  values: ret.embeddings[0],
})

// ä½¿ç”¨ embedding æŸ¥è¯¢ç›¸ä¼¼é¡¹
const matches = await this.db.query({
  table: 'products',
  values: embeddings[0],
})
```

ä½ ä¹Ÿå¯ä»¥ç”¨ `AxDBManager`ï¼Œå®ƒè‡ªåŠ¨å¤„ç†åˆ†å—ã€åµŒå…¥å’ŒæŸ¥è¯¢ï¼Œæå…¶ç®€ä¾¿ã€‚

```typescript
const manager = new AxDBManager({ ai, db })
await manager.insert(text)

const matches = await manager.query(
  'John von Neumann on human intelligence and singularity.'
)
console.log(matches)
```

## RAG æ–‡æ¡£

ç”¨ LLM å¤„ç† PDFã€DOCXã€PPTã€XLS ç­‰æ–‡æ¡£å¾ˆç¹çã€‚æˆ‘ä»¬å€ŸåŠ© Apache Tikaï¼ˆä¸€æ¬¾å¼€æºæ–‡æ¡£å¤„ç†å¼•æ“ï¼‰è®©è¿™ä¸€åˆ‡å˜å¾—ç®€å•ã€‚

å¯åŠ¨ Apache Tika

```shell
docker run -p 9998:9998 apache/tika
```

ç”¨ `AxDBManager` å°†æ–‡æ¡£è½¬ä¸ºæ–‡æœ¬å¹¶åµŒå…¥ç”¨äºæ£€ç´¢ï¼ŒåŒæ—¶æ”¯æŒ reranker å’Œ query rewriterã€‚é»˜è®¤å®ç°æœ‰ `AxDefaultResultReranker` å’Œ `AxDefaultQueryRewriter`ã€‚

```typescript
const tika = new AxApacheTika()
const text = await tika.convert('/path/to/document.pdf')

const manager = new AxDBManager({ ai, db })
await manager.insert(text)

const matches = await manager.query('Find some text')
console.log(matches)
```

## å¤šæ¨¡æ€ DSPy

å¯¹äºæ”¯æŒå¤šæ¨¡æ€æç¤ºçš„æ¨¡å‹ï¼ˆå¦‚ `GPT-4o`ã€`Gemini`ï¼‰ï¼Œæˆ‘ä»¬æ”¯æŒå›¾åƒå­—æ®µï¼Œå¹¶å¯ä¸æ•´ä¸ª DSP æµæ°´çº¿ååŒå·¥ä½œã€‚

```typescript
const image = fs
  .readFileSync('./src/examples/assets/kitten.jpeg')
  .toString('base64')

const gen = new AxChainOfThought(`question, animalImage:image -> answer`)

const res = await gen.forward(ai, {
  question: 'What family does this animal belong to?',
  animalImage: { mimeType: 'image/jpeg', data: image },
})
```

å¯¹äºæ”¯æŒéŸ³é¢‘çš„å¤šæ¨¡æ€æ¨¡å‹ï¼ˆå¦‚ `gpt-4o-audio-preview`ï¼‰ï¼Œä¹Ÿæ”¯æŒéŸ³é¢‘å­—æ®µï¼Œå…¼å®¹ DSP æµæ°´çº¿ã€‚

```typescript
const audio = fs
  .readFileSync('./src/examples/assets/comment.wav')
  .toString('base64')

const gen = new AxGen(`question, commentAudio:audio -> answer`)

const res = await gen.forward(ai, {
  question: 'What family does this animal belong to?',
  commentAudio: { format: 'wav', data: audio },
})
```

## DSPy èŠå¤©æ¥å£

å— DSPy æ¼”ç¤ºç¼–ç»‡å¯å‘ï¼ŒAx æä¾› `AxMessage` ç”¨äºæ— ç¼ç®¡ç†ä¼šè¯å†å²ã€‚å¯æ„å»ºä¸Šä¸‹æ–‡è¿è´¯çš„èŠå¤©æœºå™¨äººå’Œå¯¹è¯ä»£ç†ï¼ŒåŒæ—¶åˆ©ç”¨æç¤ºç­¾åå…¨éƒ¨èƒ½åŠ›ã€‚è¯¦æƒ…è§ç¤ºä¾‹ã€‚

```shell
GOOGLE_APIKEY=api-key npm run tsx ./src/examples/chat.ts
```

```typescript
const chatBot = new AxGen<
  { message: string } | ReadonlyArray<ChatMessage>,
  { reply: string }
>(
  `message:string "A casual message from the user" -> reply:string "A friendly, casual response"`
)

await chatBot.forward(ai, [
  {
    role: 'user',
    values: { message: 'Hi! How are you doing today?' },
  },
  {
    role: 'assistant',
    values: { reply: 'I am doing great! How about you?' },
  },
  {
    role: 'user',
    values: { message: 'Thats great!' },
  },
])
```

ä¼šè¯å†å²è‡ªåŠ¨ç¼–ç»‡è¿›æç¤ºè¯ï¼Œæ¨¡å‹å¯ä¿æŒä¸Šä¸‹æ–‡ï¼Œè¾“å‡ºè¿è´¯ã€‚è¿™ä¸ Ax çš„æ‰€æœ‰ç‰¹æ€§ï¼ˆå¦‚æµå¼ã€å‡½æ•°è°ƒç”¨ã€chain-of-thoughtï¼‰æ— ç¼åä½œã€‚

## æµå¼å¤„ç†

### æ–­è¨€

æˆ‘ä»¬æ”¯æŒæµå¼è§£æè¾“å‡ºå­—æ®µå’Œå‡½æ•°æ‰§è¡Œã€‚è¿™æ ·å¯å¿«é€Ÿå¤±è´¥å’Œçº é”™ï¼Œæ— éœ€ç­‰å¾…å…¨éƒ¨è¾“å‡ºï¼ŒèŠ‚çœ Tokenã€é™ä½å»¶è¿Ÿå’Œæˆæœ¬ã€‚æ–­è¨€å¯ç¡®ä¿è¾“å‡ºç¬¦åˆè¦æ±‚ï¼Œæµå¼å¤„ç†åŒæ ·æœ‰æ•ˆã€‚

```typescript
// è®¾ç½®æç¤ºç¨‹åº
const gen = new AxChainOfThought(
  ai,
  `startNumber:number -> next10Numbers:number[]`
)

// æ·»åŠ æ–­è¨€ï¼Œç¡®ä¿è¾“å‡ºå­—æ®µä¸åŒ…å«æ•°å­— 5
gen.addAssert(({ next10Numbers }: Readonly<{ next10Numbers: number[] }>) => {
  return next10Numbers ? !next10Numbers.includes(5) : undefined
}, 'Numbers 5 is not allowed')

// å¯ç”¨æµå¼æ‰§è¡Œ
const res = await gen.forward({ startNumber: 1 }, { stream: true })

// æˆ–ç«¯åˆ°ç«¯æµå¼æ‰§è¡Œ
const generator = await gen.streamingForward(
  { startNumber: 1 },
  {
    stream: true,
  }
)
for await (const res of generator) {
}
```

ä¸Šè¿°ä¾‹å­æ”¯æŒåœ¨æµå…¥æ—¶éªŒè¯æ•´ä¸ªè¾“å‡ºå­—æ®µã€‚è¯¥æ ¡éªŒæ—¢é€‚ç”¨äºæµå¼ï¼Œä¹Ÿé€‚ç”¨äºéæµå¼ï¼Œå®Œæ•´å­—æ®µå€¼å¯ç”¨æ—¶è§¦å‘ã€‚è¦åœ¨æµå¼è¿‡ç¨‹ä¸­çœŸæ­£é€æ­¥æ ¡éªŒï¼Œè¯·å‚è€ƒä¸‹ä¾‹ã€‚è¿™å°†æå¤§æå‡æ€§èƒ½å¹¶èŠ‚çœç”Ÿäº§ Token æˆæœ¬ã€‚

```typescript
// æ·»åŠ æµå¼æ–­è¨€ï¼Œç¡®ä¿æ‰€æœ‰è¡Œä»¥æ•°å­—åŠ ç‚¹å¼€å¤´
gen.addStreamingAssert(
  'answerInPoints',
  (value: string) => {
    const re = /^\d+\./

    // æŒ‰è¡Œåˆ†å‰²ï¼Œä¿®å‰ªï¼Œè¿‡æ»¤ç©ºè¡Œï¼Œæ£€æŸ¥æ¯è¡Œæ˜¯å¦åŒ¹é…æ­£åˆ™
    return value
      .split('\n')
      .map((x) => x.trim())
      .filter((x) => x.length > 0)
      .every((x) => re.test(x))
  },
  'Lines must start with a number and a dot. Eg: 1. This is a line.'
)

// å¯ç”¨æµå¼æ‰§è¡Œ
const res = await gen.forward(
  {
    question: 'Provide a list of optimizations to speedup LLM inference.',
  },
  { stream: true, debug: true }
)
```

### å­—æ®µå¤„ç†å™¨

å­—æ®µå¤„ç†å™¨å¯å¯¹æç¤ºä¸­çš„å­—æ®µè¿›è¡Œå¤„ç†ï¼Œåœ¨å‘é€ç»™ LLM ä¹‹å‰å¤„ç†å­—æ®µã€‚

```typescript
const gen = new AxChainOfThought(
  ai,
  `startNumber:number -> next10Numbers:number[]`
)

const streamValue = false

const processorFunction = (value) => {
  return value.map((x) => x + 1)
}

// æ·»åŠ å­—æ®µå¤„ç†å™¨
const processor = new AxFieldProcessor(
  gen,
  'next10Numbers',
  processorFunction,
  streamValue
)

const res = await gen.forward({ startNumber: 1 })
```

## æ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰

Ax æ— ç¼é›†æˆ MCPï¼ˆModel Context Protocolï¼‰ï¼Œè®©ä½ çš„ Agent å¯é€šè¿‡æ ‡å‡†æ¥å£è®¿é—®å¤–éƒ¨å·¥å…·å’Œèµ„æºã€‚

### ä½¿ç”¨ AxMCPClient

`AxMCPClient` å…è®¸ä½ è¿æ¥ä»»ä½• MCP å…¼å®¹æœåŠ¡å™¨ï¼Œå¹¶åœ¨ Ax Agent ä¸­ä½¿ç”¨å…¶èƒ½åŠ›ï¼š

```typescript
import { AxMCPClient, AxMCPStdioTransport } from '@ax-llm/ax'

// ç”¨æŒ‡å®šä¼ è¾“æ–¹å¼åˆå§‹åŒ– MCP å®¢æˆ·ç«¯
const transport = new AxMCPStdioTransport({
  command: 'npx',
  args: ['-y', '@modelcontextprotocol/server-memory'],
})

// å¯é€‰è°ƒè¯•æ¨¡å¼åˆ›å»ºå®¢æˆ·ç«¯
const client = new AxMCPClient(transport, { debug: true })

// åˆå§‹åŒ–è¿æ¥
await client.init()

// åœ¨ Agent ä¸­ä½¿ç”¨å®¢æˆ·ç«¯åŠŸèƒ½
const memoryAgent = new AxAgent({
  name: 'MemoryAssistant',
  description: 'An assistant with persistent memory',
  signature: 'input, userId -> response',
  functions: [client], // ä½œä¸ºå‡½æ•°æä¾›è€…ä¼ é€’å®¢æˆ·ç«¯
})

// æˆ–ä¸ AxGen é…åˆ
const memoryGen = new AxGen('input, userId -> response', {
  functions: [client],
})
```

### è¿œç¨‹ MCP æœåŠ¡å™¨ç¤ºä¾‹

è°ƒç”¨è¿œç¨‹ MCP æœåŠ¡å™¨å¾ˆç®€å•ã€‚æ¯”å¦‚ï¼Œä½¿ç”¨ DeepWiki MCP æœåŠ¡å™¨æŸ¥è¯¢å…¬å¼€ GitHub ä»“åº“ã€‚DeepWiki MCP æœåŠ¡å™¨åœ°å€ï¼š`https://mcp.deepwiki.com/mcp`ã€‚

```typescript
import {
  AxAgent,
  AxAI,
  AxAIOpenAIModel,
  AxMCPClient,
  AxMCPStreambleHTTPTransport,
} from '@ax-llm/ax'

// 1. è¿æ¥ DeepWiki æœåŠ¡å™¨
const transport = new AxMCPStreambleHTTPTransport(
  'https://mcp.deepwiki.com/mcp'
)

// 2. åˆ›å»º MCP å®¢æˆ·ç«¯
const mcpClient = new AxMCPClient(transport, { debug: false })
await mcpClient.init() // åˆå§‹åŒ–è¿æ¥

// 3. åˆå§‹åŒ– AI æ¨¡å‹ï¼ˆå¦‚ OpenAIï¼‰
// ç¡®ä¿ OPENAI_APIKEY ç¯å¢ƒå˜é‡å·²è®¾ç½®
const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY as string,
})

// 4. åˆ›å»ºä½¿ç”¨ MCP å®¢æˆ·ç«¯çš„ Agent
const deepwikiAgent = new AxAgent<
  {
    // è¾“å…¥ç±»å‹å®šä¹‰ï¼ŒåŒ¹é… DeepWiki å‡½æ•°
    questionAboutRepo: string
    githubRepositoryUrl: string
  },
  {
    answer: string
  }
>({
  name: 'DeepWikiQueryAgent',
  description: 'Agent to query public GitHub repositories via DeepWiki MCP.',
  signature: 'questionAboutRepo, githubRepositoryUrl -> answer',
  functions: [mcpClient], // æä¾› MCP å®¢æˆ·ç«¯
})

// 5. æé—®å¹¶è°ƒç”¨ Agent
const result = await deepwikiAgent.forward(ai, {
  questionAboutRepo: 'What is the main purpose of this library?',
  githubRepositoryUrl: 'https://github.com/dosco/ax', // ä¾‹ï¼šAx åº“æœ¬èº«
})
console.log('DeepWiki Answer:', result.answer)
```

è¯¥ç¤ºä¾‹å±•ç¤ºå¦‚ä½•è¿æ¥å…¬å¼€ MCP æœåŠ¡å™¨å¹¶åœ¨ Ax Agent ä¸­ä½¿ç”¨ã€‚Agent çš„ç­¾åï¼ˆ`questionAboutRepo, githubRepositoryUrl -> answer`ï¼‰æ˜¯å¯¹ DeepWiki æœåŠ¡æ¥å£çš„å‡è®¾ï¼Œå®é™…å¯é€šè¿‡ MCP æœåŠ¡å™¨ï¼ˆå¦‚ `mcp.getFunctions` æˆ–æ–‡æ¡£ï¼‰è·çŸ¥æ¥å£è¯¦æƒ…ã€‚

æ›´å¤æ‚ç¤ºä¾‹ï¼ˆå¦‚è®¤è¯ã€å®šåˆ¶ Headerï¼‰è¯·å‚è€ƒæœ¬ä»“åº“ `src/examples/mcp-client-pipedream.ts`ã€‚

## AI è·¯ç”±ä¸è´Ÿè½½å‡è¡¡

Ax æä¾›ä¸¤ç§å¼ºå¤§å¤š AI æœåŠ¡åä½œæ–¹å¼ï¼šè´Ÿè½½å‡è¡¡ï¼ˆé«˜å¯ç”¨ï¼‰ä¸è·¯ç”±ï¼ˆæ¨¡å‹è·¯ç”±ï¼‰ã€‚

### è´Ÿè½½å‡è¡¡

è´Ÿè½½å‡è¡¡å™¨æ ¹æ®æ€§èƒ½ä¸å¯ç”¨æ€§è‡ªåŠ¨åˆ†é…è¯·æ±‚ã€‚å¦‚é‡æœåŠ¡æ•…éšœè‡ªåŠ¨åˆ‡æ¢ã€‚

```typescript
import { AxAI, AxBalancer } from '@ax-llm/ax'

// è®¾ç½®å¤šä¸ª AI æœåŠ¡
const openai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY,
})

const ollama = new AxAI({
  name: 'ollama',
  config: { model: 'nous-hermes2' },
})

const gemini = new AxAI({
  name: 'google-gemini',
  apiKey: process.env.GOOGLE_APIKEY,
})

// åˆ›å»ºè´Ÿè½½å‡è¡¡å™¨
const balancer = new AxBalancer([openai, ollama, gemini])

// åƒæ™®é€š AI æœåŠ¡ä¸€æ ·ä½¿ç”¨â€”â€”è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æœåŠ¡
const response = await balancer.chat({
  chatPrompt: [{ role: 'user', content: 'Hello!' }],
})

// æˆ–é…åˆ AxGen ä½¿ç”¨
const gen = new AxGen(`question -> answer`)
const res = await gen.forward(balancer, { question: 'Hello!' })
```

### å¤šæœåŠ¡è·¯ç”±

è·¯ç”±å™¨è®©ä½ é€šè¿‡ç»Ÿä¸€æ¥å£ä½¿ç”¨å¤šä¸ª AI æœåŠ¡ï¼Œè‡ªåŠ¨æŒ‰æ¨¡å‹é€‰æ‹©åˆé€‚æœåŠ¡ã€‚

```typescript
import { AxAI, AxAIOpenAIModel, AxMultiServiceRouter } from '@ax-llm/ax'

// é…ç½® OpenAI æ¨¡å‹åˆ—è¡¨
const openai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY,
  models: [
    {
      key: 'basic',
      model: AxAIOpenAIModel.GPT4OMini,
      description:
        'Model for very simple tasks such as answering quick short questions',
    },
    {
      key: 'medium',
      model: AxAIOpenAIModel.GPT4O,
      description:
        'Model for semi-complex tasks such as summarizing text, writing code, and more',
    },
  ],
})

// é…ç½® Gemini æ¨¡å‹åˆ—è¡¨
const gemini = new AxAI({
  name: 'google-gemini',
  apiKey: process.env.GOOGLE_APIKEY,
  models: [
    {
      key: 'deep-thinker',
      model: 'gemini-2.0-flash-thinking',
      description:
        'Model that can think deeply about a task, best for tasks that require planning',
    },
    {
      key: 'expert',
      model: 'gemini-2.0-pro',
      description:
        'Model that is the best for very complex tasks such as writing large essays, complex coding, and more',
    },
  ],
})

const ollama = new AxAI({
  name: 'ollama',
  config: { model: 'nous-hermes2' },
})

const secretService = {
  key: 'sensitive-secret',
  service: ollama,
  description: 'Model for sensitive secrets tasks',
}

// åˆ›å»ºè·¯ç”±å™¨
const router = new AxMultiServiceRouter([openai, gemini, secretService])

// è·¯ç”±åˆ° OpenAI çš„ expert æ¨¡å‹
const openaiResponse = await router.chat({
  chatPrompt: [{ role: 'user', content: 'Hello!' }],
  model: 'expert',
})

// æˆ–é…åˆ AxGen ä½¿ç”¨
const gen = new AxGen(`question -> answer`)
const res = await gen.forward(router, { question: 'Hello!' })
```

è´Ÿè½½å‡è¡¡é€‚åˆé«˜å¯ç”¨ï¼Œè·¯ç”±é€‚åˆæŒ‰ä»»åŠ¡ç²¾å‡†åŒ¹é…æ¨¡å‹ã€‚äºŒè€…å‡æ”¯æŒ Ax æ‰€æœ‰ç‰¹æ€§ï¼ˆæµå¼ã€å‡½æ•°è°ƒç”¨ã€chain-of-thought ç­‰ï¼‰ã€‚

ä½ å¯å°†è´Ÿè½½å‡è¡¡ä¸è·¯ç”±ç»“åˆä½¿ç”¨ï¼Œè·¯ç”±å†…å«å¤šä¸ªå‡è¡¡å™¨æˆ–åä¹‹ã€‚

## OpenTelemetry æ”¯æŒ

å¯è¿½è¸ªå’Œè§‚æµ‹ LLM å·¥ä½œæµå¯¹ç”Ÿäº§ç¯å¢ƒè‡³å…³é‡è¦ã€‚OpenTelemetry æ˜¯è¡Œä¸šæ ‡å‡†ï¼Œæˆ‘ä»¬æ”¯æŒæ–° `gen_ai` å±æ€§å‘½åç©ºé—´ã€‚è¯¦æƒ…è§ `src/examples/telemetry.ts`ã€‚

```typescript
import { trace } from '@opentelemetry/api'
import {
  BasicTracerProvider,
  ConsoleSpanExporter,
  SimpleSpanProcessor,
} from '@opentelemetry/sdk-trace-base'

const provider = new BasicTracerProvider()
provider.addSpanProcessor(new SimpleSpanProcessor(new ConsoleSpanExporter()))
trace.setGlobalTracerProvider(provider)

const tracer = trace.getTracer('test')

const ai = new AxAI({
  name: 'ollama',
  config: { model: 'nous-hermes2' },
  options: { tracer },
})

const gen = new AxChainOfThought(
  ai,
  `text -> shortSummary "summarize in 5 to 10 words"`
)

const res = await gen.forward({ text })
```

```json
{
  "traceId": "ddc7405e9848c8c884e53b823e120845",
  "name": "Chat Request",
  "id": "d376daad21da7a3c",
  "kind": "SERVER",
  "timestamp": 1716622997025000,
  "duration": 14190456.542,
  "attributes": {
    "gen_ai.system": "Ollama",
    "gen_ai.request.model": "nous-hermes2",
    "gen_ai.request.max_tokens": 500,
    "gen_ai.request.temperature": 0.1,
    "gen_ai.request.top_p": 0.9,
    "gen_ai.request.frequency_penalty": 0.5,
    "gen_ai.request.llm_is_streaming": false,
    "http.request.method": "POST",
    "url.full": "http://localhost:11434/v1/chat/completions",
    "gen_ai.usage.completion_tokens": 160,
    "gen_ai.usage.prompt_tokens": 290
  }
}
```

## æç¤ºè°ƒæ•´ï¼ˆåŸºç¡€ï¼‰

ä½ å¯ä»¥ç”¨æ›´å¤§çš„æ¨¡å‹ä¼˜åŒ–æç¤ºè¯ï¼Œæé«˜æ•ˆç‡å’Œæ•ˆæœã€‚æ¯”å¦‚ç”¨ `AxBootstrapFewShot` ä¼˜åŒ–å™¨å’Œ HotPotQA æ•°æ®é›†ç¤ºä¾‹ã€‚ä¼˜åŒ–å™¨ä¼šç”Ÿæˆæ¼”ç¤º `demos`ï¼Œé…åˆæç¤ºæå‡æ•ˆç‡ã€‚

```typescript
// ä» huggingface ä¸‹è½½ HotPotQA æ•°æ®é›†
const hf = new AxHFDataLoader({
  dataset: 'hotpot_qa',
  split: 'train',
})

const examples = await hf.getData<{ question: string; answer: string }>({
  count: 100,
  fields: ['question', 'answer'],
})

const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY as string,
})

// å¾…ä¼˜åŒ–ç¨‹åº
const program = new AxChainOfThought<{ question: string }, { answer: string }>(
  ai,
  `question -> answer "in short 2 or 3 words"`
)

// é…ç½® Bootstrap Few Shot ä¼˜åŒ–å™¨
const optimize = new AxBootstrapFewShot<
  { question: string },
  { answer: string }
>({
  program,
  examples,
})

// è®¾ç½®è¯„ä¼°æŒ‡æ ‡ emï¼Œf1 ç­‰
const metricFn: AxMetricFn = ({ prediction, example }) =>
  emScore(prediction.answer as string, example.answer as string)

// æ‰§è¡Œä¼˜åŒ–ï¼Œå¹¶ä¿å­˜ç»“æœ
const result = await optimize.compile(metricFn);

// ä¿å­˜æ¼”ç¤ºåˆ°æ–‡ä»¶
// import fs from 'fs'; // å®é™…è„šæœ¬ä¸­éœ€å¼•å…¥ fs
fs.writeFileSync('bootstrap-demos.json', JSON.stringify(result.demos, null, 2));
console.log('Demos saved to bootstrap-demos.json');
```

<img width="853" alt="tune-prompt" src="https://raw.githubusercontent.com/ax-llm/ax/main/https://github.com/dosco/llm-client/assets/832235/f924baa7-8922-424c-9c2c-f8b2018d8d74">
```

## æç¤ºè°ƒæ•´ï¼ˆè¿›é˜¶ï¼ŒMipro v2ï¼‰

MiPRO v2 æ˜¯ä¸€æ¬¾é«˜çº§æç¤ºä¼˜åŒ–æ¡†æ¶ï¼ŒåŸºäºè´å¶æ–¯ä¼˜åŒ–è‡ªåŠ¨å¯»æ‰¾æœ€ä½³æŒ‡ä»¤ã€æ¼”ç¤ºå’Œç¤ºä¾‹ç»„åˆï¼Œæ— éœ€æ‰‹å·¥è°ƒä¼˜æœ€å¤§åŒ– LLM ç¨‹åºæ€§èƒ½ã€‚

### ä¸»è¦ç‰¹æ€§

- **æŒ‡ä»¤ä¼˜åŒ–**ï¼šè‡ªåŠ¨ç”Ÿæˆå¹¶æµ‹è¯•å¤šç§æŒ‡ä»¤å€™é€‰
- **Few-shot ç¤ºä¾‹ç­›é€‰**ï¼šè‡ªåŠ¨é€‰å–æœ€ä¼˜æ¼”ç¤º
- **æ™ºèƒ½è´å¶æ–¯ä¼˜åŒ–**ï¼šUCBï¼ˆä¸Šç½®ä¿¡ç•Œï¼‰ç­–ç•¥é«˜æ•ˆæ¢ç´¢
- **æå‰ç»ˆæ­¢**ï¼šæå‡åœæ»æ—¶è‡ªåŠ¨åœæ­¢ï¼ŒèŠ‚çœç®—åŠ›
- **ç¨‹åº/æ•°æ®æ„ŸçŸ¥**ï¼šç»“åˆç¨‹åºç»“æ„ä¸æ•°æ®é›†ç‰¹å¾

### å·¥ä½œåŸç†

1. ç”Ÿæˆå¤šç§æŒ‡ä»¤å€™é€‰
2. ä»æ•°æ®è‡ªä¸¾ few-shot ç¤ºä¾‹
3. ç›´æ¥ä»æ•°æ®é›†ä¸­é€‰æ‹©æ ‡æ³¨ç¤ºä¾‹
4. è´å¶æ–¯ä¼˜åŒ–æ‰¾æœ€ä½³ç»„åˆ
5. åº”ç”¨æœ€ä¼˜é…ç½®åˆ°ç¨‹åº

### åŸºæœ¬ç”¨æ³•

```typescript
import { AxAI, AxChainOfThought, AxMiPRO } from '@ax-llm/ax'

// 1. é…ç½® AI æœåŠ¡
const ai = new AxAI({
  name: 'google-gemini',
  apiKey: process.env.GOOGLE_APIKEY,
})

// 2. åˆ›å»ºç¨‹åº
const program = new AxChainOfThought(`input -> output`)

// 3. é…ç½®ä¼˜åŒ–å™¨
const optimizer = new AxMiPRO({
  ai,
  program,
  examples: trainingData, // ä½ çš„è®­ç»ƒæ ·æœ¬
  options: {
    numTrials: 20, // å°è¯•çš„é…ç½®æ•°
    auto: 'medium', // ä¼˜åŒ–çº§åˆ«
  },
})

// 4. å®šä¹‰è¯„ä¼°æŒ‡æ ‡
const metricFn = ({ prediction, example }) => {
  return prediction.output === example.output
}

// 5. æ‰§è¡Œä¼˜åŒ–
const optimizedProgram = await optimizer.compile(metricFn, {
  valset: validationData, // å¯é€‰éªŒè¯é›†
})

// 6. ä½¿ç”¨ä¼˜åŒ–åçš„ç¨‹åº
const result = await optimizedProgram.forward(ai, { input: 'test input' })
```

### é…ç½®é€‰é¡¹

MiPRO v2 æä¾›ä¸°å¯Œé…ç½®é€‰é¡¹ï¼š

| é€‰é¡¹                      | è¯´æ˜                                     | é»˜è®¤å€¼  |
| ------------------------- | ---------------------------------------- | ------- |
| `numCandidates`           | ç”ŸæˆæŒ‡ä»¤å€™é€‰æ•°                           | 5       |
| `numTrials`               | ä¼˜åŒ–å°è¯•æ¬¡æ•°                             | 30      |
| `maxBootstrappedDemos`    | è‡ªä¸¾æ¼”ç¤ºæœ€å¤§æ•°é‡                         | 3       |
| `maxLabeledDemos`         | æ ‡æ³¨ç¤ºä¾‹æœ€å¤§æ•°é‡                         | 4       |
| `minibatch`               | æ˜¯å¦ä½¿ç”¨å°æ‰¹é‡åŠ é€Ÿè¯„ä¼°                   | true    |
| `minibatchSize`           | å°æ‰¹é‡å¤§å°                               | 25      |
| `earlyStoppingTrials`     | æ— æå‡æ—¶ç»ˆæ­¢å°è¯•æ¬¡æ•°                     | 5       |
| `minImprovementThreshold` | æœ€å°å¾—åˆ†æå‡é˜ˆå€¼                         | 0.01    |
| `programAwareProposer`    | ç¨‹åºç»“æ„æ„ŸçŸ¥                              | true    |
| `dataAwareProposer`       | è€ƒè™‘æ•°æ®é›†ç‰¹å¾                            | true    |
| `verbose`                 | æ˜¾ç¤ºä¼˜åŒ–è¿‡ç¨‹è¯¦æƒ…                          | false   |
| abort-patterns.ts | å¦‚ä½•ä¸­æ­¢è¯·æ±‚ç¤ºä¾‹                              |

### ä¼˜åŒ–çº§åˆ«

ä½ å¯ç”¨ `auto` å‚æ•°å¿«é€Ÿé…ç½®ä¼˜åŒ–å¼ºåº¦ï¼š

```typescript
// è½»é‡ä¼˜åŒ–ï¼ˆå¿«ï¼Œè¦†ç›–å°‘ï¼‰
const optimizedProgram = await optimizer.compile(metricFn, { auto: 'light' })

// ä¸­ç­‰ä¼˜åŒ–ï¼ˆå¹³è¡¡ï¼‰
const optimizedProgram = await optimizer.compile(metricFn, { auto: 'medium' })

// é‡åº¦ä¼˜åŒ–ï¼ˆæ…¢ï¼Œè¦†ç›–å¤šï¼‰
const optimizedProgram = await optimizer.compile(metricFn, { auto: 'heavy' })
```

### é«˜çº§ç¤ºä¾‹ï¼šæƒ…æ„Ÿåˆ†æ

```typescript
// åˆ›å»ºæƒ…æ„Ÿåˆ†æç¨‹åº
const classifyProgram = new AxChainOfThought<
  { productReview: string },
  { label: string }
>(`productReview -> label:string "positive" or "negative"`)

// é…ç½®ä¼˜åŒ–å™¨
const optimizer = new AxMiPRO({
  ai,
  program: classifyProgram,
  examples: trainingData,
  options: {
    numCandidates: 3,
    numTrials: 10,
    maxBootstrappedDemos: 2,
    maxLabeledDemos: 3,
    earlyStoppingTrials: 3,
    programAwareProposer: true,
    dataAwareProposer: true,
    verbose: true,
  },
})

// æ‰§è¡Œä¼˜åŒ–å¹¶ä¿å­˜ç»“æœ
const optimizedProgram = await optimizer.compile(metricFn, {
  valset: validationData,
})

// ä¿å­˜é…ç½®
const programConfig = JSON.stringify(optimizedProgram, null, 2);
await fs.promises.writeFile("./optimized-config.json", programConfig);
console.log('> Done. Optimized program config saved to optimized-config.json');
```

## ä½¿ç”¨å·²è°ƒä¼˜æç¤º

åŸºç¡€ Bootstrap Few Shot ä¼˜åŒ–å™¨å’Œé«˜çº§ MiPRO v2 ä¼˜åŒ–å™¨éƒ½ä¼šç”Ÿæˆæ˜¾è‘—æå‡ç¨‹åºæ€§èƒ½çš„ **demos**ï¼ˆæ¼”ç¤ºï¼‰ã€‚è¿™äº›ç¤ºä¾‹å¯æŒ‡å¯¼ LLM å¤„ç†ç±»ä¼¼ä»»åŠ¡ã€‚

### ä»€ä¹ˆæ˜¯ Demoï¼Ÿ

Demo æ˜¯è¾“å…¥-è¾“å‡ºç¤ºä¾‹ï¼Œä¼šè‡ªåŠ¨åŒ…å«åœ¨æç¤ºè¯ä¸­ï¼Œå¼•å¯¼ LLM å­¦ä¹ ç‰¹å®šä»»åŠ¡çš„é¢„æœŸè¡Œä¸ºã€‚

### åŠ è½½ä¸ä½¿ç”¨ Demo

ä¸è®ºç”¨å“ªç§ä¼˜åŒ–å™¨ï¼ŒåŠ è½½ demo çš„æ–¹å¼ä¸€è‡´ï¼š

```typescript
import fs from 'fs'
import { AxAI, AxGen, AxChainOfThought } from '@ax-llm/ax'

// 1. é…ç½® AI æœåŠ¡
const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY,
})

// 2. åˆ›å»ºç¨‹åºï¼ˆä¸è°ƒä¼˜æ—¶ç­¾åä¸€è‡´ï¼‰
const program = new AxChainOfThought(`question -> answer "in short 2 or 3 words"`)

// 3. ä»æ–‡ä»¶åŠ è½½ demo
const demos = JSON.parse(fs.readFileSync('bootstrap-demos.json', 'utf8'))

// 4. åº”ç”¨ demo åˆ°ç¨‹åº
program.setDemos(demos)

// 5. ä½¿ç”¨å¢å¼ºåçš„ç¨‹åº
const result = await program.forward(ai, {
  question: 'What castle did David Gregory inherit?'
})

console.log(result) // ç°åœ¨è¡¨ç°æ›´ä¼˜
```

### ç®€å•ç¤ºä¾‹ï¼šæ–‡æœ¬åˆ†ç±»

ä»¥ä¸‹å®Œæ•´ç¤ºä¾‹å±•ç¤º demo å¦‚ä½•æå‡åˆ†ç±»ä»»åŠ¡ï¼š

```typescript
// åˆ›å»ºåˆ†ç±»ç¨‹åº
const classifier = new AxGen(`text -> category:class "positive, negative, neutral"`)

// åŠ è½½ Bootstrap æˆ– MiPRO è°ƒä¼˜ç”Ÿæˆçš„ demo
const savedDemos = JSON.parse(fs.readFileSync('classification-demos.json', 'utf8'))
classifier.setDemos(savedDemos)

// ç°åœ¨åˆ†ç±»å™¨å·²å­¦ä¹ ç¤ºä¾‹ï¼Œæ•ˆæœæ›´ä½³
const result = await classifier.forward(ai, {
  text: "This product exceeded my expectations!"
})

console.log(result.category) // åˆ†ç±»æ›´å‡†ç¡®
```

### Demo çš„ä¸»è¦ä¼˜åŠ¿

- **æå‡å‡†ç¡®ç‡**ï¼šä½¿ç”¨ç›¸å…³ç¤ºä¾‹ï¼Œæ•ˆæœæ˜æ˜¾æå‡
- **è¾“å‡ºä¸€è‡´æ€§**ï¼šdemo ä¿è¯è¾“å‡ºæ ¼å¼ç»Ÿä¸€
- **é™ä½å¹»è§‰**ï¼šç¤ºä¾‹å¼•å¯¼æ¨¡å‹è¡Œä¸º
- **æˆæœ¬å¯æ§**ï¼šæ— éœ€æ›´å¤§/æ›´è´µæ¨¡å‹å³å¯æå‡æ•ˆæœ

### æœ€ä½³å®è·µ

1. **ä¿å­˜ Demo**ï¼šä¸€å®šè¦ä¿å­˜ demo æ–‡ä»¶ï¼Œä¾¿äºå¤ç”¨
2. **ç­¾ååŒ¹é…**ï¼šåŠ è½½ demo æ—¶ç­¾åéœ€å®Œå…¨ä¸€è‡´
3. **ç‰ˆæœ¬ç®¡ç†**ï¼šå°† demo æ–‡ä»¶çº³å…¥ç‰ˆæœ¬ç®¡ç†ï¼Œä¿è¯å¯å¤ç°
4. **å®šæœŸæ›´æ–°**ï¼šç”¨æ–°æ•°æ®å®šæœŸè°ƒä¼˜ demo

Bootstrap Few Shot ä¸ MiPRO v2 ç”Ÿæˆçš„ demo æ ¼å¼ä¸€è‡´ï¼ŒåŠ è½½æ–¹å¼é€šç”¨ã€‚

## å†…ç½®å‡½æ•°

| åŠŸèƒ½               | åç§°                 | æè¿°                                      |
| ------------------ | -------------------- | ----------------------------------------- |
| JS è§£é‡Šå™¨          | AxJSInterpreter      | åœ¨æ²™ç®±ç¯å¢ƒæ‰§è¡Œ JS ä»£ç                     |
| Docker æ²™ç®±        | AxDockerSession      | åœ¨ Docker ç¯å¢ƒä¸­æ‰§è¡Œå‘½ä»¤                  |
| åµŒå…¥é€‚é…å™¨         | AxEmbeddingAdapter   | è·å–åµŒå…¥å¹¶ä¼ é€’ç»™ä½ çš„å‡½æ•°                  |

## æŸ¥çœ‹æ‰€æœ‰ç¤ºä¾‹

ç”¨ `tsx` å‘½ä»¤è¿è¡Œç¤ºä¾‹ï¼Œå¯ç›´æ¥è¿è¡Œ TypeScript ä»£ç ï¼Œä¹Ÿæ”¯æŒ `.env` ä¼ é€’ AI API Keyï¼Œæ— éœ€å†™åœ¨å‘½ä»¤è¡Œã€‚

```shell
OPENAI_APIKEY=api-key npm run tsx ./src/examples/marketing.ts
```

| ç¤ºä¾‹                     | æè¿°                                             |
| ------------------------ | ------------------------------------------------ |
| customer-support.ts      | ä»å®¢æˆ·æ²Ÿé€šä¸­æå–æœ‰ä»·å€¼ä¿¡æ¯                       |
| function.ts              | å•ä¸€å‡½æ•°è°ƒç”¨ç¤ºä¾‹                                 |
| food-search.ts           | å¤šæ­¥å¤šå‡½æ•°è°ƒç”¨ç¤ºä¾‹                               |
| marketing.ts             | ç”Ÿæˆç®€çŸ­é«˜æ•ˆçš„è¥é”€çŸ­ä¿¡                           |
| vectordb.ts              | åˆ†å—ã€åµŒå…¥å’Œæœç´¢æ–‡æœ¬                             |
| fibonacci.ts             | ç”¨ JS ä»£ç è§£é‡Šå™¨è®¡ç®—æ–æ³¢é‚£å¥‘æ•°                   |
| summarize.ts             | å¯¹å¤§æ®µæ–‡æœ¬ç”Ÿæˆç®€çŸ­æ‘˜è¦                           |
| chain-of-thought.ts      | ç”¨ chain-of-thought æç¤ºå›ç­”é—®é¢˜                 |
| rag.ts                   | ç”¨å¤šè·³æ£€ç´¢å›ç­”é—®é¢˜                               |
| rag-docs.ts              | PDF è½¬æ–‡æœ¬å¹¶åµŒå…¥ç”¨äº RAG æ£€ç´¢                    |
| react.ts                 | å‡½æ•°è°ƒç”¨ä¸æ¨ç†å›ç­”é—®é¢˜                           |
| agent.ts                 | Agent æ¡†æ¶ï¼ŒAgent å¯è°ƒç”¨å…¶ä»– Agentã€å·¥å…·ç­‰       |
| streaming1.ts            | æµå¼æ ¡éªŒè¾“å‡ºå­—æ®µ                                 |
| streaming2.ts            | æ¯è¾“å‡ºå­—æ®µæµå¼æ ¡éªŒ                               |
| streaming3.ts            | ç«¯åˆ°ç«¯æµå¼ç¤ºä¾‹ `streamingForward()`              |
| smart-hone.ts            | æ™ºèƒ½ä½“åœ¨æ™ºèƒ½å®¶å±…ä¸­å¯»æ‰¾ç‹—                         |
| multi-modal.ts           | è¾“å…¥å›¾ç‰‡ä¸æ–‡æœ¬è”åˆå¤„ç†                           |
| balancer.ts              | æŒ‰æˆæœ¬ç­‰å¹³è¡¡å¤š LLM                              |
| docker.ts                | ç”¨ docker æ²™ç®±æŒ‰æè¿°æŸ¥æ‰¾æ–‡ä»¶                     |
| prime.ts                 | ç”¨å­—æ®µå¤„ç†å™¨å¤„ç†æç¤ºå­—æ®µ                         |
| simple-classify.ts       | ç®€å•åˆ†ç±»å™¨åˆ†ç±»ç¤ºä¾‹                               |
| mcp-client-memory.ts     | ç”¨ MCP æœåŠ¡å™¨å®ç° Agent è®°å¿†                     |
| mcp-client-blender.ts    | ç”¨ MCP æœåŠ¡å™¨é›†æˆ Blender                        |
| mcp-client-pipedream.ts  | è¿œç¨‹ MCP é›†æˆç¤ºä¾‹                                |
| tune-bootstrap.ts        | ç”¨ bootstrap ä¼˜åŒ–å™¨æå‡æç¤ºæ•ˆç‡                  |
| tune-mipro.ts            | ç”¨ mipro v2 ä¼˜åŒ–å™¨æå‡æç¤ºæ•ˆç‡                   |
| tune-usage.ts            | ä½¿ç”¨ä¼˜åŒ–åçš„æç¤º                                 |
| telemetry.ts             | è¿½è¸ªå¹¶æ¨é€ trace åˆ° Jaeger                       |
| openai-responses.ts      | OpenAI Responses API ç¤ºä¾‹                        |
| use-examples.ts | ç”¨ 'examples' å¼•å¯¼ llm ç¤ºä¾‹                        |

## æˆ‘ä»¬çš„ç›®æ ‡

å¤§æ¨¡å‹ï¼ˆLLMï¼‰æ„ˆå‘å¼ºå¤§ï¼Œå·²èƒ½ä½œä¸ºæ•´äº§å“çš„åç«¯ã€‚ä½†å®é™…ä½¿ç”¨éœ€ç®¡ç†å¤§é‡å¤æ‚æ€§ï¼Œå¦‚æç¤ºã€æ¨¡å‹ã€æµå¼ã€å‡½æ•°è°ƒç”¨ã€é”™è¯¯ä¿®æ­£ç­‰ã€‚æˆ‘ä»¬è‡´åŠ›äºå°†æ‰€æœ‰å¤æ‚æ€§å°è£…è¿›æ˜“ç”¨ã€é«˜ç»´æŠ¤åº“ï¼Œå…¼å®¹æ‰€æœ‰ SOTA LLMï¼Œå¹¶ç”¨æœ€æ–°ç ”ç©¶æˆæœï¼ˆå¦‚ DSPyï¼‰ä¸æ–­æ‹“å±•èƒ½åŠ›ã€‚

## å¦‚ä½•ä½¿ç”¨æœ¬åº“ï¼Ÿ

### 1. é€‰æ‹©ä¸€ä¸ª AI

```ts
// é€‰æ‹© LLM
const ai = new AxOpenAI({ apiKey: process.env.OPENAI_APIKEY } as AxOpenAIArgs)
```

### 2. åˆ›å»ºé€‚ç”¨åœºæ™¯çš„æç¤ºç­¾å

```ts
// ç­¾åå®šä¹‰è¾“å…¥è¾“å‡º
const cot = new ChainOfThought(ai, `question:string -> answer:string`, { mem })
```

### 3. æ‰§è¡Œæç¤ºç¨‹åº

```ts
// ä¼ å…¥ç­¾åå®šä¹‰çš„è¾“å…¥å­—æ®µ
const res = await cot.forward({ question: 'Are we in a simulation?' })
```

### 4. æˆ–ç›´æ¥ç”¨ LLM

```ts
const res = await ai.chat([
  { role: "system", content: "Help the customer with his questions" }
  { role: "user", content: "I'm looking for a Macbook Pro M2 With 96GB RAM?" }
]);
```

## å¦‚ä½•ä½¿ç”¨å‡½æ•°è°ƒç”¨

### 1. å®šä¹‰å‡½æ•°

```ts
// å®šä¹‰ä¸€ä¸ªæˆ–å¤šä¸ªå‡½æ•°åŠå…¶å¤„ç†å™¨
const functions = [
  {
    name: 'getCurrentWeather',
    description: 'get the current weather for a location',
    parameters: {
      type: 'object',
      properties: {
        location: {
          type: 'string',
          description: 'location to get weather for',
        },
        units: {
          type: 'string',
          enum: ['imperial', 'metric'],
          default: 'imperial',
          description: 'units to use',
        },
      },
      required: ['location'],
    },
    func: async (args: Readonly<{ location: string; units: string }>) => {
      return `The weather in ${args.location} is 72 degrees`
    },
  },
]
```

### 2. å°†å‡½æ•°ä¼ ç»™æç¤º

```ts
const cot = new AxGen(ai, `question:string -> answer:string`, { functions })
```

## å¯ç”¨è°ƒè¯•æ—¥å¿—

```ts
const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY,
} as AxOpenAIArgs)
ai.setOptions({ debug: true })
```

## è”ç³»æˆ‘ä»¬

æ¬¢è¿æé—®æˆ–åŠ å…¥ Discord äº¤æµ  
[twitter/dosco](https://twitter.com/dosco)

## å¸¸è§é—®é¢˜

### 1. LLM æ‰¾ä¸åˆ°æ­£ç¡®å‡½æ•°

è¯·ä¼˜åŒ–å‡½æ•°åå’Œæè¿°ï¼Œç¡®ä¿è¯´æ˜æ¸…æ™°ï¼Œå‚æ•°æè¿°ç®€æ´è€Œç²¾ç¡®ã€‚

### 2. å¦‚ä½•æ›´æ”¹ LLM é…ç½®ï¼Ÿ

åˆ›å»º LLM å¯¹è±¡æ—¶ä¼ å…¥é…ç½®å¯¹è±¡å³å¯ã€‚

```ts
const apiKey = process.env.OPENAI_APIKEY
const conf = AxOpenAIBestConfig()
const ai = new AxOpenAI({ apiKey, conf } as AxOpenAIArgs)
```

## 3. æç¤ºå¤ªé•¿ / å¦‚ä½•è°ƒæ•´æœ€å¤§ Tokenï¼Ÿ

```ts
const conf = axOpenAIDefaultConfig() // æˆ– OpenAIBestOptions()
conf.maxTokens = 2000
```

## 4. å¦‚ä½•æ›´æ¢æ¨¡å‹ï¼ˆå¦‚ç”¨ GPT4ï¼‰ï¼Ÿ

```ts
const conf = axOpenAIDefaultConfig() // æˆ– OpenAIBestOptions()
conf.model = OpenAIModel.GPT4Turbo
```

## Monorepo å°è´´å£«

åŠ¡å¿…åªåœ¨æ ¹ç›®å½•è¿è¡Œ `npm install`ï¼Œé¿å…ç”ŸæˆåµŒå¥— `package-lock.json` å’Œæœªå»é‡çš„ `node_modules`ã€‚

å¦‚éœ€ä¸ºåŒ…æ–°å¢ä¾èµ–ï¼Œè¯·ç”¨ `npm install lodash --workspace=ax`ï¼ˆæˆ–ç›´æ¥ä¿®æ”¹å¯¹åº” `package.json` ååœ¨æ ¹ç›®å½• `npm install`ï¼‰ã€‚

---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-06-07

---