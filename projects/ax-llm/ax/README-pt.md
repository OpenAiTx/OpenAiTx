# Ax, DSPy para Typescript

Trabalhar com LLMs √© complexo ‚Äî eles nem sempre fazem o que voc√™ deseja. O DSPy facilita a constru√ß√£o de solu√ß√µes incr√≠veis com LLMs. Basta definir suas entradas e sa√≠das (assinatura) e um prompt eficiente √© gerado automaticamente e utilizado. Conecte v√°rias assinaturas para construir sistemas e fluxos de trabalho complexos usando LLMs.

E para te ajudar realmente a usar isso em produ√ß√£o, temos tudo o que voc√™ precisa, como observabilidade, streaming, suporte a outras modalidades (imagens, √°udio, etc.), corre√ß√£o de erros, chamadas de fun√ß√£o multi-etapas, MCP, RAG, etc.

[![NPM Package](https://img.shields.io/npm/v/@ax-llm/ax?style=for-the-badge&color=green)](https://www.npmjs.com/package/@ax-llm/ax)
[![Discord Chat](https://dcbadge.vercel.app/api/server/DSHg3dU7dW?style=for-the-badge)](https://discord.gg/DSHg3dU7dW)
[![Twitter](https://img.shields.io/twitter/follow/dosco?style=for-the-badge&color=red)](https://twitter.com/dosco)

<!-- header -->

## Por que usar Ax?

- Interface padr√£o para todos os principais LLMs
- Prompts compilados a partir de assinaturas simples
- Streaming nativo de ponta a ponta
- Suporte para or√ßamento de pensamento e tokens de racioc√≠nio
- Construa agentes que podem chamar outros agentes
- Suporte nativo ao MCP, Model Context Protocol
- Converta documentos de qualquer formato para texto
- RAG, chunking inteligente, embedding, consulta
- Funciona com Vercel AI SDK
- Valida√ß√£o de sa√≠da durante o streaming
- DSPy multimodal suportado
- Ajuste autom√°tico de prompts usando otimizadores
- Rastreamento / observabilidade OpenTelemetry
- C√≥digo Typescript pronto para produ√ß√£o
- Leve, sem depend√™ncias

## Pronto para Produ√ß√£o

- Sem mudan√ßas incompat√≠veis (vers√µes menores)
- Grande cobertura de testes
- Suporte nativo Open Telemetry `gen_ai`
- Amplamente usado por startups em produ√ß√£o

## O que √© uma assinatura de prompt?

<img width="860" alt="shapes at 24-03-31 00 05 55" src="https://raw.githubusercontent.com/ax-llm/ax/main/githubusercontent.com/dosco/llm-client/assets/832235/0f0306ea-1812-4a0a-9ed5-76cd908cd26b">

Prompts eficientes e type-safe s√£o gerados automaticamente a partir de uma assinatura simples. Uma assinatura de prompt √© composta por uma
`"descri√ß√£o da tarefa" campoEntrada:tipo "descri√ß√£o do campo" -> campoSa√≠da:tipo`.
A ideia por tr√°s das assinaturas de prompt √© baseada no trabalho feito no artigo
"Demonstrate-Search-Predict".

Voc√™ pode ter v√°rios campos de entrada e sa√≠da, e cada campo pode ser dos tipos `string`, `number`, `boolean`, `date`, `datetime`,
`class "class1, class2"`, `JSON`, ou um array de qualquer um destes, por exemplo, `string[]`.
Quando um tipo n√£o √© definido, o padr√£o √© `string`. O sufixo `?` torna o campo opcional (obrigat√≥rio por padr√£o) e `!` torna o campo interno, √∫til para coisas como racioc√≠nio.

## Tipos de Campo de Sa√≠da

| Tipo                     | Descri√ß√£o                             | Uso                        | Exemplo de Sa√≠da                                     |
| ------------------------ | ------------------------------------- | -------------------------- | ---------------------------------------------------- |
| `string`                 | Uma sequ√™ncia de caracteres.          | `fullName:string`          | `"exemplo"`                                          |
| `number`                 | Um valor num√©rico.                    | `price:number`             | `42`                                                 |
| `boolean`                | Um valor verdadeiro ou falso.         | `isEvent:boolean`          | `true`, `false`                                      |
| `date`                   | Um valor de data.                     | `startDate:date`           | `"2023-10-01"`                                       |
| `datetime`               | Valor de data e hora.                 | `createdAt:datetime`       | `"2023-10-01T12:00:00Z"`                             |
| `class "class1,class2"`  | Classifica√ß√£o de itens.               | `category:class`           | `["class1", "class2", "class3"]`                     |
| `string[]`               | Array de strings.                     | `tags:string[]`            | `["exemplo1", "exemplo2"]`                           |
| `number[]`               | Array de n√∫meros.                     | `scores:number[]`          | `[1, 2, 3]`                                          |
| `boolean[]`              | Array de valores booleanos.           | `permissions:boolean[]`    | `[true, false, true]`                                |
| `date[]`                 | Array de datas.                       | `holidayDates:date[]`      | `["2023-10-01", "2023-10-02"]`                       |
| `datetime[]`             | Array de datas e hor√°rios.            | `logTimestamps:datetime[]` | `["2023-10-01T12:00:00Z", "2023-10-02T12:00:00Z"]`   |
| `class[] "class1,class2"`| V√°rias classes                        | `categories:class[]`       | `["class1", "class2", "class3"]`                     |
| `code "language"`        | Bloco de c√≥digo em linguagem espec√≠fica| `code:code "python"`      | `print('Hello, world!')`                             |

## LLMs Suportados

`Google Gemini`, `OpenAI`, `Azure OpenAI`, `Anthropic`, `X Grok`, `TogetherAI`, `Cohere`, `Mistral`, `Groq`, `DeepSeek`, `Ollama`, `Reka`,
`Hugging Face`

## Instala√ß√£o

```bash
npm install @ax-llm/ax
# ou
yarn add @ax-llm/ax
```

## Exemplo: Usando chain-of-thought para resumir texto

```typescript
import { AxAI, AxChainOfThought } from '@ax-llm/ax'

const textToSummarize = `
The technological singularity‚Äîor simply the singularity[1]‚Äîis a hypothetical future point in time at which technological growth becomes uncontrollable and irreversible, resulting in unforeseeable changes to human civilization.[2][3] ...`

const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY as string,
})

const gen = new AxChainOfThought(
  `textToSummarize -> textType:class "note, email, reminder", shortSummary "summarize in 5 to 10 words"`
)

const res = await gen.forward(ai, { textToSummarize })

console.log('>', res)
```

## Exemplo: Construindo um agente

Use o prompt de agente (framework) para construir agentes que trabalham com outros agentes para completar tarefas. Agentes s√£o f√°ceis de criar com assinaturas de prompt. Experimente o exemplo de agente.

```typescript
# npm run tsx ./src/examples/agent.ts

const researcher = new AxAgent({
  name: 'researcher',
  description: 'Researcher agent',
  signature: `physicsQuestion "physics questions" -> answer "reply in bullet points"`
});

const summarizer = new AxAgent({
  name: 'summarizer',
  description: 'Summarizer agent',
  signature: `text "text so summarize" -> shortSummary "summarize in 5 to 10 words"`
});

const agent = new AxAgent({
  name: 'agent',
  description: 'A an agent to research complex topics',
  signature: `question -> answer`,
  agents: [researcher, summarizer]
});

agent.forward(ai, { questions: "How many atoms are there in the universe" })
```

## Suporte a Modelos de Pensamento

Ax oferece suporte nativo para modelos com capacidades de pensamento, permitindo controlar o or√ßamento de tokens de pensamento e acessar o racioc√≠nio do modelo. Esse recurso ajuda a entender o processo de racioc√≠nio do modelo e otimizar o uso de tokens.

```typescript
const ai = new AxAI({
  name: 'google-gemini',
  apiKey: process.env.GOOGLE_APIKEY as string,
  config: {
    model: AxAIGoogleGeminiModel.Gemini25Flash,
    thinking: { includeThoughts: true },
  },
})

// Ou controle o or√ßamento de pensamento por requisi√ß√£o
const gen = new AxChainOfThought(`question -> answer`)
const res = await gen.forward(
  ai,
  { question: 'What is quantum entanglement?' },
  { thinkingTokenBudget: 'medium' } // 'minimal', 'low', 'medium' ou 'high'
)

// Acesse pensamentos na resposta
console.log(res.thoughts) // Mostra o processo de racioc√≠nio do modelo
```

## Bancos de Dados Vetoriais Suportados

Bancos de dados vetoriais s√£o essenciais para construir fluxos de trabalho com LLM. Temos abstra√ß√µes limpas sobre bancos de dados vetoriais populares e nosso pr√≥prio banco de dados vetorial em mem√≥ria.

| Provedor    | Testado |
| ----------- | ------- |
| Em Mem√≥ria  | üü¢ 100% |
| Weaviate    | üü¢ 100% |
| Cloudflare  | üü° 50%  |
| Pinecone    | üü° 50%  |

```typescript
// Crie embeddings de texto usando um LLM
const ret = await this.ai.embed({ texts: 'hello world' })

// Crie um banco vetorial em mem√≥ria
const db = new axDB('memory')

// Insira no banco vetorial
await this.db.upsert({
  id: 'abc',
  table: 'products',
  values: ret.embeddings[0],
})

// Consulte por entradas similares usando embeddings
const matches = await this.db.query({
  table: 'products',
  values: embeddings[0],
})
```

Alternativamente, voc√™ pode usar o `AxDBManager` que gerencia chunking, embedding e consultas automaticamente, facilitando tudo.

```typescript
const manager = new AxDBManager({ ai, db })
await manager.insert(text)

const matches = await manager.query(
  'John von Neumann on human intelligence and singularity.'
)
console.log(matches)
```

## Documentos RAG

Usar documentos como PDF, DOCX, PPT, XLS, etc., com LLMs √© complicado. Facilitamos isso com o Apache Tika, um mecanismo open-source de processamento de documentos.

Inicie o Apache Tika

```shell
docker run -p 9998:9998 apache/tika
```

Converta documentos para texto e fa√ßa o embedding para recupera√ß√£o usando o `AxDBManager`, que tamb√©m suporta reranker e rewriter de consulta. Duas implementa√ß√µes padr√£o, `AxDefaultResultReranker` e `AxDefaultQueryRewriter`, est√£o dispon√≠veis.

```typescript
const tika = new AxApacheTika()
const text = await tika.convert('/path/to/document.pdf')

const manager = new AxDBManager({ ai, db })
await manager.insert(text)

const matches = await manager.query('Find some text')
console.log(matches)
```

## DSPy Multimodal

Ao usar modelos como `GPT-4o` e `Gemini` que suportam prompts multimodais, √© poss√≠vel usar campos de imagem, funcionando com todo o pipeline DSP.

```typescript
const image = fs
  .readFileSync('./src/examples/assets/kitten.jpeg')
  .toString('base64')

const gen = new AxChainOfThought(`question, animalImage:image -> answer`)

const res = await gen.forward(ai, {
  question: 'What family does this animal belong to?',
  animalImage: { mimeType: 'image/jpeg', data: image },
})
```

Ao usar modelos como `gpt-4o-audio-preview` que suportam prompts multimodais com √°udio, tamb√©m √© poss√≠vel usar campos de √°udio, funcionando em todo o pipeline DSP.

```typescript
const audio = fs
  .readFileSync('./src/examples/assets/comment.wav')
  .toString('base64')

const gen = new AxGen(`question, commentAudio:audio -> answer`)

const res = await gen.forward(ai, {
  question: 'What family does this animal belong to?',
  commentAudio: { format: 'wav', data: audio },
})
```

## API de Chat DSPy

Inspirado pelo demonstration weaving do DSPy, Ax fornece o `AxMessage` para gerenciamento de hist√≥rico de conversas. Isso permite criar chatbots e agentes conversacionais que mant√™m contexto em m√∫ltiplas intera√ß√µes, aproveitando todo o poder das assinaturas de prompt. Veja o exemplo para mais detalhes.

```shell
GOOGLE_APIKEY=api-key npm run tsx ./src/examples/chat.ts
```

```typescript
const chatBot = new AxGen<
  { message: string } | ReadonlyArray<ChatMessage>,
  { reply: string }
>(
  `message:string "A casual message from the user" -> reply:string "A friendly, casual response"`
)

await chatBot.forward(ai, [
  {
    role: 'user',
    values: { message: 'Hi! How are you doing today?' },
  },
  {
    role: 'assistant',
    values: { reply: 'I am doing great! How about you?' },
  },
  {
    role: 'user',
    values: { message: 'Thats great!' },
  },
])
```

O hist√≥rico da conversa √© automaticamente incorporado ao prompt, permitindo que o modelo mantenha o contexto e forne√ßa respostas coerentes. Isso funciona perfeitamente com todos os recursos do Ax, incluindo streaming, chamadas de fun√ß√£o e chain-of-thought.

## Streaming

### Asser√ß√µes

Suportamos parsing de campos de sa√≠da e execu√ß√£o de fun√ß√µes durante o streaming. Isso permite falhas r√°pidas e corre√ß√£o de erros sem esperar pela sa√≠da completa, economizando tokens e reduzindo lat√™ncia. Asser√ß√µes s√£o uma forma poderosa de garantir que a sa√≠da atenda seus requisitos; elas tamb√©m funcionam com streaming.

```typescript
// configurar o programa de prompt
const gen = new AxChainOfThought(
  ai,
  `startNumber:number -> next10Numbers:number[]`
)

// adicionar uma asser√ß√£o para garantir que o n√∫mero 5 n√£o est√° em um campo de sa√≠da
gen.addAssert(({ next10Numbers }: Readonly<{ next10Numbers: number[] }>) => {
  return next10Numbers ? !next10Numbers.includes(5) : undefined
}, 'Numbers 5 is not allowed')

// executar o programa com streaming habilitado
const res = await gen.forward({ startNumber: 1 }, { stream: true })

// ou execute o programa com streaming end-to-end
const generator = await gen.streamingForward(
  { startNumber: 1 },
  {
    stream: true,
  }
)
for await (const res of generator) {
}
```

O exemplo acima permite validar campos inteiros de sa√≠da √† medida que s√£o transmitidos. Essa valida√ß√£o funciona tanto com streaming quanto sem, e √© acionada quando o valor do campo inteiro est√° dispon√≠vel. Para valida√ß√£o real durante o streaming, veja o exemplo abaixo. Isso melhora muito o desempenho e economiza tokens em escala de produ√ß√£o.

```typescript
// adicionar uma asser√ß√£o para garantir que todas as linhas come√ßam com um n√∫mero e um ponto
gen.addStreamingAssert(
  'answerInPoints',
  (value: string) => {
    const re = /^\d+\./

    // divide o valor em linhas, remove espa√ßos, filtra linhas vazias e verifica se todas come√ßam com o regex
    return value
      .split('\n')
      .map((x) => x.trim())
      .filter((x) => x.length > 0)
      .every((x) => re.test(x))
  },
  'Lines must start with a number and a dot. Eg: 1. This is a line.'
)

// execute o programa com streaming habilitado
const res = await gen.forward(
  {
    question: 'Provide a list of optimizations to speedup LLM inference.',
  },
  { stream: true, debug: true }
)
```

### Processadores de Campo

Processadores de campo s√£o uma forma poderosa de processar campos em um prompt antes de envi√°-lo ao LLM.

```typescript
const gen = new AxChainOfThought(
  ai,
  `startNumber:number -> next10Numbers:number[]`
)

const streamValue = false

const processorFunction = (value) => {
  return value.map((x) => x + 1)
}

// Adicione um processador de campo ao programa
const processor = new AxFieldProcessor(
  gen,
  'next10Numbers',
  processorFunction,
  streamValue
)

const res = await gen.forward({ startNumber: 1 })
```

## Model Context Protocol (MCP)

Ax oferece integra√ß√£o perfeita com o Model Context Protocol (MCP), permitindo que seus agentes acessem ferramentas externas e recursos atrav√©s de uma interface padronizada.

### Usando AxMCPClient

O `AxMCPClient` permite conectar-se a qualquer servidor compat√≠vel com MCP e usar suas capacidades dentro dos agentes Ax:

```typescript
import { AxMCPClient, AxMCPStdioTransport } from '@ax-llm/ax'

// Inicialize um cliente MCP com um transporte
const transport = new AxMCPStdioTransport({
  command: 'npx',
  args: ['-y', '@modelcontextprotocol/server-memory'],
})

// Crie o cliente com modo debug opcional
const client = new AxMCPClient(transport, { debug: true })

// Inicialize a conex√£o
await client.init()

// Use as fun√ß√µes do cliente em um agente
const memoryAgent = new AxAgent({
  name: 'MemoryAssistant',
  description: 'An assistant with persistent memory',
  signature: 'input, userId -> response',
  functions: [client], // Passe o cliente como provedor de fun√ß√£o
})

// Ou use o cliente com AxGen
const memoryGen = new AxGen('input, userId -> response', {
  functions: [client],
})
```

### Usando AxMCPClient com um Servidor Remoto

Chamar um servidor MCP remoto com Ax √© simples. Por exemplo, veja como usar o servidor DeepWiki MCP para perguntar sobre qualquer reposit√≥rio p√∫blico do GitHub. O servidor DeepWiki MCP est√° dispon√≠vel em `https://mcp.deepwiki.com/mcp`.

```typescript
import {
  AxAgent,
  AxAI,
  AxAIOpenAIModel,
  AxMCPClient,
  AxMCPStreambleHTTPTransport,
} from '@ax-llm/ax'

// 1. Inicialize o transporte MCP para o servidor DeepWiki
const transport = new AxMCPStreambleHTTPTransport(
  'https://mcp.deepwiki.com/mcp'
)

// 2. Crie o cliente MCP
const mcpClient = new AxMCPClient(transport, { debug: false })
await mcpClient.init() // Inicialize a conex√£o

// 3. Inicialize seu modelo AI (ex: OpenAI)
// Garanta que a vari√°vel de ambiente OPENAI_APIKEY esteja definida
const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY as string,
})

// 4. Crie um AxAgent que usa o cliente MCP
const deepwikiAgent = new AxAgent<
  {
    // Defina tipos de entrada para clareza, combinando com uma fun√ß√£o poss√≠vel do DeepWiki
    questionAboutRepo: string
    githubRepositoryUrl: string
  },
  {
    answer: string
  }
>({
  name: 'DeepWikiQueryAgent',
  description: 'Agent to query public GitHub repositories via DeepWiki MCP.',
  signature: 'questionAboutRepo, githubRepositoryUrl -> answer',
  functions: [mcpClient], // Forne√ßa o cliente MCP ao agente
})

// 5. Formule uma pergunta e chame o agente
const result = await deepwikiAgent.forward(ai, {
  questionAboutRepo: 'What is the main purpose of this library?',
  githubRepositoryUrl: 'https://github.com/dosco/ax', // Exemplo: biblioteca Ax
})
console.log('DeepWiki Answer:', result.answer)
```

Este exemplo mostra como conectar-se a um servidor MCP p√∫blico e us√°-lo dentro de um agente Ax. A assinatura do agente (`questionAboutRepo, githubRepositoryUrl -> answer`) √© uma suposi√ß√£o de como interagir com o servi√ßo DeepWiki; normalmente, voc√™ descobriria as fun√ß√µes dispon√≠veis e suas assinaturas diretamente do servidor MCP (ex: via chamada `mcp.getFunctions`, se suportada, ou documenta√ß√£o).

Para um exemplo mais complexo envolvendo autentica√ß√£o e cabe√ßalhos customizados com um servidor MCP remoto, consulte o arquivo `src/examples/mcp-client-pipedream.ts` neste reposit√≥rio.

## Roteamento de IA e Balanceamento de Carga

Ax oferece duas formas poderosas de trabalhar com m√∫ltiplos servi√ßos de IA: um balanceador de carga para alta disponibilidade e um roteador para roteamento espec√≠fico por modelo.

### Balanceador de Carga

O balanceador de carga distribui automaticamente requisi√ß√µes entre v√°rios servi√ßos de IA com base em desempenho e disponibilidade. Se um servi√ßo falhar, ele faz failover automaticamente para o pr√≥ximo dispon√≠vel.

```typescript
import { AxAI, AxBalancer } from '@ax-llm/ax'

// Configure m√∫ltiplos servi√ßos de IA
const openai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY,
})

const ollama = new AxAI({
  name: 'ollama',
  config: { model: 'nous-hermes2' },
})

const gemini = new AxAI({
  name: 'google-gemini',
  apiKey: process.env.GOOGLE_APIKEY,
})

// Crie um balanceador com todos os servi√ßos
const balancer = new AxBalancer([openai, ollama, gemini])

// Use como um servi√ßo IA regular - utiliza automaticamente o melhor servi√ßo dispon√≠vel
const response = await balancer.chat({
  chatPrompt: [{ role: 'user', content: 'Hello!' }],
})

// Ou use o balanceador com AxGen
const gen = new AxGen(`question -> answer`)
const res = await gen.forward(balancer, { question: 'Hello!' })
```

### Roteador Multi-Servi√ßo

O roteador permite usar m√∫ltiplos servi√ßos de IA por uma √∫nica interface, roteando automaticamente as requisi√ß√µes para o servi√ßo correto baseado no modelo especificado.

```typescript
import { AxAI, AxAIOpenAIModel, AxMultiServiceRouter } from '@ax-llm/ax'

// Configure OpenAI com lista de modelos
const openai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY,
  models: [
    {
      key: 'basic',
      model: AxAIOpenAIModel.GPT4OMini,
      description:
        'Modelo para tarefas muito simples como responder perguntas r√°pidas e curtas',
    },
    {
      key: 'medium',
      model: AxAIOpenAIModel.GPT4O,
      description:
        'Modelo para tarefas semi-complexas como resumir textos, escrever c√≥digo e mais',
    },
  ],
})

// Configure Gemini com lista de modelos
const gemini = new AxAI({
  name: 'google-gemini',
  apiKey: process.env.GOOGLE_APIKEY,
  models: [
    {
      key: 'deep-thinker',
      model: 'gemini-2.0-flash-thinking',
      description:
        'Modelo que pode pensar profundamente sobre uma tarefa, ideal para tarefas que exigem planejamento',
    },
    {
      key: 'expert',
      model: 'gemini-2.0-pro',
      description:
        'Modelo ideal para tarefas muito complexas como escrever ensaios longos, c√≥digos complexos e mais',
    },
  ],
})

const ollama = new AxAI({
  name: 'ollama',
  config: { model: 'nous-hermes2' },
})

const secretService = {
  key: 'sensitive-secret',
  service: ollama,
  description: 'Modelo para tarefas sens√≠veis',
}

// Crie um roteador com todos os servi√ßos
const router = new AxMultiServiceRouter([openai, gemini, secretService])

// Roteie para o modelo expert do OpenAI
const openaiResponse = await router.chat({
  chatPrompt: [{ role: 'user', content: 'Hello!' }],
  model: 'expert',
})

// Ou use o roteador com AxGen
const gen = new AxGen(`question -> answer`)
const res = await gen.forward(router, { question: 'Hello!' })
```

O balanceador √© ideal para alta disponibilidade, enquanto o roteador √© perfeito quando voc√™ precisa de modelos espec√≠ficos para tarefas espec√≠ficas. Ambos podem ser usados com qualquer recurso do Ax, como streaming, chamadas de fun√ß√£o e prompting chain-of-thought.

Voc√™ tamb√©m pode usar balanceador e roteador juntos ‚Äî m√∫ltiplos balanceadores podem ser usados com o roteador ou vice-versa.

## Suporte OpenTelemetry

A habilidade de rastrear e observar seu fluxo de trabalho LLM √© fundamental para construir fluxos de produ√ß√£o. OpenTelemetry √© o padr√£o da ind√∫stria, e suportamos o novo namespace de atributos `gen_ai`. Confira `src/examples/telemetry.ts` para mais informa√ß√µes.

```typescript
import { trace } from '@opentelemetry/api'
import {
  BasicTracerProvider,
  ConsoleSpanExporter,
  SimpleSpanProcessor,
} from '@opentelemetry/sdk-trace-base'

const provider = new BasicTracerProvider()
provider.addSpanProcessor(new SimpleSpanProcessor(new ConsoleSpanExporter()))
trace.setGlobalTracerProvider(provider)

const tracer = trace.getTracer('test')

const ai = new AxAI({
  name: 'ollama',
  config: { model: 'nous-hermes2' },
  options: { tracer },
})

const gen = new AxChainOfThought(
  ai,
  `text -> shortSummary "summarize in 5 to 10 words"`
)

const res = await gen.forward({ text })
```

```json
{
  "traceId": "ddc7405e9848c8c884e53b823e120845",
  "name": "Chat Request",
  "id": "d376daad21da7a3c",
  "kind": "SERVER",
  "timestamp": 1716622997025000,
  "duration": 14190456.542,
  "attributes": {
    "gen_ai.system": "Ollama",
    "gen_ai.request.model": "nous-hermes2",
    "gen_ai.request.max_tokens": 500,
    "gen_ai.request.temperature": 0.1,
    "gen_ai.request.top_p": 0.9,
    "gen_ai.request.frequency_penalty": 0.5,
    "gen_ai.request.llm_is_streaming": false,
    "http.request.method": "POST",
    "url.full": "http://localhost:11434/v1/chat/completions",
    "gen_ai.usage.completion_tokens": 160,
    "gen_ai.usage.prompt_tokens": 290
  }
}
```

## Ajustando os Prompts (B√°sico)

Voc√™ pode ajustar seus prompts usando um modelo maior para ajud√°-los a funcionar de forma mais eficiente e gerar melhores resultados. Isso √© feito usando um otimizador como `AxBootstrapFewShot` com exemplos do popular dataset `HotPotQA`. O otimizador gera demonstra√ß√µes (`demos`) que, quando usadas com o prompt, ajudam a melhorar sua efici√™ncia.

```typescript
// Baixe o dataset HotPotQA do huggingface
const hf = new AxHFDataLoader({
  dataset: 'hotpot_qa',
  split: 'train',
})

const examples = await hf.getData<{ question: string; answer: string }>({
  count: 100,
  fields: ['question', 'answer'],
})

const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY as string,
})

// Configure o programa para ajustar
const program = new AxChainOfThought<{ question: string }, { answer: string }>(
  ai,
  `question -> answer "in short 2 or 3 words"`
)

// Configure um otimizador Bootstrap Few Shot para ajustar o programa acima
const optimize = new AxBootstrapFewShot<
  { question: string },
  { answer: string }
>({
  program,
  examples,
})

// Configure uma m√©trica de avalia√ß√£o em, f1 scores s√£o formas populares de medir performance de recupera√ß√£o.
const metricFn: AxMetricFn = ({ prediction, example }) =>
  emScore(prediction.answer as string, example.answer as string)

// Execute o otimizador e lembre-se de salvar o resultado para uso posterior
const result = await optimize.compile(metricFn);

// Salve as demos geradas em um arquivo
// import fs from 'fs'; // Certifique-se de importar fs em seu script
fs.writeFileSync('bootstrap-demos.json', JSON.stringify(result.demos, null, 2));
console.log('Demos saved to bootstrap-demos.json');
```

<img width="853" alt="tune-prompt" src="https://raw.githubusercontent.com/ax-llm/ax/main/githubusercontent.com/dosco/llm-client/assets/832235/f924baa7-8922-424c-9c2c-f8b2018d8d74">

## Ajustando os Prompts (Avan√ßado, Mipro v2)

MiPRO v2 √© um framework avan√ßado de otimiza√ß√£o de prompts que usa otimiza√ß√£o Bayesiana para encontrar automaticamente as melhores instru√ß√µes, demonstra√ß√µes e exemplos para seus programas LLM. Ao explorar sistematicamente diferentes configura√ß√µes de prompt, o MiPRO v2 ajuda a maximizar a performance do modelo sem ajustes manuais.

### Principais Recursos

- **Otimiza√ß√£o de instru√ß√µes**: Gera e testa automaticamente m√∫ltiplas instru√ß√µes
- **Sele√ß√£o de exemplos few-shot**: Encontra demonstra√ß√µes ideais no seu dataset
- **Otimiza√ß√£o Bayesiana inteligente**: Usa UCB para explorar configura√ß√µes eficientemente
- **Parada antecipada**: Interrompe a otimiza√ß√£o quando n√£o h√° melhorias
- **Ciente do programa e dos dados**: Considera estrutura do programa e caracter√≠sticas do dataset

### Como Funciona

1. Gera v√°rias instru√ß√µes candidatas
2. Bootstrapa exemplos few-shot a partir dos seus dados
3. Seleciona exemplos rotulados diretamente do seu dataset
4. Usa otimiza√ß√£o Bayesiana para encontrar a combina√ß√£o ideal
5. Aplica a melhor configura√ß√£o ao seu programa

### Uso B√°sico

```typescript
import { AxAI, AxChainOfThought, AxMiPRO } from '@ax-llm/ax'

// 1. Configure seu servi√ßo AI
const ai = new AxAI({
  name: 'google-gemini',
  apiKey: process.env.GOOGLE_APIKEY,
})

// 2. Crie seu programa
const program = new AxChainOfThought(`input -> output`)

// 3. Configure o otimizador
const optimizer = new AxMiPRO({
  ai,
  program,
  examples: trainingData, // Seus exemplos de treinamento
  options: {
    numTrials: 20, // N√∫mero de configura√ß√µes a tentar
    auto: 'medium', // N√≠vel de otimiza√ß√£o
  },
})

// 4. Defina sua m√©trica de avalia√ß√£o
const metricFn = ({ prediction, example }) => {
  return prediction.output === example.output
}

// 5. Execute a otimiza√ß√£o
const optimizedProgram = await optimizer.compile(metricFn, {
  valset: validationData, // Conjunto de valida√ß√£o opcional
})

// 6. Use o programa otimizado
const result = await optimizedProgram.forward(ai, { input: 'test input' })
```

### Op√ß√µes de Configura√ß√£o

MiPRO v2 oferece op√ß√µes de configura√ß√£o extensivas:

| Op√ß√£o                    | Descri√ß√£o                                     | Padr√£o  |
| ------------------------ | --------------------------------------------- | ------- |
| `numCandidates`          | N√∫mero de instru√ß√µes candidatas a gerar       | 5       |
| `numTrials`              | N√∫mero de tentativas de otimiza√ß√£o            | 30      |
| `maxBootstrappedDemos`   | M√°ximo de demonstra√ß√µes bootstrapped          | 3       |
| `maxLabeledDemos`        | M√°ximo de exemplos rotulados                  | 4       |
| `minibatch`              | Usa minibatches para avalia√ß√£o mais r√°pida    | true    |
| `minibatchSize`          | Tamanho dos minibatches de avalia√ß√£o          | 25      |
| `earlyStoppingTrials`    | Para se n√£o houver melhoria ap√≥s N tentativas | 5       |
| `minImprovementThreshold`| Limite m√≠nimo de melhoria                     | 0.01    |
| `programAwareProposer`   | Usa estrutura do programa para propostas      | true    |
| `dataAwareProposer`      | Considera caracter√≠sticas do dataset          | true    |
| `verbose`                | Mostra progresso detalhado da otimiza√ß√£o      | false   |
| abort-patterns.ts | Exemplo de como abortar requisi√ß√µes |

### N√≠veis de Otimiza√ß√£o

Voc√™ pode configurar rapidamente a intensidade da otimiza√ß√£o com o par√¢metro `auto`:

```typescript
// Otimiza√ß√£o leve (r√°pida, menos minuciosa)
const optimizedProgram = await optimizer.compile(metricFn, { auto: 'light' })

// Otimiza√ß√£o m√©dia (balanceada)
const optimizedProgram = await optimizer.compile(metricFn, { auto: 'medium' })

// Otimiza√ß√£o pesada (mais lenta, mais minuciosa)
const optimizedProgram = await optimizer.compile(metricFn, { auto: 'heavy' })
```

### Exemplo Avan√ßado: An√°lise de Sentimento

```typescript
// Crie um programa de an√°lise de sentimento
const classifyProgram = new AxChainOfThought<
  { productReview: string },
  { label: string }
>(`productReview -> label:string "positive" or "negative"`)

// Configure o otimizador com op√ß√µes avan√ßadas
const optimizer = new AxMiPRO({
  ai,
  program: classifyProgram,
  examples: trainingData,
  options: {
    numCandidates: 3,
    numTrials: 10,
    maxBootstrappedDemos: 2,
    maxLabeledDemos: 3,
    earlyStoppingTrials: 3,
    programAwareProposer: true,
    dataAwareProposer: true,
    verbose: true,
  },
})

// Execute a otimiza√ß√£o e salve o resultado
const optimizedProgram = await optimizer.compile(metricFn, {
  valset: validationData,
})

// Salve a configura√ß√£o para uso futuro
const programConfig = JSON.stringify(optimizedProgram, null, 2);
await fs.promises.writeFile("./optimized-config.json", programConfig);
console.log('> Done. Optimized program config saved to optimized-config.json');
```

## Usando os Prompts Ajustados

Tanto o otimizador Bootstrap Few Shot quanto o avan√ßado MiPRO v2 geram **demos** (demonstra√ß√µes) que melhoram significativamente a performance do seu programa. Essas demos s√£o exemplos que mostram ao LLM como lidar corretamente com tarefas similares.

### O que s√£o Demos?

Demos s√£o exemplos de entrada-sa√≠da que s√£o automaticamente inclu√≠dos nos seus prompts para guiar o LLM. Eles atuam como exemplos de few-shot learning, mostrando ao modelo o comportamento esperado para sua tarefa espec√≠fica.

### Carregando e Usando Demos

Seja usando Bootstrap Few Shot ou MiPRO v2, o processo de uso das demos geradas √© o mesmo:

```typescript
import fs from 'fs'
import { AxAI, AxGen, AxChainOfThought } from '@ax-llm/ax'

// 1. Configure seu servi√ßo AI
const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY,
})

// 2. Crie seu programa (mesma assinatura usada no ajuste)
const program = new AxChainOfThought(`question -> answer "in short 2 or 3 words"`)

// 3. Carregue as demos do arquivo salvo
const demos = JSON.parse(fs.readFileSync('bootstrap-demos.json', 'utf8'))

// 4. Aplique as demos ao seu programa
program.setDemos(demos)

// 5. Use seu programa aprimorado
const result = await program.forward(ai, {
  question: 'What castle did David Gregory inherit?'
})

console.log(result) // Agora com melhor desempenho usando exemplos aprendidos
```

### Exemplo Simples: Classifica√ß√£o de Texto

Veja um exemplo completo mostrando como demos melhoram uma tarefa de classifica√ß√£o:

```typescript
// Crie um programa de classifica√ß√£o
const classifier = new AxGen(`text -> category:class "positive, negative, neutral"`)

// Carregue demos geradas de Bootstrap ou MiPRO tuning
const savedDemos = JSON.parse(fs.readFileSync('classification-demos.json', 'utf8'))
classifier.setDemos(savedDemos)

// Agora o classificador aprendeu com exemplos e tem melhor desempenho
const result = await classifier.forward(ai, {
  text: "This product exceeded my expectations!"
})

console.log(result.category) // Classifica√ß√£o mais precisa
```

### Principais Benef√≠cios do Uso de Demos

- **Maior Precis√£o**: Programas performam muito melhor com exemplos relevantes
- **Sa√≠da Consistente**: Demos ajudam a manter formatos de resposta consistentes
- **Reduz Alucina√ß√µes**: Exemplos direcionam o modelo para comportamentos esperados
- **Custo-efetivo**: Melhores resultados sem precisar de modelos maiores/caros

### Boas Pr√°ticas

1. **Salve suas Demos**: Sempre salve demos geradas para reutiliza√ß√£o
2. **Combine Assinaturas**: Use exatamente a mesma assinatura ao carregar demos
3. **Controle de Vers√£o**: Mantenha arquivos de demos sob controle de vers√£o para reprodutibilidade
4. **Atualize Regularmente**: Reajuste periodicamente com novos dados para melhorar as demos

Tanto Bootstrap Few Shot quanto MiPRO v2 geram demos no mesmo formato, ent√£o voc√™ pode usar o mesmo padr√£o de carregamento independentemente do otimizador.

## Fun√ß√µes Embutidas

| Fun√ß√£o              | Nome                | Descri√ß√£o                                     |
| ------------------- | ------------------- | --------------------------------------------- |
| Interpretador JS    | AxJSInterpreter     | Executa c√≥digo JS em ambiente isolado         |
| Docker Sandbox      | AxDockerSession     | Executa comandos em ambiente Docker           |
| Adaptador de Embeddings | AxEmbeddingAdapter | Busca e passa embedding para sua fun√ß√£o    |

## Veja todos os exemplos

Use o comando `tsx` para rodar os exemplos. Ele permite rodar c√≥digo Typescript no Node. Tamb√©m suporta uso de arquivo `.env` para passar as chaves da API de IA ao inv√©s de colocar na linha de comando.

```shell
OPENAI_APIKEY=api-key npm run tsx ./src/examples/marketing.ts
```

| Exemplo                  | Descri√ß√£o                                               |
| ------------------------ | ------------------------------------------------------ |
| customer-support.ts      | Extrai detalhes de comunica√ß√µes com clientes           |
| function.ts              | Exemplo simples de chamada de fun√ß√£o                   |
| food-search.ts           | Exemplo multi-etapas e multi-fun√ß√µes                   |
| marketing.ts             | Gera mensagens SMS de marketing curtas e eficazes      |
| vectordb.ts              | Chunk, embed e busca de texto                          |
| fibonacci.ts             | Usa interpretador JS para calcular fibonacci           |
| summarize.ts             | Gera resumo curto de um texto grande                   |
| chain-of-thought.ts      | Usa prompting chain-of-thought para responder quest√µes |
| rag.ts                   | Usa multi-hop retrieval para responder quest√µes        |
| rag-docs.ts              | Converte PDF em texto e faz embedding para RAG search  |
| react.ts                 | Usa chamadas de fun√ß√£o e racioc√≠nio para responder     |
| agent.ts                 | Framework de agentes; agentes podem usar outros agentes|
| streaming1.ts            | Valida√ß√£o de campos de sa√≠da durante streaming         |
| streaming2.ts            | Valida√ß√£o por campo durante streaming                  |
| streaming3.ts            | Exemplo de streaming end-to-end `streamingForward()`   |
| smart-hone.ts            | Agente procura cachorro em casa inteligente            |
| multi-modal.ts           | Usa imagem como entrada junto com texto                |
| balancer.ts              | Balanceia entre v√°rios LLMs por custo, etc             |
| docker.ts                | Usa sandbox Docker para encontrar arquivos por descri√ß√£o|
| prime.ts                 | Usa processadores de campo em prompt                   |
| simple-classify.ts       | Usa classificador simples para classificar itens       |
| mcp-client-memory.ts     | Exemplo de MCP server para mem√≥ria com Ax              |
| mcp-client-blender.ts    | Exemplo de MCP server para Blender com Ax              |
| mcp-client-pipedream.ts  | Exemplo de integra√ß√£o com MCP remoto                   |
| tune-bootstrap.ts        | Usa otimizador bootstrap para efici√™ncia de prompt     |
| tune-mipro.ts            | Usa otimizador mipro v2 para efici√™ncia de prompt      |
| tune-usage.ts            | Usa prompts otimizados                                 |
| telemetry.ts             | Trace e envia traces para servi√ßo Jaeger               |
| openai-responses.ts      | Exemplo usando OpenAI Responses API                    |
| use-examples.ts | Exemplo de uso de 'examples' para direcionar o llm              |

## Nosso Objetivo

Grandes modelos de linguagem (LLMs) est√£o cada vez mais poderosos e j√° podem funcionar como backend de um produto inteiro. Por√©m, ainda h√° muita complexidade a ser gerenciada, como prompts corretos, modelos, streaming, chamadas de fun√ß√£o, corre√ß√£o de erros e muito mais. Nosso objetivo √© empacotar toda essa complexidade em uma biblioteca bem mantida e f√°cil de usar, que funcione com todos os LLMs de ponta. Al√©m disso, utilizamos as pesquisas mais recentes para adicionar novas capacidades como DSPy √† biblioteca.

## Como usar esta biblioteca?

### 1. Escolha uma IA para trabalhar

```ts
// Escolha um LLM
const ai = new AxOpenAI({ apiKey: process.env.OPENAI_APIKEY } as AxOpenAIArgs)
```

### 2. Crie uma assinatura de prompt baseada no seu caso de uso

```ts
// A assinatura define as entradas e sa√≠das do seu programa de prompt
const cot = new ChainOfThought(ai, `question:string -> answer:string`, { mem })
```

### 3. Execute este novo programa de prompt

```ts
// Passe os campos de entrada definidos na assinatura acima
const res = await cot.forward({ question: 'Are we in a simulation?' })
```

### 4. Ou se quiser usar o LLM diretamente

```ts
const res = await ai.chat([
  { role: "system", content: "Help the customer with his questions" }
  { role: "user", content: "I'm looking for a Macbook Pro M2 With 96GB RAM?" }
]);
```

## Como usar chamadas de fun√ß√£o

### 1. Defina as fun√ß√µes

```ts
// defina uma ou mais fun√ß√µes e um handler de fun√ß√£o
const functions = [
  {
    name: 'getCurrentWeather',
    description: 'get the current weather for a location',
    parameters: {
      type: 'object',
      properties: {
        location: {
          type: 'string',
          description: 'location to get weather for',
        },
        units: {
          type: 'string',
          enum: ['imperial', 'metric'],
          default: 'imperial',
          description: 'units to use',
        },
      },
      required: ['location'],
    },
    func: async (args: Readonly<{ location: string; units: string }>) => {
      return `The weather in ${args.location} is 72 degrees`
    },
  },
]
```

### 2. Passe as fun√ß√µes para um prompt

```ts
const cot = new AxGen(ai, `question:string -> answer:string`, { functions })
```

## Ativar logs de debug

```ts
const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY,
} as AxOpenAIArgs)
ai.setOptions({ debug: true })
```

## Entre em contato

Estamos felizes em ajudar! Entre em contato se tiver d√∫vidas ou junte-se ao Discord
[twitter/dosco](https://twitter.com/dosco)

## FAQ

### 1. O LLM n√£o encontra a fun√ß√£o correta

Melhore o nome e a descri√ß√£o da fun√ß√£o. Seja claro sobre o que a fun√ß√£o faz. Al√©m disso, garanta que os par√¢metros tenham boas descri√ß√µes. As descri√ß√µes podem ser curtas, mas precisam ser precisas.

### 2. Como altero a configura√ß√£o do LLM que estou usando?

Voc√™ pode passar um objeto de configura√ß√£o como segundo par√¢metro ao criar um novo objeto LLM.

```ts
const apiKey = process.env.OPENAI_APIKEY
const conf = AxOpenAIBestConfig()
const ai = new AxOpenAI({ apiKey, conf } as AxOpenAIArgs)
```

## 3. Meu prompt est√° muito longo / posso alterar o max tokens?

```ts
const conf = axOpenAIDefaultConfig() // ou OpenAIBestOptions()
conf.maxTokens = 2000
```

## 4. Como altero o modelo? (ex: quero usar o GPT4)

```ts
const conf = axOpenAIDefaultConfig() // ou OpenAIBestOptions()
conf.model = OpenAIModel.GPT4Turbo
```

## Dicas & truques para Monorepo

√â essencial lembrar que devemos rodar `npm install` apenas no diret√≥rio raiz. Isso previne a cria√ß√£o de arquivos `package-lock.json` aninhados e evita `node_modules` n√£o deduplicados.

Adicionar novas depend√™ncias em pacotes deve ser feito com, por exemplo,
`npm install lodash --workspace=ax` (ou apenas modificar o `package.json` apropriado e rodar `npm install` do root).

---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-06-07

---