<div align="right">
  <details>
    <summary >üåê Langue</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="#" title="Coming soon">ÁπÅÈ´î‰∏≠Êñá (bient√¥t)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="#" title="Coming soon">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä (bient√¥t)</a> |
        | <a href="#" title="Coming soon">‡πÑ‡∏ó‡∏¢ (bient√¥t)</a> |
        | <a href="#" title="Coming soon">Fran√ßais (bient√¥t)</a>
        | <a href="#" title="Coming soon">Deutsch (bient√¥t)</a>
        | <a href="#" title="Coming soon">Espa√±ol (bient√¥t)</a>
        | <a href="#" title="Coming soon">Italiano (bient√¥t)</a>
        | <a href="#" title="Coming soon">–†—É—Å—Å–∫–∏–π (bient√¥t)</a>
        | <a href="#" title="Coming soon">Portugu√™s (bient√¥t)</a>
        | <a href="#" title="Coming soon">Nederlands (bient√¥t)</a>
        | <a href="#" title="Coming soon">Polski (bient√¥t)</a>
        | <a href="#" title="Coming soon">ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (bient√¥t)</a>
        | <a href="#" title="Coming soon">ŸÅÿßÿ±ÿ≥€å (bient√¥t)</a>
        | <a href="#" title="Coming soon">T√ºrk√ße (bient√¥t)</a>
        | <a href="#" title="Coming soon">Ti·∫øng Vi·ªát (bient√¥t)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (bient√¥t)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM
Un LLM entra√Æn√© uniquement sur des donn√©es de certaines p√©riodes pour r√©duire les biais modernes.

Imaginez si un mod√®le d‚ÄôIA ne faisait pas que pr√©tendre √™tre historique, mais l‚Äô√©tait r√©ellement.

Bas√© sur [nanoGPT par Andrej Karpathy](https://github.com/karpathy/nanoGPT). Les scripts de base d‚Äôentra√Ænement et l‚Äôarchitecture du mod√®le sont son travail.

# Objectifs du projet

TimeCapsule LLM est un projet exp√©rimental qui ne sera entra√Æn√© que sur des textes √©crits durant certaines p√©riodes. L‚Äôobjectif est de simuler la vision du monde et le langage d‚Äô√©poques historiques sp√©cifiques.

# Pourquoi le fine tuning ne suffit pas

Si vous vous contentez d‚Äôaffiner un mod√®le pr√©-entra√Æn√©, votre LLM conna√Ætra toujours des concepts modernes. Bien s√ªr, atteindre z√©ro biais moderne est difficile mais je veux m‚Äôen rapprocher autant que possible. Pour n‚Äôavoir aucun biais moderne, il faut entra√Æner un mod√®le depuis z√©ro.

# R√©sultats attendus

J‚Äôesp√®re qu‚Äôune fois termin√©, ce mod√®le ne conna√Ætra pas de concepts modernes et ne pourra pas raisonner au-del√† de ce sur quoi il a √©t√© entra√Æn√©. Il ne devrait pas reconna√Ætre les concepts/vocabulaire modernes et j‚Äôesp√®re qu‚Äôil n‚Äôhallucinera pas de connaissances modernes.

# Mises √† jour de progression

## 9 juillet 2025

J‚Äôai d√©fini ma p√©riode √† 1800-1850 et la r√©gion : Londres

J‚Äôai rassembl√© une liste de textes, livres, documents

J‚Äôen ai r√©cup√©r√© 50 au format txt et je vais bient√¥t commencer l‚Äôentra√Ænement de NanoGPT

Je mettrai √† jour ceci tant que des progr√®s seront r√©alis√©s

## 13 juillet 2025

Entra√Æn√© nanoGPT avec 187 Mo de textes historiques.

## 15 juillet 2025

J‚Äôai commenc√© √† t√©l√©charger des textes pour la deuxi√®me session d‚Äôentra√Ænement. Je r√©cup√®re tout depuis Internet Archive et j‚Äôai √©largi la p√©riode √† 1800-1875. Pour obtenir une diversit√© de textes, vous pouvez utiliser les filtres de sujet et de recherche pour le lieu de publication, la p√©riode et les sujets sur Internet Archive.

![Filtres de recherche](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## 16 juillet 2025

J‚Äôai t√©l√©charg√© environ 500 fichiers txt depuis Internet Archive et apr√®s les avoir nettoy√©s (simplement en supprimant les espaces, en-t√™tes Gutenberg, etc), j‚Äôai environ 500 Mo de donn√©es. C‚Äôest un tout petit jeu de donn√©es mais la derni√®re fois j‚Äôai entra√Æn√© sur 187 Mo donc il devrait au moins y avoir une diff√©rence notable dans la sortie apr√®s avoir entra√Æn√© le second mod√®le. J‚Äôesp√®re que ce mod√®le pourra au moins produire des phrases plus coh√©rentes qui ont un certain sens. Ce n‚Äôest bien s√ªr pas garanti puisque le jeu de donn√©es reste minuscule, mais c‚Äôest plus que la derni√®re fois.

C‚Äôest faisable sur mon propre mat√©riel, c‚Äôest bien aussi car je pourrai, esp√©rons-le, voir quelques am√©liorations avant de passer √† un jeu de donn√©es plus gros qui m‚Äôobligerait √† louer un GPU. Mais ne vous inqui√©tez pas, je compte toujours louer un GPU bient√¥t, mais avant cela je veux m‚Äôassurer que mon jeu de donn√©es est aussi s√©lectionn√© et propre que possible. L‚Äôun des probl√®mes que j‚Äôai est le nettoyage, beaucoup de ces fichiers txt contiennent des caract√®res sans sens. Les scripts que j‚Äôai utilis√©s pour nettoyer fonctionnent mais ne sont pas efficaces √† 100%.

Je vais entra√Æner ce jeu de donn√©es aujourd‚Äôhui et cela devrait prendre environ 4-5 heures. Une fois termin√© et test√©, je donnerai des nouvelles. Merci encore √† tous ceux qui consultent mon projet, j‚Äôai m√™me eu des gens qui m‚Äôont envoy√© des liens vers des ressources OCR donc merci ! J‚Äôesp√®re que plus de gens essaieront et exp√©rimenteront avec leurs propres jeux de donn√©es.

### Mise √† jour entra√Ænement

J‚Äôai commenc√© l‚Äôentra√Ænement sur un corpus de 435 Mo (108 M tokens), √ßa se passe plut√¥t bien pour l‚Äôinstant. La perte d‚Äôentra√Ænement est pass√©e de 10.9 √† 4.9 lors des 2800 premi√®res it√©rations. Je pense que cela prendra environ 8 ou 9 heures pour compl√©ter. Je posterai une autre mise √† jour une fois termin√©.

## 17 juillet 2025 2:13AM

L‚Äôentra√Ænement est termin√© pour le second mod√®le, mon 4060 a mis environ 8 heures et 40 minutes (3 900 it√©rations/h) pour 33 000 it√©rations (5 √©poques). La perte d‚Äôentra√Ænement finale √©tait de 3,73. Les r√©sultats √©taient √©tonnamment bons, il g√©n√®re d√©sormais r√©ellement des phrases coh√©rentes dans le style du XIXe si√®cle.

# Comportement & Limites du mod√®le V0

Les premiers prompts montrent que le mod√®le r√©pond avec le langage et le comportement des ann√©es 1800. Par exemple, je lui ai demand√© "Who art Henry?" et il a r√©pondu "I know that man, I have did not a black, the storm." et oui cette phrase n‚Äôa pas de sens mais le LLM reconna√Æt que je demande √† propos d‚Äôune personne.

![Exemple de sortie TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

Il n‚Äôy a aucune mention de concepts modernes, les sorties contiennent principalement des mots et tournures des ann√©es 1800.

Il reste encore beaucoup √† faire, s‚Äôentra√Æner sur 187 Mo ne donnera pas un mod√®le qui produit des textes avec un raisonnement complexe.

Pour l‚Äôinstant il produit des phrases qui manquent de structure compl√®te et globalement n‚Äôont pas de sens, mais c‚Äôest normal pour cette taille d‚Äôentra√Ænement.

# Comportement et limitations du mod√®le V0.5

Ceci repr√©sente une belle am√©lioration par rapport au mod√®le pr√©c√©dent. Le style d'√©criture et le vocabulaire sont victoriens et presque chaque phrase est grammaticalement correcte avec la ponctuation appropri√©e. Encore une fois, il s'agit d'un entra√Ænement depuis z√©ro donc il reste centr√© sur les sujets des ann√©es 1800.

![Exemple de sortie TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

Il y a beaucoup d'hallucinations factuelles. Beaucoup (comme 100%) des d√©tails (dates, √©v√©nements, personnages historiques) sont invent√©s. De plus, les phrases ne sont pas vraiment li√©es entre elles, parfois peut-√™tre 2 phrases se rapportent l'une √† l'autre mais pas plus. Un autre probl√®me est qu'il arrive parfois qu'un pied de page ‚ÄúDigitized by Google‚Äù apparaisse, donc la prochaine fois que j'entra√Ænerai, je devrai vraiment m'assurer que les textes soient bien nettoy√©s. Globalement, je suis tr√®s satisfait des r√©sultats, ce n'est pas encore un LLM mais c'est clairement un g√©n√©rateur de phrases.

J'apprends beaucoup et je vais commencer √† d√©terminer ce que je dois am√©liorer dans les semaines √† venir. Je vais bient√¥t t√©l√©charger les fichiers !

# Plans √† venir

(Termin√©) Je vais commencer √† travailler sur la version 0.5, au lieu d'utiliser 50 livres pour l'entra√Ænement, j'utiliserai id√©alement 500-600. Pour l'instant, j'entra√Æne nanoGPT avec des livres de 1800-1850 et sp√©cifiquement de Londres. Il y a des d√©fis comme s'assurer que les livres trouv√©s ne soient pas des versions mises √† jour ou avec des interpr√©tations modernes mais des livres non modifi√©s publi√©s dans la p√©riode choisie.

Je veux entra√Æner un nouveau mod√®le (v1) avec un corpus beaucoup plus grand, peut-√™tre 5 √† 10 fois plus grand que celui utilis√© pour v0.5. Mon objectif est de voir si je peux obtenir l'√©mergence de capacit√©s de raisonnement √† partir du seul entra√Ænement temporel s√©lectif, ce sera une t√¢che plus difficile et je ne suis m√™me pas s√ªr que cela soit possible √† cause des limitations des donn√©es historiques. Dans les prochaines semaines, j'essaierai de rassembler suffisamment de donn√©es pour un corpus de 5 √† 10 Go. Je pense que si j'obtiens des donn√©es propres et de haute qualit√© et que je loue un GPU, il y aura des progr√®s.

# Comment utiliser ce projet

Ce projet se concentre principalement sur la curation de donn√©es historiques, leur pr√©paration pour l'entra√Ænement et la construction d'un tokenizer. Je ne couvre pas ici l'int√©gralit√© du processus d'entra√Ænement d'un LLM, pour cela r√©f√©rez-vous √† nanoGPT par Andrej Karpathy.

# √âtape 1 : Collecter et pr√©parer des textes historiques

Rassemblez des fichiers .txt de livres du domaine public, documents, etc. de la p√©riode choisie (par exemple, Londres 1800-1850)

Vous pouvez utiliser download_texts_improved.py pour t√©l√©charger des livres si besoin.

Nettoyez les fichiers texte √† l'aide d'un script ou retirez manuellement les en-t√™tes/pieds de page de Project Gutenberg, les annotations modernes ou des erreurs d'OCR.

prepare_dataset.py devrait fonctionner correctement.

# √âtape 2 : Construire un tokenizer personnalis√©

Lancez train_tokenizer.py ou train_tokenizer_hf.py sur les donn√©es nettoy√©es.
Cela vous donnera vocab.json et merges.txt

Ces fichiers d√©finissent le vocabulaire et les r√®gles de fusion pour votre mod√®le

# √âtape 3 : Entra√Æner votre mod√®le (nanoGPT)

R√©f√©rez-vous √† [nanoGPT par Andrej Karpathy](https://github.com/karpathy/nanoGPT) pour le processus d'entra√Ænement.

Vous pouvez entra√Æner un autre LLM si vous le souhaitez, mais j'ai utilis√© nanoGPT

# FAQ

## Qu'est-ce que l'entra√Ænement temporel s√©lectif ?

L'entra√Ænement temporel s√©lectif (STT) est une m√©thodologie d'apprentissage automatique o√π toutes les donn√©es d'entra√Ænement sont soigneusement s√©lectionn√©es pour appartenir √† une p√©riode historique sp√©cifique. C'est fait afin de mod√©liser le langage et les connaissances de cette √©poque sans influence des concepts modernes. Par exemple, le mod√®le actuel (v0.5) est entra√Æn√© uniquement avec des donn√©es de 1800-1875, il n'est pas affin√© mais entra√Æn√© depuis z√©ro, produisant ainsi des sorties refl√©tant le style linguistique et le contexte historique de cette p√©riode.

## Pourquoi ne pas simplement utiliser le fine-tuning ou LoRA ?

Pour ce projet, j'essaie de cr√©er un mod√®le de langage non biais√© par la modernit√©. Si je fais un fine-tuning sur GPT-2, il est d√©j√† pr√©-entra√Æn√© et ces informations ne dispara√Ætront pas. Si j'entra√Æne depuis z√©ro, le mod√®le de langage ne fera pas semblant d'√™tre ancien, il le sera simplement. L'objectif actuel de ce projet est de cr√©er quelque chose capable de raisonner uniquement avec les connaissances provenant de livres londoniens publi√©s entre 1800 et 1850.

## Quel type de donn√©es avez-vous utilis√© pour l'entra√Ænement ?

J'utilise des livres, documents juridiques, journaux et autres √©crits de Londres 1800‚Äì1850. La liste que j'ai partag√©e en contient environ 200 mais pour le premier entra√Ænement j'ai utilis√© seulement 50 fichiers d'environ 187 Mo. Vous pouvez voir la liste des documents :
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## Quelle est la taille du mod√®le Version 0 ?

Ce mod√®le est tr√®s petit pour l'instant, je fais √ßa pour le plaisir et je suis une r√®gle stricte de ne pas utiliser de sources modernes. Il a presque 16 millions de param√®tres mais je vais commencer √† rassembler plus de textes anciens pour commencer un nouvel entra√Ænement. Je donnerai des nouvelles au fur et √† mesure.

## Sp√©cifications d'entra√Ænement ?

GPU : Geforce rtx 4060
CPU : i5-13400F
Ram : 16 Go DDR5.



---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-21

---