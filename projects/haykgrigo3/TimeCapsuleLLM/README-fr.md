
<div align="right">
  <details>
    <summary >üåê Langue</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="#" title="Coming soon">ÁπÅÈ´î‰∏≠Êñá (bient√¥t disponible)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="#" title="Coming soon">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä (bient√¥t disponible)</a> |
        | <a href="#" title="Coming soon">‡πÑ‡∏ó‡∏¢ (bient√¥t disponible)</a> |
        | <a href="#" title="Coming soon">Fran√ßais (bient√¥t disponible)</a>
        | <a href="#" title="Coming soon">Deutsch (bient√¥t disponible)</a>
        | <a href="#" title="Coming soon">Espa√±ol (bient√¥t disponible)</a>
        | <a href="#" title="Coming soon">Italiano (bient√¥t disponible)</a>
        | <a href="#" title="Coming soon">–†—É—Å—Å–∫–∏–π (bient√¥t disponible)</a>
        | <a href="#" title="Coming soon">Portugu√™s (bient√¥t disponible)</a>
        | <a href="#" title="Coming soon">Nederlands (bient√¥t disponible)</a>
        | <a href="#" title="Coming soon">Polski (bient√¥t disponible)</a>
        | <a href="#" title="Coming soon">ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (bient√¥t disponible)</a>
        | <a href="#" title="Coming soon">ŸÅÿßÿ±ÿ≥€å (bient√¥t disponible)</a>
        | <a href="#" title="Coming soon">T√ºrk√ße (bient√¥t disponible)</a>
        | <a href="#" title="Coming soon">Ti·∫øng Vi·ªát (bient√¥t disponible)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (bient√¥t disponible)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM
Un LLM entra√Æn√© uniquement sur des donn√©es de certaines p√©riodes pour r√©duire le biais moderne.

Imaginez si un mod√®le d‚ÄôIA ne faisait pas que pr√©tendre √™tre historique mais l‚Äô√©tait r√©ellement.

Construit sur [nanoGPT par Andrej Karpathy](https://github.com/karpathy/nanoGPT). Les scripts de formation principaux et l‚Äôarchitecture du mod√®le sont son ≈ìuvre.

# Objectifs du projet

TimeCapsule LLM est un projet exp√©rimental qui ne sera entra√Æn√© que sur des textes √©crits pendant certaines p√©riodes. L‚Äôobjectif est de simuler la vision du monde et le langage d‚Äô√©poques historiques sp√©cifiques.

# Pourquoi l‚Äôajustement fin ne suffit pas

Si vous ne faites qu‚Äôajuster un mod√®le pr√©-entra√Æn√©, votre LLM conna√Ætra toujours des concepts modernes. Bien s√ªr, obtenir z√©ro biais moderne est difficile mais je veux m‚Äôen approcher le plus possible. Pour √©liminer le biais moderne, il faut entra√Æner un mod√®le √† partir de z√©ro.

# R√©sultats attendus

J‚Äôesp√®re qu‚Äôune fois termin√©, ce mod√®le ne conna√Ætra pas les concepts modernes et ne pourra pas raisonner au-del√† de ce sur quoi il a √©t√© entra√Æn√©. Il ne devrait pas reconna√Ætre les concepts/vocabulaire modernes et j‚Äôesp√®re qu‚Äôil ne g√©n√®rera pas de connaissances modernes par hallucination.

# Suivi de l‚Äôavancement

## 9 juillet 2025

J‚Äôai fix√© ma p√©riode √† 1800-1850 et la r√©gion : Londres

J‚Äôai rassembl√© une liste de textes, livres, documents

J‚Äôen ai obtenu 50 sous forme de fichiers txt jusqu‚Äôici et je vais bient√¥t commencer l‚Äôentra√Ænement de NanoGPT

Je mettrai √† jour ceci tant que des progr√®s seront r√©alis√©s

## 13 juillet 2025

Entra√Æn√© nanoGPT avec 187 Mo de donn√©es textuelles historiques.

## 15 juillet 2025

J‚Äôai commenc√© √† t√©l√©charger des textes pour la deuxi√®me session d‚Äôentra√Ænement. Je r√©cup√®re tout sur Internet Archive et j‚Äôai √©largi la p√©riode √† 1800-1875. Pour obtenir une gamme vari√©e de textes, vous pouvez utiliser les filtres de sujet et de recherche pour le lieu de publication, la p√©riode et les sujets sur Internet Archive.

![Filtres de recherche](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## 16 juillet 2025

J‚Äôai t√©l√©charg√© environ 500 fichiers txt depuis Internet Archive et apr√®s les avoir nettoy√©s (juste suppression des espaces, en-t√™tes Gutenberg, etc.) j‚Äôai environ 500 Mo de donn√©es. C‚Äôest un petit jeu de donn√©es mais la derni√®re fois j‚Äôai entra√Æn√© sur 187 Mo donc il devrait y avoir au moins une diff√©rence notable dans les r√©sultats apr√®s l‚Äôentra√Ænement du deuxi√®me mod√®le. J‚Äôesp√®re que ce mod√®le pourra au moins produire des phrases plus coh√©rentes qui ont un certain sens. Ce n‚Äôest bien s√ªr pas garanti car le jeu de donn√©es reste minuscule, mais c‚Äôest mieux que la derni√®re fois.

Ceci devrait √™tre faisable avec mon propre mat√©riel, c‚Äôest une bonne chose car j‚Äôesp√®re voir quelques am√©liorations avant de passer √† un jeu de donn√©es plus important qui m‚Äôobligerait √† louer un GPU. Mais ne vous inqui√©tez pas, je pr√©vois toujours d‚Äôen louer un bient√¥t, mais avant cela je veux m‚Äôassurer que mon jeu de donn√©es est aussi bien s√©lectionn√© et propre que possible. Un des probl√®mes que j‚Äôai est le nettoyage, beaucoup de ces fichiers txt contiennent du charabia. Les scripts que j‚Äôutilise pour nettoyer fonctionnent, mais ils ne sont pas efficaces √† 100 %.

Je vais entra√Æner ce jeu de donn√©es aujourd‚Äôhui et cela devrait prendre environ 4 √† 5 heures. Une fois termin√© et test√©, je donnerai des nouvelles. Merci encore √† tous ceux qui consultent mon projet, j‚Äôai m√™me eu des personnes qui m‚Äôont envoy√© des liens vers des ressources OCR alors merci ! J‚Äôesp√®re que d‚Äôautres essaieront aussi et exp√©rimenteront avec leurs propres jeux de donn√©es.


### Mise √† jour de l‚Äôentra√Ænement

J‚Äôai commenc√© l‚Äôentra√Ænement sur un corpus de 435 Mo (108 M tokens), tout se passe bien pour l‚Äôinstant. La perte d‚Äôentra√Ænement est pass√©e de 10,9 √† 4,9 lors des 2800 premi√®res it√©rations. Je pense que cela prendra environ 8 ou 9 heures pour terminer. Je posterai une autre mise √† jour une fois termin√©.

## 17 juillet 2025

L‚Äôentra√Ænement est termin√© pour le deuxi√®me mod√®le, mon 4060 a mis environ 8h40 (3 900 iters/h) pour 33 000 it√©rations (5 √©poques). La perte d‚Äôentra√Ænement finale √©tait de 3,73. Les r√©sultats √©taient √©tonnamment bons, il g√©n√®re d√©sormais vraiment des phrases coh√©rentes dans le style du XIXe si√®cle.

## 28 juillet 2025

J‚Äôai mis en ligne la version 0.5 sur Hugging Face, [jetez-y un coup d‚Äô≈ìil](https://huggingface.co/haykgrigorian/TimeCapsuleLLM) si vous le souhaitez. Vous pouvez maintenant t√©l√©charger mon d√©p√¥t et l‚Äôex√©cuter localement. Malheureusement, nanoGPT ne fonctionne pas nativement avec HuggingFace, il faudra donc t√©l√©charger et ex√©cuter le mod√®le en local.

Je vais aussi commencer √† s√©lectionner des donn√©es pour mon prochain entra√Ænement, je pense qu‚Äôil me faudra peut-√™tre 5 √† 10 fois plus de donn√©es pour atteindre des capacit√©s de raisonnement.

## 2 ao√ªt 2025

Je vais bient√¥t commencer √† travailler sur la Version 1. Je vais devoir passer de l‚Äôarchitecture de nanoGPT √† quelque chose de plus moderne. J‚Äôai plusieurs architectures LLM open source en t√™te, dont : OpenLLaMA v3, Phi-2 et Qwen 1.5B. Et pour passer √† la V1, il me faudra soigneusement s√©lectionner un jeu de donn√©es bien plus grand et diversifi√©. J‚Äôaurai besoin d‚Äôau moins 5 Go de donn√©es d‚Äôentra√Ænement propres.

# Comportement et limitations du mod√®le V0

Les premiers prompts montrent que le mod√®le r√©pond avec le langage et le comportement des ann√©es 1800. Par exemple, je lui ai demand√© "Who art Henry ?" et il a r√©pondu "Je connais cet homme, je n'ai point fait un noir, la temp√™te." et oui, cette phrase n'a aucun sens mais le LLM reconna√Æt que je demande √† propos d'une personne.

![Exemple de sortie TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

Il n'y a aucune mention de concepts modernes, les r√©sultats contiennent principalement des mots et des tournures de phrases des ann√©es 1800.

Il reste encore beaucoup de travail √† faire, s'entra√Æner sur 187 Mo ne donnera pas un mod√®le capable de produire du texte avec un raisonnement complexe.

Actuellement, il produit des phrases qui manquent de structure compl√®te et, globalement, n'ont aucun sens, mais c'est normal avec cette taille d'entra√Ænement.

# Comportement et limitations du mod√®le V0.5

C'est une nette am√©lioration par rapport au pr√©c√©dent mod√®le. Le style d'√©criture et le vocabulaire sont victoriens et presque chaque phrase est grammaticalement correcte avec une bonne ponctuation. Et encore une fois, ce mod√®le est entra√Æn√© depuis z√©ro donc il reste centr√© sur les sujets des ann√©es 1800.

![Exemple de sortie TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

Il y a beaucoup d'hallucinations factuelles. Beaucoup (environ 100%) des d√©tails (dates, √©v√©nements, personnages historiques) sont invent√©s. De plus, les phrases n'ont pas vraiment de liens entre elles, parfois deux phrases peuvent se rapporter l'une √† l'autre mais pas au-del√†. Un autre probl√®me est qu'il arrive parfois qu'un pied de page ‚ÄúDigitized by Google‚Äù apparaisse, donc la prochaine fois que j'entra√Ænerai, il faudra vraiment que je nettoie mieux les textes. Globalement, je suis tr√®s satisfait des r√©sultats, ce n'est pas encore un LLM mais d√©finitivement un g√©n√©rateur de phrases.

J'apprends beaucoup et je vais commencer √† d√©terminer ce que je dois am√©liorer dans les prochaines semaines. Je mettrai bient√¥t les fichiers en ligne !

# Plans √† venir

(Termin√©) Je vais commencer √† travailler sur la version 0.5, au lieu de m'entra√Æner avec 50 livres, je vais utiliser id√©alement 500-600. Actuellement, j'entra√Æne nanoGPT avec des livres de 1800-1850 et sp√©cifiquement de Londres. Il y a quelques d√©fis comme s'assurer que les livres trouv√©s ne sont pas mis √† jour ou poss√®dent des interpr√©tations modernes mais qu'ils sont bien des livres intacts publi√©s durant la p√©riode choisie.

Je souhaite entra√Æner un nouveau mod√®le (v1) avec un corpus bien plus grand, peut-√™tre 5 √† 10 fois plus grand que celui utilis√© pour v0.5. Mon objectif est de voir si des capacit√©s de raisonnement peuvent √©merger du seul entra√Ænement temporel s√©lectif, ce sera une t√¢che plus difficile et je ne suis m√™me pas certain que ce soit possible en raison des limites des donn√©es historiques. Dans les prochaines semaines, j'essaierai de rassembler assez de donn√©es pour un corpus de 5 √† 10 Go. Je pense que si j'arrive √† obtenir des donn√©es propres et de haute qualit√© et √† louer un GPU, il y aura des progr√®s.

# Comment utiliser ce projet

Ce projet se concentre surtout sur la curation de donn√©es historiques, leur pr√©paration pour l'entra√Ænement et la cr√©ation d'un tokenizer. Je ne vais pas couvrir tout le processus d'entra√Ænement d'un LLM, pour cela, r√©f√©rez-vous √† nanoGPT par Andrej Karpathy.

# √âtape 1 : Rassembler et pr√©parer les textes historiques

Collectez des fichiers .txt de livres, documents, etc. du domaine public issus de la p√©riode de votre choix (par ex. Londres 1800-1850)

Vous pouvez utiliser download_texts_improved.py pour t√©l√©charger des livres si besoin.

Nettoyez les fichiers textes √† l'aide d'un script ou manuellement pour supprimer les en-t√™tes/pieds de page de Project Gutenberg, annotations modernes ou erreurs d'OCR.

prepare_dataset.py devrait fonctionner correctement.

# √âtape 2 : Construire un tokenizer personnalis√©

Lancez train_tokenizer.py ou train_tokenizer_hf.py sur les donn√©es nettoy√©es.
Cela vous donnera vocab.json et merges.txt

Ces fichiers d√©finissent le vocabulaire et les r√®gles de fusion pour votre mod√®le

# √âtape 3 : Entra√Æner votre mod√®le (nanoGPT)

R√©f√©rez-vous √† [nanoGPT par Andrej Karpathy](https://github.com/karpathy/nanoGPT) pour le processus d'entra√Ænement.

Vous pouvez entra√Æner un autre LLM si vous le souhaitez, mais j'ai utilis√© nanoGPT

# FAQ

## Qu'est-ce que l'entra√Ænement temporel s√©lectif ?

L'entra√Ænement temporel s√©lectif (Selective Temporal Training, STT) est une m√©thodologie d'apprentissage automatique o√π toutes les donn√©es d'entra√Ænement sont sp√©cifiquement s√©lectionn√©es pour appartenir √† une p√©riode historique pr√©cise. Cela permet de mod√©liser la langue et les connaissances de cette √©poque sans influence de concepts modernes. Par exemple, le mod√®le actuel (v0.5) est entra√Æn√© uniquement sur des donn√©es de 1800 √† 1875, il n'est pas affin√© mais entra√Æn√© depuis z√©ro, ce qui donne un r√©sultat refl√©tant le style linguistique et le contexte historique de cette p√©riode.

## Pourquoi ne pas simplement utiliser du fine-tuning ou LoRA ?

Pour ce projet, j'essaie de cr√©er un mod√®le de langage non biais√© par la modernit√©. Si je fais du fine-tuning sur un mod√®le comme GPT-2, il est d√©j√† pr√©-entra√Æn√© et cette information ne dispara√Ætra pas. Si j'entra√Æne depuis z√©ro, le mod√®le de langage ne fera pas semblant d'√™tre ancien, il le sera. L'objectif de ce projet est actuellement de cr√©er un mod√®le capable de raisonner uniquement avec les connaissances des livres londoniens publi√©s entre 1800 et 1850.

## Quel type de donn√©es avez-vous utilis√© pour l'entra√Ænement ?

J'utilise des livres, des documents juridiques, des journaux et d'autres √©crits de Londres entre 1800 et 1850. La liste que j'ai partag√©e en contient environ 200 mais pour le premier entra√Ænement, je n'ai utilis√© que 50 fichiers soit environ 187 Mo. Vous pouvez consulter la liste des documents :
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## Quelle est la taille du mod√®le Version 0 ?

Ce mod√®le est encore tr√®s petit, je fais √ßa pour m'amuser et je suis une r√®gle stricte d'entra√Ænement sans sources modernes. Il compte presque 16 millions de param√®tres mais je vais commencer √† rassembler plus de textes anciens pour d√©buter une nouvelle phase d'entra√Ænement. Je donnerai des nouvelles au fur et √† mesure.

## Sp√©cifications d'entra√Ænement ?

GPU : Geforce rtx 4060
CPU : i5-13400F
Ram : 16GB DDR5.



---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-08-07

---