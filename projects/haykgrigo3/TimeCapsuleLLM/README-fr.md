<div align="right">
  <details>
    <summary >ğŸŒ Langue</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ç®€ä½“ä¸­æ–‡</a>
        | <a href="#" title="Coming soon">ç¹é«”ä¸­æ–‡ (bientÃ´t disponible)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">æ—¥æœ¬èª</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">í•œêµ­ì–´</a>
        | <a href="#" title="Coming soon">à¤¹à¤¿à¤¨à¥à¤¦à¥€ (bientÃ´t disponible)</a> |
        | <a href="#" title="Coming soon">à¹„à¸—à¸¢ (bientÃ´t disponible)</a> |
        | <a href="#" title="Coming soon">FranÃ§ais (bientÃ´t disponible)</a>
        | <a href="#" title="Coming soon">Deutsch (bientÃ´t disponible)</a>
        | <a href="#" title="Coming soon">EspaÃ±ol (bientÃ´t disponible)</a>
        | <a href="#" title="Coming soon">Italiano (bientÃ´t disponible)</a>
        | <a href="#" title="Coming soon">Ğ ÑƒÑÑĞºĞ¸Ğ¹ (bientÃ´t disponible)</a>
        | <a href="#" title="Coming soon">PortuguÃªs (bientÃ´t disponible)</a>
        | <a href="#" title="Coming soon">Nederlands (bientÃ´t disponible)</a>
        | <a href="#" title="Coming soon">Polski (bientÃ´t disponible)</a>
        | <a href="#" title="Coming soon">Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© (bientÃ´t disponible)</a>
        | <a href="#" title="Coming soon">ÙØ§Ø±Ø³ÛŒ (bientÃ´t disponible)</a>
        | <a href="#" title="Coming soon">TÃ¼rkÃ§e (bientÃ´t disponible)</a>
        | <a href="#" title="Coming soon">Tiáº¿ng Viá»‡t (bientÃ´t disponible)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (bientÃ´t disponible)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM
Un LLM entraÃ®nÃ© uniquement sur des donnÃ©es de certaines pÃ©riodes pour rÃ©duire le biais moderne.

Imaginez si un modÃ¨le dâ€™IA ne se contentait pas de faire semblant dâ€™Ãªtre historique, mais lâ€™Ã©tait rÃ©ellement.

BasÃ© sur [nanoGPT par Andrej Karpathy](https://github.com/karpathy/nanoGPT) Les scripts dâ€™entraÃ®nement principaux et lâ€™architecture du modÃ¨le sont son travail.

# Objectifs du projet

TimeCapsule LLM est un projet expÃ©rimental qui ne sera entraÃ®nÃ© que sur des textes Ã©crits durant certaines pÃ©riodes historiques. L'objectif est de simuler la vision du monde et le langage d'Ã©poques spÃ©cifiques.

# Pourquoi lâ€™affinage ne suffit pas

Si vous vous contentez dâ€™affiner un modÃ¨le prÃ©-entraÃ®nÃ©, votre LLM connaÃ®tra toujours des concepts modernes. Bien sÃ»r, atteindre zÃ©ro biais moderne est difficile mais je veux mâ€™en rapprocher autant que possible. Pour Ã©liminer tout biais moderne, il faut entraÃ®ner un modÃ¨le depuis zÃ©ro.

# RÃ©sultats attendus

Une fois terminÃ©, ce modÃ¨le ne devrait pas connaÃ®tre de concepts modernes et ne pourra pas raisonner au-delÃ  de ce sur quoi il a Ã©tÃ© entraÃ®nÃ©. Il ne devrait pas reconnaÃ®tre de vocabulaire/concepts modernes et ne devrait pas halluciner de connaissances contemporaines.

# Mises Ã  jour de lâ€™avancement

## 9 juillet 2025

Jâ€™ai dÃ©fini ma pÃ©riode : 1800-1850 et la rÃ©gion : Londres

Jâ€™ai rassemblÃ© une liste de textes, livres, documents

Pour lâ€™instant jâ€™en ai rÃ©cupÃ©rÃ© 50 au format txt et je vais bientÃ´t commencer lâ€™entraÃ®nement avec NanoGPT

Je mettrai Ã  jour ce fil tant que des progrÃ¨s seront faits

## 13 juillet 2025

EntraÃ®nÃ© nanoGPT avec 187 Mo de donnÃ©es textuelles historiques.

## 15 juillet 2025

Jâ€™ai commencÃ© Ã  tÃ©lÃ©charger des textes pour le deuxiÃ¨me entraÃ®nement. Je rÃ©cupÃ¨re tout sur Internet Archive et jâ€™ai Ã©tendu la pÃ©riode Ã  1800-1875. Pour obtenir une diversitÃ© de textes, vous pouvez utiliser les filtres par sujet et par lieu de publication, pÃ©riode et thÃ¨mes sur Internet Archive.

![Filtres de recherche](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## 16 juillet 2025

Jâ€™ai tÃ©lÃ©chargÃ© environ 500 fichiers txt sur Internet Archive et aprÃ¨s nettoyage (juste suppression des espaces, en-tÃªtes Gutenberg, etc.) jâ€™ai environ 500 Mo de donnÃ©es. Câ€™est un minuscule jeu de donnÃ©es mais la derniÃ¨re fois jâ€™ai entraÃ®nÃ© sur 187 Mo donc il devrait y avoir au moins une diffÃ©rence notable dans la sortie aprÃ¨s lâ€™entraÃ®nement du second modÃ¨le. Jâ€™espÃ¨re que ce modÃ¨le pourra au moins produire des phrases plus cohÃ©rentes qui ont un certain sens. Ce nâ€™est pas garanti bien sÃ»r car câ€™est toujours un trÃ¨s petit jeu de donnÃ©es, mais câ€™est plus que ce que jâ€™ai utilisÃ© prÃ©cÃ©demment.

Cela devrait Ãªtre faisable sur mon propre matÃ©riel, câ€™est bien car je pourrai peut-Ãªtre constater quelques amÃ©liorations avant de passer Ã  un jeu de donnÃ©es plus grand qui nÃ©cessiterait de louer un GPU. Mais ne vous inquiÃ©tez pas, je prÃ©vois toujours de louer un GPU bientÃ´t, mais avant cela je veux mâ€™assurer que mon jeu de donnÃ©es est aussi bien sÃ©lectionnÃ© et propre que possible. Lâ€™un des problÃ¨mes que jâ€™ai est le nettoyage, beaucoup de ces fichiers txt contiennent des caractÃ¨res absurdes mÃ©langÃ©s. Les scripts que jâ€™utilise pour le nettoyage fonctionnent mais ne sont pas 100 % efficaces.

Je vais entraÃ®ner ce jeu de donnÃ©es aujourdâ€™hui et cela devrait prendre environ 4-5 heures. Une fois terminÃ© et testÃ©, je donnerai des nouvelles. Merci encore Ã  tous ceux qui suivent mon projet, jâ€™ai mÃªme eu des personnes qui mâ€™ont donnÃ© des liens vers des ressources OCR alors merci ! Jâ€™espÃ¨re que plus de gens essaieront et expÃ©rimenteront avec leurs propres jeux de donnÃ©es.


## 12 aoÃ»t 2025

La version 1 est terminÃ©e, cette version utilise Phi 1.5 (700M paramÃ¨tres) et est entraÃ®nÃ©e sur un jeu de donnÃ©es contenant prÃ¨s de 7000 textes. La principale amÃ©lioration de ce modÃ¨le est sa capacitÃ© Ã  parfois rÃ©fÃ©rencer des noms rÃ©els et des Ã©vÃ©nements historiques. Les versions 0 et 0.5 nâ€™y parvenaient pas, lorsque je les sollicitais avec une phrase du type Â« En lâ€™an 1821 Â», la sortie nâ€™Ã©tait que des hallucinations, des choses qui ne se sont jamais produites. Un exemple avec la version 1 :

Mon prompt : Câ€™Ã©tait lâ€™an de grÃ¢ce 1834,

La sortie : Â« Câ€™Ã©tait lâ€™an de grÃ¢ce 1834 et les rues de Londres Ã©taient pleines de protestations et de pÃ©titions. La cause, comme beaucoup lâ€™ont racontÃ©, nâ€™Ã©tait pas privÃ©e, mais ayant pris le mÃªme jour sous le jour de Lord Palmerston, le public recevra un court exposÃ© des difficultÃ©s auxquelles le jour de la loi nous a conduits. Il est profondÃ©ment regrettable que les Ã©vÃ©nements prÃ©sents dans lâ€™histoire du monde soient clairs et seront donc connus. Il nâ€™est pas vrai que les tout premiers installÃ©s de lâ€™Ã‰vangile Ã  JÃ©rusalem devraient avoir un registre aussi vaste et intÃ©ressant de prospÃ©ritÃ© et de prospÃ©ritÃ© Â»

Au dÃ©but jâ€™ai pensÃ© que câ€™Ã©tait une coÃ¯ncidence mais regardez ceci : ![1834protest](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)

### Mise Ã  jour de lâ€™entraÃ®nement

Jâ€™ai commencÃ© lâ€™entraÃ®nement sur un corpus de 435 Mo (108 M de tokens), Ã§a se passe plutÃ´t bien pour lâ€™instant. La perte dâ€™entraÃ®nement est passÃ©e de 10,9 Ã  4,9 lors des 2800 premiÃ¨res itÃ©rations. Je pense que cela prendra environ 8 ou 9 heures pour terminer. Je publierai une autre mise Ã  jour une fois terminÃ©.

## 17 juillet 2025

Lâ€™entraÃ®nement du second modÃ¨le est terminÃ©, mon 4060 a mis environ 8 heures et 40 minutes (3 900 itÃ©rations/h) pour 33 000 itÃ©rations (5 Ã©poques). La perte dâ€™entraÃ®nement finale Ã©tait de 3,73. Les sorties Ã©taient Ã©tonnamment bonnes, il gÃ©nÃ¨re dÃ©sormais de vraies phrases cohÃ©rentes dans le style du XIXe siÃ¨cle.

## 28 juillet 2025

Jâ€™ai mis en ligne v0.5 sur Hugging Face, [Jetez-y un Å“il](https://huggingface.co/haykgrigorian/TimeCapsuleLLM) si vous voulez. Vous pouvez maintenant tÃ©lÃ©charger mon dÃ©pÃ´t et lâ€™exÃ©cuter localement. Malheureusement, nanoGPT ne fonctionne pas nativement avec HuggingFace, donc il faudra tÃ©lÃ©charger et exÃ©cuter le modÃ¨le en local.

Je vais aussi commencer Ã  sÃ©lectionner des donnÃ©es pour mon prochain entraÃ®nement, je pense quâ€™il me faudra 5 Ã  10 fois plus de donnÃ©es pour atteindre des capacitÃ©s de raisonnement.

## 2 aoÃ»t 2025

Je vais bientÃ´t commencer Ã  travailler sur la version 1. Je devrai passer de lâ€™architecture de nanoGPT Ã  quelque chose de plus moderne. Jâ€™ai plusieurs architectures LLM open-source en tÃªte, dont : OpenLLaMA v3, Phi-2 et Qwen 1.5B. Et pour supporter le saut vers V1, il faudra que je sÃ©lectionne soigneusement un jeu de donnÃ©es beaucoup plus grand et diversifiÃ©. Jâ€™aurai besoin dâ€™au moins 5 Go de donnÃ©es dâ€™entraÃ®nement propres.

# Comportement et limitations du modÃ¨le V0

Les premiers prompts montrent que le modÃ¨le rÃ©pond avec le langage et le comportement des annÃ©es 1800. Par exemple, je lui ai demandÃ© Â« Who art Henry? Â» et il a rÃ©pondu Â« I know that man, I have did not a black, the storm. Â» et oui cette phrase nâ€™a pas de sens mais le LLM reconnaÃ®t que je parle dâ€™une personne.

![TimeLockLLM Exemple de sortie](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

Il nâ€™y a aucune mention de concepts modernes, les sorties contiennent principalement des mots et expressions du XIXe siÃ¨cle.

Il reste encore beaucoup de travail, sâ€™entraÃ®ner sur 187 Mo ne donnera pas un modÃ¨le capable de produire des textes avec un raisonnement complexe.


Actuellement, il produit des phrases qui manquent de structure complÃ¨te et n'ont globalement aucun sens, mais cela est normal pour la taille d'entraÃ®nement.

# Comportement et limitations du modÃ¨le V0.5

C'est une belle amÃ©lioration par rapport au dernier modÃ¨le. Le style d'Ã©criture et le vocabulaire sont victoriens et presque chaque phrase est grammaticalement correcte avec une ponctuation appropriÃ©e. Et encore une fois, ce modÃ¨le est entraÃ®nÃ© Ã  partir de zÃ©ro, donc il reste sur des sujets des annÃ©es 1800.

![Exemple de sortie TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

Il y a beaucoup dâ€™hallucinations factuelles. Beaucoup (comme 100%) des dÃ©tails (dates, Ã©vÃ©nements, personnages historiques) sont inventÃ©s. De plus, les phrases nâ€™ont pas vraiment de liens entre elles, parfois peut-Ãªtre 2 phrases se rapportent entre elles mais au-delÃ  de Ã§a non. Un autre problÃ¨me est qu'il arrive quâ€™un pied de page â€œDigitized by Googleâ€ apparaisse, donc la prochaine fois que jâ€™entraÃ®ne, je devrai vraiment m'assurer que les textes soient bien nettoyÃ©s. Globalement, je suis trÃ¨s satisfait des rÃ©sultats, ce n'est pas encore un LLM mais c'est assurÃ©ment un gÃ©nÃ©rateur de phrases.

Jâ€™apprends beaucoup et je vais commencer Ã  dÃ©terminer ce que je dois amÃ©liorer dans les prochaines semaines. Je vais bientÃ´t mettre les fichiers en ligne !

# Comportement et limitations du modÃ¨le V1

Je vais bientÃ´t mettre en ligne des exemples de sorties et aussi faire des comparaisons entre les 3 modÃ¨les avec le mÃªme prompt. Je vais aussi uploader V1 sur huggingface comme je lâ€™ai fait avec ma derniÃ¨re version, vous pouvez trouver mon compte huggingface ici : https://huggingface.co/haykgrigorian/TimeCapsuleLLM

# Projets Ã  venir

(TerminÃ©) Je vais commencer Ã  travailler sur la version 0.5, au lieu d'utiliser 50 livres pour l'entraÃ®nement, je vais idÃ©alement en utiliser 500-600. Actuellement, jâ€™entraÃ®ne nanoGPT en utilisant des livres de 1800-1850 et spÃ©cifiquement de Londres. Il y a quelques dÃ©fis comme sâ€™assurer que les livres que je trouve nâ€™ont pas Ã©tÃ© mis Ã  jour ou nâ€™ont pas dâ€™interprÃ©tations modernes, mais des livres intacts publiÃ©s pendant la pÃ©riode choisie.

Je veux entraÃ®ner un nouveau modÃ¨le (v1) avec un corpus beaucoup plus large, peut-Ãªtre 5 Ã  10 fois plus grand que celui utilisÃ© pour v0.5. Mon objectif est de voir si je peux faire Ã©merger des capacitÃ©s de raisonnement uniquement grÃ¢ce Ã  lâ€™entraÃ®nement temporel sÃ©lectif, ce sera une tÃ¢che plus difficile et je ne suis mÃªme pas sÃ»r que ce soit possible Ã  cause des limitations des donnÃ©es historiques. Dans les semaines Ã  venir, je vais essayer de rÃ©unir assez de donnÃ©es pour un corpus de 5 Ã  10 Go. Je pense que si jâ€™arrive Ã  obtenir des donnÃ©es propres et de haute qualitÃ© et louer un GPU, il y aura des progrÃ¨s.

# Comment utiliser ce projet

Ce projet se concentre principalement sur la collecte de donnÃ©es historiques, leur prÃ©paration pour lâ€™entraÃ®nement et la crÃ©ation dâ€™un tokenizer. Je ne vais pas couvrir tout le processus dâ€™entraÃ®nement LLM, pour cela rÃ©fÃ©rez-vous Ã  nanoGPT par Andrej Karpathy.

# Ã‰tape 1 : Collecter et prÃ©parer des textes historiques

Rassemblez des fichiers .txt de livres du domaine public, documents, etc. de la pÃ©riode choisie (par exemple, Londres 1800-1850)

Vous pouvez utiliser download_texts_improved.py pour tÃ©lÃ©charger des livres si besoin.

Nettoyez les fichiers textes Ã  lâ€™aide dâ€™un script ou retirez manuellement les en-tÃªtes/pieds de page de Project Gutenberg, les annotations modernes ou les erreurs OCR.

prepare_dataset.py devrait fonctionner correctement.

# Ã‰tape 2 : Construire un tokenizer personnalisÃ©

Lancez train_tokenizer.py ou train_tokenizer_hf.py sur les donnÃ©es nettoyÃ©es.
Cela vous donnera vocab.json et merges.txt

Ces fichiers dÃ©finissent le vocabulaire et les rÃ¨gles de fusion pour votre modÃ¨le

# Ã‰tape 3 : EntraÃ®nez votre modÃ¨le (nanoGPT)

Reportez-vous Ã  [nanoGPT par Andrej Karpathy](https://github.com/karpathy/nanoGPT) pour le processus d'entraÃ®nement.

Vous pouvez entraÃ®ner un autre LLM si vous le souhaitez, mais j'ai utilisÃ© nanoGPT

# FAQ

## Qu'est-ce que l'entraÃ®nement temporel sÃ©lectif ?

L'entraÃ®nement temporel sÃ©lectif (STT) est une mÃ©thodologie d'apprentissage automatique oÃ¹ toutes les donnÃ©es d'entraÃ®nement sont spÃ©cialement sÃ©lectionnÃ©es pour appartenir Ã  une pÃ©riode historique spÃ©cifique. Cela permet de modÃ©liser la langue et les connaissances de cette Ã©poque sans influence de concepts modernes. Par exemple, le modÃ¨le actuel que j'ai (v0.5) est entraÃ®nÃ© exclusivement sur des donnÃ©es de 1800 Ã  1875, il n'est pas affinÃ© mais entraÃ®nÃ© depuis zÃ©ro, ce qui donne une sortie qui reflÃ¨te le style linguistique et le contexte historique de cette pÃ©riode.

## Pourquoi ne pas simplement utiliser le fine-tuning ou LoRA ?

Pour ce projet, j'essaie de crÃ©er un modÃ¨le de langue qui n'est pas biaisÃ© par la modernitÃ©. Si j'affine un modÃ¨le comme GPT-2, il est dÃ©jÃ  prÃ©-entraÃ®nÃ© et cette information ne disparaÃ®tra pas. Si j'entraÃ®ne le modÃ¨le depuis zÃ©ro, il ne fera pas semblant d'Ãªtre ancien, il le sera vraiment. L'objectif de ce projet est de crÃ©er quelque chose qui puisse raisonner exclusivement avec les connaissances provenant de livres londoniens publiÃ©s entre 1800 et 1850.

## Quel type de donnÃ©es avez-vous utilisÃ© pour l'entraÃ®nement ?

J'utilise des livres, des documents juridiques, des journaux et autres Ã©crits de Londres entre 1800 et 1850. La liste que j'ai partagÃ©e contient environ 200 documents, mais pour le premier entraÃ®nement j'ai seulement utilisÃ© 50 fichiers pour environ ~187 Mo. Vous pouvez consulter la liste des documents :
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## Quelle est la taille des modÃ¨les ?

V0 : 16M paramÃ¨tres

V0.5 : 123M paramÃ¨tres

V1 : 700M paramÃ¨tres

# SpÃ©cifications d'entraÃ®nement ?

#V0/V0.5
GPU : Geforce RTX 4060
CPU : i5-13400F
RAM : 16 Go DDR5.

#V1
GPU: A100 rented









---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-08-12

---