
<div align="right">
  <details>
    <summary >üåê Langue</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="#" title="Coming soon">ÁπÅÈ´î‰∏≠Êñá (√† venir)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="#" title="Coming soon">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä (√† venir)</a> |
        | <a href="#" title="Coming soon">‡πÑ‡∏ó‡∏¢ (√† venir)</a> |
        | <a href="#" title="Coming soon">Fran√ßais (√† venir)</a>
        | <a href="#" title="Coming soon">Deutsch (√† venir)</a>
        | <a href="#" title="Coming soon">Espa√±ol (√† venir)</a>
        | <a href="#" title="Coming soon">Italiano (√† venir)</a>
        | <a href="#" title="Coming soon">–†—É—Å—Å–∫–∏–π (√† venir)</a>
        | <a href="#" title="Coming soon">Portugu√™s (√† venir)</a>
        | <a href="#" title="Coming soon">Nederlands (√† venir)</a>
        | <a href="#" title="Coming soon">Polski (√† venir)</a>
        | <a href="#" title="Coming soon">ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (√† venir)</a>
        | <a href="#" title="Coming soon">ŸÅÿßÿ±ÿ≥€å (√† venir)</a>
        | <a href="#" title="Coming soon">T√ºrk√ße (√† venir)</a>
        | <a href="#" title="Coming soon">Ti·∫øng Vi·ªát (√† venir)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (√† venir)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM

*Un mod√®le linguistique entra√Æn√© **depuis z√©ro** exclusivement sur des donn√©es provenant de certains lieux et p√©riodes afin de r√©duire les biais modernes et d‚Äô√©muler la voix, le vocabulaire et la vision du monde de l‚Äô√©poque.*

Imaginez si un mod√®le d‚ÄôIA n‚Äôessayait pas seulement d‚Äô√™tre historique, mais l‚Äô√©tait r√©ellement.

v0 et v0.5 construits sur [nanoGPT par Andrej Karpathy](https://github.com/karpathy/nanoGPT) Les scripts d'entra√Ænement principaux et l'architecture du mod√®le sont son ≈ìuvre. 

v1 construit sur [Phi 1.5 par Microsoft](https://huggingface.co/microsoft/phi-1_5)

##  Comportement du mod√®le & Limitations

### **v0**  

Les premiers prompts montrent le mod√®le r√©pondant avec un langage et un comportement du XIXe si√®cle.  
Exemple : Prompt : "Who art Henry ?" et il a r√©pondu "Je connais cet homme, je n'ai pas fait de noir, la temp√™te."

![Exemple de sortie TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

- Aucune mention de concepts modernes  
- Vocabulaire majoritairement fid√®le √† l'√©poque  
- Phrases g√©n√©ralement incoh√©rentes (attendu pour ~187 Mo de donn√©es d'entra√Ænement)

### **v0.5** 

Am√©lioration significative par rapport √† v0.  
- Style d'√©criture victorien, ponctuation correcte, phrases majoritairement grammaticales  
- Taux √©lev√© de hallucinations factuelles  
- Bruit OCR ("Digitized by Google") encore pr√©sent dans les sorties

![Exemple de sortie TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

### **v1**

Premier mod√®le √† rappeler et relier un √©v√©nement historique r√©el √† une figure r√©elle du jeu de donn√©es.

Exemple : Prompt : "It was the year of our Lord 1834"

La sortie : "C'√©tait l'ann√©e de notre Seigneur 1834 et les rues de Londres √©taient remplies de protestations et de p√©titions. La cause, comme beaucoup l'ont racont√©, n'√©tait pas li√©e au priv√©, mais ayant pris le m√™me jour dans le jour de Lord Palmerston, le public recevra une br√®ve d√©claration des difficult√©s auxquelles le jour de la loi nous a conduits. Il est profond√©ment regrettable que les √©v√©nements actuels de l'histoire du monde soient clairs, et par cons√©quent seront connus. Il n'est pas vrai que les tout premiers hommes qui se sont install√©s dans l'√âvangile √† J√©rusalem devraient avoir un dossier aussi vaste et aussi int√©ressant de la prosp√©rit√© et de la prosp√©rit√©."

Au d√©but, j'ai suppos√© qu'une protestation avait pu avoir lieu la m√™me ann√©e par hasard, mais regardez ceci : ![1834protest](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)

### Pourquoi c'est important :

C'est le premier exemple d'un de mes mod√®les reliant une ann√©e √† la fois √† un √©v√©nement historique r√©el et √† une personne r√©elle associ√©e √† cet √©v√©nement (Lord Palmerston). Les mod√®les pr√©c√©dents (v0 et v0.5) savaient imiter les styles d'√©criture du XIXe si√®cle mais hallucinaient toujours les √©v√©nements, les personnes et les faits. Cela montre que le mod√®le commence √† se souvenir des √©l√©ments du jeu de donn√©es.

## Plans √† venir

- Il y a pr√®s de 175 000 textes publi√©s √† Londres de 1800 √† 1875 sur Internet Archive 
- Je pr√©vois d‚Äô√©tendre le corpus et de le nettoyer davantage pour am√©liorer les capacit√©s de raisonnement
- Extension √† diff√©rentes r√©gions et p√©riodes pour des mod√®les historiques plus vari√©s


## Comment utiliser

Ce projet se concentre principalement sur la curation de donn√©es historiques, leur pr√©paration pour l‚Äôentra√Ænement et la cr√©ation d‚Äôun tokenizer. Je ne vais pas couvrir tout le processus d‚Äôentra√Ænement d‚Äôun LLM ; pour cela, r√©f√©rez-vous √† nanoGPT d‚ÄôAndrej Karpathy.

### √âtape 1 : Collecter et pr√©parer des textes historiques 

- Rassemblez des fichiers .txt de livres, documents, etc. du domaine public de la p√©riode choisie (ex. : Londres 1800-1850) 
- Gardez-les dans votre fen√™tre de temps/lieu choisie  
- Nettoyez les fichiers texte √† l‚Äôaide d‚Äôun script ou retirez manuellement les en-t√™tes/pieds de page de Project Gutenberg, les annotations modernes ou les erreurs OCR.

### √âtape 2 : Construire un tokenizer personnalis√©

- Ex√©cutez train_tokenizer.py ou train_tokenizer_hf.py sur les donn√©es nettoy√©es.
- Cela vous donnera vocab.json et merges.txt
- Ces fichiers d√©finissent le vocabulaire et les r√®gles de fusion pour votre mod√®le

### √âtape 3 : Entra√Æner votre mod√®le 

- R√©f√©rez-vous √† [nanoGPT d‚ÄôAndrej Karpathy](https://github.com/karpathy/nanoGPT) pour le processus d‚Äôentra√Ænement ou aux documents de l‚Äôarchitecture que vous avez choisie.

# FAQ

## Qu‚Äôest-ce que l‚Äôentra√Ænement temporel s√©lectif ?

L‚Äôentra√Ænement temporel s√©lectif (STT) est une m√©thodologie d‚Äôapprentissage automatique o√π toutes les donn√©es d‚Äôentra√Ænement sont sp√©cifiquement s√©lectionn√©es pour appartenir √† une p√©riode historique d√©finie. Cela permet de mod√©liser la langue et les connaissances de cette √©poque sans influence de concepts modernes. Par exemple, le mod√®le actuel (v0.5) est entra√Æn√© uniquement sur des donn√©es de 1800 √† 1875 ; il n‚Äôest pas affin√© mais entra√Æn√© depuis z√©ro, ce qui donne un r√©sultat qui refl√®te le style linguistique et le contexte historique de cette p√©riode.

## Pourquoi ne pas simplement utiliser le fine-tuning ou LoRA ?

Pour ce projet, je cherche √† cr√©er un mod√®le linguistique non biais√© par la modernit√©. Si je fais du fine-tuning sur un mod√®le comme GPT-2, il est d√©j√† pr√©-entra√Æn√© et cette information ne dispara√Ætra pas. Si j‚Äôentra√Æne un mod√®le depuis z√©ro, il ne fera pas semblant d‚Äô√™tre ancien, il le sera v√©ritablement. L‚Äôobjectif ici est de cr√©er un mod√®le capable de raisonner exclusivement √† partir des connaissances des livres londoniens publi√©s entre 1800 et 1875.

## Quel type de donn√©es avez-vous utilis√© pour l‚Äôentra√Ænement ?

J‚Äôutilise des livres, des documents juridiques, des journaux et autres √©crits de Londres entre 1800 et 1875. La liste que j‚Äôai partag√©e (pour v0) en compte environ 200, mais pour le premier entra√Ænement j‚Äôai juste utilis√© 50 fichiers pour environ 187 Mo. Vous pouvez consulter la liste des documents :
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt


Tailles des ensembles de donn√©es :
v0 : ~187 Mo
v0.5 : ~435 Mo 
v1 : ~6,25 Go 

## Quelle est la taille des mod√®les ?

V0 : 16M param√®tres

V0.5 : 123M param√®tres

V1 : 700M param√®tres

# Sp√©cifications d'entra√Ænement ?

# V0/V0.5
GPU : Geforce RTX 4060
CPU : i5-13400F 
RAM : 16 Go DDR5.

# V1
GPU : A100 lou√©e














---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-08-19

---