
<div align="right">
  <details>
    <summary >üåê Langue</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="#" title="Coming soon">ÁπÅÈ´î‰∏≠Êñá (√† venir)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="#" title="Coming soon">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä (√† venir)</a> |
        | <a href="#" title="Coming soon">‡πÑ‡∏ó‡∏¢ (√† venir)</a> |
        | <a href="#" title="Coming soon">Fran√ßais (√† venir)</a>
        | <a href="#" title="Coming soon">Deutsch (√† venir)</a>
        | <a href="#" title="Coming soon">Espa√±ol (√† venir)</a>
        | <a href="#" title="Coming soon">Italiano (√† venir)</a>
        | <a href="#" title="Coming soon">–†—É—Å—Å–∫–∏–π (√† venir)</a>
        | <a href="#" title="Coming soon">Portugu√™s (√† venir)</a>
        | <a href="#" title="Coming soon">Nederlands (√† venir)</a>
        | <a href="#" title="Coming soon">Polski (√† venir)</a>
        | <a href="#" title="Coming soon">ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (√† venir)</a>
        | <a href="#" title="Coming soon">ŸÅÿßÿ±ÿ≥€å (√† venir)</a>
        | <a href="#" title="Coming soon">T√ºrk√ße (√† venir)</a>
        | <a href="#" title="Coming soon">Ti·∫øng Vi·ªát (√† venir)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (√† venir)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM

*Un mod√®le linguistique entra√Æn√© **depuis z√©ro** exclusivement sur des donn√©es provenant de certains lieux et p√©riodes afin de r√©duire les biais modernes et d‚Äô√©muler la voix, le vocabulaire et la vision du monde de l‚Äô√©poque.*

Imaginez si un mod√®le d‚ÄôIA n‚Äôessayait pas seulement d‚Äô√™tre historique, mais l‚Äô√©tait r√©ellement.

v0 et v0.5 construits sur [nanoGPT par Andrej Karpathy](https://github.com/karpathy/nanoGPT) Les scripts d'entra√Ænement principaux et l'architecture du mod√®le sont son ≈ìuvre. 

v1 construit sur [Phi 1.5 par Microsoft](https://huggingface.co/microsoft/phi-1_5)

[Lien Hugging Face](https://huggingface.co/haykgrigorian/TimeCapsuleLLM)


##  Comportement du Mod√®le & Limitations

### **v0**  

Les premiers prompts montrent que le mod√®le r√©pond avec le langage et le comportement des ann√©es 1800.  
Exemple : Prompt : "Who art Henry?" et il a r√©pondu "I know that man, I have did not a black, the storm." 

![Exemple de sortie TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

- Aucune mention de concepts modernes  
- Vocabulaire principalement fid√®le √† l'√©poque  
- Les phrases sont majoritairement incoh√©rentes (attendu pour un jeu de donn√©es d'environ 187 Mo)

### **v0.5** 

Une am√©lioration significative par rapport √† v0.  
- Style d'√©criture victorien, ponctuation correcte, phrases g√©n√©ralement grammaticales  
- Taux √©lev√© d'hallucinations factuelles persistant  
- Bruit OCR (¬´ Digitized by Google ¬ª) toujours pr√©sent dans les sorties

![Exemple de sortie TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

### **v1**

Premier mod√®le √† rappeler et relier un √©v√©nement historique r√©el √† une figure r√©elle du jeu de donn√©es.

Exemple : Prompt : "It was the year of our Lord 1834" 

La sortie : "It was the year of our Lord 1834 and the streets of London were filled with protest and petition. The cause, as many re counted, was not bound in the way of private, but having taken up the same day in the day of Lord Palmerston, the public will receive a short statement of the difficulties under which the day of law has reached us. It is a matter of deep regret, that the present events in the history of the world are clear, and consequently will be'known. It is not true that the very men who first settled in the Gospel at Jerusalem should have so extensive and so interesting a record of the prosperity and prosperity" 

Au d√©but, j'ai suppos√© qu'une protestation avait pu avoir lieu par co√Øncidence la m√™me ann√©e, mais regardez ceci : ![1834protest](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)

### Pourquoi c'est important :

C'est le premier exemple d'un de mes mod√®les reliant une ann√©e √† la fois √† un √©v√©nement historique r√©el et √† une personne r√©elle li√©e √† cet √©v√©nement (Lord Palmerston). Les versions pr√©c√©dentes (v0 et v0.5) pouvaient imiter les styles d'√©criture du XIXe si√®cle mais hallucinaient syst√©matiquement les √©v√©nements, personnes et faits. Cela montre que le mod√®le commence √† se souvenir d'√©l√©ments du jeu de donn√©es.

## Plans √† venir 

- Il y a pr√®s de 175 000 textes publi√©s √† Londres entre 1800 et 1875 sur Internet Archive 
- Je pr√©vois d‚Äô√©largir le corpus et de le nettoyer davantage pour am√©liorer les capacit√©s de raisonnement
- Extension vers diff√©rentes r√©gions et p√©riodes pour des mod√®les plus historiques


## Comment utiliser

Ce projet porte principalement sur la curation de donn√©es historiques, leur pr√©paration pour l‚Äôentra√Ænement et la construction d‚Äôun tokenizer. Je ne vais pas couvrir le processus complet d‚Äôentra√Ænement d‚Äôun LLM, pour cela r√©f√©rez-vous √† nanoGPT par Andrej Karpathy.

### √âtape 1 : Rassembler et pr√©parer les textes historiques 

- Collectez des fichiers .txt de livres, documents, etc du domaine public issus de votre p√©riode choisie (ex : Londres 1800-1850) 
- Gardez-les dans la fen√™tre de temps/lieu choisie  
- Nettoyez les fichiers texte en utilisant un script ou retirez manuellement les en-t√™tes/pieds de page de Project Gutenberg, les annotations modernes ou des erreurs OCR.

### √âtape 2 : Construire un tokenizer personnalis√©

- Ex√©cutez train_tokenizer.py ou train_tokenizer_hf.py sur les donn√©es nettoy√©es.
- Cela vous donnera vocab.json et merges.txt
- Ces fichiers d√©finissent le vocabulaire et les r√®gles de fusion pour votre mod√®le

### √âtape 3 : Entra√Æner votre mod√®le 

- R√©f√©rez-vous √† [nanoGPT par Andrej Karpathy](https://github.com/karpathy/nanoGPT) pour le processus d‚Äôentra√Ænement ou la documentation de l‚Äôarchitecture de votre choix.

# FAQ

## Qu‚Äôest-ce que l‚ÄôEntra√Ænement Temporel S√©lectif ?

L‚ÄôEntra√Ænement Temporel S√©lectif (ETS) est une m√©thodologie d‚Äôapprentissage automatique o√π toutes les donn√©es d‚Äôentra√Ænement sont soigneusement s√©lectionn√©es pour appartenir √† une p√©riode historique sp√©cifique. Cela permet de mod√©liser la langue et le savoir de cette √©poque sans influence des concepts modernes. Par exemple, le mod√®le actuel (v0.5) est entra√Æn√© exclusivement sur des donn√©es de 1800 √† 1875 ; il n‚Äôest pas simplement affin√© mais entra√Æn√© depuis z√©ro, ce qui donne une sortie refl√©tant le style linguistique et le contexte historique de cette p√©riode.

## Pourquoi ne pas utiliser simplement le fine-tuning ou LoRA ?

Pour ce projet, j‚Äôessaie de cr√©er un mod√®le de langue non biais√© par la modernit√©. Si je fais un fine-tuning sur quelque chose comme GPT-2, il est d√©j√† pr√©-entra√Æn√© et cette information ne dispara√Ætra pas. Si je l‚Äôentra√Æne depuis z√©ro, le mod√®le de langue ne fera pas semblant d‚Äô√™tre ancien, il le sera vraiment. L‚Äôobjectif actuel est de cr√©er quelque chose qui puisse raisonner exclusivement √† partir des connaissances des livres londoniens publi√©s entre 1800 et 1875.

## Quel type de donn√©es avez-vous utilis√© pour l‚Äôentra√Ænement ?

J'utilise des livres, des documents juridiques, des journaux et d'autres √©crits provenant de Londres entre 1800 et 1875. La liste que j'ai li√©e (pour v0) en contient environ 200, mais pour le premier entra√Ænement, j'ai seulement utilis√© 50 fichiers pour environ ~187 Mo. Vous pouvez consulter la liste des documents :
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

Tailles des jeux de donn√©es :
v0 : ~187Mo
v0.5 : ~435Mo
v1 : ~6,25Go

## Quelle est la taille des mod√®les ?

V0 : 16M Param√®tres

V0.5 123M Param√®tres

V1 : 700M Param√®tres

# Sp√©cifications d'entra√Ænement ?

# V0/V0.5
GPU : Geforce rtx 4060
CPU : i5-13400F
Ram : 16Go DDR5.

# V1
GPU : A100 lou√©e















---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-09-30

---