
<div align="right">
  <details>
    <summary >üåê Langue</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="#" title="Coming soon">ÁπÅÈ´î‰∏≠Êñá (bient√¥t disponible)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="#" title="Coming soon">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä (bient√¥t disponible)</a> |
        | <a href="#" title="Coming soon">‡πÑ‡∏ó‡∏¢ (bient√¥t disponible)</a> |
        | <a href="#" title="Coming soon">Fran√ßais (bient√¥t disponible)</a>
        | <a href="#" title="Coming soon">Deutsch (bient√¥t disponible)</a>
        | <a href="#" title="Coming soon">Espa√±ol (bient√¥t disponible)</a>
        | <a href="#" title="Coming soon">Italiano (bient√¥t disponible)</a>
        | <a href="#" title="Coming soon">–†—É—Å—Å–∫–∏–π (bient√¥t disponible)</a>
        | <a href="#" title="Coming soon">Portugu√™s (bient√¥t disponible)</a>
        | <a href="#" title="Coming soon">Nederlands (bient√¥t disponible)</a>
        | <a href="#" title="Coming soon">Polski (bient√¥t disponible)</a>
        | <a href="#" title="Coming soon">ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (bient√¥t disponible)</a>
        | <a href="#" title="Coming soon">ŸÅÿßÿ±ÿ≥€å (bient√¥t disponible)</a>
        | <a href="#" title="Coming soon">T√ºrk√ße (bient√¥t disponible)</a>
        | <a href="#" title="Coming soon">Ti·∫øng Vi·ªát (bient√¥t disponible)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (bient√¥t disponible)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM
Un LLM entra√Æn√© uniquement sur des donn√©es provenant de certaines p√©riodes pour r√©duire le biais moderne.

Imaginez si un mod√®le d‚ÄôIA ne faisait pas que pr√©tendre √™tre historique mais l‚Äô√©tait r√©ellement.

Bas√© sur [nanoGPT par Andrej Karpathy](https://github.com/karpathy/nanoGPT) Les scripts principaux d‚Äôentra√Ænement et l‚Äôarchitecture du mod√®le sont son ≈ìuvre. 

# Objectifs du projet

TimeCapsule LLM est un projet exp√©rimental qui ne sera entra√Æn√© que sur des textes √©crits pendant certaines p√©riodes historiques. L‚Äôobjectif est de simuler la vision du monde et la langue d‚Äô√©poques sp√©cifiques.

# Pourquoi l‚Äôaffinage ne suffit pas

Si vous ne faites qu‚Äôaffiner un mod√®le pr√©-entra√Æn√©, votre LLM conna√Ætra tout de m√™me des concepts modernes. Bien s√ªr, atteindre z√©ro biais moderne est difficile mais je veux m‚Äôen approcher le plus possible. √âliminer tout biais moderne n√©cessite d‚Äôentra√Æner un mod√®le √† partir de z√©ro.

# R√©sultats attendus

J‚Äôesp√®re qu‚Äôune fois termin√©, ce mod√®le ne conna√Ætra pas les concepts modernes et ne pourra pas raisonner au-del√† de ce sur quoi il a √©t√© entra√Æn√©. Il ne devrait pas reconna√Ætre les concepts/vocabulaire modernes et j‚Äôesp√®re qu‚Äôil n‚Äôhallucinera pas de connaissances modernes.

# Mises √† jour de progression

## 9 juillet 2025

J‚Äôai choisi ma p√©riode : 1800-1850 et la r√©gion : Londres

J‚Äôai rassembl√© une liste de textes, livres, documents

Pour l‚Äôinstant j‚Äôen ai obtenu 50 en fichiers txt et je vais bient√¥t commencer l‚Äôentra√Ænement sur NanoGPT

Je mettrai √† jour ceci tant que des progr√®s seront r√©alis√©s

## 13 juillet 2025

Entra√Ænement de nanoGPT avec 187 Mo de textes historiques.

## 15 juillet 2025

J‚Äôai commenc√© √† t√©l√©charger des textes pour la seconde session d‚Äôentra√Ænement. Je prends tout depuis Internet Archive et j‚Äôai √©largi la p√©riode √† 1800-1875. Pour obtenir une gamme diversifi√©e de textes, on peut utiliser les filtres de sujet et de recherche pour le lieu de publication, la p√©riode et les sujets sur Internet Archive.

![Filtres de recherche](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## 16 juillet 2025

J‚Äôai t√©l√©charg√© environ 500 fichiers txt depuis Internet Archive et apr√®s nettoyage (suppression des espaces, en-t√™tes Gutenberg, etc.) j‚Äôai environ 500 Mo de donn√©es. C‚Äôest un petit jeu de donn√©es mais la derni√®re fois j‚Äôai entra√Æn√© sur 187 Mo donc il devrait y avoir une diff√©rence au moins perceptible dans la sortie apr√®s l‚Äôentra√Ænement du second mod√®le. J‚Äôesp√®re que ce mod√®le pourra au moins produire des phrases plus coh√©rentes qui ont du sens. Ce n‚Äôest pas garanti bien s√ªr puisque le jeu de donn√©es reste tr√®s petit, mais c‚Äôest plus que la fois pr√©c√©dente.

Cela devrait √™tre faisable sur mon propre mat√©riel, c‚Äôest bien aussi car j‚Äôesp√®re voir une am√©lioration avant de passer √† un jeu de donn√©es plus grand qui n√©cessiterait de louer un GPU. Mais ne vous inqui√©tez pas je pr√©vois toujours d‚Äôen louer un bient√¥t, mais avant √ßa je veux m‚Äôassurer que mon jeu de donn√©es est le plus soign√© et propre possible. Un des probl√®mes que j‚Äôai c‚Äôest le nettoyage, beaucoup de ces fichiers txt contiennent du charabia. Les scripts que j‚Äôai utilis√©s pour nettoyer fonctionnent mais ne sont pas efficaces √† 100 %.

Je vais entra√Æner ce jeu de donn√©es aujourd‚Äôhui et cela devrait prendre environ 4 √† 5 heures. Une fois termin√© et test√©, je donnerai des nouvelles. Merci encore √† tous ceux qui suivent mon projet, j‚Äôai m√™me eu des gens qui m‚Äôont envoy√© des liens vers des ressources OCR alors merci ! J‚Äôesp√®re que davantage de personnes essaieront et exp√©rimenteront avec leurs propres jeux de donn√©es.

## 28 juillet 2025

J‚Äôai mis en ligne la version v0.5 sur Hugging Face, [jetez-y un ≈ìil](https://huggingface.co/haykgrigorian/TimeCapsuleLLM) si vous le souhaitez. Vous pouvez maintenant t√©l√©charger mon d√©p√¥t et l‚Äôex√©cuter localement. Malheureusement, nanoGPT ne fonctionne pas nativement avec HuggingFace, donc il faudra t√©l√©charger et ex√©cuter le mod√®le localement.

Je vais aussi commencer √† s√©lectionner des donn√©es pour la prochaine session d‚Äôentra√Ænement, je pense qu‚Äôil me faudra peut-√™tre 5 √† 10 fois plus de donn√©es pour atteindre des capacit√©s de raisonnement.

### Mise √† jour sur l‚Äôentra√Ænement

J‚Äôai commenc√© l‚Äôentra√Ænement sur un corpus de 435 Mo (108 M de tokens), tout se passe bien pour l‚Äôinstant. La perte d‚Äôentra√Ænement est pass√©e de 10,9 √† 4,9 lors des 2800 premi√®res it√©rations. Je pense qu‚Äôil faudra environ 8 ou 9 heures pour terminer. Je posterai une autre mise √† jour une fois fini.

## 17 juillet 2025 2:13AM

L‚Äôentra√Ænement du second mod√®le est termin√©, mon 4060 a mis environ 8 heures et 40 minutes (3 900 it√©rations/heure) pour 33 000 it√©rations (5 √©poques). La perte finale d‚Äôentra√Ænement √©tait de 3,73. Les r√©sultats √©taient √©tonnamment bons, il g√©n√®re vraiment maintenant des phrases coh√©rentes au style du XIXe si√®cle.

# Comportement & limitations du mod√®le V0

Les premiers prompts montrent que le mod√®le r√©pond avec le langage et le comportement des ann√©es 1800. Par exemple, je lui ai demand√© ¬´ Who art Henry? ¬ª et il a r√©pondu ¬´ I know that man, I have did not a black, the storm. ¬ª et oui cette phrase n‚Äôa pas de sens mais le LLM reconna√Æt que je demande √† propos d‚Äôune personne.

![Exemple de sortie TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

Il n'y a aucune mention de concepts modernes, les sorties contiennent principalement des mots et des formulations du XIXe si√®cle.

Il reste encore beaucoup de travail √† faire, s'entra√Æner sur 187 Mo ne vous donnera pas un mod√®le produisant un texte avec un raisonnement complexe.

Actuellement, il produit des phrases qui manquent de structure compl√®te et, dans l'ensemble, n'ont aucun sens, mais c'est normal pour cette taille de donn√©es d'entra√Ænement.

# Comportement et limitations du mod√®le V0.5

C'est une belle am√©lioration par rapport au dernier mod√®le. Le style d'√©criture et le vocabulaire sont victoriens et presque chaque phrase est grammaticalement correcte avec une ponctuation appropri√©e. Et encore une fois, ce mod√®le est entra√Æn√© √† partir de z√©ro, donc il s'en tient aux sujets des ann√©es 1800.

![Exemple de sortie TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

Il y a beaucoup d'hallucinations factuelles. Beaucoup (comme 100%) des d√©tails (dates, √©v√©nements, personnages historiques) sont invent√©s. De plus, les phrases n'ont pas vraiment de liens entre elles, parfois peut-√™tre que 2 phrases se rapportent entre elles, mais pas au-del√†. Un autre probl√®me est qu'il arrive parfois qu'un pied de page ‚ÄúDigitized by Google‚Äù apparaisse, donc la prochaine fois que j'entra√Ænerai, je devrai vraiment m'assurer que les textes sont bien nettoy√©s. Globalement, je suis tr√®s satisfait des r√©sultats, ce n'est pas encore un LLM mais clairement un g√©n√©rateur de phrases.

J'apprends beaucoup et je vais commencer √† d√©terminer ce que je dois am√©liorer dans les semaines √† venir. Je vais bient√¥t t√©l√©verser les fichiers !

# Projets √† venir

(Termin√©) Je vais commencer √† travailler sur la version 0.5, au lieu d'entra√Æner avec 50 livres, je vais entra√Æner id√©alement avec 500-600. En ce moment, j'entra√Æne nanoGPT avec des livres de 1800-1850, sp√©cifiquement de Londres. Il y a quelques d√©fis comme s'assurer que les livres trouv√©s ne sont pas mis √† jour ou avec des interpr√©tations modernes, mais des livres intacts publi√©s dans ma p√©riode choisie.

Je veux entra√Æner un nouveau mod√®le (v1) avec un corpus beaucoup plus grand, peut-√™tre 5 √† 10 fois plus grand que celui utilis√© pour v0.5. Mon objectif est de voir si je peux faire √©merger des capacit√©s de raisonnement gr√¢ce au Selective Temporal Training seul, ce sera une t√¢che plus difficile et je ne suis m√™me pas s√ªr que ce soit possible √† cause des limitations des donn√©es historiques. Dans les semaines √† venir, j'essaierai de rassembler suffisamment de donn√©es pour un corpus de 5-10 Go. Je crois que si j'obtiens des donn√©es propres et de haute qualit√© et que je loue un GPU, il y aura des progr√®s.

# Comment utiliser ce projet

Ce projet se concentre principalement sur la curation de donn√©es historiques, leur pr√©paration pour l'entra√Ænement et la cr√©ation d'un tokenizer. Je ne vais pas couvrir le processus complet d'entra√Ænement LLM, pour cela r√©f√©rez-vous √† nanoGPT par Andrej Karpathy.

# √âtape 1 : Rassembler et pr√©parer des textes historiques

Rassemblez des fichiers .txt de livres du domaine public, documents, etc. de votre p√©riode choisie (par ex. Londres 1800-1850)

Vous pouvez utiliser download_texts_improved.py pour t√©l√©charger des livres si besoin.

Nettoyez les fichiers texte √† l'aide d'un script ou retirez manuellement les en-t√™tes/pieds de page de Project Gutenberg, les annotations modernes ou les erreurs OCR.

prepare_dataset.py devrait fonctionner correctement.

# √âtape 2 : Construire un tokenizer personnalis√©

Lancez train_tokenizer.py ou train_tokenizer_hf.py sur les donn√©es nettoy√©es.
Cela vous donnera vocab.json et merges.txt

Ces fichiers d√©finissent le vocabulaire et les r√®gles de fusion pour votre mod√®le

# √âtape 3 : Entra√Ænez votre mod√®le (nanoGPT)

R√©f√©rez-vous √† [nanoGPT par Andrej Karpathy](https://github.com/karpathy/nanoGPT) pour le processus d'entra√Ænement.

Vous pouvez entra√Æner un autre LLM si vous le souhaitez, mais j'ai utilis√© nanoGPT

# FAQ

## Qu'est-ce que le Selective Temporal Training ?

Le Selective Temporal Training (STT) est une m√©thodologie d'apprentissage automatique o√π toutes les donn√©es d'entra√Ænement sont pr√©cis√©ment s√©lectionn√©es pour appartenir √† une p√©riode historique sp√©cifique. Cela permet de mod√©liser la langue et les connaissances de cette √©poque sans influence de concepts modernes. Par exemple, le mod√®le que j'ai actuellement (v0.5) est entra√Æn√© sur des donn√©es exclusivement de 1800-1875, il n'est pas affin√© mais entra√Æn√© √† partir de z√©ro, ce qui donne une sortie refl√©tant le style linguistique et le contexte historique de cette p√©riode.

## Pourquoi ne pas simplement utiliser le fine-tuning ou LoRA ?

Pour ce projet, j'essaie de cr√©er un mod√®le de langage sans biais moderne. Si j'affine quelque chose comme GPT-2, il est d√©j√† pr√©-entra√Æn√© et cette information ne dispara√Ætra pas. Si j'entra√Æne √† partir de z√©ro, le mod√®le de langage ne fera pas semblant d'√™tre ancien, il le sera simplement. Le but de ce projet, pour l'instant, est de cr√©er quelque chose qui puisse raisonner uniquement √† partir des connaissances issues de livres londoniens publi√©s entre 1800 et 1850.

## Quelles donn√©es avez-vous utilis√©es pour l'entra√Ænement ?

J'utilise des livres, documents juridiques, journaux et autres √©crits de Londres, 1800‚Äì1850. La liste que j'ai partag√©e en compte environ 200, mais pour le premier entra√Ænement j'ai utilis√© seulement 50 fichiers pour ~187 Mo. Vous pouvez voir une liste des documents :
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## Quelle est la taille du mod√®le Version 0 ?

Ce mod√®le est tr√®s petit pour l'instant, je fais cela pour le plaisir en suivant la r√®gle stricte de n'utiliser aucune source moderne. Il a presque 16 millions de param√®tres mais je vais commencer √† rassembler plus de vieux textes pour commencer un autre entra√Ænement. Je donnerai des nouvelles au fur et √† mesure.

## Sp√©cifications d'entra√Ænement ?

GPU : Geforce rtx 4060
CPU : i5-13400F
Ram : 16GB DDR5.


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-29

---