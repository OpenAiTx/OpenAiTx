
<div align="right">
  <details>
    <summary >üåê Langue</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="#" title="Coming soon">ÁπÅÈ´î‰∏≠Êñá (bient√¥t disponible)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="#" title="Coming soon">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä (bient√¥t disponible)</a> |
        | <a href="#" title="Coming soon">‡πÑ‡∏ó‡∏¢ (bient√¥t disponible)</a> |
        | <a href="#" title="Coming soon">Fran√ßais (bient√¥t disponible)</a>
        | <a href="#" title="Coming soon">Deutsch (bient√¥t disponible)</a>
        | <a href="#" title="Coming soon">Espa√±ol (bient√¥t disponible)</a>
        | <a href="#" title="Coming soon">Italiano (bient√¥t disponible)</a>
        | <a href="#" title="Coming soon">–†—É—Å—Å–∫–∏–π (bient√¥t disponible)</a>
        | <a href="#" title="Coming soon">Portugu√™s (bient√¥t disponible)</a>
        | <a href="#" title="Coming soon">Nederlands (bient√¥t disponible)</a>
        | <a href="#" title="Coming soon">Polski (bient√¥t disponible)</a>
        | <a href="#" title="Coming soon">ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (bient√¥t disponible)</a>
        | <a href="#" title="Coming soon">ŸÅÿßÿ±ÿ≥€å (bient√¥t disponible)</a>
        | <a href="#" title="Coming soon">T√ºrk√ße (bient√¥t disponible)</a>
        | <a href="#" title="Coming soon">Ti·∫øng Vi·ªát (bient√¥t disponible)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (bient√¥t disponible)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM
Un LLM entra√Æn√© uniquement sur des donn√©es provenant de certaines p√©riodes pour r√©duire le biais moderne.

Imaginez si un mod√®le d'IA ne se contentait pas de faire semblant d'√™tre historique, mais l'√©tait r√©ellement.

Bas√© sur [nanoGPT par Andrej Karpathy](https://github.com/karpathy/nanoGPT) Les scripts d'entra√Ænement principaux et l'architecture du mod√®le sont de lui.

# Objectifs du projet

TimeCapsule LLM est un projet exp√©rimental qui ne sera entra√Æn√© que sur des textes √©crits durant certaines p√©riodes. L'objectif est de simuler la vision du monde et la langue d'√©poques historiques sp√©cifiques.

# Pourquoi l'affinage n'est pas suffisant

Si vous vous contentez d'affiner un mod√®le pr√©-entra√Æn√©, votre LLM conna√Ætra encore des concepts modernes. Bien s√ªr, atteindre un biais moderne nul est difficile, mais je veux m'en approcher le plus possible. Obtenir un mod√®le sans biais moderne n√©cessite un entra√Ænement depuis z√©ro.

# R√©sultats attendus

J'esp√®re que, une fois termin√©, ce mod√®le n'aura pas connaissance des concepts modernes et ne sera pas capable de raisonner au-del√† de ce sur quoi il a √©t√© entra√Æn√©. Il ne devrait pas reconna√Ætre les concepts/vocabulaire modernes et j'esp√®re qu'il n'hallucinera pas de connaissances modernes.

# Mises √† jour de l'avancement

## 9 juillet 2025

J'ai d√©fini ma p√©riode √† 1800-1850 et la r√©gion : Londres

J'ai rassembl√© une liste de textes, livres, documents

Jusqu'√† pr√©sent, j'en ai obtenu 50 sous forme de fichiers txt et je vais bient√¥t commencer l'entra√Ænement de NanoGPT

Je mettrai √† jour ceci tant que des progr√®s seront r√©alis√©s

## 13 juillet 2025

NanoGPT entra√Æn√© avec 187 Mo de donn√©es textuelles historiques.

## 15 juillet 2025

J'ai commenc√© √† t√©l√©charger des textes pour la seconde session d'entra√Ænement. Je prends tout sur Internet Archive et j'ai √©largi la p√©riode √† 1800-1875. Pour obtenir une gamme diversifi√©e de textes, vous pouvez utiliser les filtres de sujet et de recherche sur Internet Archive selon le lieu de publication, la p√©riode et les sujets.

![Filtres de recherche](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## 16 juillet 2025

J'ai t√©l√©charg√© environ 500 fichiers txt depuis Internet Archive et apr√®s les avoir nettoy√©s (juste suppression des espaces, en-t√™tes Gutenberg, etc.) j'ai environ 500 Mo de donn√©es. C'est un petit jeu de donn√©es mais la derni√®re fois j'ai entra√Æn√© sur 187 Mo donc il devrait y avoir au moins une diff√©rence notable dans la sortie apr√®s l'entra√Ænement du second mod√®le. J'esp√®re que ce mod√®le pourra au moins produire des phrases plus coh√©rentes qui ont un certain sens. Ce n'est bien s√ªr pas garanti puisque c'est encore un tout petit jeu de donn√©es, mais c'est plus que ce que j'avais utilis√© la derni√®re fois.

Cela devrait √™tre faisable sur mon propre mat√©riel, c'est aussi bien car je pourrai voir, je l'esp√®re, des am√©liorations avant de passer √† un plus grand jeu de donn√©es qui n√©cessiterait de louer un GPU. Mais ne vous inqui√©tez pas, je pr√©vois toujours de louer un GPU bient√¥t, mais avant cela, je veux m'assurer que mon jeu de donn√©es est aussi bien s√©lectionn√© et propre que possible. L'un des probl√®mes que j'ai est le nettoyage, beaucoup de ces fichiers txt contiennent du charabia m√©lang√©. Les scripts que j'ai utilis√©s pour le nettoyage fonctionnent mais ne sont pas efficaces √† 100 %.

Je vais entra√Æner ce jeu de donn√©es aujourd'hui et cela devrait prendre environ 4 √† 5 heures. Une fois termin√© et test√©, je donnerai des nouvelles. Merci encore √† tous ceux qui consultent mon projet, certains m'ont m√™me envoy√© des liens vers des ressources OCR alors merci ! J'esp√®re que plus de gens essaieront cela et exp√©rimenteront avec leurs propres jeux de donn√©es.


### Mise √† jour de l'entra√Ænement

J'ai commenc√© l'entra√Ænement sur un corpus de 435 Mo (108 M de tokens), √ßa se passe plut√¥t bien pour l'instant. La perte d'entra√Ænement est pass√©e de 10,9 √† 4,9 lors des 2800 premi√®res it√©rations. Je pense que cela prendra environ 8 ou 9 heures pour se terminer. Je posterai une autre mise √† jour une fois que ce sera fait.

## 17 juillet 2025 2:13AM

L'entra√Ænement du second mod√®le est termin√©, cela a pris environ 8 heures et 40 minutes √† mon 4060 (3 900 iters/h) pour 33 000 it√©rations (5 √©poques). La perte d'entra√Ænement finale √©tait de 3,73. Les sorties √©taient √©tonnamment bonnes, il g√©n√®re maintenant vraiment des phrases de style XIXe si√®cle coh√©rentes.

## 28 juillet 2025

J'ai upload√© la v0.5 sur Hugging Face, [Regardez ici](https://huggingface.co/haykgrigorian/TimeCapsuleLLM) si vous voulez. Vous pouvez maintenant t√©l√©charger mon repo et l'ex√©cuter localement. Malheureusement nanoGPT ne fonctionne pas nativement avec HuggingFace, donc vous devrez t√©l√©charger et ex√©cuter le mod√®le localement.

Je vais √©galement commencer √† s√©lectionner les donn√©es pour mon prochain entra√Ænement, je pense qu'il me faudra peut-√™tre 5 √† 10 fois plus de donn√©es pour atteindre des capacit√©s de raisonnement.


# Comportement et limitations du mod√®le V0

Les premiers prompts montrent le mod√®le r√©pondant avec la langue et le comportement des ann√©es 1800. Par exemple, je lui ai demand√© "Who art Henry?" et il a r√©pondu "I know that man, I have did not a black, the storm." et oui, cette phrase n'a aucun sens mais le LLM reconna√Æt que je demande √† propos d'une personne.

![Exemple de sortie TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

Aucune mention de concepts modernes, les sorties contiennent principalement des mots et des formulations du XIXe si√®cle.

Il reste encore beaucoup de travail, s‚Äôentra√Æner sur 187 Mo ne permet pas d‚Äôobtenir un mod√®le capable de produire un texte avec un raisonnement complexe.

Actuellement, il produit des phrases qui manquent de structure compl√®te et globalement n‚Äôont aucun sens, mais c‚Äôest normal compte tenu de la taille de l‚Äôentra√Ænement.

# Comportement et limitations du mod√®le V0.5

C‚Äôest une belle am√©lioration par rapport au dernier mod√®le. Le style d‚Äô√©criture et le vocabulaire sont victoriens et presque chaque phrase est grammaticalement correcte avec une ponctuation appropri√©e. Et encore une fois, ce mod√®le est entra√Æn√© √† partir de z√©ro donc il reste sur des sujets du XIXe si√®cle.

![Exemple de sortie TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

Il y a beaucoup d‚Äôhallucinations factuelles. Beaucoup (pratiquement 100 %) des d√©tails (dates, √©v√©nements, personnages historiques) sont invent√©s. De plus, les phrases n‚Äôont pas vraiment de liens entre elles, parfois 2 phrases peuvent √™tre li√©es mais pas au-del√†. Un autre probl√®me est qu‚Äôil arrive qu‚Äôun pied de page ¬´ Digitized by Google ¬ª apparaisse, donc la prochaine fois que j‚Äôentra√Æne, je dois vraiment m‚Äôassurer que les textes sont bien nettoy√©s. Globalement, je suis tr√®s content des r√©sultats, on est loin d‚Äôun LLM mais c‚Äôest d√©finitivement un g√©n√©rateur de phrases.

J‚Äôapprends beaucoup et je vais commencer √† identifier ce que je dois am√©liorer dans les semaines √† venir. Je mettrai bient√¥t les fichiers en ligne !

# Projets √† venir

(Termin√©) Je vais commencer √† travailler sur la version 0.5, au lieu d‚Äôentra√Æner sur 50 livres, j‚Äôentra√Ænerai id√©alement sur 500-600. Actuellement, j‚Äôentra√Æne nanoGPT en utilisant des livres de 1800-1850, sp√©cifiquement de Londres. Il y a quelques d√©fis comme s‚Äôassurer que les livres trouv√©s ne sont pas modernis√©s ou interpr√©t√©s, mais bien des livres intacts publi√©s dans la p√©riode choisie.

Je veux entra√Æner un nouveau mod√®le (v1) avec un corpus beaucoup plus large, peut-√™tre 5 √† 10 fois plus grand que celui utilis√© pour v0.5. Mon objectif est de voir si des capacit√©s de raisonnement peuvent √©merger gr√¢ce au Selective Temporal Training seul, ce qui sera plus difficile et je ne suis m√™me pas s√ªr que ce soit possible √† cause des limitations de donn√©es historiques. Dans les semaines √† venir, je vais essayer de r√©unir assez de donn√©es pour un corpus de 5 √† 10 Go. Je crois qu‚Äôavec des donn√©es propres et de haute qualit√© et en louant un GPU, il y aura des progr√®s.

# Comment utiliser ce projet

Ce projet se concentre surtout sur la curation de donn√©es historiques, leur pr√©paration pour l‚Äôentra√Ænement et la cr√©ation d‚Äôun tokenizer. Je ne couvrirai pas tout le processus d‚Äôentra√Ænement d‚Äôun LLM, pour cela r√©f√©rez-vous √† nanoGPT d‚ÄôAndrej Karpathy.

# √âtape 1 : Rassembler et pr√©parer les textes historiques

Collectez des fichiers .txt de livres, documents, etc. du domaine public de la p√©riode choisie (par ex. Londres 1800-1850)

Vous pouvez utiliser download_texts_improved.py pour t√©l√©charger des livres si besoin.

Nettoyez les fichiers texte √† l‚Äôaide d‚Äôun script ou manuellement pour retirer les en-t√™tes/pieds de page de Project Gutenberg, annotations modernes ou erreurs d‚ÄôOCR.

prepare_dataset.py devrait fonctionner correctement.

# √âtape 2 : Construire un tokenizer personnalis√©

Lancez train_tokenizer.py ou train_tokenizer_hf.py sur les donn√©es nettoy√©es.
Cela vous donnera vocab.json et merges.txt

Ces fichiers d√©finissent le vocabulaire et les r√®gles de fusion pour votre mod√®le

# √âtape 3 : Entra√Ænez votre mod√®le (nanoGPT)

R√©f√©rez-vous √† [nanoGPT d‚ÄôAndrej Karpathy](https://github.com/karpathy/nanoGPT) pour le processus d‚Äôentra√Ænement.

Vous pouvez entra√Æner un autre LLM si vous le souhaitez, mais j‚Äôai utilis√© nanoGPT

# FAQ

## Qu‚Äôest-ce que le Selective Temporal Training ?

Le Selective Temporal Training (STT) est une m√©thodologie d‚Äôapprentissage automatique o√π toutes les donn√©es d‚Äôentra√Ænement sont sp√©cifiquement s√©lectionn√©es pour appartenir √† une p√©riode historique pr√©cise. Cela permet de mod√©liser la langue et les connaissances de cette √©poque sans influence de concepts modernes. Par exemple, le mod√®le actuel (v0.5) est entra√Æn√© uniquement sur des donn√©es de 1800-1875, il n‚Äôest pas affin√© mais entra√Æn√© √† partir de z√©ro, ce qui donne une sortie refl√©tant le style linguistique et le contexte historique de cette p√©riode.

## Pourquoi ne pas simplement utiliser le fine-tuning ou LoRA ?

Pour ce projet, j‚Äôessaie de cr√©er un mod√®le de langue non biais√© par la modernit√©. Si je fais du fine-tuning sur quelque chose comme GPT-2, il est d√©j√† pr√©-entra√Æn√© et cette information ne dispara√Ætra pas. Si j‚Äôentra√Æne √† partir de z√©ro, le mod√®le ne fera pas semblant d‚Äô√™tre ancien, il le sera vraiment. L‚Äôobjectif de ce projet actuellement est de cr√©er quelque chose qui puisse raisonner exclusivement avec les connaissances issues de livres londoniens publi√©s entre 1800 et 1850.

## Quel type de donn√©es as-tu utilis√© pour l‚Äôentra√Ænement ?

J‚Äôutilise des livres, documents juridiques, journaux et autres √©crits de Londres 1800‚Äì1850. La liste que j‚Äôai partag√©e contient environ 200 documents, mais pour le premier entra√Ænement je n‚Äôai utilis√© que 50 fichiers d‚Äôenviron ~187 Mo. Vous pouvez consulter la liste des documents :
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## Quelle est la taille du mod√®le Version 0 ?

Ce mod√®le est tr√®s petit pour l‚Äôinstant, je fais √ßa pour le plaisir et je respecte la r√®gle stricte de n‚Äôutiliser aucune source moderne. Il a pr√®s de 16 millions de param√®tres mais je vais commencer √† rassembler plus de vieux textes pour d√©buter un nouvel entra√Ænement. Je donnerai des nouvelles au fur et √† mesure.

## Sp√©cifications d‚Äôentra√Ænement ?

GPU : Geforce RTX 4060
CPU : i5-13400F
Ram : 16 Go DDR5.


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-08-02

---