<div align="right">
  <details>
    <summary >ğŸŒ Langue</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ç®€ä½“ä¸­æ–‡</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-TW">ç¹é«”ä¸­æ–‡</a> 
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">æ—¥æœ¬èª</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">í•œêµ­ì–´</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=as">à¤¹à¤¿à¤¨à¥à¤¦à¥€</a> 
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=th">à¹„à¸—à¸¢</a> 
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=fr">FranÃ§ais</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=es">EspaÃ±ol</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=it">Italiano</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=ru">Ğ ÑƒÑÑĞºĞ¸Ğ¹</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=pt">PortuguÃªs</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=ar">Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©</a>
        | <a href="#" title="Coming soon">ÙØ§Ø±Ø³ÛŒ (bientÃ´t disponible)</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=tr">TÃ¼rkÃ§e</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=vi">Tiáº¿ng Viá»‡t</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (bientÃ´t disponible)</a>

      </div>
    </div>
  </details>
</div>


# TimeCapsule LLM

*Un modÃ¨le de langage entraÃ®nÃ© **Ã  partir de zÃ©ro** exclusivement sur des donnÃ©es de certains lieux et pÃ©riodes afin de rÃ©duire le biais moderne et dâ€™Ã©muler la voix, le vocabulaire et la vision du monde de lâ€™Ã©poque.*

Imaginez quâ€™un modÃ¨le IA ne fasse pas seulement semblant dâ€™Ãªtre historique, mais le soit rÃ©ellement.

v0 et v0.5 construits sur [nanoGPT par Andrej Karpathy](https://github.com/karpathy/nanoGPT) Les scripts dâ€™entraÃ®nement principaux et lâ€™architecture du modÃ¨le sont son Å“uvre. 

v1 construit sur [Phi 1.5 par Microsoft](https://huggingface.co/microsoft/phi-1_5)

[Lien Hugging Face](https://huggingface.co/haykgrigorian/TimeCapsuleLLM)

##  Comportement du modÃ¨le & Limitations

### **v0**  

Les premiers prompts montrent que le modÃ¨le rÃ©pond avec le langage et le comportement des annÃ©es 1800.  
Exemple : Prompt : Â« Qui est Henry ? Â» et il a rÃ©pondu Â« Je connais cet homme, je nâ€™ai pas fait de noir, la tempÃªte. Â»  

![Exemple de sortie TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

- Aucune mention de concepts modernes  
- Vocabulaire principalement fidÃ¨le Ã  lâ€™Ã©poque  
- Les phrases sont majoritairement incohÃ©rentes (attendu pour ~187MB de donnÃ©es dâ€™entraÃ®nement)

### **v0.5** 

AmÃ©lioration significative par rapport Ã  v0.  
- Style dâ€™Ã©criture victorien, ponctuation correcte, phrases majoritairement grammaticales  
- Taux Ã©levÃ© dâ€™hallucination factuelle persistant  
- Bruit OCR (Â« Digitized by Google Â») encore prÃ©sent dans les sorties

![Exemple de sortie TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

### **v1**

Premier modÃ¨le Ã  rappeler et relier un Ã©vÃ©nement historique rÃ©el Ã  une figure authentique du jeu de donnÃ©es.

Exemple : Prompt : Â« Câ€™Ã©tait lâ€™an de grÃ¢ce 1834 Â» 

La sortie : Â« Câ€™Ã©tait lâ€™an de grÃ¢ce 1834 et les rues de Londres Ã©taient remplies de protestation et de pÃ©tition. La cause, comme beaucoup lâ€™ont racontÃ©, nâ€™Ã©tait pas liÃ©e Ã  la voie privÃ©e, mais ayant pris le mÃªme jour lors du jour de Lord Palmerston, le public recevra une brÃ¨ve dÃ©claration des difficultÃ©s sous lesquelles la loi du jour nous a atteints. Câ€™est un profond regret que les Ã©vÃ©nements actuels dans lâ€™histoire du monde soient clairs, et seront donc connus. Il nâ€™est pas vrai que les tout premiers hommes installÃ©s dans lâ€™Ã‰vangile Ã  JÃ©rusalem devraient avoir un compte rendu aussi vaste et aussi intÃ©ressant de la prospÃ©ritÃ© et de la prospÃ©ritÃ© Â» 

Au dÃ©but, je pensais quâ€™une protestation avait pu avoir lieu la mÃªme annÃ©e par coÃ¯ncidence mais regardez ceci : ![1834protest](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)

### **v2mini-eval1**

EntraÃ®nÃ© Ã  lâ€™aide dâ€™un Ã©chantillon de 15 Go provenant du jeu de donnÃ©es de 90 Go de v2.

ModÃ¨le entraÃ®nÃ© uniquement jusqu'Ã  10 000 Ã©tapes.

Il y a eu un problÃ¨me de tokenisation qui fait que la sortie ressemble Ã  ceci :

INVITE :
Qui est Charles Dickens ?


W ho is Charles D ic ens ? D oes that work more of h ise x cell ent st ir ring , in his pl ays , int he G reat C omp any 's f arm ? What I have y et to qu ote from J ack P ick ett ? D oy oun ot know th att hey were as pe cies of g all oping , ors aw ing oft heir bre asts , or what w ast ob ed one about the t im ew hen J acks on was looking ont hew ind ow ? What ist hes uccess of an English man , and which h isson has not be end is cover ing t ome , where by to acc omp lish such a we ight ? D idy ou ever m ake ap ass age int othe old road way , ort o an anch or - b ree ze att he foot of our boat , which you m ust le ave us ? The fact is , that whe ther the wind would rise up from the pl ain on S atur day night orn o ont ide , ort ill then orth , or other wise , wes hall be com pelled t od esc ribe af orm idable bar rier , with t hes ame e ffect s ast he p resent . In th iss itu ation , at le ast , itis not to omuch t os ay that we have le ft that room . I bel ieve there are three c op ies int he " F ive H undred - fold ," to bere fer red to , ast he first n umber of our read ers wh ow is ht

CorrigÃ© :

Â« Qui est Charles Dickens ? Est-ce que cela fonctionne davantage de son excellente agitation, dans ses piÃ¨ces, Ã  la ferme de la Great Company ? Que dois-je encore citer de Jack Pickett ? Ne savez-vous pas qu'ils Ã©taient une sorte de galop, ou de sciage de leur poitrine, ou que fallait-il faire Ã  l'Ã©poque oÃ¹ Jackson regardait par la fenÃªtre ? Quel est le succÃ¨s d'un Anglais, et que son fils ne m'a pas encore rÃ©vÃ©lÃ©, afin d'accomplir un tel poids ? Avez-vous dÃ©jÃ  fait un passage sur l'ancienne chaussÃ©e, ou vers une brise d'ancre au pied de notre bateau, que vous devez nous quitter ? Le fait est que, que le vent se lÃ¨ve de la plaine le samedi soir ou Ã  midi, ou vers le nord, ou autrement, nous serons obligÃ©s de dÃ©crire une barriÃ¨re redoutable, avec les mÃªmes effets que ceux du prÃ©sent. Dans cette situation, du moins, il n'est pas exagÃ©rÃ© de dire que nous avons quittÃ© cette piÃ¨ce. Je crois qu'il y a trois exemplaires dans le Â« Cinq-Cents fois Â», Ã  consulter, en tant que premier nombre de nos lecteurs qui souhaitent t[...]... Â»

### **v2mini-eval2**

EntraÃ®nÃ© avec le mÃªme Ã©chantillon de 15 Go utilisÃ© pour lâ€™entraÃ®nement de eval1.
Le modÃ¨le est de nouveau entraÃ®nÃ© sur seulement 10 000 Ã©tapes.

PROMPT : Charles Darwin

Charles DarwinECCEMACY. Monsieur, â€” Le cas suivant mâ€™intÃ©resse : â€” Jâ€™Ã©tais Ã  Londres pendant quinze jours, et jâ€™ai Ã©tÃ© fortement affectÃ© par une attaque de rhumatisme. La premiÃ¨re attaque de rhumatisme a eu lieu une semaine avant que je vous voie, la deuxiÃ¨me quand je vous ai vu, la troisiÃ¨me quand je vous ai vu, et la troisiÃ¨me en mÃªme temps. La deuxiÃ¨me attaque de goutte, cependant, nâ€™Ã©tait pas accompagnÃ©e de symptÃ´mes fÃ©briles, mais dâ€™une augmentation du flux urinaire, et dâ€™une Ã©vacuation urinaire plus abondante. La troisiÃ¨me attaque a eu lieu une heure aprÃ¨s que je vous ai vu, et a Ã©tÃ© suivie dâ€™un retour du paroxysme de goutte, et dâ€™un retour plus rapide de la goutte. La quatriÃ¨me attaque Ã©tait Ã©galement accompagnÃ©e de fiÃ¨vre, mais nâ€™Ã©tait pas toujours accompagnÃ©e de symptÃ´mes fÃ©briles. La troisiÃ¨me attaque de goutte a eu lieu quinze jours aprÃ¨s que vous avez Ã©tÃ© malade, et la quatriÃ¨me a Ã©tÃ© suivie dâ€™un paroxysme de goutte. La quatriÃ¨me attaque est survenue quinze jours aprÃ¨s que vous avez Ã©tÃ© attaquÃ©, et Ã©tait accompagnÃ©e dâ€™une sensation


##  Jeux de donnÃ©es

### **v2**

- 90 Go de textes londoniens de 1800 Ã  1875
- 136 344 documents
- Les 90 Go complets ne sont pas encore disponibles car ils nâ€™ont pas encore Ã©tÃ© tokenisÃ©s, mais vous pouvez trouver un Ã©chantillon de 15 Go ici : https://huggingface.co/datasets/haykgrigorian/TimeCapsuleLLM-London-1800-1875-v2-15GB

 ### Statistiques sur les biais
  ![Biais de pronom](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v2mini_eval1/pronoun_bias.png)

  ![Biais gÃ©ographique](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v2mini_eval1/geographic_bias.png)

  ![Biais temporel](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v2mini_eval1/temporal_bias.png)

RÃ©fÃ©rez-vous au [rapport sur les biais v2](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v2mini_eval1/v2_bias_report.json) pour plus dâ€™informations.


## Comment utiliser

Ce projet se concentre principalement sur la collecte de donnÃ©es historiques, leur prÃ©paration pour lâ€™entraÃ®nement et la construction dâ€™un tokenizer. Je ne vais pas couvrir tout le processus dâ€™entraÃ®nement LLM, pour cela, rÃ©fÃ©rez-vous Ã  nanoGPT par Andrej Karpathy.

### Ã‰tape 1 : Rassembler et prÃ©parer des textes historiques

- Collectez des fichiers .txt de livres du domaine public, de documents, etc. de votre pÃ©riode choisie (par exemple, Londres 1800-1850)
- Gardez-les dans la fenÃªtre temporelle/lieu que vous avez choisie  
- Nettoyez les fichiers texte Ã  lâ€™aide dâ€™un script ou retirez manuellement les en-tÃªtes/pieds de page de Project Gutenberg, les annotations modernes ou les erreurs OCR.

### Ã‰tape 2 : Construire un tokenizer personnalisÃ©

- ExÃ©cutez train_tokenizer.py ou train_tokenizer_hf.py sur les donnÃ©es nettoyÃ©es.
- Cela vous donnera vocab.json et merges.txt
- Ces fichiers dÃ©finissent le vocabulaire et les rÃ¨gles de fusion pour votre modÃ¨le

### Ã‰tape 3 : EntraÃ®nez votre modÃ¨le 

- Reportez-vous Ã  [nanoGPT par Andrej Karpathy](https://github.com/karpathy/nanoGPT) pour le processus dâ€™entraÃ®nement ou la documentation de lâ€™architecture choisie.

# FAQ

## Quâ€™est-ce que lâ€™entraÃ®nement temporel sÃ©lectif ?

Lâ€™entraÃ®nement temporel sÃ©lectif (Selective Temporal Training, STT) est une mÃ©thodologie dâ€™apprentissage automatique oÃ¹ toutes les donnÃ©es dâ€™entraÃ®nement sont soigneusement sÃ©lectionnÃ©es pour appartenir Ã  une pÃ©riode historique spÃ©cifique. Cela permet de modÃ©liser la langue et les connaissances de cette Ã©poque sans influence des concepts modernes. Par exemple, le modÃ¨le actuel que je possÃ¨de (v0.5) est entraÃ®nÃ© exclusivement sur des donnÃ©es de 1800 Ã  1875, il nâ€™est pas ajustÃ© mais entraÃ®nÃ© depuis zÃ©ro, ce qui donne une sortie reflÃ©tant le style linguistique et le contexte historique de cette pÃ©riode.

## Pourquoi ne pas simplement utiliser le fine-tuning ou LoRA ?

Pour ce projet, jâ€™essaie de crÃ©er un modÃ¨le de langage non biaisÃ© par la modernitÃ©. Si je fais du fine-tuning sur GPT-2, il est dÃ©jÃ  prÃ©-entraÃ®nÃ© et cette information ne disparaÃ®tra pas. Si jâ€™entraÃ®ne Ã  partir de zÃ©ro, le modÃ¨le de langage ne fera pas semblant dâ€™Ãªtre ancien, il le sera rÃ©ellement. Lâ€™objectif pour ce projet est de crÃ©er quelque chose qui puisse raisonner exclusivement Ã  partir de connaissances tirÃ©es de livres londoniens publiÃ©s entre 1800 et 1875.

## Quel type de donnÃ©es avez-vous utilisÃ© pour lâ€™entraÃ®nement ?

Jâ€™utilise des livres, documents juridiques, journaux et autres Ã©crits de Londres entre 1800 et 1875. La liste que jâ€™ai partagÃ©e (pour v0) en contient environ 200, mais pour le premier entraÃ®nement, jâ€™ai utilisÃ© seulement 50 fichiers pour environ ~187 Mo. Vous pouvez consulter la liste des documents :
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

Taille des ensembles de donnÃ©es :
- v0 : ~187 Mo
- v0.5 : ~435 Mo 
- v1 : ~6,25 Go 
- v2mini-eval1 : 15 Go

## Quelle est la taille des modÃ¨les ?

v0 : 16M de paramÃ¨tres

v0.5 123M de paramÃ¨tres

v1: 700M Parameters

v2mini-eval1: 300M Parameters

# Training Specs ? 

# v0/v0.5
GPU: Geforce rtx 4060
CPU: i5-13400F 
Ram: 16GB DDR5.

# v1
GPU: A100 SXM rented

# v2mini-eval1

GPU: A100 SXM rented





























---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2026-01-09

---