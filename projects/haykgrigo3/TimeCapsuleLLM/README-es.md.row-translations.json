[
  {
    "row": 1,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "<div align=\"right\">"
  },
  {
    "row": 2,
    "rowsha": "cWgam+tnnXudu7i74+ahMEGk/A9dQS+EwWLAIfi3dHk=",
    "originContent": "<div align=\"right\">",
    "translatedContent": "  <details>"
  },
  {
    "row": 3,
    "rowsha": "orOcu5ARna/hb3RUkj6dBI8pHTM3WHeTvby17l5E0h0=",
    "originContent": "  <details>",
    "translatedContent": "    <summary >üåê Idioma</summary>"
  },
  {
    "row": 4,
    "rowsha": "TtgkLzblnvP0q9aAIVXt6s2LczXjy5k+QvHKcU0/5Ms=",
    "originContent": "    <summary >üåê Language</summary>",
    "translatedContent": "    <div>"
  },
  {
    "row": 5,
    "rowsha": "fZtk4rPTAJEEslnbhSVkHEcPlsctYSzAV7CDPL3rJmA=",
    "originContent": "    <div>",
    "translatedContent": "      <div align=\"center\">"
  },
  {
    "row": 6,
    "rowsha": "9KQxOeJSigvTmGWO+mtnl8kZY9zQfueoy8sk4lYm09Q=",
    "originContent": "      <div align=\"center\">",
    "translatedContent": "        <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en\">English</a>"
  },
  {
    "row": 7,
    "rowsha": "CeOhdpchZBoZSEUDtSE417JEcMBSZw18jeJuHJBKB2Y=",
    "originContent": "        <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en\">English</a>",
    "translatedContent": "        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN\">ÁÆÄ‰Ωì‰∏≠Êñá</a>"
  },
  {
    "row": 8,
    "rowsha": "ToO7MFa3QrNNljdQWIagsnOPxe8cXuuA2m5msIm+Kbs=",
    "originContent": "        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN\">ÁÆÄ‰Ωì‰∏≠Êñá</a>",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">ÁπÅÈ´î‰∏≠Êñá (pr√≥ximamente)</a> |"
  },
  {
    "row": 9,
    "rowsha": "MRATmWdRMRw0JU4u9h5pMb6GU17lQFgG9v/bpGLr9pM=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">ÁπÅÈ´î‰∏≠Êñá (coming soon)</a> |",
    "translatedContent": "        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja\">Êó•Êú¨Ë™û</a>"
  },
  {
    "row": 10,
    "rowsha": "GY7LXxG3rk5eFh9itcqM0cTtmHybyjLTf1icB3jN31I=",
    "originContent": "        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja\">Êó•Êú¨Ë™û</a>",
    "translatedContent": "        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko\">ÌïúÍµ≠Ïñ¥</a>"
  },
  {
    "row": 11,
    "rowsha": "b5TwunGJh+gsAe7aQU3dkfobXF/nknCEta1msDa7XBU=",
    "originContent": "        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko\">ÌïúÍµ≠Ïñ¥</a>",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä (pr√≥ximamente)</a> |"
  },
  {
    "row": 12,
    "rowsha": "1/HCgPsVh2ChqMY+k/VVxEWHPRRmWWCjy5nDRibi3mM=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä (coming soon)</a> |",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">‡πÑ‡∏ó‡∏¢ (pr√≥ximamente)</a> |"
  },
  {
    "row": 13,
    "rowsha": "3lfEHT+5HYFEvbE5cl+xujQPYjtVmzTifT37iqPTWII=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">‡πÑ‡∏ó‡∏¢ (coming soon)</a> |",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Fran√ßais (pr√≥ximamente)</a>"
  },
  {
    "row": 14,
    "rowsha": "KmG3P0px2E3bt1lU/w3eGop+zeA1j8xL0k280Zd9m2s=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">Fran√ßais (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Deutsch (pr√≥ximamente)</a>"
  },
  {
    "row": 15,
    "rowsha": "CSdHSEXgIs3M2Q/6zIIJ8NbKkZWhydhBqNus94qrPvg=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">Deutsch (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Espa√±ol (pr√≥ximamente)</a>"
  },
  {
    "row": 16,
    "rowsha": "8wz7pDuXc3dk+ZcqZ1jmmh8zh6xN3Wb6qWbCjxAj7dA=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">Espa√±ol (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Italiano (pr√≥ximamente)</a>"
  },
  {
    "row": 17,
    "rowsha": "op/NqIZs7OjCSpNgpXk8RnqDnTegVPyWUQhuQxvTR7U=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">Italiano (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">–†—É—Å—Å–∫–∏–π (pr√≥ximamente)</a>"
  },
  {
    "row": 18,
    "rowsha": "tAvlfwut/Ad9q1huxc8EREZGv7vYHbrEujzUS8xoaQo=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">–†—É—Å—Å–∫–∏–π (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Portugu√™s (pr√≥ximamente)</a>"
  },
  {
    "row": 19,
    "rowsha": "WhhSpeeCUUAqJiVTS4Fvyc6A2c+24Jnj3MW7XLQuIcI=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">Portugu√™s (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Nederlands (pr√≥ximamente)</a>"
  },
  {
    "row": 20,
    "rowsha": "0yPXPrWh+Vzc6FBE9iiciw5HwpOSmo05HNe36wfTWCI=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">Nederlands (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Polski (pr√≥ximamente)</a>"
  },
  {
    "row": 21,
    "rowsha": "mdW6YUUXf5KzI4CwZxrE08ofaLonUOMnJpN3vPR7Y2A=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">Polski (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (pr√≥ximamente)</a>"
  },
  {
    "row": 22,
    "rowsha": "sw1AXxAGQNvn4eSG9enTWNkwKH0yr6LlVtXBH1j9z8s=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">ŸÅÿßÿ±ÿ≥€å (pr√≥ximamente)</a>"
  },
  {
    "row": 23,
    "rowsha": "I8dh9zmXisU0+CpddA55QQgvujH03J/dEnXgj5aFtQM=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">ŸÅÿßÿ±ÿ≥€å (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">T√ºrk√ße (pr√≥ximamente)</a>"
  },
  {
    "row": 24,
    "rowsha": "7VFv8o6de72ciJrbh3mctfrEgCJhNvuKGWJNOmCaPdM=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">T√ºrk√ße (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Ti·∫øng Vi·ªát (pr√≥ximamente)</a>"
  },
  {
    "row": 25,
    "rowsha": "C+XRvFz/D3o9/JyPqwitsxtskFZleJC/oFUr4SEeiHA=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">Ti·∫øng Vi·ªát (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Bahasa Indonesia (pr√≥ximamente)</a>"
  },
  {
    "row": 26,
    "rowsha": "ntGI5B+n9x96pV3ZG5GG83nmocQbxTJjKY7VVwa6Rq8=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">Bahasa Indonesia (coming soon)</a>",
    "translatedContent": ""
  },
  {
    "row": 27,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "      </div>"
  },
  {
    "row": 28,
    "rowsha": "0OM5wNEm0TO56MEBvQzL7AUZM7/3OpgIeqRf2zFre3Q=",
    "originContent": "      </div>",
    "translatedContent": "    </div>"
  },
  {
    "row": 29,
    "rowsha": "fcjTfY+fs8YnY5slBs1sZvWPAqEQR7tzaBDO54skkGQ=",
    "originContent": "    </div>",
    "translatedContent": "  </details>"
  },
  {
    "row": 30,
    "rowsha": "+fQNH2ldI7UM/rqRscP3hUSWAmw1HvQ2wEKDN8JagT0=",
    "originContent": "  </details>",
    "translatedContent": "</div>"
  },
  {
    "row": 31,
    "rowsha": "qsMmUbEPVnxGG5tPJV1vsfpoWbU2jYvZpRr5IKshzyM=",
    "originContent": "</div>",
    "translatedContent": ""
  },
  {
    "row": 32,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "# TimeCapsule LLM"
  },
  {
    "row": 33,
    "rowsha": "VRGjp0FtfvQ89lbX/wJLis2ypCRtNJwe8ViIi29+Rko=",
    "originContent": "# TimeCapsule LLM",
    "translatedContent": "Un LLM entrenado solo con datos de ciertos periodos de tiempo para reducir el sesgo moderno."
  },
  {
    "row": 34,
    "rowsha": "XGlykErifWX9oIzV4ZXDc4AUnsuesz8LvpruG76e6uY=",
    "originContent": "An LLM trained only on data from certain time periods to reduce modern bias.",
    "translatedContent": ""
  },
  {
    "row": 35,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "Imagina si un modelo de IA no solo pretendiera ser hist√≥rico sino que realmente lo fuera."
  },
  {
    "row": 36,
    "rowsha": "06wDXO9Un3ot9kUKAGg7CaRsIVSkfS1d2m+EQ6HOFog=",
    "originContent": "Imagine if an AI model didnt just pretend to be historical but actually was.",
    "translatedContent": ""
  },
  {
    "row": 37,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "Construido sobre [nanoGPT por Andrej Karpathy](https://github.com/karpathy/nanoGPT) Los scripts de entrenamiento principales y la arquitectura del modelo son su trabajo."
  },
  {
    "row": 38,
    "rowsha": "2773v/qIXSAsW4pN2HtYcVltfzG1vzgVbHgfjStBQIY=",
    "originContent": "Built on [nanoGPT by Andrej Karpathy](https://github.com/karpathy/nanoGPT) Core training scripts and model architecture are his work. ",
    "translatedContent": ""
  },
  {
    "row": 39,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "# Objetivos del Proyecto"
  },
  {
    "row": 40,
    "rowsha": "wITJJBD/4abiy4E37iMdOcGmifkmz4dALLyk6AhA1kc=",
    "originContent": "# Project Goals ",
    "translatedContent": ""
  },
  {
    "row": 41,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "TimeCapsule LLM es un proyecto experimental que solo ser√° entrenado con textos escritos durante ciertos periodos hist√≥ricos. El objetivo es simular la visi√≥n del mundo y el lenguaje de √©pocas hist√≥ricas espec√≠ficas."
  },
  {
    "row": 42,
    "rowsha": "LlW7r/H8NhftFgGAHce1f4KThGgsoT8aJ88/IsiLntc=",
    "originContent": "TimeCapsule LLM is an expirimental project that will only be trained on texts written during certain time periods. The goal is to simulate the worldview and language of specific historical eras.",
    "translatedContent": ""
  },
  {
    "row": 43,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "# Por qu√© el ajuste fino no es suficiente"
  },
  {
    "row": 44,
    "rowsha": "obYFMCTDj8qHZGo0BQtA2AlwA8JgNcjDK9WlMRI4eq8=",
    "originContent": "# Why fine tuning isn't enough ",
    "translatedContent": ""
  },
  {
    "row": 45,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "Si solo ajustas un modelo preentrenado, tu LLM todav√≠a conocer√° conceptos modernos. Por supuesto, lograr cero sesgo moderno es dif√≠cil, pero quiero acercarme lo m√°s posible a esto. Eliminar el sesgo moderno requiere entrenar un modelo desde cero."
  },
  {
    "row": 46,
    "rowsha": "yNEBOKV/RnG7CvDjiWhkXKK6vqbwki9QKC+Zs+8PzbM=",
    "originContent": "If you just fine tune a pre-trained model, your LLM is still gonna know modern concepts. Of course achieving zero modern bias is difficult but I want to get as close as possible to this. Getting no modern bias requires training a model from scratch.",
    "translatedContent": ""
  },
  {
    "row": 47,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "# Resultados esperados"
  },
  {
    "row": 48,
    "rowsha": "SdJkrN/DUD4+aOCh9lfDM3AAqMxlyukDfye/nzXzxN0=",
    "originContent": "# Expected outcomes ",
    "translatedContent": ""
  },
  {
    "row": 49,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "Con suerte, cuando est√© terminado, este modelo no conocer√° conceptos modernos y no podr√° razonar m√°s all√° de lo que ha sido entrenado. No deber√≠a reconocer conceptos/vocabulario modernos y espero que no alucine conocimiento moderno."
  },
  {
    "row": 50,
    "rowsha": "bsSMnG6qSBf/pVtCQNGFlaKye8GxBKV660amPA/pINE=",
    "originContent": "Hopefully when finished, this model will not know modern concepts and will not be able to reason beyond what it's been trained on. It shouldnt recognize modern concepts/vocab and I hope it doesn't hallucinate modern knowledge.",
    "translatedContent": ""
  },
  {
    "row": 51,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "# Actualizaciones de Progreso"
  },
  {
    "row": 52,
    "rowsha": "8EWRxPaogE2BaXxVJE1VFNAXNdS6KUYPLDFN8xlQ9LE=",
    "originContent": "# Progress Updates",
    "translatedContent": ""
  },
  {
    "row": 53,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "## 9 de julio de 2025"
  },
  {
    "row": 54,
    "rowsha": "oq91hnNV5WwmrEF0amya8kSN7gu21MN5nOcR2dPRBZ0=",
    "originContent": "## July 9th, 2025",
    "translatedContent": ""
  },
  {
    "row": 55,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "He establecido mi periodo de tiempo en 1800-1850 y regi√≥n: Londres"
  },
  {
    "row": 56,
    "rowsha": "yU3u8taDdAaD23tJB9+n/2wmry0GfF+KXqADT4YLuJ8=",
    "originContent": "I've set my time period for 1800-1850 and region: London ",
    "translatedContent": ""
  },
  {
    "row": 57,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "He reunido una lista de textos, libros, documentos"
  },
  {
    "row": 58,
    "rowsha": "uQe95shOfOi8NA0M2/CQCXlOjNsiSmGrt5dZbeP4ANs=",
    "originContent": "I've gathered a list of texts, books, documents ",
    "translatedContent": ""
  },
  {
    "row": 59,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "Hasta ahora he conseguido 50 en archivos txt y pronto comenzar√© a entrenar NanoGPT"
  },
  {
    "row": 60,
    "rowsha": "i9Kzka7MMa5yKjfdslauZFzKk+gAcnyILwscFoaepYs=",
    "originContent": "So far I've gotten 50 as txt files and will begin training NanoGPT soon ",
    "translatedContent": ""
  },
  {
    "row": 61,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "Actualizar√© esto siempre que haya progreso"
  },
  {
    "row": 62,
    "rowsha": "Wov5RgnyTA0P0gtJKLL0GTcSf7t8WrgFcAzDufE5Xh4=",
    "originContent": "Will update this as long as progress is made",
    "translatedContent": ""
  },
  {
    "row": 63,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "## 13 de julio de 2025"
  },
  {
    "row": 64,
    "rowsha": "FSOOW2G6pyPozg+3u76To6E4Pthd9lRoZE396fwY2I4=",
    "originContent": "## July 13th, 2025",
    "translatedContent": ""
  },
  {
    "row": 65,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "Entren√© nanoGPT con 187 MB de datos hist√≥ricos en texto."
  },
  {
    "row": 66,
    "rowsha": "GszU76q+4dgDkDO4uNZpx/9WhQTTCZlq4VYIt0ZdgD0=",
    "originContent": "Trained nanoGPT with 187MB of historial text data. ",
    "translatedContent": ""
  },
  {
    "row": 67,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "## 15 de julio de 2025"
  },
  {
    "row": 68,
    "rowsha": "pGPL3z/t2hDtXa67ubNgmRC/+b4O2Yp/0ff3R/9mraE=",
    "originContent": "## July 15th, 2025",
    "translatedContent": ""
  },
  {
    "row": 69,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "Comenc√© a descargar textos para la segunda ronda de entrenamiento. Estoy obteniendo todo del Internet Archive y he ampliado el periodo de tiempo a 1800-1875. Para obtener una gama diversa de textos, puedes usar filtros de tema y b√∫squeda por ubicaci√≥n de publicaci√≥n, periodo de tiempo y temas en Internet Archive."
  },
  {
    "row": 70,
    "rowsha": "+jwjqBw9Cr+lRnmxUzCQ0SnVBfXYeJDycuYf/p0JJgg=",
    "originContent": "I started downloading texts for the second training run. I'm getting everything from Internet Archive and I've expanded the time period to 1800-1875. To get a diverse range of texts, you can use subject and search filters for publication location, time period and subjects on Internet Archive. ",
    "translatedContent": ""
  },
  {
    "row": 71,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "![Filtros de B√∫squeda](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)"
  },
  {
    "row": 72,
    "rowsha": "XE9Xts6Q8wsZVZHg8uD/1ZXBQ/j2uFLsR9HwsiaqMds=",
    "originContent": "![Search Filters](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)",
    "translatedContent": ""
  },
  {
    "row": 73,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "## 16 de julio de 2025"
  },
  {
    "row": 74,
    "rowsha": "iREIjirFxs+ic0QmjQG1FQYKHC5brQaZ5JjPaEto+lU=",
    "originContent": "## July 16th, 2025",
    "translatedContent": ""
  },
  {
    "row": 75,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "Descargu√© alrededor de 500 archivos txt de Internet Archive y despu√©s de limpiarlos (solo eliminando espacios en blanco, encabezados de Gutenberg, etc.) tengo alrededor de 500 MB de datos. Es un conjunto de datos peque√±o pero la √∫ltima vez entren√© con 187 MB as√≠ que deber√≠a haber al menos alguna diferencia notable en la salida despu√©s de entrenar el segundo modelo. Espero que este modelo pueda al menos producir oraciones m√°s coherentes que tengan algo de sentido. Por supuesto, no es una garant√≠a ya que sigue siendo un conjunto de datos muy peque√±o, pero es m√°s de lo que us√© la vez pasada."
  },
  {
    "row": 76,
    "rowsha": "c1Ww8CUkqpg5TNm17QY7m130dQycuSFaAia2gfx/uLw=",
    "originContent": "I downloaded around 500 txt files from Internet Archive and after cleaning them (just deleting whitespaces, Gutenberg headers, etc) I have around 500MB of data. It's a tiny dataset but last time I trained off of 187MB so there should be at least some kind of noticable difference in the output after I train the second model. I'm hoping this model can at least produce more coherent sentences that kind of make sense. It's not a guarantee of course since this is still a tiny tiny dataset, but it's more than what I used last time. ",
    "translatedContent": ""
  },
  {
    "row": 77,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "Esto deber√≠a ser posible con mi propio hardware, lo cual es bueno porque espero ver alg√∫n tipo de mejora antes de pasar a un conjunto de datos m√°s grande que requerir√≠a alquilar una GPU. Pero no te preocupes, todav√≠a planeo alquilar una GPU pronto, pero antes de hacerlo quiero asegurarme de que mi conjunto de datos est√© lo m√°s curado y limpio posible. Uno de los problemas que tengo es la limpieza, muchos de estos archivos txt tienen textos sin sentido mezclados. Los scripts que he usado para limpiar funcionan pero no son 100% efectivos."
  },
  {
    "row": 78,
    "rowsha": "h/hyxvgOlOm5er9sn3CL2wmktMoq2q+qZi5Vi7upXGI=",
    "originContent": "This should be doable on my own hardware, it's good too because I can hopefully see some kind of improvements before I jump to a bigger dataset which would require me to rent a GPU. But don't worry I still plan on renting a GPU soon, but before I do that I wanna make sure my dataset is as curated and clean as possible. One of the issues I have is cleaning, a lot of these txt files have gibberish mixed in. The scripts I've used for cleaning do work but they're not 100% effective. ",
    "translatedContent": ""
  },
  {
    "row": 79,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "Entrenar√© este conjunto de datos hoy y deber√≠a tomar alrededor de 4-5 horas. Una vez que termine y lo pruebe, dar√© actualizaciones. ¬°Gracias de nuevo a todos los que est√°n viendo mi proyecto, incluso ha habido personas que me han enviado enlaces a recursos de OCR as√≠ que gracias! Espero que m√°s gente pruebe esto y experimente con sus propios conjuntos de datos."
  },
  {
    "row": 80,
    "rowsha": "8LYxYjHwrXnU59N+13Fd9m653Tgyom3yQQAWGIrSPSc=",
    "originContent": "I will train this dataset today and it should take around 4-5 hours. Once it's done and I test it, I will give updates. Thank you again to everyone whos checking out my project, I've even had some people even giving me links to OCR resources so Thank you! I hope more people try this out and expirement with they're own datasets. ",
    "translatedContent": ""
  },
  {
    "row": 81,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "### Actualizaci√≥n de Entrenamiento"
  },
  {
    "row": 82,
    "rowsha": "mNjt3ebwxfwyPM2/AK7E/GDTFM6i95HG8GZ/8TZs9EA=",
    "originContent": "### Training Update ",
    "translatedContent": ""
  },
  {
    "row": 83,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "Comenc√© a entrenar con un corpus de 435 MB (108 M de tokens), va bastante bien hasta ahora. La p√©rdida de entrenamiento baj√≥ de 10.9 a 4.9 en las primeras 2800 iteraciones. Espero que tome alrededor de 8 o 9 horas completar. Publicar√© otra actualizaci√≥n una vez que termine."
  },
  {
    "row": 84,
    "rowsha": "RxsWSGTgM12Md9v3+GBLg3PyoUxQ9kl1AglpanRmZqE=",
    "originContent": "I started training on a 435MB (108 M tokens) corpus, it's going pretty smooth right now. Train loss dropped from 10.9 to 4.9 in the first 2800 iterations. I expect it'll take around 8 or 9 hours to complete. I'll post another update once it's done.",
    "translatedContent": ""
  },
  {
    "row": 85,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "## 17 de julio de 2025 2:13AM"
  },
  {
    "row": 86,
    "rowsha": "CLW3lPsO6v+tT6iir338rG5+IYXM+JqRoag5w7K8exE=",
    "originContent": "## July 17th, 2025 2:13AM",
    "translatedContent": ""
  },
  {
    "row": 87,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "El entrenamiento termin√≥ para el segundo modelo, a mi 4060 le tom√≥ alrededor de 8 horas y 40 minutos (3,900 iteraciones/hora) para 33,000 iteraciones (5 √©pocas). La p√©rdida final de entrenamiento fue de 3.73. Las salidas fueron sorprendentemente buenas, ahora realmente genera oraciones coherentes al estilo del siglo XIX."
  },
  {
    "row": 88,
    "rowsha": "Q0uM34dBNqytALNUZSPxoBQZT3LxqlwyioEi3nTshXQ=",
    "originContent": "The training is done for the second model, it took my 4060 around 8 hours and 40 minutes (3,900 iters/hr) for 33,000 iters (5 epochs). Final train loss was 3.73. The outputs were suprisingly good it genuinely generates coherent 19th century style sentences now. ",
    "translatedContent": ""
  },
  {
    "row": 89,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "# Comportamiento y Limitaciones del Modelo V0"
  },
  {
    "row": 90,
    "rowsha": "YbYVsndAe75CgxPU/J34HsXOaqcQlIjTCm06o4eVQIg=",
    "originContent": "# V0 Model Behavior & Limitations ",
    "translatedContent": ""
  },
  {
    "row": 91,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "Las primeras pruebas muestran que el modelo responde con lenguaje y comportamiento de los a√±os 1800. Por ejemplo, lo prob√© con \"Who art Henry?\" y respondi√≥ \"I know that man, I have did not a black, the storm.\" y s√≠, esa oraci√≥n no tiene sentido pero el LLM est√° reconociendo que le estoy preguntando por una persona."
  },
  {
    "row": 92,
    "rowsha": "OCh97kyOITXqFpKyZsol6voS+nrzc9n9flpDpNyf35w=",
    "originContent": "Early prompts show the model responding with 1800's language and behavior. For example, I prompted it with \"Who art Henry?\" and it replied \"I know that man, I have did not a black, the storm.\" and yeah that sentence makes no sense but the LLM is recognizing I'm asking about a person. ",
    "translatedContent": ""
  },
  {
    "row": 93,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "![Salida de Muestra de TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)"
  },
  {
    "row": 94,
    "rowsha": "yKIR0teTc66wVDG+jdIyNmAzItXb2JH2ld3D7tm4qnM=",
    "originContent": "![TimeLockLLM Sample Output](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)",
    "translatedContent": ""
  },
  {
    "row": 95,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "No hay menci√≥n de conceptos modernos, las salidas contienen principalmente palabras y frases de los a√±os 1800."
  },
  {
    "row": 96,
    "rowsha": "/GR84OQ/Xp3d+Yv3/vVIIQFyl8ExKQLgjT1Go5rPplE=",
    "originContent": "There is no mention of modern concetps, outputs contain mostly words and phrasing from the 1800's.",
    "translatedContent": ""
  },
  {
    "row": 97,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "A√∫n necesita mucho trabajo, entrenar con 187 MB no te dar√° un modelo que produzca texto con razonamiento complejo."
  },
  {
    "row": 98,
    "rowsha": "aojQk3KjkX8shkLnJSZvUZBo0vk4tKDT9aysMfY9yK4=",
    "originContent": "It still needs alot of work, training off of 187MB will not give you a model that produces text with complex reasoning. ",
    "translatedContent": ""
  },
  {
    "row": 99,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "Ahora mismo produce oraciones que carecen de estructura completa y en general simplemente no tienen sentido, pero esto es normal para este tama√±o de entrenamiento."
  },
  {
    "row": 100,
    "rowsha": "SzQ3fs08W+JGBPGw6+aEzLsm/vGfQNDJ6YZnfFE0F4I=",
    "originContent": "Right now it produces sentences that lack full sentence structure and overall just make no sense but this is normal for the training size. ",
    "translatedContent": ""
  },
  {
    "row": 101,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "# Comportamiento y Limitaciones del Modelo V0.5"
  },
  {
    "row": 102,
    "rowsha": "eeaXlcgOJhwk4liDhaA6/6S4URmAdV8vj9hlqGr3LFw=",
    "originContent": "# V0.5 Model Behavior & Limitations",
    "translatedContent": ""
  },
  {
    "row": 103,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "Esta es una mejora notable en comparaci√≥n con el modelo anterior. El estilo de escritura y el vocabulario son victorianos y casi todas las oraciones son gramaticalmente correctas y con la puntuaci√≥n adecuada. Y de nuevo, este modelo est√° entrenado desde cero, por lo que se ci√±e a temas de los a√±os 1800."
  },
  {
    "row": 104,
    "rowsha": "tII9OX/M7KpPoQKPIphmQBWtsSVucUevvTnzVsTXcaI=",
    "originContent": "This is a nice improvement compared to the last model. The writing style and vocab is Victorian and almost every sentence is grammatically correct with proper punctuation. And again this is trained from scratch so it sticks to 1800's subjects. ",
    "translatedContent": ""
  },
  {
    "row": 105,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "![Salida de muestra de TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)"
  },
  {
    "row": 106,
    "rowsha": "8DhXpgpVtg05XdyplRHf49EFOQNCJVzXA9RpmJQ+y9U=",
    "originContent": "![TimeLockLLM Sample Output](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)",
    "translatedContent": ""
  },
  {
    "row": 107,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "Hay muchas alucinaciones factuales. Muchas (como el 100%) de los detalles (fechas, eventos, figuras hist√≥ricas) son inventados. Adem√°s, las oraciones realmente no tienen conexiones entre s√≠, a veces tal vez 2 oraciones se relacionan, pero m√°s all√° de eso no lo hacen. Otro problema es que a veces aparece un pie de p√°gina errante de ‚ÄúDigitized by Google‚Äù, as√≠ que la pr√≥xima vez que entrene realmente tengo que asegurarme de limpiar bien los textos. En general, estoy muy contento con los resultados; todav√≠a no es un LLM pero definitivamente es un generador de oraciones."
  },
  {
    "row": 108,
    "rowsha": "YgdLunUOAWWHsq+eAfyWcDz1fCc0rCcobnRpEXHoK94=",
    "originContent": "There are a lot of factual hallucinations. A lot (like 100%) of the details (dates, events, historical figures)  are made up. Also the sentences don't really have connections to each other, sometimes maybe 2 sentences will relate to each other but beyond that they dont. Another issue is sometimes a stray ‚ÄúDigitized by Google‚Äù footer shows up, so the next time I train I really have to make sure the texts are cleaned well. Overall I'm very happy with the results, it's nowhere near an LLM yet but definitely a sentence generator. ",
    "translatedContent": ""
  },
  {
    "row": 109,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "Estoy aprendiendo mucho y en las pr√≥ximas semanas empezar√© a averiguar qu√© necesito mejorar. ¬°Pronto subir√© archivos!"
  },
  {
    "row": 110,
    "rowsha": "V44Ne2sN8v7ZVpRZ5vo7B2aUZFjGppYthg0fjsQAqd0=",
    "originContent": "I'm learning a lot and will start figuring out what I need to do better in the coming weeks. I will upload files soon! ",
    "translatedContent": ""
  },
  {
    "row": 111,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "# Planes Futuros"
  },
  {
    "row": 112,
    "rowsha": "iMvQyl++GAiLpFMT+58v3e9/hb7zXnIJauchG5p986Y=",
    "originContent": "# Upcoming Plans ",
    "translatedContent": ""
  },
  {
    "row": 113,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "(Completado) Voy a empezar a trabajar en la versi√≥n 0.5, en lugar de entrenar usando 50 libros, entrenar√© usando idealmente 500-600. Ahora mismo estoy entrenando nanoGPT usando libros de 1800-1850 y espec√≠ficamente de Londres. Hay algunos desaf√≠os, como asegurarme de que los libros que encuentro no hayan sido actualizados o tengan interpretaciones modernas, sino libros intactos publicados dentro del periodo de tiempo que eleg√≠."
  },
  {
    "row": 114,
    "rowsha": "OIt/QAbIp2G7kt87dbAIf6Ec0U3HnwjOW6LYRmdf68I=",
    "originContent": "(Completed) I'm going to start work on version 0.5 , instead of training using 50 books, I'll train using ideally 500-600. Right now I'm training nanoGPT using books from 1800-1850 and specifically from London. There is some challeneges like making sure the books I find are not updated or have modern interpretations but untouched books published withtin my chosen time period.",
    "translatedContent": ""
  },
  {
    "row": 115,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "Quiero entrenar un nuevo modelo (v1) con un corpus mucho m√°s grande, tal vez de 5 a 10 veces mayor que el que us√© para v0.5. Mi objetivo es ver si puedo lograr que emerjan habilidades de razonamiento s√≥lo a partir del Entrenamiento Temporal Selectivo, ser√° una tarea m√°s dif√≠cil y ni siquiera estoy seguro de si es posible debido a las limitaciones de datos hist√≥ricos. En las pr√≥ximas semanas intentar√© recopilar suficiente informaci√≥n para un corpus de 5-10GB. Creo que si puedo obtener datos limpios y de alta calidad y alquilar una GPU, habr√° avances."
  },
  {
    "row": 116,
    "rowsha": "13sW7eekt8pSDdOxr/oKukwKEOVmw/M8xiAJC1VFXZc=",
    "originContent": "I want to train a new model (v1) with a much larger corpus, maybe 5-10x larger than the one I used for v0.5. My goal is to see if I can get reasoning abilities to emerge from Selective Temporal Training alone, this will be a more difficult task and I'm not even entirely sure if it's possible due to the fact that there is historical data limitations. In the upcoming weeks I will try to curate enough data for a 5-10GB corpus. I believe If I can get clean clean high quality data and rent a GPU, there will be progress.",
    "translatedContent": ""
  },
  {
    "row": 117,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "# C√≥mo Usar Este Proyecto"
  },
  {
    "row": 118,
    "rowsha": "sO+voevLpUtEbqum6Gntntle+nPVa66c5GATAMLrgf0=",
    "originContent": "# How to Use This Project ",
    "translatedContent": ""
  },
  {
    "row": 119,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "Este proyecto se centra principalmente en recopilar datos hist√≥ricos, prepararlos para el entrenamiento y construir un tokenizador. No voy a cubrir todo el proceso de entrenamiento de un LLM; para eso, consulta nanoGPT de Andrej Karpathy."
  },
  {
    "row": 120,
    "rowsha": "XVoXr9uzZwN09vboETojEQJe057RBzcMUjXmQRCB/jo=",
    "originContent": "This project focuses mostly on curating historical data, preparing it for training and building a tokenizer. I am not going to cover the full LLM training process, for that refer to nanoGPT by Andrej Karpathy.",
    "translatedContent": ""
  },
  {
    "row": 121,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "# Paso 1: Reunir y Preparar Textos Hist√≥ricos"
  },
  {
    "row": 122,
    "rowsha": "kDK7XtqFkTiZvD804yYG4VEojvLrRbdEhmHEBzDAQz4=",
    "originContent": "# Step 1: Gather and Prepare Historical Texts ",
    "translatedContent": ""
  },
  {
    "row": 123,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "Re√∫ne archivos .txt de libros de dominio p√∫blico, documentos, etc. del periodo de tiempo elegido (por ejemplo, Londres 1800-1850)"
  },
  {
    "row": 124,
    "rowsha": "oRoOYaG+mx3SqTnhGcOKeH7W3W4wSRQmhZS4jPGMH8s=",
    "originContent": "Collect .txt files of public domain books, documents, etc from your chosen time period (e.g., London 1800-1850)",
    "translatedContent": ""
  },
  {
    "row": 125,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "Puedes usar download_texts_improved.py para descargar libros si lo necesitas."
  },
  {
    "row": 126,
    "rowsha": "EBDhZvTogrL4hqRuH095O7/tXXoy2OEskLQlang/pOA=",
    "originContent": "You can use download_texts_improved.py to download books for you if you need to.",
    "translatedContent": ""
  },
  {
    "row": 127,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "Limpia los archivos de texto usando un script o elimina manualmente encabezados/pies de p√°gina de Project Gutenberg, anotaciones modernas o errores de OCR."
  },
  {
    "row": 128,
    "rowsha": "q5OO8x+9kIbzaQRWimI/Vo9ZowzVBtCX+TodObLsQoY=",
    "originContent": "Clean the text files using a script or manually remove headers/footer from Project Gutenberg, Modern annotations or things like OCR errors.",
    "translatedContent": ""
  },
  {
    "row": 129,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "prepare_dataset.py deber√≠a funcionar bien."
  },
  {
    "row": 130,
    "rowsha": "N4wsIjC0LRlClodmfCtYX3qswttJnk0psU28/mlCRTw=",
    "originContent": "prepare_dataset.py should work fine.",
    "translatedContent": ""
  },
  {
    "row": 131,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "# Paso 2: Construir un Tokenizador Personalizado"
  },
  {
    "row": 132,
    "rowsha": "jDM2lr7pP+MT0pt+L0cd5nBXI83IPoT27NzIgplt7R8=",
    "originContent": "# Step 2: Build a Custom Tokenizer",
    "translatedContent": ""
  },
  {
    "row": 133,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "Ejecuta train_tokenizer.py o train_tokenizer_hf.py sobre los datos limpios."
  },
  {
    "row": 134,
    "rowsha": "+90EorsgO/X2bFK2pYJw+vZIpjIbuMm4QR6W/xfj8C8=",
    "originContent": "Run train_tokenizer.py or train_tokenizer_hf.py on the cleaned data.",
    "translatedContent": "Esto te dar√° vocab.json y merges.txt"
  },
  {
    "row": 135,
    "rowsha": "tkP3Eg1rWphTQMNhN2yYg/1+AA1IdcXbGT96aRMpnwc=",
    "originContent": "This will give you vocab.json and merges.txt",
    "translatedContent": ""
  },
  {
    "row": 136,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "Estos archivos definen el vocabulario y las reglas de fusi√≥n para tu modelo"
  },
  {
    "row": 137,
    "rowsha": "/wqxgtOu72+x3a2xi7q23jDkx+WQv2SHrJddzpvm1Ys=",
    "originContent": "Thes files define vocab and merge rules for your model",
    "translatedContent": ""
  },
  {
    "row": 138,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "# Paso 3: Entrena Tu Modelo (nanoGPT)"
  },
  {
    "row": 139,
    "rowsha": "vKPAEsPxc9uYtzjTX9/rNADSDnxkKwYcYpX/aiAp8Hc=",
    "originContent": "# Step 3: Train Your Model (nanoGPT) ",
    "translatedContent": ""
  },
  {
    "row": 140,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "Consulta [nanoGPT de Andrej Karpathy](https://github.com/karpathy/nanoGPT) para el proceso de entrenamiento."
  },
  {
    "row": 141,
    "rowsha": "tCDY5iXt+Z7YYeTPouMSYDX5uuFnGROxZMvHyTOIblY=",
    "originContent": "Refer to [nanoGPT by Andrej Karpathy](https://github.com/karpathy/nanoGPT) for the training process.",
    "translatedContent": ""
  },
  {
    "row": 142,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "Puedes entrenar un LLM diferente si quieres, pero yo us√© nanoGPT"
  },
  {
    "row": 143,
    "rowsha": "bLpr9snECDSJ5ejrqDWYNT8VXSupCQKQxyLxvAmQ2Kc=",
    "originContent": "You can train a different LLM if you want, but I used nanoGPT ",
    "translatedContent": ""
  },
  {
    "row": 144,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "# Preguntas Frecuentes"
  },
  {
    "row": 145,
    "rowsha": "OoCxyGfPN5TmdzAkaPphtPx303MJJ7vpfWbKrufGH5g=",
    "originContent": "# FAQ",
    "translatedContent": ""
  },
  {
    "row": 146,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "## ¬øQu√© es el Entrenamiento Temporal Selectivo?"
  },
  {
    "row": 147,
    "rowsha": "+5dDgPw4ILEotxso4tjjjz1cxwUei16yNQPDUKbgxoo=",
    "originContent": "## What is Selective Temporal Training ?",
    "translatedContent": ""
  },
  {
    "row": 148,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "El Entrenamiento Temporal Selectivo (STT) es una metodolog√≠a de aprendizaje autom√°tico donde todos los datos de entrenamiento son seleccionados espec√≠ficamente para que caigan dentro de un periodo hist√≥rico concreto. Se hace para modelar el lenguaje y el conocimiento de esa √©poca sin la influencia de conceptos modernos. Por ejemplo, el modelo que tengo ahora (v0.5) est√° entrenado con datos exclusivamente de 1800-1875; no est√° afinado, sino entrenado desde cero, lo que da como resultado una salida que refleja el estilo ling√º√≠stico y el contexto hist√≥rico de ese periodo."
  },
  {
    "row": 149,
    "rowsha": "hooEARKH4r/sDPh7JUtZAZ6TYMvBkTLZIcfw3g83xos=",
    "originContent": "Selective Temporal Training (STT) is a machine learning methodology where all training data is specifically curated to fall within a specific historical time period. It's done in order to model the language and knowledge of that era without influence from modern concepts. For example, the current model I have now (v0.5) is trained on data exclusively from 1800-1875, it's not fine tuned but trained from scratch resulting in output that reflects the linguistic style and historical context of that time period.",
    "translatedContent": ""
  },
  {
    "row": 150,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "## ¬øPor qu√© no simplemente usar fine-tuning o LoRA?"
  },
  {
    "row": 151,
    "rowsha": "dVMKQ2mPI1Spc6x6r/jNG0PIR5YKpalU4MXx9JmKp/I=",
    "originContent": "## Why not just use fine-tuning or LoRA?",
    "translatedContent": ""
  },
  {
    "row": 152,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "Para este proyecto intento crear un modelo de lenguaje sin sesgos modernos. Si hago fine-tuning a algo como GPT-2, ya est√° preentrenado y esa informaci√≥n no se elimina. Si entreno desde cero, el modelo de lenguaje no fingir√° ser antiguo, simplemente lo ser√°. El objetivo de este proyecto ahora es crear algo que pueda razonar exclusivamente usando conocimiento de libros de Londres publicados entre 1800 y 1850."
  },
  {
    "row": 153,
    "rowsha": "oNvWlJHtQSyq1TwlqJyGtMzk4Z4mBIn8AW2SudzvUYs=",
    "originContent": "For this project I'm trying to create a language model that is unclouded from modern bias. If I fine-tune something like GPT-2, it's already pre-trained and that information won't go away. If I train from scratch the language model won't pretend to be old, it just will be. The Goal for this project right now is to create something can reason exclusively using knowledge from London books published between 1800 and 1850.",
    "translatedContent": ""
  },
  {
    "row": 154,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "## ¬øQu√© tipo de datos usaste para el entrenamiento?"
  },
  {
    "row": 155,
    "rowsha": "ByP4WlNmMoG6WIiLJNd6b080/DSciCgWmj9aYSJjAF0=",
    "originContent": "## What kind of data did you use for training?",
    "translatedContent": ""
  },
  {
    "row": 156,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "Estoy usando libros, documentos legales, peri√≥dicos y otros escritos de Londres entre 1800 y 1850. La lista que enlac√© tiene como 200, pero para el primer entrenamiento s√≥lo us√© 50 archivos de unos ~187 MB. Puedes ver una lista de los documentos:"
  },
  {
    "row": 157,
    "rowsha": "Kj6EF7wZdUrAFg4ErGmJuh9Q5Xujmb+tunpfssPKXkA=",
    "originContent": "I'm using books, legal documents, newspapers, and other writings from 1800‚Äì1850 London. The list I linked has like 200 but for the first training I just used 50 files about ~187 MB. You can view a list of the documents:",
    "translatedContent": "https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt"
  },
  {
    "row": 158,
    "rowsha": "0mxyGiLJxzp9JPCg1oA+nbIwAKJbEC4ei9kSV3Gp84Y=",
    "originContent": "https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt",
    "translatedContent": ""
  },
  {
    "row": 159,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "## ¬øQu√© tama√±o tiene el modelo Versi√≥n 0?"
  },
  {
    "row": 160,
    "rowsha": "RiLgksbH2sYLZRyKsFhVyfjC6nNxLsxWsc7XnFS061A=",
    "originContent": "## How large is the Version 0 model ?",
    "translatedContent": ""
  },
  {
    "row": 161,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "Este modelo es muy peque√±o por ahora, s√≥lo lo hago por diversi√≥n y siguiendo una estricta regla de entrenamiento sin fuentes modernas. Tiene casi 16 millones de par√°metros, pero voy a empezar a recopilar m√°s textos antiguos para iniciar otro entrenamiento de modelo. Ir√© dando actualizaciones a medida que avance."
  },
  {
    "row": 162,
    "rowsha": "55Ce1mBEzOreC728R5HRfuMIC/nZRg3Zcr87GVkjVVY=",
    "originContent": "This model is very small right now, I'm just doing this for fun and following a strict training rule of no modern sources. It has almost 16 million parameters but I'm gonna start gathering more old texts to begin another model training. Will give updates as I go.",
    "translatedContent": ""
  },
  {
    "row": 163,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "## ¬øEspecificaciones de entrenamiento?"
  },
  {
    "row": 164,
    "rowsha": "A40eQ7ZJiqr+bs9cl0Cb4QKKuS9z7/PA1ZaGn1TSehI=",
    "originContent": "## Training Specs ? ",
    "translatedContent": ""
  },
  {
    "row": 165,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "GPU: Geforce rtx 4060"
  },
  {
    "row": 166,
    "rowsha": "EH8H1HW/C4Tb7LfJgVUnVGsk4pF9l40Rlev8tAkKhjI=",
    "originContent": "GPU: Geforce rtx 4060",
    "translatedContent": "CPU: i5-13400F"
  },
  {
    "row": 167,
    "rowsha": "vo3FdN37kY6VUB7PruRKfBPJDgsVJyBHIUCn/g8mt68=",
    "originContent": "CPU: i5-13400F ",
    "translatedContent": "Ram: 16GB DDR5."
  },
  {
    "row": 168,
    "rowsha": "W8fXPiQKUkoNso0PPfTvjYMy0IYo85j+gNXmB0aERO4=",
    "originContent": "Ram: 16GB DDR5.",
    "translatedContent": ""
  },
  {
    "row": 169,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  }
]