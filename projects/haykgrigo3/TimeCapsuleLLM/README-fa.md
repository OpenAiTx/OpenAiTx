<div align="right">
  <details>
    <summary >🌐 زبان</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">انگلیسی</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">简体中文</a>
        | <a href="#" title="Coming soon">繁體中文 (به زودی)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">ژاپنی</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">کره‌ای</a>
        | <a href="#" title="Coming soon">هندی (به زودی)</a> |
        | <a href="#" title="Coming soon">تایلندی (به زودی)</a> |
        | <a href="#" title="Coming soon">فرانسوی (به زودی)</a>
        | <a href="#" title="Coming soon">آلمانی (به زودی)</a>
        | <a href="#" title="Coming soon">اسپانیایی (به زودی)</a>
        | <a href="#" title="Coming soon">ایتالیایی (به زودی)</a>
        | <a href="#" title="Coming soon">روسی (به زودی)</a>
        | <a href="#" title="Coming soon">پرتغالی (به زودی)</a>
        | <a href="#" title="Coming soon">هلندی (به زودی)</a>
        | <a href="#" title="Coming soon">لهستانی (به زودی)</a>
        | <a href="#" title="Coming soon">عربی (به زودی)</a>
        | <a href="#" title="Coming soon">فارسی (به زودی)</a>
        | <a href="#" title="Coming soon">ترکی (به زودی)</a>
        | <a href="#" title="Coming soon">ویتنامی (به زودی)</a>
        | <a href="#" title="Coming soon">اندونزیایی (به زودی)</a>

      </div>
    </div>
  </details>
</div>

# تایم‌کپسول LLM
یک مدل زبان بزرگ که تنها بر اساس داده‌های دوره‌های زمانی مشخص آموزش داده شده است تا سوگیری مدرن را کاهش دهد.

تصور کنید یک مدل هوش مصنوعی نه فقط وانمود کند تاریخی است، بلکه واقعا همینطور باشد.

بر پایه [nanoGPT نوشته آندری کارپاتی](https://github.com/karpathy/nanoGPT) اسکریپت‌های اصلی آموزش و معماری مدل متعلق به اوست.

# اهداف پروژه

تایم‌کپسول LLM یک پروژه آزمایشی است که تنها با متون نوشته شده در بازه‌های زمانی مشخص آموزش خواهد دید. هدف، شبیه‌سازی جهان‌بینی و زبان دوران‌های تاریخی خاص است.

# چرا صرفا تنظیم دقیق کافی نیست

اگر فقط یک مدل از پیش‌آموزش‌دیده را تنظیم دقیق کنید، مدل شما همچنان مفاهیم مدرن را خواهد دانست. البته رسیدن به سوگیری کاملا صفر مدرن دشوار است اما من می‌خواهم تا حد ممکن به این هدف نزدیک شوم. برای حذف کامل سوگیری مدرن، باید مدل را از ابتدا آموزش داد.

# نتایج مورد انتظار

امیدوارم وقتی مدل تکمیل شد، دیگر مفاهیم مدرن را نشناسد و نتواند فراتر از آنچه آموزش دیده است، استدلال کند. نباید واژگان/مفاهیم مدرن را بشناسد و امیدوارم دانش مدرن را به اشتباه تولید نکند.

# به‌روزرسانی‌های پیشرفت

## ۹ ژوئیه ۲۰۲۵

دوره زمانی خود را بین ۱۸۰۰ تا ۱۸۵۰ و منطقه: لندن تعیین کردم

فهرستی از متون، کتاب‌ها و اسناد جمع‌آوری کردم

تا کنون ۵۰ فایل متنی به دست آورده‌ام و به زودی آموزش NanoGPT را شروع خواهم کرد

تا زمانی که پیشرفتی حاصل شود، این بخش را به‌روزرسانی می‌کنم

## ۱۳ ژوئیه ۲۰۲۵

nanoGPT را با ۱۸۷ مگابایت داده متنی تاریخی آموزش دادم.

## ۱۵ ژوئیه ۲۰۲۵

شروع به دانلود متون برای دومین دور آموزش کردم. همه چیز را از Internet Archive می‌گیرم و دوره زمانی را به ۱۸۰۰ تا ۱۸۷۵ گسترش داده‌ام. برای داشتن طیف متنوعی از متون، می‌توانید از فیلترهای موضوعی و جستجو برای محل انتشار، بازه زمانی و موضوعات در Internet Archive استفاده کنید.

![فیلترهای جستجو](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## ۱۶ ژوئیه ۲۰۲۵

حدود ۵۰۰ فایل متنی از Internet Archive دانلود کردم و بعد از پاکسازی (حذف فاصله‌ها، هدرهای گوتنبرگ و غیره) حدود ۵۰۰ مگابایت داده دارم. دیتاست کوچکی است اما بار قبل با ۱۸۷ مگابایت آموزش دادم، پس باید حداقل تفاوت محسوسی در خروجی مدل دوم وجود داشته باشد. امیدوارم این مدل بتواند جملات منسجم‌تری که تا حدی قابل فهم باشند تولید کند. البته تضمینی نیست چون دیتاست هنوز خیلی کوچک است، اما از دفعه قبل بیشتر است.

این کار روی سخت‌افزار خودم قابل انجام است، که خوب است چون می‌توانم امیدوار باشم قبل از رفتن به سراغ دیتاست بزرگ‌تر که نیازمند اجاره GPU است، برخی پیشرفت‌ها را ببینم. اما نگران نباشید هنوز قصد دارم به زودی GPU اجاره کنم، اما پیش از آن می‌خواهم مطمئن شوم دیتاست من تا حد امکان گزینش‌شده و پاک است. یکی از مشکلاتم پاکسازی است، خیلی از این فایل‌های متنی شامل مطالب بی‌معنی هستند. اسکریپت‌هایی که برای پاکسازی استفاده کرده‌ام کار می‌کنند اما ۱۰۰٪ موثر نیستند.

امروز این دیتاست را آموزش خواهم داد و باید حدود ۴ تا ۵ ساعت طول بکشد. وقتی تمام شد و تست کردم، به‌روزرسانی خواهم داد. دوباره از همه کسانی که پروژه من را بررسی می‌کنند متشکرم، حتی برخی افراد منابع OCR را هم معرفی کردند پس متشکرم! امیدوارم افراد بیشتری این کار را امتحان کنند و با دیتاست‌های خودشان آزمایش کنند.

## ۲۸ ژوئیه ۲۰۲۵

نسخه v0.5 را در Hugging Face آپلود کرده‌ام، [اینجا ببینید](https://huggingface.co/haykgrigorian/TimeCapsuleLLM) اگر علاقه دارید. حالا می‌توانید مخزن من را دانلود و به صورت محلی اجرا کنید. متاسفانه nanoGPT به طور بومی با HuggingFace کار نمی‌کند، پس باید مدل را دانلود و به صورت محلی اجرا کنید.

همچنین شروع به گردآوری داده برای آموزش بعدی کرده‌ام، فکر می‌کنم برای دستیابی به توانایی استدلال به ۵ تا ۱۰ برابر داده بیشتر نیاز دارم.

### به‌روزرسانی آموزش

آموزش را روی یک کورپوس ۴۳۵ مگابایتی (۱۰۸ میلیون توکن) آغاز کردم، فعلاً روند آموزش روان پیش می‌رود. میزان loss آموزش از ۱۰.۹ به ۴.۹ در ۲۸۰۰ تکرار اول کاهش یافت. انتظار دارم حدود ۸ یا ۹ ساعت طول بکشد تا کامل شود. پس از اتمام، به‌روزرسانی دیگری خواهم داد.

## ۱۷ ژوئیه ۲۰۲۵ ساعت ۲:۱۳ بامداد

آموزش مدل دوم تمام شد، کارت گرافیک ۴۰۶۰ من حدود ۸ ساعت و ۴۰ دقیقه (۳۹۰۰ تکرار در ساعت) برای ۳۳,۰۰۰ تکرار (۵ دوره) زمان برد. loss نهایی آموزش ۳.۷۳ بود. خروجی‌ها فوق‌العاده خوب بودند و اکنون واقعاً جملات منسجم به سبک قرن نوزدهم تولید می‌کند.

# رفتار و محدودیت‌های مدل نسخه v0

در ابتدای آزمایش، مدل با زبان و رفتار قرن ۱۸۰۰ پاسخ می‌دهد. مثلاً به آن نوشتم "Who art Henry?" و پاسخ داد: "I know that man, I have did not a black, the storm." که البته جمله بی‌معنی است اما مدل تشخیص می‌دهد که درباره یک شخص سؤال شده است.

![نمونه خروجی TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

هیچ اشاره‌ای به مفاهیم مدرن نشده است، خروجی‌ها عمدتاً شامل واژه‌ها و عبارات متعلق به قرن نوزدهم هستند.

هنوز نیاز به کار زیادی دارد، آموزش با ۱۸۷ مگابایت به شما مدلی نخواهد داد که متنی با استدلال پیچیده تولید کند.

در حال حاضر جملاتی تولید می‌کند که ساختار کامل جمله را ندارند و به طور کلی بی‌معنی هستند، اما این برای این حجم آموزش طبیعی است.

# رفتار مدل V0.5 و محدودیت‌ها

این نسبت به مدل قبلی بهبود خوبی است. سبک نگارش و واژگان ویکتوریایی است و تقریباً هر جمله از نظر دستوری صحیح و با نقطه‌گذاری درست است. و باز هم این مدل از ابتدا آموزش دیده پس به موضوعات قرن نوزدهم پایبند است.

![نمونه خروجی TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

خطاهای واقعی زیادی وجود دارد. بسیاری (تقریباً ۱۰۰٪) از جزئیات (تاریخ‌ها، رویدادها، شخصیت‌های تاریخی) ساخته شده‌اند. همچنین جملات واقعاً به هم مرتبط نیستند، گاهی شاید دو جمله به هم مربوط باشند اما فراتر از آن نه. مشکل دیگر این است که گاهی یک پاورقی پراکنده مانند “Digitized by Google” ظاهر می‌شود، بنابراین دفعه بعد باید مطمئن شوم که متن‌ها خوب پاک‌سازی شده‌اند. در کل از نتایج بسیار راضی هستم، هنوز به مدل زبانی نزدیک نیست اما قطعاً یک جمله‌ساز است.

خیلی چیزها یاد می‌گیرم و در هفته‌های آینده شروع به فهمیدن اینکه چه باید بهتر انجام دهم می‌کنم. به زودی فایل‌ها را آپلود می‌کنم!

# برنامه‌های آینده

(انجام شده) قرار است کار روی نسخه ۰.۵ را شروع کنم، به جای آموزش با ۵۰ کتاب، ایده‌آل است با ۵۰۰-۶۰۰ کتاب آموزش ببینم. در حال حاضر nanoGPT را با کتاب‌هایی از سال‌های ۱۸۰۰ تا ۱۸۵۰ و به طور خاص از لندن آموزش می‌دهم. چالش‌هایی مثل اطمینان از اینکه کتاب‌هایی که پیدا می‌کنم به‌روزرسانی نشده‌اند یا تفسیرهای مدرن ندارند ولی کتاب‌های دست‌نخورده در بازه زمانی انتخابی من منتشر شده‌اند وجود دارد.

می‌خواهم مدلی جدید (v1) با مجموعه داده‌ای بسیار بزرگ‌تر، شاید ۵ تا ۱۰ برابر بزرگ‌تر از آنچه برای v0.5 استفاده کردم آموزش دهم. هدف من این است که ببینم آیا می‌توانم فقط با آموزش زمانی انتخابی، توانایی استدلال را ظاهر کنم، این کار دشوارتر خواهد بود و حتی مطمئن نیستم که ممکن باشد، چون محدودیت داده تاریخی وجود دارد. در هفته‌های آینده تلاش می‌کنم داده کافی برای یک مجموعه ۵ تا ۱۰ گیگابایتی گردآوری کنم. باور دارم اگر داده پاک و باکیفیت تهیه کنم و یک GPU اجاره کنم، پیشرفت حاصل خواهد شد.

# نحوه استفاده از این پروژه

این پروژه عمدتاً بر گردآوری داده‌های تاریخی، آماده‌سازی آن برای آموزش و ساخت یک توکنایزر تمرکز دارد. قرار نیست کل فرآیند آموزش مدل زبانی را پوشش دهم، برای آن به nanoGPT نوشته Andrej Karpathy مراجعه کنید.

# گام ۱: گردآوری و آماده‌سازی متون تاریخی

فایل‌های .txt کتاب‌ها، اسناد و ... دامنه عمومی از بازه زمانی انتخابی خود (مثلاً لندن ۱۸۰۰-۱۸۵۰) جمع‌آوری کنید.

در صورت نیاز می‌توانید از download_texts_improved.py برای دانلود کتاب‌ها استفاده کنید.

فایل‌های متنی را با اسکریپت یا به صورت دستی پاک‌سازی کنید؛ سرصفحه/پاورقی پروژه گوتنبرگ، یادداشت‌های مدرن یا خطاهای OCR را حذف کنید.

prepare_dataset.py باید به خوبی کار کند.

# گام ۲: ساخت توکنایزر اختصاصی

train_tokenizer.py یا train_tokenizer_hf.py را روی داده پاک اجرا کنید.
این کار vocab.json و merges.txt را به شما می‌دهد.

این فایل‌ها واژگان و قواعد ادغام مدل شما را تعریف می‌کنند.

# گام ۳: آموزش مدل خود (nanoGPT)

برای فرآیند آموزش به [nanoGPT نوشته Andrej Karpathy](https://github.com/karpathy/nanoGPT) مراجعه کنید.

اگر خواستید می‌توانید مدل زبانی دیگری آموزش دهید، اما من از nanoGPT استفاده کردم.

# پرسش‌های متداول

## آموزش زمانی انتخابی چیست؟

آموزش زمانی انتخابی (Selective Temporal Training یا STT) روشی در یادگیری ماشین است که در آن تمام داده‌های آموزشی به طور خاص برای قرار گرفتن در یک بازه زمانی تاریخی معین گردآوری می‌شوند. این کار برای مدل‌سازی زبان و دانش آن دوران بدون تأثیر مفاهیم مدرن انجام می‌شود. مثلاً مدل فعلی من (v0.5) فقط با داده‌های ۱۸۰۰-۱۸۷۵ آموزش دیده، از ابتدا آموزش داده شده و خروجی آن سبک زبانی و زمینه تاریخی آن دوره را منعکس می‌کند.

## چرا فقط از فاین‌تیون یا LoRA استفاده نمی‌کنید؟

در این پروژه سعی دارم مدل زبانی بسازم که تحت تأثیر سوگیری مدرن نباشد. اگر چیزی مثل GPT-2 را فاین‌تیون کنم، از قبل آموزش دیده و آن اطلاعات حذف نخواهد شد. اگر از ابتدا آموزش دهم، مدل زبانی وانمود نمی‌کند قدیمی است، واقعاً قدیمی خواهد بود. هدف فعلی پروژه این است که چیزی بسازم که فقط با دانش کتاب‌های لندن منتشرشده بین ۱۸۰۰ تا ۱۸۵۰ استدلال کند.

## از چه نوع داده‌ای برای آموزش استفاده کردید؟

از کتاب‌ها، اسناد حقوقی، روزنامه‌ها و سایر نوشته‌های لندن ۱۸۰۰ تا ۱۸۵۰ استفاده می‌کنم. فهرستی که پیوند دادم حدود ۲۰۰ مورد دارد اما برای آموزش اول فقط از ۵۰ فایل حدوداً ۱۸۷ مگابایتی استفاده کردم. می‌توانید فهرست اسناد را ببینید:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## اندازه مدل نسخه صفر چقدر است؟

این مدل الان خیلی کوچک است، فقط برای سرگرمی و با رعایت قانون آموزش بدون منابع مدرن این کار را می‌کنم. تقریباً ۱۶ میلیون پارامتر دارد اما قصد دارم متون قدیمی بیشتری جمع‌آوری کنم تا آموزش مدل دیگری را شروع کنم. به‌روزرسانی‌ها را اعلام می‌کنم.

## مشخصات آموزش؟

GPU: Geforce rtx 4060
CPU: i5-13400F
رم: ۱۶ گیگابایت DDR5.



---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-29

---