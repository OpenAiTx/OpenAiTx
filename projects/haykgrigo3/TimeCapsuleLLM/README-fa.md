<div align="right">
  <details>
    <summary >🌐 زبان</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">انگلیسی</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">简体中文</a>
        | <a href="#" title="Coming soon">繁體中文 (به زودی)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">ژاپنی</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">کره‌ای</a>
        | <a href="#" title="Coming soon">هندی (به زودی)</a> |
        | <a href="#" title="Coming soon">تایلندی (به زودی)</a> |
        | <a href="#" title="Coming soon">فرانسوی (به زودی)</a>
        | <a href="#" title="Coming soon">آلمانی (به زودی)</a>
        | <a href="#" title="Coming soon">اسپانیایی (به زودی)</a>
        | <a href="#" title="Coming soon">ایتالیایی (به زودی)</a>
        | <a href="#" title="Coming soon">روسی (به زودی)</a>
        | <a href="#" title="Coming soon">پرتغالی (به زودی)</a>
        | <a href="#" title="Coming soon">هلندی (به زودی)</a>
        | <a href="#" title="Coming soon">لهستانی (به زودی)</a>
        | <a href="#" title="Coming soon">عربی (به زودی)</a>
        | <a href="#" title="Coming soon">فارسی (به زودی)</a>
        | <a href="#" title="Coming soon">ترکی (به زودی)</a>
        | <a href="#" title="Coming soon">ویتنامی (به زودی)</a>
        | <a href="#" title="Coming soon">اندونزیایی (به زودی)</a>

      </div>
    </div>
  </details>
</div>

# تایم‌کپسول ال‌ال‌ام
یک مدل زبانی بزرگ که فقط با داده‌های دوره‌های زمانی خاص آموزش داده شده تا سوگیری مدرن را کاهش دهد.

تصور کنید اگر یک مدل هوش مصنوعی فقط نقش تاریخی را بازی نمی‌کرد، بلکه واقعاً تاریخی بود.

بر پایه [nanoGPT اثر آندری کارپاتی](https://github.com/karpathy/nanoGPT) اسکریپت‌های اصلی آموزش و معماری مدل از ایشان است.

# اهداف پروژه

تایم‌کپسول ال‌ال‌ام یک پروژه آزمایشی است که فقط با متونی آموزش داده می‌شود که در دوره‌های زمانی خاص نوشته شده‌اند. هدف، شبیه‌سازی جهان‌بینی و زبان دوره‌های تاریخی معین است.

# چرا تنظیم دقیق کافی نیست

اگر فقط یک مدل آموزش‌دیده را تنظیم دقیق کنید، مدل شما همچنان مفاهیم مدرن را خواهد دانست. البته رسیدن به صفر سوگیری مدرن دشوار است اما من می‌خواهم تا حد ممکن به این هدف نزدیک شوم. برای حذف کامل سوگیری مدرن باید مدل را از ابتدا آموزش داد.

# نتایج مورد انتظار

امیدوارم وقتی مدل کامل شد، دانشی از مفاهیم مدرن نداشته باشد و نتواند فراتر از آموزش‌هایش استدلال کند. نباید مفاهیم/واژگان مدرن را تشخیص دهد و امیدوارم دانش مدرن را جعل نکند.

# به‌روزرسانی‌های پیشرفت

## ۹ ژوئیه ۲۰۲۵

دوره زمانی من ۱۸۰۰-۱۸۵۰ و منطقه: لندن تعیین شد

فهرستی از متون، کتاب‌ها و اسناد جمع‌آوری کرده‌ام

تاکنون ۵۰ متن را به صورت فایل txt تهیه کرده‌ام و به زودی آموزش NanoGPT را آغاز خواهم کرد

تا زمانی که پیشرفت حاصل شود، این بخش را به‌روزرسانی می‌کنم

## ۱۳ ژوئیه ۲۰۲۵

مدل nanoGPT را با ۱۸۷ مگابایت داده متنی تاریخی آموزش دادم.

## ۱۵ ژوئیه ۲۰۲۵

برای دور دوم آموزش شروع به دانلود متون کردم. همه چیز را از Internet Archive دریافت می‌کنم و بازه زمانی را به ۱۸۰۰-۱۸۷۵ گسترش دادم. برای تنوع بیشتر متون، می‌توانید از فیلترهای موضوع و جستجو برای محل انتشار، بازه زمانی و موضوعات در Internet Archive استفاده کنید.

![فیلترهای جستجو](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## ۱۶ ژوئیه ۲۰۲۵

حدود ۵۰۰ فایل txt از Internet Archive دانلود کردم و پس از پاک‌سازی (حذف فاصله‌های اضافی، هدرهای گوتنبرگ و غیره) حدود ۵۰۰ مگابایت داده دارم. این یک مجموعه داده کوچک است، اما دفعه قبل فقط با ۱۸۷ مگابایت آموزش دادم، پس باید دست‌کم تفاوت قابل توجهی در خروجی پس از آموزش مدل دوم مشاهده شود. امیدوارم این مدل بتواند جملات منسجم‌تری تولید کند که تا حدی معنادار باشند. البته تضمینی نیست چون هنوز هم داده‌ها خیلی کم هستند، اما از دفعه قبل بیشتر است.

این کار باید روی سخت‌افزار خودم قابل انجام باشد، این هم خوب است چون می‌توانم قبل از رفتن به سمت مجموعه داده بزرگ‌تر که نیاز به اجاره GPU دارد، برخی بهبودها را ببینم. اما نگران نباشید، هنوز برنامه دارم به زودی GPU اجاره کنم، ولی قبل از آن می‌خواهم مطمئن شوم داده‌هایم تا حد امکان پاک و گزینش شده‌اند. یکی از مشکلاتم پاک‌سازی است، بسیاری از این فایل‌های txt مخلوطی از آشفتگی دارند. اسکریپت‌هایی که برای پاک‌سازی استفاده کردم تا حدی کار می‌کنند اما صد درصد موثر نیستند.

امروز این مجموعه داده را آموزش می‌دهم و باید حدود ۴-۵ ساعت طول بکشد. وقتی تمام شد و تست کردم، به‌روزرسانی خواهم داد. دوباره از همه کسانی که پروژه‌ام را بررسی می‌کنند ممنونم، حتی بعضی‌ها منابع OCR را به من معرفی کردند، پس متشکرم! امیدوارم افراد بیشتری این کار را امتحان کنند و با داده‌های خودشان آزمایش کنند.


### به‌روزرسانی آموزش

آموزش را با یک مجموعه ۴۳۵ مگابایتی (۱۰۸ میلیون توکن) آغاز کردم، فعلاً روند خوبی دارد. خطای آموزش از ۱۰.۹ به ۴.۹ در ۲۸۰۰ تکرار اول کاهش یافت. انتظار دارم حدود ۸ یا ۹ ساعت طول بکشد تا کامل شود. پس از اتمام، یک به‌روزرسانی دیگر منتشر می‌کنم.

## ۱۷ ژوئیه ۲۰۲۵

آموزش مدل دوم تمام شد، کارت گرافیک ۴۰۶۰ من در حدود ۸ ساعت و ۴۰ دقیقه (۳۹۰۰ تکرار/ساعت) برای ۳۳,۰۰۰ تکرار (۵ اپوک) آن را انجام داد. خطای آموزش نهایی ۳.۷۳ بود. خروجی‌ها به طرز شگفت‌انگیزی خوب بودند و واقعاً جملات منسجم به سبک قرن نوزدهم تولید می‌کند.

## ۲۸ ژوئیه ۲۰۲۵

نسخه ۰.۵ را در Hugging Face آپلود کردم، [اینجا ببینید](https://huggingface.co/haykgrigorian/TimeCapsuleLLM) اگر دوست دارید. حالا می‌توانید مخزن من را دانلود و به صورت محلی اجرا کنید. متأسفانه nanoGPT به طور بومی با HuggingFace کار نمی‌کند، پس باید مدل را دانلود و به صورت محلی اجرا کنید.

همچنین به زودی شروع به گزینش داده برای دور بعدی آموزش می‌کنم، فکر می‌کنم برای دستیابی به قابلیت استدلال به ۵ تا ۱۰ برابر داده بیشتر نیاز دارم.

## ۲ اوت ۲۰۲۵

به زودی کار روی نسخه ۱ را آغاز خواهم کرد. باید از معماری nanoGPT به چیزی مدرن‌تر مهاجرت کنم. چند معماری LLM متن‌باز مد نظر دارم، از جمله: OpenLLaMA v3، Phi-2 و Qwen 1.5B. و برای جهش به نسخه ۱، باید یک مجموعه داده بسیار بزرگ‌تر و متنوع‌تر را با دقت گزینش کنم. دست‌کم به ۵ گیگابایت داده آموزشی پاک نیاز دارم.


# رفتار و محدودیت‌های مدل V0

دستورهای اولیه نشان می‌دهند که مدل با زبان و رفتار قرن ۱۸۰۰ پاسخ می‌دهد. برای مثال، من با جمله «چه کسی هنری است؟» از آن پرسیدم و جواب داد «من آن مرد را می‌شناسم، هرگز سیاه نکرده‌ام، طوفان.» و بله این جمله بی‌معنی است اما مدل LLM متوجه می‌شود که درباره یک شخص سؤال پرسیده‌ام.

![نمونه خروجی TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

هیچ اشاره‌ای به مفاهیم مدرن وجود ندارد، خروجی‌ها عمدتاً شامل واژگان و عبارات متعلق به قرن ۱۸۰۰ هستند.

هنوز به کار زیادی نیاز دارد، آموزش روی ۱۸۷ مگابایت داده مدلی نمی‌سازد که متنی با استدلال پیچیده تولید کند.

در حال حاضر جملاتی تولید می‌کند که ساختار جمله کامل ندارند و در کل بی‌معنی هستند، اما این برای اندازه مجموعه آموزش طبیعی است.

# رفتار و محدودیت‌های مدل V0.5

این نسبت به مدل قبلی پیشرفت خوبی است. سبک نگارش و واژگان ویکتوریایی است و تقریباً هر جمله از لحاظ گرامری صحیح و با نشانه‌گذاری درست است. و باز هم چون از ابتدا آموزش دیده، به موضوعات قرن ۱۸۰۰ وفادار است.

![نمونه خروجی TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

تعداد زیادی خطاهای واقعی (hallucination) وجود دارد. بسیاری (تقریباً ۱۰۰٪) از جزئیات (تاریخ‌ها، رویدادها، شخصیت‌های تاریخی) ساختگی هستند. همچنین جملات واقعاً به هم مرتبط نیستند، شاید فقط دو جمله به هم مربوط باشند اما بیشتر از آن نه. مشکل دیگر این است که گاهی یک فوتر اضافی مانند «Digitized by Google» ظاهر می‌شود، پس دفعه بعد باید واقعاً مطمئن شوم که متون به‌خوبی پاک‌سازی شده‌اند. در کل از نتایج راضی هستم، هنوز به LLM نزدیک نیست اما قطعاً یک جمله‌ساز است.

خیلی چیزها یاد می‌گیرم و در هفته‌های آینده سعی می‌کنم بفهمم چه کارهایی باید بهتر انجام دهم. به‌زودی فایل‌ها را بارگذاری می‌کنم!

# برنامه‌های آینده

(انجام شد) قصد دارم کار روی نسخه ۰.۵ را شروع کنم، به‌جای آموزش با ۵۰ کتاب، ترجیحاً با ۵۰۰ تا ۶۰۰ کتاب آموزش می‌دهم. در حال حاضر دارم nanoGPT را با کتاب‌هایی از سال ۱۸۰۰ تا ۱۸۵۰ و به طور خاص از لندن آموزش می‌دهم. چالش‌هایی وجود دارد مثل اینکه مطمئن شوم کتاب‌هایی که پیدا می‌کنم نسخه به‌روزرسانی شده یا تفسیرهای مدرن ندارند و کتاب‌های دست‌نخورده‌ای هستند که در بازه زمانی انتخابی من منتشر شده‌اند.

می‌خواهم یک مدل جدید (v1) با مجموعه داده بسیار بزرگ‌تر، شاید ۵ تا ۱۰ برابر بزرگ‌تر از v0.5 آموزش دهم. هدفم این است ببینم آیا می‌توانم صرفاً با آموزش زمانی انتخابی (Selective Temporal Training) توانایی استدلال را ظاهر کنم، این کار دشوارتر خواهد بود و حتی مطمئن نیستم به دلیل محدودیت داده‌های تاریخی امکان‌پذیر باشد. در هفته‌های آینده سعی می‌کنم داده کافی برای یک مجموعه ۵ تا ۱۰ گیگابایتی جمع‌آوری کنم. معتقدم اگر بتوانم داده‌های تمیز و با کیفیت بالا تهیه کنم و یک GPU اجاره کنم، پیشرفت حاصل خواهد شد.

# چگونه از این پروژه استفاده کنیم

این پروژه عمدتاً بر جمع‌آوری داده‌های تاریخی، آماده‌سازی آن برای آموزش و ساخت یک توکنایزر متمرکز است. من کل فرایند آموزش LLM را پوشش نمی‌دهم، برای آن به nanoGPT از آندری کارپاتی مراجعه کنید.

# گام ۱: جمع‌آوری و آماده‌سازی متون تاریخی

فایل‌های .txt کتاب‌ها، اسناد و غیره را از بازه زمانی مورد نظر خود (مثلاً لندن ۱۸۰۰-۱۸۵۰) جمع‌آوری کنید.

در صورت نیاز می‌توانید از download_texts_improved.py برای دانلود کتاب‌ها استفاده کنید.

فایل‌های متنی را با یک اسکریپت یا به صورت دستی از هدر/فوتر پروژه گوتنبرگ، یادداشت‌های مدرن یا خطاهای OCR پاک کنید.

prepare_dataset.py باید به‌خوبی کار کند.

# گام ۲: ساخت توکنایزر سفارشی

train_tokenizer.py یا train_tokenizer_hf.py را روی داده‌های پاک‌شده اجرا کنید.
این کار vocab.json و merges.txt را به شما می‌دهد.

این فایل‌ها واژگان و قواعد ادغام توکن‌ها برای مدل شما را تعریف می‌کنند.

# گام ۳: آموزش مدل خود (nanoGPT)

برای فرایند آموزش به [nanoGPT اثر آندری کارپاتی](https://github.com/karpathy/nanoGPT) مراجعه کنید.

اگر بخواهید می‌توانید یک LLM متفاوت آموزش دهید، اما من از nanoGPT استفاده کردم.

# پرسش‌های متداول

## آموزش زمانی انتخابی (Selective Temporal Training) چیست؟

آموزش زمانی انتخابی (STT) یک روش یادگیری ماشین است که در آن تمام داده‌های آموزشی به طور خاص در یک بازه زمانی تاریخی خاص جمع‌آوری می‌شوند. این کار به منظور مدل‌سازی زبان و دانش آن دوره بدون تأثیر مفاهیم مدرن انجام می‌شود. برای مثال، مدل فعلی من (v0.5) فقط با داده‌های سال‌های ۱۸۰۰-۱۸۷۵ آموزش دیده است، ریزتنظیم نشده بلکه از ابتدا آموزش داده شده و خروجی آن بازتاب‌دهنده سبک زبانی و زمینه تاریخی همان زمان است.

## چرا فقط از ریزتنظیم یا LoRA استفاده نمی‌کنید؟

در این پروژه سعی دارم یک مدل زبانی بسازم که از سوگیری مدرن دور باشد. اگر چیزی مثل GPT-2 را ریزتنظیم کنم، از قبل آموزش دیده و آن اطلاعات حذف نمی‌شود. اگر از ابتدا آموزش دهم، مدل زبانی تظاهر به قدیمی بودن نمی‌کند، بلکه واقعاً قدیمی خواهد بود. هدف فعلی این پروژه ساخت مدلی است که فقط با دانش کتاب‌های لندن بین سال‌های ۱۸۰۰ تا ۱۸۵۰ استدلال کند.

## از چه داده‌هایی برای آموزش استفاده کردید؟

من از کتاب‌ها، اسناد حقوقی، روزنامه‌ها و سایر نوشته‌های لندن ۱۸۰۰-۱۸۵۰ استفاده می‌کنم. لیست پیوند داده شده حدود ۲۰۰ مورد دارد اما برای اولین آموزش فقط از ۵۰ فایل با حجم حدود ۱۸۷ مگابایت استفاده کردم. می‌توانید فهرست اسناد را مشاهده کنید:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## مدل نسخه ۰ چقدر بزرگ است؟

این مدل در حال حاضر بسیار کوچک است، این کار را صرفاً برای سرگرمی انجام می‌دهم و قانون آموزشی سختگیرانه‌ای برای عدم استفاده از منابع مدرن دارم. تقریباً ۱۶ میلیون پارامتر دارد اما قصد دارم متون قدیمی بیشتری جمع‌آوری کنم تا آموزش مدل دیگری را آغاز کنم. به‌مرور به‌روزرسانی خواهم داد.

## مشخصات آموزش؟

GPU: Geforce rtx 4060
CPU: i5-13400F
رم: ۱۶ گیگابایت DDR5.



---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-08-07

---