<div align="right">
  <details>
    <summary >🌐 زبان</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">简体中文</a>
        | <a href="#" title="Coming soon">繁體中文 (به زودی)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">한국어</a>
        | <a href="#" title="Coming soon">हिन्दी (به زودی)</a> |
        | <a href="#" title="Coming soon">ไทย (به زودی)</a> |
        | <a href="#" title="Coming soon">Français (به زودی)</a>
        | <a href="#" title="Coming soon">Deutsch (به زودی)</a>
        | <a href="#" title="Coming soon">Español (به زودی)</a>
        | <a href="#" title="Coming soon">Italiano (به زودی)</a>
        | <a href="#" title="Coming soon">Русский (به زودی)</a>
        | <a href="#" title="Coming soon">Português (به زودی)</a>
        | <a href="#" title="Coming soon">Nederlands (به زودی)</a>
        | <a href="#" title="Coming soon">Polski (به زودی)</a>
        | <a href="#" title="Coming soon">العربية (به زودی)</a>
        | <a href="#" title="Coming soon">فارسی (به زودی)</a>
        | <a href="#" title="Coming soon">Türkçe (به زودی)</a>
        | <a href="#" title="Coming soon">Tiếng Việt (به زودی)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (به زودی)</a>

      </div>
    </div>
  </details>
</div>

# تایم‌کپسول ال‌ال‌ام

*یک مدل زبانی که **از صفر** فقط با داده‌های مربوط به مکان‌ها و دوره‌های زمانی خاص آموزش داده شده تا سوگیری‌های مدرن را کاهش داده و صدای، واژگان و جهان‌بینی آن عصر را شبیه‌سازی کند.*

تصور کنید اگر یک مدل هوش مصنوعی فقط وانمود به تاریخی بودن نکند، بلکه واقعا تاریخی باشد.

نسخه‌های v0 و v0.5 بر پایه [nanoGPT توسط آندری کارپاتی](https://github.com/karpathy/nanoGPT) ساخته شده‌اند. اسکریپت‌های اصلی آموزش و معماری مدل متعلق به اوست.


نسخه v1 بر پایه [Phi 1.5 توسط مایکروسافت](https://huggingface.co/microsoft/phi-1_5) ساخته شده است

[لینک Hugging Face](https://huggingface.co/haykgrigorian/TimeCapsuleLLM)

##  رفتار مدل و محدودیت‌ها

### **نسخه v0**  

پرامپت‌های اولیه نشان می‌دهند که مدل با زبان و رفتار دهه ۱۸۰۰ پاسخ می‌دهد.  
مثال: پرامپت: "چه کسی هنری است؟" و پاسخ داد: "من آن مرد را می‌شناسم، من نکرده‌ام سیاه، طوفان."  

![نمونه خروجی TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

- بدون اشاره به مفاهیم مدرن  
- عمدتاً واژگان مطابق با دوره زمانی  
- جملات عمدتاً نامفهوم (انتظار می‌رفت با حدود ۱۸۷ مگابایت داده آموزشی)

### **نسخه v0.5** 

بهبود قابل توجه نسبت به نسخه v0  
- سبک نوشتار ویکتوریایی، نقطه‌گذاری صحیح، جملات عمدتاً دستوری  
- هنوز نرخ بالای هذیان‌گویی واقعی  
- نویز OCR ("Digitized by Google") همچنان در خروجی‌ها وجود دارد

![نمونه خروجی TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

### **نسخه v1**

اولین مدلی که توانست یک رویداد تاریخی واقعی را با یک شخصیت حقیقی از مجموعه داده مرتبط کند.

مثال: پرامپت: "در سال پروردگارمان ۱۸۳۴ بود" 

خروجی: "در سال پروردگارمان ۱۸۳۴ بود و خیابان‌های لندن پر از اعتراض و دادخواست بود. علت، همانطور که بسیاری نقل کردند، به شیوه خصوصی محدود نبود، بلکه همان روز در روز لرد پالمرستون مطرح شد، عموم مردم بیانیه کوتاهی از دشواری‌هایی که قانون به ما رسانده است دریافت خواهند کرد. این موضوعی است که جای تأسف عمیق دارد، که رویدادهای کنونی در تاریخ جهان واضح هستند و بنابراین شناخته خواهند شد. درست نیست که همان مردانی که برای اولین بار در انجیل در اورشلیم ساکن شدند، باید چنین سابقه گسترده و جالبی از رونق و رفاه داشته باشند"

در ابتدا تصور می‌کردم که شاید به طور تصادفی اعتراضی در همان سال رخ داده باشد اما به این توجه کنید: ![1834protest](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)

### چرا این موضوع اهمیت دارد:

این اولین نمونه‌ای است که یکی از مدل‌های من یک سال را هم به یک رویداد تاریخی واقعی و هم به یک فرد واقعی مرتبط با آن رویداد (لرد پالمرستون) متصل می‌کند. مدل‌های قبلی (v0 و v0.5) می‌توانستند سبک نوشتاری قرن نوزدهم را تقلید کنند اما همواره رویدادها، افراد و حقایق را هذیان می‌گفتند. این نشان می‌دهد که مدل شروع به به‌خاطر سپردن اطلاعات از مجموعه داده کرده است.


## برنامه‌های آتی

- تقریباً ۱۷۵,۰۰۰ متن منتشر شده در لندن از سال ۱۸۰۰ تا ۱۸۷۵ در Internet Archive وجود دارد
- قصد دارم مجموعه داده را گسترش داده و آن را بیشتر پاک‌سازی کنم تا توانایی استدلال بهتری حاصل شود
- گسترش به مناطق و دوره‌های زمانی مختلف برای مدل‌های تاریخی بیشتر

## نحوه استفاده

این پروژه عمدتاً بر گردآوری داده‌های تاریخی، آماده‌سازی آن برای آموزش و ساخت توکنایزر تمرکز دارد. من فرآیند کامل آموزش LLM را پوشش نمی‌دهم؛ برای این منظور به nanoGPT اثر Andrej Karpathy مراجعه کنید.

### مرحله ۱: گردآوری و آماده‌سازی متون تاریخی

- فایل‌های .txt از کتاب‌ها، اسناد و... دامنه عمومی را از دوره زمانی انتخابی خود جمع‌آوری کنید (مثلاً لندن ۱۸۰۰-۱۸۵۰)
- آن‌ها را در بازه زمانی/مکانی انتخابی خود نگه دارید
- فایل‌های متنی را با یک اسکریپت یا به صورت دستی پاک‌سازی کنید و سرصفحه/پاصفحه پروژه گوتنبرگ، توضیحات مدرن یا خطاهای OCR را حذف کنید.

### مرحله ۲: ساخت توکنایزر سفارشی

- train_tokenizer.py یا train_tokenizer_hf.py را روی داده‌های پاک‌سازی شده اجرا کنید.
- این کار vocab.json و merges.txt را به شما می‌دهد
- این فایل‌ها واژگان و قواعد ادغام برای مدل شما را تعریف می‌کنند

### مرحله ۳: آموزش مدل خود

- برای فرآیند آموزش یا مستندات معماری انتخابی، به [nanoGPT اثر Andrej Karpathy](https://github.com/karpathy/nanoGPT) مراجعه کنید.

# پرسش‌های متداول

## آموزش زمانی گزینشی چیست؟

آموزش زمانی گزینشی (STT) روشی در یادگیری ماشین است که تمام داده‌های آموزشی به طور خاص برای قرار گرفتن در یک دوره تاریخی مشخص انتخاب می‌شوند. هدف آن مدل‌سازی زبان و دانش آن دوره بدون تأثیر مفاهیم مدرن است. به عنوان مثال، مدل فعلی من (نسخه ۰.۵) فقط با داده‌های ۱۸۰۰ تا ۱۸۷۵ آموزش دیده است؛ از ابتدا آموزش دیده و نه فقط تنظیم دقیق شده، بنابراین خروجی آن بازتاب‌دهنده سبک زبانی و زمینه تاریخی همان دوره است.

## چرا فقط از تنظیم دقیق یا LoRA استفاده نمی‌کنید؟

برای این پروژه، هدفم ساخت یک مدل زبانی بدون سوگیری مدرن است. اگر چیزی مثل GPT-2 را تنظیم دقیق کنم، آن قبلاً آموزش دیده و اطلاعات آن باقی خواهد ماند. اگر مدل را از ابتدا آموزش دهم، مدل زبانی وانمود به قدیمی بودن نمی‌کند، واقعاً قدیمی خواهد بود. هدف فعلی این پروژه ساخت مدلی است که بتواند منحصراً با دانش کتاب‌های لندن منتشر شده بین ۱۸۰۰ و ۱۸۷۵ استدلال کند.

## از چه نوع داده‌ای برای آموزش استفاده کردید؟


من از کتاب‌ها، اسناد حقوقی، روزنامه‌ها و سایر نوشته‌های لندن بین سال‌های ۱۸۰۰ تا ۱۸۷۵ استفاده می‌کنم. لیستی که لینک کردم (برای نسخه v0) حدود ۲۰۰ مورد دارد اما برای اولین آموزش فقط ۵۰ فایل به حجم تقریبی ۱۸۷ مگابایت را استفاده کردم. شما می‌توانید لیست اسناد را مشاهده کنید:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

اندازه دیتاست‌ها:
v0: تقریباً ۱۸۷ مگابایت
v0.5: تقریباً ۴۳۵ مگابایت 
v1: تقریباً ۶.۲۵ گیگابایت 

## مدل‌ها چه اندازه هستند؟

V0: ۱۶ میلیون پارامتر

V0.5: ۱۲۳ میلیون پارامتر

V1: ۷۰۰ میلیون پارامتر

# مشخصات آموزش؟

# V0/V0.5
کارت گرافیک: Geforce rtx 4060
پردازنده: i5-13400F 
رم: ۱۶ گیگابایت DDR5.

# V1
کارت گرافیک: A100 اجاره‌ای

















---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-09-30

---