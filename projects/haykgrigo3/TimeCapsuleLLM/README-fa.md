<div align="right">
  <details>
    <summary >🌐 زبان</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">انگلیسی</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">简体中文</a>
        | <a href="#" title="Coming soon">繁體中文 (به‌زودی)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">ژاپنی</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">کره‌ای</a>
        | <a href="#" title="Coming soon">هندی (به‌زودی)</a> |
        | <a href="#" title="Coming soon">تایلندی (به‌زودی)</a> |
        | <a href="#" title="Coming soon">فرانسوی (به‌زودی)</a>
        | <a href="#" title="Coming soon">آلمانی (به‌زودی)</a>
        | <a href="#" title="Coming soon">اسپانیایی (به‌زودی)</a>
        | <a href="#" title="Coming soon">ایتالیایی (به‌زودی)</a>
        | <a href="#" title="Coming soon">روسی (به‌زودی)</a>
        | <a href="#" title="Coming soon">پرتغالی (به‌زودی)</a>
        | <a href="#" title="Coming soon">هلندی (به‌زودی)</a>
        | <a href="#" title="Coming soon">لهستانی (به‌زودی)</a>
        | <a href="#" title="Coming soon">عربی (به‌زودی)</a>
        | <a href="#" title="Coming soon">فارسی (به‌زودی)</a>
        | <a href="#" title="Coming soon">ترکی (به‌زودی)</a>
        | <a href="#" title="Coming soon">ویتنامی (به‌زودی)</a>
        | <a href="#" title="Coming soon">اندونزیایی (به‌زودی)</a>

      </div>
    </div>
  </details>
</div>

# تایم‌کپسول ال‌ال‌ام
مدلی از نوع LLM که فقط بر اساس داده‌های دوره‌های زمانی خاصی آموزش دیده تا سوگیری‌های مدرن کاهش یابد.

تصور کنید یک مدل هوش مصنوعی فقط وانمود نمی‌کند که تاریخی است، بلکه واقعا اینگونه باشد.

بر اساس [nanoGPT از آندری کارپاتی](https://github.com/karpathy/nanoGPT) اسکریپت‌های اصلی آموزش و معماری مدل کار اوست.

# اهداف پروژه

تایم‌کپسول LLM یک پروژه آزمایشی است که فقط بر روی متون نوشته‌شده در دوره‌های زمانی خاص آموزش داده می‌شود. هدف این است که جهان‌بینی و زبان دوران‌های تاریخی خاص را شبیه‌سازی کند.

# چرا فقط تنظیم دقیق کافی نیست

اگر فقط یک مدل از پیش‌آموزش‌دیده را فاین‌تیون کنید، مدل شما همچنان مفاهیم مدرن را خواهد شناخت. البته رسیدن به بی‌سوگیری کامل مدرن سخت است اما من می‌خواهم تا حد ممکن به این هدف نزدیک شوم. برای نداشتن سوگیری مدرن باید مدل را از ابتدا آموزش داد.

# انتظارات

امیدوارم پس از اتمام، این مدل هیچ مفهومی از مفاهیم مدرن نداشته باشد و نتواند فراتر از آنچه آموزش دیده است، استدلال کند. نباید مفاهیم/واژگان مدرن را تشخیص دهد و امیدوارم دانشی از عصر مدرن را هذیان‌گویی نکند.

# به‌روزرسانی‌های پیشرفت

## ۹ ژوئیه ۲۰۲۵

دوره زمانی خود را ۱۸۰۰-۱۸۵۰ و منطقه: لندن تنظیم کرده‌ام

فهرستی از متون، کتاب‌ها و اسناد جمع‌آوری کرده‌ام

تا کنون ۵۰ فایل متنی به دست آورده‌ام و به زودی آموزش NanoGPT را آغاز می‌کنم

تا زمانی که پیشرفت داشته باشم این را به‌روزرسانی خواهم کرد

## ۱۳ ژوئیه ۲۰۲۵

nanoGPT را با ۱۸۷ مگابایت داده متنی تاریخی آموزش دادم.

## ۱۵ ژوئیه ۲۰۲۵

شروع به دانلود متون برای دومین دور آموزش کردم. همه چیز را از اینترنت آرشیو می‌گیرم و بازه زمانی را به ۱۸۰۰-۱۸۷۵ گسترش داده‌ام. برای به دست آوردن طیف متنوعی از متون، می‌توانید از فیلترهای موضوعی و جستجو برای مکان انتشار، بازه زمانی و موضوعات در اینترنت آرشیو استفاده کنید.

![فیلترهای جستجو](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## ۱۶ ژوئیه ۲۰۲۵

حدود ۵۰۰ فایل متنی از اینترنت آرشیو دانلود کردم و پس از پاک‌سازی (فقط حذف فاصله‌های اضافی، هدرهای گوتنبرگ و غیره) حدود ۵۰۰ مگابایت داده به دست آوردم. این مجموعه داده کوچکی است اما آخرین بار با ۱۸۷ مگابایت آموزش دادم، پس باید حداقل تفاوت قابل توجهی در خروجی پس از آموزش مدل دوم دیده شود. امیدوارم این مدل بتواند جملات منسجم‌تری تولید کند که تا حدی معنا داشته باشند. البته این تضمین نیست چون همچنان مجموعه داده خیلی کوچکی است، اما بیشتر از دفعه قبل است.

این کار باید روی سخت‌افزار خودم قابل انجام باشد، این هم خوب است چون امیدوارم بتوانم پیشرفت‌هایی را ببینم قبل از اینکه سراغ مجموعه داده بزرگ‌تری بروم که نیاز به اجاره GPU دارد. اما نگران نباشید، هنوز برنامه دارم به زودی GPU اجاره کنم، ولی قبلش می‌خواهم مطمئن شوم مجموعه داده‌ام تا حد امکان گزینش‌شده و پاک باشد. یکی از مشکلاتم پاک‌سازی است، بسیاری از این فایل‌های متنی حاوی نوشته‌های بی‌معنا هستند. اسکریپت‌هایی که برای پاک‌سازی استفاده کرده‌ام کار می‌کنند اما صد درصد موثر نیستند.

امروز این مجموعه داده را آموزش می‌دهم و باید حدود ۴-۵ ساعت طول بکشد. وقتی تمام شد و تست کردم، به‌روزرسانی خواهم داد. دوباره از همه کسانی که پروژه من را دنبال می‌کنند تشکر می‌کنم، حتی افرادی منابع OCR برایم فرستاده‌اند، پس ممنون! امیدوارم افراد بیشتری این کار را امتحان کنند و با مجموعه داده‌های خودشان آزمایش انجام دهند.

### به‌روزرسانی آموزش

آموزش را روی مجموعه‌ای ۴۳۵ مگابایتی (۱۰۸ میلیون توکن) شروع کردم، فعلاً روند آموزش خوب پیش می‌رود. زیان آموزش از ۱۰.۹ به ۴.۹ در ۲۸۰۰ تکرار اول کاهش یافت. انتظار دارم حدود ۸ یا ۹ ساعت طول بکشد. پس از اتمام، به‌روزرسانی دیگری خواهم گذاشت.

## ۱۷ ژوئیه ۲۰۲۵ ساعت ۲:۱۳ بامداد

آموزش مدل دوم تمام شد، کارت ۴۰۶۰ من حدود ۸ ساعت و ۴۰ دقیقه (۳۹۰۰ تکرار در ساعت) برای ۳۳,۰۰۰ تکرار (۵ اپک) زمان برد. زیان نهایی آموزش ۳.۷۳ بود. خروجی‌ها به طرز شگفت‌انگیزی خوب بودند و واقعاً جملاتی با سبک قرن نوزدهم تولید می‌کند.

# رفتار و محدودیت‌های مدل نسخه V0

در ابتدای کار، مدل با زبان و رفتار دهه ۱۸۰۰ پاسخ می‌دهد. مثلاً با درخواست "Who art Henry?" پاسخ داد: "I know that man, I have did not a black, the storm." و بله این جمله بی‌معنی است اما LLM تشخیص می‌دهد که درباره یک شخص سؤال شده.

![خروجی نمونه TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

هیچ اشاره‌ای به مفاهیم مدرن وجود ندارد، خروجی‌ها عمدتاً شامل واژگان و عبارات قرن نوزدهم هستند.

هنوز کار زیادی لازم است، آموزش با ۱۸۷ مگابایت به شما مدلی با استدلال پیچیده نمی‌دهد.

در حال حاضر جملاتی تولید می‌کند که ساختار جمله کامل ندارند و به طور کلی بی‌معنی هستند، اما برای این اندازه آموزش طبیعی است.

# رفتار و محدودیت‌های مدل V0.5

این نسبت به مدل قبلی پیشرفت خوبی است. سبک نوشتار و واژگان ویکتوریایی بوده و تقریباً هر جمله از نظر دستوری صحیح و با نقطه‌گذاری مناسب است. و باز هم این مدل از ابتدا آموزش داده شده پس به موضوعات دهه ۱۸۰۰ پایبند است.

![نمونه خروجی TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

تعداد زیادی توهمات واقعی وجود دارد. بسیاری (تقریباً ۱۰۰٪) از جزئیات (تاریخ‌ها، رویدادها، شخصیت‌های تاریخی) ساختگی هستند. همچنین جملات واقعاً ارتباطی با هم ندارند، گاهی شاید دو جمله به هم مرتبط باشند اما بیشتر از آن نه. یک مشکل دیگر این است که گاهی پاورقی پراکنده "Digitized by Google" ظاهر می‌شود، بنابراین دفعه بعد که آموزش می‌دهم باید مطمئن شوم متون به خوبی پاک‌سازی شده‌اند. در مجموع از نتایج بسیار راضی‌ام، هنوز به یک LLM نزدیک نیست اما قطعاً یک جمله‌ساز است.

خیلی یاد می‌گیرم و در هفته‌های آینده شروع به بررسی اینکه چه چیزهایی باید بهتر انجام دهم خواهم کرد. به زودی فایل‌ها را بارگذاری می‌کنم!

# برنامه‌های آینده

(انجام شده) قرار است کار روی نسخه ۰.۵ را شروع کنم، به جای آموزش با استفاده از ۵۰ کتاب، ایده‌آل این است که از ۵۰۰ تا ۶۰۰ کتاب استفاده کنم. اکنون در حال آموزش nanoGPT با استفاده از کتاب‌های ۱۸۰۰ تا ۱۸۵۰ و به طور خاص از لندن هستم. چالش‌هایی وجود دارد مانند اطمینان از اینکه کتاب‌هایی که پیدا می‌کنم به‌روزرسانی نشده‌اند یا تفسیرهای مدرن ندارند و فقط کتاب‌های دست‌نخورده منتشر شده در بازه زمانی انتخابی‌ام هستند.

می‌خواهم یک مدل جدید (v1) با یک پیکره بسیار بزرگ‌تر، شاید ۵ تا ۱۰ برابر بزرگ‌تر از آنچه برای v0.5 استفاده کردم، آموزش دهم. هدفم این است که ببینم آیا می‌توانم فقط با آموزش زمانی انتخابی، توانایی استدلال را ایجاد کنم، این کار دشوارتر خواهد بود و حتی مطمئن نیستم شدنی باشد چون محدودیت داده تاریخی وجود دارد. در هفته‌های آینده سعی می‌کنم به اندازه کافی داده برای یک پیکره ۵ تا ۱۰ گیگابایتی جمع کنم. معتقدم اگر بتوانم داده‌های تمیز و باکیفیت تهیه کنم و یک GPU اجاره کنم، پیشرفت حاصل خواهد شد.

# چگونه از این پروژه استفاده کنیم

این پروژه بیشتر بر جمع‌آوری داده‌های تاریخی، آماده‌سازی برای آموزش و ساخت توکنایزر متمرکز است. فرایند کامل آموزش LLM را پوشش نمی‌دهم، برای این منظور به nanoGPT توسط آندری کارپاتی مراجعه کنید.

# مرحله ۱: جمع‌آوری و آماده‌سازی متون تاریخی

فایل‌های .txt از کتاب‌ها، اسناد و غیره مربوط به بازه زمانی انتخابی خود (مثلاً لندن ۱۸۰۰-۱۸۵۰) جمع‌آوری کنید.

می‌توانید از download_texts_improved.py برای دانلود کتاب‌ها در صورت نیاز استفاده کنید.

فایل‌های متنی را با اسکریپت یا به صورت دستی از سربرگ/پاورقی‌های پروژه گوتنبرگ، یادداشت‌های مدرن یا خطاهای OCR پاک‌سازی کنید.

prepare_dataset.py باید به خوبی کار کند.

# مرحله ۲: ساخت توکنایزر سفارشی

train_tokenizer.py یا train_tokenizer_hf.py را روی داده‌های پاک‌شده اجرا کنید.
این کار به شما vocab.json و merges.txt می‌دهد.

این فایل‌ها واژگان و قوانین ادغام مدل شما را تعریف می‌کنند.

# مرحله ۳: آموزش مدل (nanoGPT)

برای فرایند آموزش به [nanoGPT توسط آندری کارپاتی](https://github.com/karpathy/nanoGPT) مراجعه کنید.

اگر خواستید می‌توانید یک LLM دیگر را آموزش دهید، اما من از nanoGPT استفاده کردم.

# پرسش‌های متداول

## آموزش زمانی انتخابی چیست؟

آموزش زمانی انتخابی (STT) یک روش یادگیری ماشین است که در آن تمام داده‌های آموزشی به طور خاص طوری انتخاب می‌شوند که در یک بازه زمانی تاریخی خاص قرار بگیرند. این کار برای مدل‌سازی زبان و دانش آن دوره بدون تأثیر مفاهیم مدرن انجام می‌شود. به عنوان مثال، مدل فعلی من (v0.5) منحصراً با داده‌های ۱۸۰۰-۱۸۷۵ آموزش دیده و از ابتدا ساخته شده، بنابراین خروجی آن بازتاب سبک زبانی و بستر تاریخی همان دوره است.

## چرا فقط از فاین‌تیون یا LoRA استفاده نمی‌کنید؟

در این پروژه سعی دارم یک مدل زبانی بسازم که تحت تأثیر سوگیری مدرن نباشد. اگر چیزی مثل GPT-2 را فاین‌تیون کنم، آن مدل قبلاً پیش‌آموزش دیده و این اطلاعات از بین نمی‌رود. اگر از ابتدا آموزش دهم، مدل زبان وانمود نمی‌کند قدیمی است، واقعاً قدیمی خواهد بود. هدف فعلی این پروژه این است که مدلی بسازم که صرفاً بر اساس دانش کتاب‌های لندن منتشرشده بین ۱۸۰۰ تا ۱۸۵۰ استدلال کند.

## از چه نوع داده‌ای برای آموزش استفاده کردید؟

من از کتاب‌ها، اسناد حقوقی، روزنامه‌ها و سایر نوشته‌های لندن بین ۱۸۰۰ تا ۱۸۵۰ استفاده می‌کنم. لیستی که پیوند دادم حدود ۲۰۰ مورد دارد اما برای اولین آموزش فقط از ۵۰ فایل به حجم تقریبی ۱۸۷ مگابایت استفاده کردم. لیست اسناد را می‌توانید در اینجا ببینید:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## اندازه مدل نسخه ۰ چقدر است؟

این مدل در حال حاضر بسیار کوچک است، من فقط برای سرگرمی و با رعایت قاعده سختگیرانه عدم استفاده از منابع مدرن این کار را انجام می‌دهم. تقریباً ۱۶ میلیون پارامتر دارد اما قرار است متون قدیمی بیشتری جمع‌آوری کنم تا آموزش مدل بعدی را شروع کنم. به‌روزرسانی‌ها را اعلام خواهم کرد.

## مشخصات آموزش؟

GPU: Geforce rtx 4060
CPU: i5-13400F
رم: ۱۶ گیگابایت DDR5.



---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-21

---