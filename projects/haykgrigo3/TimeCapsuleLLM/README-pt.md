
<div align="right">
  <details>
    <summary >üåê Idioma</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="#" title="Em breve">ÁπÅÈ´î‰∏≠Êñá (em breve)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="#" title="Em breve">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä (em breve)</a> |
        | <a href="#" title="Em breve">‡πÑ‡∏ó‡∏¢ (em breve)</a> |
        | <a href="#" title="Em breve">Fran√ßais (em breve)</a>
        | <a href="#" title="Em breve">Deutsch (em breve)</a>
        | <a href="#" title="Em breve">Espa√±ol (em breve)</a>
        | <a href="#" title="Em breve">Italiano (em breve)</a>
        | <a href="#" title="Em breve">–†—É—Å—Å–∫–∏–π (em breve)</a>
        | <a href="#" title="Em breve">Portugu√™s (em breve)</a>
        | <a href="#" title="Em breve">Nederlands (em breve)</a>
        | <a href="#" title="Em breve">Polski (em breve)</a>
        | <a href="#" title="Em breve">ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (em breve)</a>
        | <a href="#" title="Em breve">ŸÅÿßÿ±ÿ≥€å (em breve)</a>
        | <a href="#" title="Em breve">T√ºrk√ße (em breve)</a>
        | <a href="#" title="Em breve">Ti·∫øng Vi·ªát (em breve)</a>
        | <a href="#" title="Em breve">Bahasa Indonesia (em breve)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM

*Um modelo de linguagem treinado **do zero** exclusivamente com dados de determinados locais e per√≠odos para reduzir o vi√©s moderno e emular a voz, o vocabul√°rio e a vis√£o de mundo da √©poca.*

Imagine se um modelo de IA n√£o apenas fingisse ser hist√≥rico, mas realmente fosse.

v0 e v0.5 constru√≠dos sobre [nanoGPT de Andrej Karpathy](https://github.com/karpathy/nanoGPT) Os scripts principais de treinamento e arquitetura do modelo s√£o trabalho dele. 

v1 constru√≠do sobre [Phi 1.5 da Microsoft](https://huggingface.co/microsoft/phi-1_5)

##  Comportamento do Modelo & Limita√ß√µes

### **v0**  

Os primeiros prompts mostram o modelo respondendo com linguagem e comportamento do s√©culo XIX. 
Exemplo: Prompt: "Who art Henry?" e ele respondeu "Eu conhe√ßo esse homem, eu n√£o fiz um negro, a tempestade."

![Sa√≠da de Exemplo do TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

- Nenhuma men√ß√£o a conceitos modernos  
- Vocabul√°rio majoritariamente fiel √† √©poca  
- Frases s√£o na maioria incoerentes (esperado para ~187MB de dados de treinamento)

### **v0.5** 

Uma melhoria significativa em rela√ß√£o ao v0.  
- Estilo de escrita vitoriano, pontua√ß√£o adequada, frases na maioria gramaticais  
- Taxa de alucina√ß√£o factual ainda alta  
- Ru√≠do de OCR (‚ÄúDigitized by Google‚Äù) ainda presente nas respostas

![Sa√≠da de Exemplo do TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

### **v1**

Primeiro modelo a recordar e conectar um evento hist√≥rico real com uma figura do conjunto de dados.

Exemplo: Prompt: "Foi o ano de nosso Senhor 1834" 

A resposta: "Foi o ano de nosso Senhor 1834 e as ruas de Londres estavam cheias de protestos e peti√ß√µes. A causa, como muitos relataram, n√£o estava vinculada ao modo privado, mas tendo sido tomada no mesmo dia no dia de Lord Palmerston, o p√∫blico receber√° uma breve declara√ß√£o das dificuldades sob as quais o dia da lei chegou at√© n√≥s. √â motivo de profundo pesar que os eventos atuais na hist√≥ria do mundo sejam claros e, consequentemente, ser√£o conhecidos. N√£o √© verdade que os pr√≥prios homens que primeiro se estabeleceram no Evangelho em Jerusal√©m deveriam ter um registro t√£o extenso e t√£o interessante da prosperidade e prosperidade"

No in√≠cio presumi que um protesto poderia ter acontecido coincidentemente no mesmo ano, mas veja isto: ![1834protest](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)

### Por que isso importa:

Este √© o primeiro exemplo de um dos meus modelos conectando um ano a um evento hist√≥rico real e a uma pessoa real ligada a esse evento (Lord Palmerston). Modelos anteriores (v0 e v0.5) conseguiam imitar estilos de escrita do s√©culo XIX, mas sempre alucinavam eventos, pessoas e fatos. Isso mostra que o modelo est√° come√ßando a lembrar coisas do conjunto de dados

## Planos Futuros 

- Existem quase 175.000 textos publicados em Londres de 1800-1875 dispon√≠veis no Internet Archive
- Pretendo expandir o corpus e limp√°-lo mais para melhorar as capacidades de racioc√≠nio
- Expandindo para diferentes regi√µes e per√≠odos para modelos hist√≥ricos mais diversos


## Como Usar

Este projeto foca principalmente em curar dados hist√≥ricos, prepar√°-los para treinamento e construir um tokenizador. N√£o vou cobrir o processo completo de treinamento de LLM, para isso, consulte o nanoGPT por Andrej Karpathy.

### Passo 1: Coletar e Preparar Textos Hist√≥ricos

- Colete arquivos .txt de livros de dom√≠nio p√∫blico, documentos, etc do per√≠odo escolhido (ex.: Londres 1800-1850)
- Mantenha-os dentro da janela de tempo/local escolhida
- Limpe os arquivos de texto usando um script ou remova manualmente cabe√ßalhos/rodap√©s do Project Gutenberg, anota√ß√µes modernas ou erros de OCR.

### Passo 2: Construir um Tokenizador Personalizado

- Execute train_tokenizer.py ou train_tokenizer_hf.py nos dados limpos.
- Isso ir√° gerar vocab.json e merges.txt
- Esses arquivos definem o vocabul√°rio e as regras de fus√£o para seu modelo

### Passo 3: Treinar Seu Modelo

- Consulte [nanoGPT por Andrej Karpathy](https://github.com/karpathy/nanoGPT) para o processo de treinamento ou a documenta√ß√£o da arquitetura escolhida.

# FAQ

## O que √© Treinamento Temporal Seletivo?

Treinamento Temporal Seletivo (STT) √© uma metodologia de aprendizado de m√°quina onde todos os dados de treinamento s√£o especificamente selecionados para estar dentro de um per√≠odo hist√≥rico determinado. Isso √© feito para modelar a linguagem e o conhecimento daquela √©poca sem influ√™ncia de conceitos modernos. Por exemplo, o modelo atual que possuo (v0.5) foi treinado exclusivamente com dados de 1800-1875, n√£o √© fine-tuned, mas treinado do zero, resultando em uma sa√≠da que reflete o estilo lingu√≠stico e contexto hist√≥rico desse per√≠odo.

## Por que n√£o apenas usar fine-tuning ou LoRA?

Para este projeto, estou tentando criar um modelo de linguagem livre de vi√©s moderno. Se eu fizer fine-tuning em algo como GPT-2, ele j√° est√° pr√©-treinado e essa informa√ß√£o n√£o desaparece. Se eu treinar do zero, o modelo de linguagem n√£o vai apenas fingir ser antigo, ele realmente ser√°. O objetivo deste projeto agora √© criar algo que possa raciocinar exclusivamente usando conhecimento de livros de Londres publicados entre 1800 e 1875.

## Que tipo de dados voc√™ usou para o treinamento?

Estou usando livros, documentos legais, jornais e outros escritos de Londres de 1800‚Äì1875. A lista que linkei (para v0) tem cerca de 200, mas para o primeiro treinamento usei apenas 50 arquivos totalizando ~187 MB. Voc√™ pode ver a lista de documentos:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt


Tamanhos dos conjuntos de dados:
v0: ~187MB
v0.5: ~435MB
v1: ~6,25GB

## Qual o tamanho dos modelos?

V0: 16M Par√¢metros

V0.5 123M Par√¢metros

V1: 700M Par√¢metros

# Especifica√ß√µes de treinamento?

# V0/V0.5
GPU: Geforce RTX 4060
CPU: i5-13400F
Ram: 16GB DDR5.

# V1
GPU: A100 alugada














---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-08-19

---