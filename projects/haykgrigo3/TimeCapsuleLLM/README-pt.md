<div align="right">
  <details>
    <summary >üåê Idioma</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-TW">ÁπÅÈ´î‰∏≠Êñá</a> 
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=as">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä</a> 
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=th">‡πÑ‡∏ó‡∏¢</a> 
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=fr">Fran√ßais</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=es">Espa√±ol</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=it">Italiano</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=ru">–†—É—Å—Å–∫–∏–π</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=pt">Portugu√™s</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=ar">ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</a>
        | <a href="#" title="Coming soon">ŸÅÿßÿ±ÿ≥€å (em breve)</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=tr">T√ºrk√ße</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=vi">Ti·∫øng Vi·ªát</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (em breve)</a>

      </div>
    </div>
  </details>
</div>


# TimeCapsule LLM

*Um modelo de linguagem treinado **do zero** exclusivamente com dados de determinados locais e per√≠odos hist√≥ricos para reduzir o vi√©s moderno e emular a voz, o vocabul√°rio e a vis√£o de mundo da √©poca.*

Imagine se um modelo de IA n√£o apenas fingisse ser hist√≥rico, mas realmente fosse.

v0 e v0.5 constru√≠dos sobre [nanoGPT por Andrej Karpathy](https://github.com/karpathy/nanoGPT) Os scripts principais de treinamento e a arquitetura do modelo s√£o de autoria dele. 

v1 constru√≠do com base no [Phi 1.5 da Microsoft](https://huggingface.co/microsoft/phi-1_5)

v2 constru√≠do com base no llamaforcausallm

[Link do Hugging Face](https://huggingface.co/haykgrigorian/TimeCapsuleLLM)

##  Comportamento do Modelo & Limita√ß√µes

### **v0**  

Os primeiros prompts mostram o modelo respondendo com linguagem e comportamento do s√©culo XIX. 
Exemplo: Prompt: "Who art Henry?" e ele respondeu "I know that man, I have did not a black, the storm." 

![Sa√≠da de Exemplo do TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

- Nenhuma men√ß√£o a conceitos modernos  
- Vocabul√°rio majoritariamente fiel √† √©poca  
- Frases na maioria das vezes incoerentes (esperado para ~187MB de dados de treinamento)

### **v0.5** 

Uma melhoria significativa em rela√ß√£o ao v0.  
- Estilo de escrita vitoriana, pontua√ß√£o adequada, frases na maioria das vezes gramaticais  
- Ainda apresenta alta taxa de alucina√ß√£o factual  
- Ru√≠do de OCR (‚ÄúDigitized by Google‚Äù) ainda presente nas sa√≠das

![Sa√≠da de Exemplo do TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

### **v1**

Primeiro modelo a recordar e conectar um evento hist√≥rico real com uma figura real do conjunto de dados.

Exemplo: Prompt: "It was the year of our Lord 1834" 

A sa√≠da: "It was the year of our Lord 1834 and the streets of London were filled with protest and petition. The cause, as many re counted, was not bound in the way of private, but having taken up the same day in the day of Lord Palmerston, the public will receive a short statement of the difficulties under which the day of law has reached us. It is a matter of deep regret, that the present events in the history of the world are clear, and consequently will be'known. It is not true that the very men who first settled in the Gospel at Jerusalem should have so extensive and so interesting a record of the prosperity and prosperity" 

No in√≠cio, presumi que um protesto poderia ter ocorrido coincidentemente no mesmo ano, mas veja isto: ![1834protest](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)

### **v2mini-eval1**


Treinado usando uma amostra de 15GB do conjunto de dados de 90GB da v2.
Modelo treinado apenas at√© 10.000 etapas.

Houve um problema com a tokeniza√ß√£o que faz com que a sa√≠da fique assim:

PROMPT:
Quem √© Charles Dickens?


W ho is Charles D ic ens ? D oes that work more of h ise x cell ent st ir ring , in his pl ays , int he G reat C omp any 's f arm ? What I have y et to qu ote from J ack P ick ett ? D oy oun ot know th att hey were as pe cies of g all oping , ors aw ing oft heir bre asts , or what w ast ob ed one about the t im ew hen J acks on was looking ont hew ind ow ? What ist hes uccess of an English man , and which h isson has not be end is cover ing t ome , where by to acc omp lish such a we ight ? D idy ou ever m ake ap ass age int othe old road way , ort o an anch or - b ree ze att he foot of our boat , which you m ust le ave us ? The fact is , that whe ther the wind would rise up from the pl ain on S atur day night orn o ont ide , ort ill then orth , or other wise , wes hall be com pelled t od esc ribe af orm idable bar rier , with t hes ame e ffect s ast he p resent . In th iss itu ation , at le ast , itis not to omuch t os ay that we have le ft that room . I bel ieve there are three c op ies int he " F ive H undred - fold ," to bere fer red to , ast he first n umber of our read ers wh ow is ht
Corrigido:

"Quem √© Charles Dickens? Isso funciona mais da sua excelente como√ß√£o, em suas pe√ßas, na fazenda da Grande Companhia? O que ainda tenho a citar de Jack Pickett? Voc√™ n√£o sabe que eles eram uma esp√©cie de galope, ou serra√ß√£o de seus peitos, ou o que deveria ser feito na √©poca em que Jackson olhava pela janela? Qual √© o sucesso de um ingl√™s, e o que seu filho ainda n√£o descobriu para mim, pelo qual realizar tal peso? Voc√™ j√° fez uma passagem para a velha estrada, ou para uma brisa de √¢ncora ao p√© do nosso barco, que voc√™ deve nos deixar? O fato √© que, se o vento se levantaria da plan√≠cie no s√°bado √† noite ou ao meio-dia, ou at√© o norte, ou de outra forma, seremos obrigados a descrever uma barreira formid√°vel, com os mesmos efeitos do presente. Nesta situa√ß√£o, pelo menos, n√£o √© exagero dizer que deixamos aquele quarto. Acredito que h√° tr√™s c√≥pias no 'Quinhentas vezes', para serem referidas, como o primeiro n√∫mero de nossos leitores que desejam t[o]..."

### **v2mini-eval2**

Treinado usando a mesma amostra de 15GB usada para treinar o eval1.
O modelo foi novamente treinado por apenas 10K passos.

PROMPT:Charles Darwin

Charles DarwinECCEMACY. Senhor, ‚Äî O seguinte caso √© interessante para mim: ‚Äî Estive em Londres por uma quinzena e fui muito afetado por um ataque de reumatismo. O primeiro ataque de reumatismo foi uma semana antes de eu v√™-lo, e o segundo quando o vi, e o terceiro quando o vi, e o terceiro no mesmo tempo. O segundo ataque de gota, no entanto, n√£o foi acompanhado por sintomas febris, mas foi acompanhado por um aumento no fluxo de urina, e por uma descarga mais copiosa de urina. O terceiro ataque foi uma hora depois que o vi, e foi sucedido por um retorno de um paroxismo de gota, e um retorno mais r√°pido da gota. O quarto ataque tamb√©m foi acompanhado por uma febre, mas nem sempre foi acompanhado por sintomas febris. O terceiro ataque de gota foi uma quinzena depois que voc√™ esteve doente, e o quarto foi seguido por um paroxismo de gota. O quarto ataque foi uma quinzena depois que voc√™ foi atacado, e foi acompanhado por uma sensa√ß√£o


##  Conjuntos de Dados

### **v2**

- 90GB de textos londrinos de 1800-1875
- 136.344 documentos
- Os 90GB completos ainda n√£o est√£o dispon√≠veis pois n√£o foram tokenizados, mas voc√™ pode encontrar uma amostra de 15GB aqui: https://huggingface.co/datasets/haykgrigorian/TimeCapsuleLLM-London-1800-1875-v2-15GB

 ### Estat√≠sticas de Vi√©s 
  ![Vi√©s de pronome](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v2mini_eval1/pronoun_bias.png)

  ![Vi√©s geogr√°fico](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v2mini_eval1/geographic_bias.png)

  ![Vi√©s temporal](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v2mini_eval1/temporal_bias.png)

Consulte o [relat√≥rio de vi√©s v2](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v2mini_eval1/v2_bias_report.json) para mais informa√ß√µes.


## Como Usar

Este projeto foca principalmente na curadoria de dados hist√≥ricos, prepara√ß√£o para treinamento e constru√ß√£o de um tokenizador. N√£o vou abordar o processo completo de treinamento de LLM, para isso consulte o nanoGPT de Andrej Karpathy.

### Passo 1: Reunir e Preparar Textos Hist√≥ricos 

- Colete arquivos .txt de livros de dom√≠nio p√∫blico, documentos, etc do per√≠odo escolhido (ex: Londres 1800-1850)

- Mantenha-os dentro do intervalo de tempo/local escolhido  
- Limpe os arquivos de texto usando um script ou remova manualmente cabe√ßalhos/rodap√©s do Project Gutenberg, anota√ß√µes modernas ou coisas como erros de OCR.

### Passo 2: Construa um Tokenizador Personalizado

- Execute train_tokenizer.py ou train_tokenizer_hf.py nos dados limpos.
- Isso ir√° gerar vocab.json e merges.txt
- Esses arquivos definem o vocabul√°rio e as regras de mesclagem para seu modelo

### Passo 3: Treine Seu Modelo

- Consulte [nanoGPT de Andrej Karpathy](https://github.com/karpathy/nanoGPT) para o processo de treinamento ou a documenta√ß√£o da arquitetura escolhida.

# FAQ

## O que √© Treinamento Temporal Seletivo?

O Treinamento Temporal Seletivo (STT) √© uma metodologia de aprendizado de m√°quina onde todos os dados de treinamento s√£o especificamente selecionados para se enquadrar em um determinado per√≠odo hist√≥rico. √â feito para modelar a linguagem e o conhecimento daquela √©poca sem influ√™ncia de conceitos modernos. Por exemplo, o modelo que possuo agora (v0.5) foi treinado exclusivamente com dados de 1800-1875, n√£o foi ajustado, mas treinado do zero, resultando em uma sa√≠da que reflete o estilo lingu√≠stico e contexto hist√≥rico daquele per√≠odo.

## Por que n√£o apenas usar fine-tuning ou LoRA?

Para este projeto, estou tentando criar um modelo de linguagem livre de vieses modernos. Se eu ajustar algo como o GPT-2, ele j√° est√° pr√©-treinado e essa informa√ß√£o n√£o ser√° removida. Se eu treinar do zero, o modelo de linguagem n√£o vai fingir ser antigo, ele simplesmente ser√°. O objetivo deste projeto agora √© criar algo que possa raciocinar exclusivamente usando conhecimento de livros de Londres publicados entre 1800 e 1875.

## Que tipo de dados voc√™ usou para o treinamento?

Estou usando livros, documentos legais, jornais e outros escritos de Londres entre 1800‚Äì1875. A lista que eu linkei (para v0) tem cerca de 200, mas para o primeiro treinamento usei apenas 50 arquivos de aproximadamente 187 MB. Voc√™ pode ver uma lista dos documentos:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

Tamanhos dos datasets:
- v0: ~187MB
- v0.5: ~435MB 
- v1: ~6,25GB 
- v2mini-eval1: 15GB

## Qual o tamanho dos modelos?

v0: 16M Par√¢metros

v0.5 123M Par√¢metros

v1: 700M Parameters

v2mini-eval1: 300M Parameters

# Training Specs ? 

# v0/v0.5
GPU: Geforce rtx 4060
CPU: i5-13400F 
Ram: 16GB DDR5.

# v1
GPU: A100 SXM rented

# v2mini-eval1

GPU: A100 SXM rented






























---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2026-01-13

---