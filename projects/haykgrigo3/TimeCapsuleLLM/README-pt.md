
<div align="right">
  <details>
    <summary >üåê Idioma</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="#" title="Em breve">ÁπÅÈ´î‰∏≠Êñá (em breve)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="#" title="Em breve">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä (em breve)</a> |
        | <a href="#" title="Em breve">‡πÑ‡∏ó‡∏¢ (em breve)</a> |
        | <a href="#" title="Em breve">Fran√ßais (em breve)</a>
        | <a href="#" title="Em breve">Deutsch (em breve)</a>
        | <a href="#" title="Em breve">Espa√±ol (em breve)</a>
        | <a href="#" title="Em breve">Italiano (em breve)</a>
        | <a href="#" title="Em breve">–†—É—Å—Å–∫–∏–π (em breve)</a>
        | <a href="#" title="Em breve">Portugu√™s (em breve)</a>
        | <a href="#" title="Em breve">Nederlands (em breve)</a>
        | <a href="#" title="Em breve">Polski (em breve)</a>
        | <a href="#" title="Em breve">ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (em breve)</a>
        | <a href="#" title="Em breve">ŸÅÿßÿ±ÿ≥€å (em breve)</a>
        | <a href="#" title="Em breve">T√ºrk√ße (em breve)</a>
        | <a href="#" title="Em breve">Ti·∫øng Vi·ªát (em breve)</a>
        | <a href="#" title="Em breve">Bahasa Indonesia (em breve)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM

*Um modelo de linguagem treinado **do zero** exclusivamente com dados de determinados locais e per√≠odos para reduzir o vi√©s moderno e emular a voz, o vocabul√°rio e a vis√£o de mundo da √©poca.*

Imagine se um modelo de IA n√£o apenas fingisse ser hist√≥rico, mas realmente fosse.

v0 e v0.5 constru√≠dos sobre [nanoGPT de Andrej Karpathy](https://github.com/karpathy/nanoGPT) Os scripts principais de treinamento e arquitetura do modelo s√£o trabalho dele. 

v1 constru√≠do sobre [Phi 1.5 da Microsoft](https://huggingface.co/microsoft/phi-1_5)

[Link Hugging Face](https://huggingface.co/haykgrigorian/TimeCapsuleLLM)

##  Comportamento do Modelo & Limita√ß√µes

### **v0**  

Os primeiros prompts mostram o modelo respondendo com linguagem e comportamento do s√©culo XIX. 
Exemplo: Prompt: "Who art Henry?" e ele respondeu "I know that man, I have did not a black, the storm." 

![TimeLockLLM Sa√≠da de Exemplo](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

- Sem men√ß√£o a conceitos modernos  
- Vocabul√°rio majoritariamente preciso √† √©poca  
- Frases em sua maioria incoerentes (esperado para ~187MB de dados de treinamento)

### **v0.5** 

Uma melhoria significativa em rela√ß√£o ao v0.  
- Estilo de escrita vitoriano, pontua√ß√£o correta, frases majoritariamente gramaticais  
- Ainda alta taxa de alucina√ß√£o factual  
- Ru√≠do de OCR (‚ÄúDigitized by Google‚Äù) ainda presente nas sa√≠das

![TimeLockLLM Sa√≠da de Exemplo](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

### **v1**

Primeiro modelo a recordar e conectar um evento hist√≥rico real com uma figura real do conjunto de dados.

Exemplo: Prompt: "It was the year of our Lord 1834" 

A sa√≠da: "It was the year of our Lord 1834 and the streets of London were filled with protest and petition. The cause, as many re counted, was not bound in the way of private, but having taken up the same day in the day of Lord Palmerston, the public will receive a short statement of the difficulties under which the day of law has reached us. It is a matter of deep regret, that the present events in the history of the world are clear, and consequently will be'known. It is not true that the very men who first settled in the Gospel at Jerusalem should have so extensive and so interesting a record of the prosperity and prosperity" 

A princ√≠pio assumi que um protesto poderia ter ocorrido coincidentemente no mesmo ano, mas olhe isto: ![1834protesto](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)

### Por que isso importa:

Este √© o primeiro exemplo de um dos meus modelos conectando um ano a tanto um evento hist√≥rico real quanto a uma pessoa real vinculada a esse evento (Lord Palmerston). Modelos anteriores (v0 e v0.5) conseguiam imitar estilos de escrita do s√©culo XIX, mas sempre alucinavam eventos, pessoas e fatos. Isso mostra que o modelo est√° come√ßando a lembrar coisas do conjunto de dados


## Planos Futuros

- Existem quase 175.000 textos publicados em Londres de 1800-1875 no Internet Archive
- Pretendo expandir o corpus e limp√°-lo mais para melhorar as capacidades de racioc√≠nio
- Expandir para diferentes regi√µes e per√≠odos de tempo para modelos hist√≥ricos mais abrangentes

## Como Usar

Este projeto foca principalmente em curar dados hist√≥ricos, prepar√°-los para treinamento e construir um tokenizador. N√£o vou cobrir todo o processo de treinamento de LLM, para isso consulte o nanoGPT de Andrej Karpathy.

### Passo 1: Coletar e Preparar Textos Hist√≥ricos

- Colete arquivos .txt de livros de dom√≠nio p√∫blico, documentos, etc. do per√≠odo escolhido (ex.: Londres 1800-1850)
- Mantenha-os dentro da janela de tempo/local escolhida
- Limpe os arquivos de texto usando um script ou removendo manualmente cabe√ßalhos/rodap√©s do Projeto Gutenberg, anota√ß√µes modernas ou erros de OCR.

### Passo 2: Construir um Tokenizador Personalizado

- Execute train_tokenizer.py ou train_tokenizer_hf.py nos dados limpos.
- Isso ir√° gerar vocab.json e merges.txt
- Esses arquivos definem vocabul√°rio e regras de mesclagem para seu modelo

### Passo 3: Treine Seu Modelo

- Consulte [nanoGPT de Andrej Karpathy](https://github.com/karpathy/nanoGPT) para o processo de treinamento ou a documenta√ß√£o da arquitetura escolhida.

# FAQ

## O que √© Treinamento Temporal Seletivo?

Treinamento Temporal Seletivo (STT) √© uma metodologia de aprendizado de m√°quina onde todos os dados de treinamento s√£o especificamente curados para se enquadrar em um per√≠odo hist√≥rico determinado. Isso √© feito para modelar a linguagem e o conhecimento daquela √©poca sem influ√™ncia de conceitos modernos. Por exemplo, o modelo atual que tenho (v0.5) foi treinado exclusivamente com dados de 1800-1875, n√£o foi apenas ajustado, mas treinado do zero, resultando em uma sa√≠da que reflete o estilo lingu√≠stico e o contexto hist√≥rico daquele per√≠odo.

## Por que n√£o usar apenas fine-tuning ou LoRA?

Neste projeto estou tentando criar um modelo de linguagem livre de vieses modernos. Se eu fizer fine-tuning em algo como GPT-2, ele j√° est√° pr√©-treinado e essa informa√ß√£o n√£o desaparece. Se eu treinar do zero, o modelo de linguagem n√£o vai fingir ser antigo, ele simplesmente ser√°. O objetivo do projeto agora √© criar algo que possa raciocinar exclusivamente usando conhecimento de livros de Londres publicados entre 1800 e 1875.

## Que tipo de dados voc√™ usou para treinar?


Estou usando livros, documentos legais, jornais e outros escritos de Londres entre 1800‚Äì1875. A lista que mencionei (para v0) tem cerca de 200, mas para o primeiro treinamento usei apenas 50 arquivos, aproximadamente ~187 MB. Voc√™ pode ver uma lista dos documentos:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

Tamanhos dos conjuntos de dados:
v0: ~187MB
v0.5: ~435MB 
v1: ~6.25GB 

## Qual o tamanho dos modelos ?

V0: 16M Par√¢metros

V0.5 123M Par√¢metros

V1: 700M Par√¢metros

# Especifica√ß√µes de Treinamento ? 

# V0/V0.5
GPU: Geforce rtx 4060
CPU: i5-13400F 
Ram: 16GB DDR5.

# V1
GPU: A100 alugada















---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-09-30

---