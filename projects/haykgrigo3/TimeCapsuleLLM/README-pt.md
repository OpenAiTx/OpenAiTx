<div align="right">
  <details>
    <summary >üåê Idioma</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="#" title="Coming soon">ÁπÅÈ´î‰∏≠Êñá (em breve)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="#" title="Coming soon">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä (em breve)</a> |
        | <a href="#" title="Coming soon">‡πÑ‡∏ó‡∏¢ (em breve)</a> |
        | <a href="#" title="Coming soon">Fran√ßais (em breve)</a>
        | <a href="#" title="Coming soon">Deutsch (em breve)</a>
        | <a href="#" title="Coming soon">Espa√±ol (em breve)</a>
        | <a href="#" title="Coming soon">Italiano (em breve)</a>
        | <a href="#" title="Coming soon">–†—É—Å—Å–∫–∏–π (em breve)</a>
        | <a href="#" title="Coming soon">Portugu√™s (em breve)</a>
        | <a href="#" title="Coming soon">Nederlands (em breve)</a>
        | <a href="#" title="Coming soon">Polski (em breve)</a>
        | <a href="#" title="Coming soon">ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (em breve)</a>
        | <a href="#" title="Coming soon">ŸÅÿßÿ±ÿ≥€å (em breve)</a>
        | <a href="#" title="Coming soon">T√ºrk√ße (em breve)</a>
        | <a href="#" title="Coming soon">Ti·∫øng Vi·ªát (em breve)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (em breve)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM
Um LLM treinado apenas com dados de certos per√≠odos para reduzir o vi√©s moderno.

Imagine se um modelo de IA n√£o apenas fingisse ser hist√≥rico, mas realmente fosse.

Constru√≠do sobre o [nanoGPT de Andrej Karpathy](https://github.com/karpathy/nanoGPT) Os scripts principais de treinamento e a arquitetura do modelo s√£o de sua autoria.

# Objetivos do Projeto

TimeCapsule LLM √© um projeto experimental que ser√° treinado apenas em textos escritos durante determinados per√≠odos hist√≥ricos. O objetivo √© simular a vis√£o de mundo e a linguagem de eras hist√≥ricas espec√≠ficas.

# Por que o fine tuning n√£o √© suficiente

Se voc√™ apenas fizer fine tuning em um modelo pr√©-treinado, seu LLM ainda conhecer√° conceitos modernos. Claro que conseguir zero vi√©s moderno √© dif√≠cil, mas quero chegar o mais pr√≥ximo poss√≠vel disso. Para n√£o ter vi√©s moderno, √© preciso treinar um modelo do zero.

# Resultados Esperados

Espero que, quando finalizado, esse modelo n√£o conhe√ßa conceitos modernos e n√£o seja capaz de raciocinar al√©m do que foi treinado. N√£o deve reconhecer conceitos/vocabul√°rio modernos e espero que n√£o alucine conhecimentos atuais.

# Atualiza√ß√µes de Progresso

## 9 de julho de 2025

Defini meu per√≠odo de tempo para 1800-1850 e regi√£o: Londres

Reuni uma lista de textos, livros, documentos

At√© agora consegui 50 arquivos txt e vou come√ßar o treinamento do NanoGPT em breve

Vou atualizar isso enquanto houver progresso

## 13 de julho de 2025

Treinei o nanoGPT com 187MB de dados textuais hist√≥ricos.

## 15 de julho de 2025

Comecei a baixar textos para a segunda rodada de treinamento. Estou pegando tudo do Internet Archive e expandi o per√≠odo para 1800-1875. Para obter uma gama diversificada de textos, voc√™ pode usar filtros de assunto e pesquisa para local de publica√ß√£o, per√≠odo e temas no Internet Archive.

![Filtros de Pesquisa](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## 16 de julho de 2025

Baixei cerca de 500 arquivos txt do Internet Archive e, ap√≥s limp√°-los (apenas removendo espa√ßos em branco, cabe√ßalhos do Gutenberg, etc), tenho cerca de 500MB de dados. √â um dataset pequeno, mas da √∫ltima vez treinei com 187MB, ent√£o deve haver pelo menos alguma diferen√ßa percept√≠vel na sa√≠da depois que treinar o segundo modelo. Espero que este modelo consiga pelo menos produzir frases mais coerentes que fa√ßam algum sentido. Claro que n√£o √© garantia, pois ainda √© um dataset muito pequeno, mas j√° √© mais do que usei da √∫ltima vez.

Isso deve ser poss√≠vel no meu pr√≥prio hardware, o que √© bom tamb√©m porque espero ver algum tipo de melhora antes de pular para um dataset maior, o que exigiria alugar uma GPU. Mas n√£o se preocupe, ainda pretendo alugar uma GPU em breve, mas antes quero garantir que meu dataset esteja o mais curado e limpo poss√≠vel. Um dos problemas que tenho √© a limpeza, muitos desses arquivos txt t√™m lixo misturado. Os scripts que usei para limpar funcionam, mas n√£o s√£o 100% eficazes.

Vou treinar esse dataset hoje e deve levar cerca de 4-5 horas. Assim que terminar e eu testar, darei atualiza√ß√µes. Obrigado novamente a todos que est√£o acompanhando meu projeto, j√° teve at√© gente me enviando links de recursos de OCR, ent√£o obrigado! Espero que mais pessoas tentem isso e experimentem com seus pr√≥prios datasets.


### Atualiza√ß√£o do Treinamento

Comecei a treinar em um corpus de 435MB (108 M tokens), est√° indo bem at√© agora. O train loss caiu de 10,9 para 4,9 nas primeiras 2800 itera√ß√µes. Acho que vai levar cerca de 8 ou 9 horas para terminar. Postarei outra atualiza√ß√£o assim que acabar.

## 17 de julho de 2025

O treinamento do segundo modelo terminou, levou cerca de 8 horas e 40 minutos no meu 4060 (3.900 iters/h) para 33.000 itera√ß√µes (5 √©pocas). O train loss final foi 3,73. As sa√≠das ficaram surpreendentemente boas, realmente gera frases coerentes no estilo do s√©culo XIX agora.

## 28 de julho de 2025

Fui em frente e subi a v0.5 no Hugging Face, [confira aqui](https://huggingface.co/haykgrigorian/TimeCapsuleLLM) se quiser. Agora voc√™ pode baixar meu reposit√≥rio e rodar localmente. Infelizmente o nanoGPT n√£o funciona nativamente com o HuggingFace, ent√£o voc√™ ter√° que baixar e rodar o modelo localmente.

Tamb√©m vou come√ßar a curar dados para minha pr√≥xima rodada de treinamento, acredito que vou precisar de 5 a 10 vezes mais dados para alcan√ßar capacidade de racioc√≠nio.

## 2 de agosto de 2025

Vou come√ßar a trabalhar na Vers√£o 1 em breve. Vou precisar fazer a transi√ß√£o da arquitetura do nanoGPT para algo mais moderno. Tenho v√°rias arquiteturas LLM open-source em mente, incluindo: OpenLLaMA v3, Phi-2 e Qwen 1.5B. E para suportar o salto para a V1, vou precisar curar cuidadosamente um dataset muito maior e mais diverso. Preciso de pelo menos 5GB de dados de treinamento limpos.


# Comportamento e Limita√ß√µes do Modelo V0

Os primeiros prompts mostram o modelo respondendo com linguagem e comportamento do s√©culo XIX. Por exemplo, eu perguntei "Who art Henry?" e ele respondeu "I know that man, I have did not a black, the storm." e sim, essa frase n√£o faz sentido, mas o LLM reconheceu que estou perguntando sobre uma pessoa.

![Exemplo de Sa√≠da do TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

N√£o h√° men√ß√£o a conceitos modernos, as sa√≠das cont√™m principalmente palavras e frases do s√©culo XIX.

Ainda precisa de muito trabalho, treinar com 187MB n√£o resultar√° em um modelo que produza texto com racioc√≠nio complexo.

No momento, ele produz frases que n√£o t√™m estrutura completa e, em geral, n√£o fazem sentido, mas isso √© normal para esse tamanho de treinamento.

# Comportamento e Limita√ß√µes do Modelo V0.5

Esta √© uma boa melhoria em rela√ß√£o ao modelo anterior. O estilo de escrita e o vocabul√°rio s√£o vitorianos e quase todas as frases s√£o gramaticalmente corretas com a pontua√ß√£o adequada. E novamente, foi treinado do zero, ent√£o mant√©m os temas do s√©culo XIX.

![Exemplo de Sa√≠da do TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

H√° muitas alucina√ß√µes factuais. Muitas (tipo 100%) das informa√ß√µes (datas, eventos, figuras hist√≥ricas) s√£o inventadas. Al√©m disso, as frases n√£o t√™m muita conex√£o entre si; √†s vezes, talvez duas frases se relacionem, mas al√©m disso n√£o se conectam. Outro problema √© que, √†s vezes, aparece um rodap√© ‚ÄúDigitized by Google‚Äù, ent√£o na pr√≥xima vez que eu treinar preciso garantir que os textos estejam bem limpos. No geral, estou muito satisfeito com os resultados, ainda n√£o √© um LLM, mas definitivamente um gerador de frases.

Estou aprendendo muito e come√ßarei a entender o que preciso melhorar nas pr√≥ximas semanas. Em breve, vou fazer upload dos arquivos!

# Planos Futuros

(Conclu√≠do) Vou come√ßar a trabalhar na vers√£o 0.5, em vez de treinar com 50 livros, vou treinar idealmente com 500-600. No momento estou treinando o nanoGPT com livros de 1800-1850 e especificamente de Londres. H√° alguns desafios, como garantir que os livros encontrados n√£o sejam atualizados ou tenham interpreta√ß√µes modernas, mas sim livros intocados publicados dentro do per√≠odo escolhido.

Quero treinar um novo modelo (v1) com um corpus muito maior, talvez 5-10x maior do que o que usei para o v0.5. Meu objetivo √© ver se consigo fazer com que habilidades de racioc√≠nio emerjam apenas do Treinamento Temporal Seletivo, ser√° uma tarefa mais dif√≠cil e nem tenho certeza se √© poss√≠vel devido √†s limita√ß√µes de dados hist√≥ricos. Nas pr√≥ximas semanas, tentarei reunir dados suficientes para um corpus de 5-10GB. Acredito que se eu conseguir dados limpos e de alta qualidade e alugar uma GPU, haver√° progresso.

# Como Usar Este Projeto

Este projeto foca principalmente na curadoria de dados hist√≥ricos, prepara√ß√£o para treinamento e constru√ß√£o de um tokenizador. N√£o vou cobrir o processo completo de treinamento de LLM, para isso consulte o nanoGPT de Andrej Karpathy.

# Passo 1: Coletar e Preparar Textos Hist√≥ricos

Colete arquivos .txt de livros de dom√≠nio p√∫blico, documentos, etc. do per√≠odo escolhido (ex: Londres 1800-1850)

Voc√™ pode usar o download_texts_improved.py para baixar livros se precisar.

Limpe os arquivos de texto usando um script ou remova manualmente cabe√ßalhos/rodap√©s do Project Gutenberg, anota√ß√µes modernas ou erros de OCR.

prepare_dataset.py deve funcionar bem.

# Passo 2: Construir um Tokenizador Personalizado

Execute train_tokenizer.py ou train_tokenizer_hf.py nos dados limpos.
Isso ir√° gerar vocab.json e merges.txt

Esses arquivos definem o vocabul√°rio e as regras de jun√ß√£o para o seu modelo

# Passo 3: Treinar seu Modelo (nanoGPT)

Consulte [nanoGPT por Andrej Karpathy](https://github.com/karpathy/nanoGPT) para o processo de treinamento.

Voc√™ pode treinar outro LLM se quiser, mas eu usei o nanoGPT

# FAQ

## O que √© Treinamento Temporal Seletivo?

Treinamento Temporal Seletivo (STT) √© uma metodologia de machine learning onde todos os dados de treinamento s√£o especificamente selecionados para pertencer a um determinado per√≠odo hist√≥rico. √â feito para modelar a linguagem e o conhecimento daquela √©poca sem influ√™ncia de conceitos modernos. Por exemplo, o modelo atual (v0.5) foi treinado exclusivamente com dados de 1800-1875, n√£o foi ajustado, mas treinado do zero, resultando em sa√≠das que refletem o estilo lingu√≠stico e o contexto hist√≥rico daquele per√≠odo.

## Por que n√£o apenas usar fine-tuning ou LoRA?

Para este projeto, estou tentando criar um modelo de linguagem sem vi√©s moderno. Se eu fizer fine-tuning em algo como o GPT-2, ele j√° foi pr√©-treinado e essa informa√ß√£o n√£o ser√° removida. Se eu treinar do zero, o modelo de linguagem n√£o vai fingir ser antigo, ele realmente ser√°. O objetivo do projeto agora √© criar algo que possa raciocinar exclusivamente usando conhecimentos de livros de Londres publicados entre 1800 e 1850.

## Que tipo de dados voc√™ usou para o treinamento?

Estou usando livros, documentos legais, jornais e outros escritos de Londres de 1800‚Äì1850. A lista que eu linkei tem cerca de 200, mas para o primeiro treinamento usei apenas 50 arquivos, cerca de 187 MB. Voc√™ pode ver a lista dos documentos:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## Qual o tamanho do modelo Vers√£o 0?

Este modelo √© bem pequeno no momento, estou fazendo isso por divers√£o e seguindo uma regra r√≠gida de treinamento sem fontes modernas. Tem quase 16 milh√µes de par√¢metros, mas vou come√ßar a reunir mais textos antigos para iniciar outro treinamento de modelo. Vou dar atualiza√ß√µes conforme avan√ßo.

## Especifica√ß√µes de Treinamento?

GPU: Geforce rtx 4060
CPU: i5-13400F
Ram: 16GB DDR5.



---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-08-07

---