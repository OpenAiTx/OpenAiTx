<div align="right">
  <details>
    <summary >üåê Idioma</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="#" title="Em breve">ÁπÅÈ´î‰∏≠Êñá (em breve)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="#" title="Em breve">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä (em breve)</a> |
        | <a href="#" title="Em breve">‡πÑ‡∏ó‡∏¢ (em breve)</a> |
        | <a href="#" title="Em breve">Fran√ßais (em breve)</a>
        | <a href="#" title="Em breve">Deutsch (em breve)</a>
        | <a href="#" title="Em breve">Espa√±ol (em breve)</a>
        | <a href="#" title="Em breve">Italiano (em breve)</a>
        | <a href="#" title="Em breve">–†—É—Å—Å–∫–∏–π (em breve)</a>
        | <a href="#" title="Em breve">Portugu√™s (em breve)</a>
        | <a href="#" title="Em breve">Nederlands (em breve)</a>
        | <a href="#" title="Em breve">Polski (em breve)</a>
        | <a href="#" title="Em breve">ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (em breve)</a>
        | <a href="#" title="Em breve">ŸÅÿßÿ±ÿ≥€å (em breve)</a>
        | <a href="#" title="Em breve">T√ºrk√ße (em breve)</a>
        | <a href="#" title="Em breve">Ti·∫øng Vi·ªát (em breve)</a>
        | <a href="#" title="Em breve">Bahasa Indonesia (em breve)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM
Um LLM treinado apenas com dados de determinados per√≠odos para reduzir o vi√©s moderno.

Imagine se um modelo de IA n√£o apenas fingisse ser hist√≥rico, mas realmente fosse.

Baseado no [nanoGPT de Andrej Karpathy](https://github.com/karpathy/nanoGPT). Os scripts principais de treinamento e arquitetura do modelo s√£o de sua autoria. 

# Objetivos do Projeto 

TimeCapsule LLM √© um projeto experimental que ser√° treinado apenas com textos escritos durante determinados per√≠odos hist√≥ricos. O objetivo √© simular a vis√£o de mundo e a linguagem de eras hist√≥ricas espec√≠ficas.

# Por que o fine tuning n√£o √© suficiente 

Se voc√™ apenas fizer fine tuning em um modelo pr√©-treinado, seu LLM ainda conhecer√° conceitos modernos. Claro que atingir zero vi√©s moderno √© dif√≠cil, mas quero chegar o mais pr√≥ximo poss√≠vel disso. Obter aus√™ncia de vi√©s moderno requer treinar um modelo do zero.

# Resultados esperados 

Espero que, quando finalizado, este modelo n√£o conhe√ßa conceitos modernos e n√£o consiga raciocinar al√©m do que foi treinado. Ele n√£o deve reconhecer conceitos/vocabul√°rio modernos e espero que n√£o alucine conhecimentos atuais.

# Atualiza√ß√µes de Progresso

## 9 de julho de 2025

Defini meu per√≠odo hist√≥rico para 1800-1850 e regi√£o: Londres 

Reuni uma lista de textos, livros, documentos 

At√© agora consegui 50 arquivos txt e em breve iniciarei o treinamento do NanoGPT 

Atualizarei isso sempre que houver progresso

## 13 de julho de 2025

Treinei o nanoGPT com 187MB de dados textuais hist√≥ricos. 

## 15 de julho de 2025

Comecei a baixar textos para a segunda rodada de treinamento. Estou buscando tudo no Internet Archive e ampliei o per√≠odo para 1800-1875. Para obter uma variedade maior de textos, voc√™ pode usar filtros de assunto e pesquisa por local de publica√ß√£o, per√≠odo e temas no Internet Archive. 

![Filtros de Pesquisa](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## 16 de julho de 2025

Baixei cerca de 500 arquivos txt do Internet Archive e ap√≥s limp√°-los (apenas removendo espa√ßos em branco, cabe√ßalhos do Gutenberg, etc.) tenho cerca de 500MB de dados. √â um conjunto de dados pequeno, mas da √∫ltima vez treinei com 187MB, ent√£o deve haver pelo menos alguma diferen√ßa percept√≠vel no resultado ap√≥s treinar o segundo modelo. Espero que este modelo consiga pelo menos produzir frases mais coerentes que fa√ßam algum sentido. N√£o √© garantia, claro, pois ainda √© um conjunto de dados muito pequeno, mas √© mais do que usei da √∫ltima vez. 

Isso deve ser vi√°vel no meu pr√≥prio hardware, e √© bom pois espero ver algum tipo de melhoria antes de partir para um conjunto de dados maior, o que exigiria alugar uma GPU. Mas n√£o se preocupe, ainda pretendo alugar uma GPU em breve, mas antes quero garantir que meu conjunto de dados esteja o mais curado e limpo poss√≠vel. Um dos problemas que tenho √© a limpeza, muitos desses arquivos txt t√™m informa√ß√µes sem sentido misturadas. Os scripts que usei para limpeza funcionam, mas n√£o s√£o 100% eficazes. 

Vou treinar esse conjunto de dados hoje e deve levar cerca de 4-5 horas. Assim que terminar e eu testar, darei atualiza√ß√µes. Obrigado novamente a todos que est√£o acompanhando meu projeto, inclusive j√° recebi links de recursos de OCR, ent√£o obrigado! Espero que mais pessoas tentem isso e experimentem com seus pr√≥prios conjuntos de dados. 

## 28 de julho de 2025 

J√° subi a vers√£o v0.5 no Hugging Face, [Confira aqui](https://huggingface.co/haykgrigorian/TimeCapsuleLLM) se quiser. Agora voc√™ pode baixar meu reposit√≥rio e rodar localmente. Infelizmente, o nanoGPT n√£o funciona nativamente com o HuggingFace, ent√£o voc√™ precisar√° baixar e rodar o modelo localmente. 

Tamb√©m vou come√ßar a curar dados para minha pr√≥xima rodada de treinamento, acredito que precisarei de 5 a 10 vezes mais dados para atingir capacidade de racioc√≠nio. 

### Atualiza√ß√£o de Treinamento 

Comecei o treinamento com um corpus de 435MB (108 milh√µes de tokens), est√° indo bem at√© agora. O train loss caiu de 10.9 para 4.9 nas primeiras 2800 itera√ß√µes. Espero que leve cerca de 8 ou 9 horas para completar. Postarei outra atualiza√ß√£o quando terminar.

## 17 de julho de 2025 2:13AM

O treinamento est√° conclu√≠do para o segundo modelo, levou cerca de 8 horas e 40 minutos no meu 4060 (3.900 iters/hora) para 33.000 itera√ß√µes (5 √©pocas). O train loss final foi 3,73. Os resultados foram surpreendentemente bons, agora ele gera frases coerentes no estilo do s√©culo XIX. 

# Comportamento do Modelo V0 & Limita√ß√µes 

Os primeiros prompts mostram o modelo respondendo com linguagem e comportamento dos anos 1800. Por exemplo, solicitei "Who art Henry?" e ele respondeu "I know that man, I have did not a black, the storm." e sim, essa frase n√£o faz sentido, mas o LLM reconheceu que estou perguntando sobre uma pessoa. 

![Sa√≠da de Exemplo do TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

N√£o h√° men√ß√£o de conceitos modernos, as sa√≠das cont√™m principalmente palavras e frases do s√©culo XIX.

Ainda precisa de muito trabalho, treinar com 187MB n√£o lhe dar√° um modelo que produza textos com racioc√≠nio complexo.

Neste momento, ele produz frases que n√£o possuem estrutura completa e, no geral, simplesmente n√£o fazem sentido, mas isso √© normal para o tamanho do treinamento.

# Comportamento do Modelo V0.5 & Limita√ß√µes

Esta √© uma boa melhoria em compara√ß√£o com o modelo anterior. O estilo de escrita e o vocabul√°rio s√£o vitorianos e quase todas as frases est√£o gramaticalmente corretas, com pontua√ß√£o adequada. E novamente, foi treinado do zero, ent√£o se mant√©m em assuntos do s√©culo XIX.

![Sa√≠da de Exemplo TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

H√° muitas alucina√ß√µes factuais. Muitas (tipo 100%) das informa√ß√µes (datas, eventos, figuras hist√≥ricas) s√£o inventadas. Al√©m disso, as frases realmente n√£o t√™m conex√£o umas com as outras, √†s vezes talvez 2 frases se relacionem, mas fora isso n√£o se conectam. Outro problema √© que √†s vezes aparece um rodap√© ‚ÄúDigitized by Google‚Äù, ent√£o na pr√≥xima vez que treinar preciso garantir que os textos estejam bem limpos. No geral, estou muito satisfeito com os resultados, ainda est√° longe de ser um LLM, mas definitivamente √© um gerador de frases.

Estou aprendendo muito e come√ßarei a descobrir o que preciso melhorar nas pr√≥ximas semanas. Em breve vou fazer upload dos arquivos!

# Planos Futuros

(Conclu√≠do) Vou come√ßar a trabalhar na vers√£o 0.5, ao inv√©s de treinar usando 50 livros, vou treinar usando idealmente 500-600. Neste momento estou treinando o nanoGPT usando livros de 1800-1850 e especificamente de Londres. Existem alguns desafios como garantir que os livros encontrados n√£o foram atualizados ou possuem interpreta√ß√µes modernas, mas livros intocados publicados dentro do per√≠odo escolhido.

Quero treinar um novo modelo (v1) com um corpus muito maior, talvez 5-10x maior que o usado no v0.5. Meu objetivo √© ver se consigo fazer habilidades de racioc√≠nio emergirem apenas do Treinamento Temporal Seletivo, ser√° uma tarefa mais dif√≠cil e nem tenho certeza se √© poss√≠vel devido √†s limita√ß√µes dos dados hist√≥ricos. Nas pr√≥ximas semanas tentarei selecionar dados suficientes para um corpus de 5-10GB. Acredito que se eu conseguir dados limpos e de alta qualidade e alugar uma GPU, haver√° progresso.

# Como Usar Este Projeto

Este projeto foca principalmente na curadoria de dados hist√≥ricos, prepara√ß√£o para treinamento e constru√ß√£o de um tokenizador. N√£o vou cobrir o processo completo de treinamento de LLM, para isso consulte o nanoGPT de Andrej Karpathy.

# Passo 1: Coletar e Preparar Textos Hist√≥ricos

Colete arquivos .txt de livros de dom√≠nio p√∫blico, documentos, etc. do per√≠odo escolhido (por exemplo, Londres 1800-1850)

Voc√™ pode usar o download_texts_improved.py para baixar os livros, se precisar.

Limpe os arquivos de texto usando um script ou remova manualmente cabe√ßalhos/rodap√©s do Project Gutenberg, anota√ß√µes modernas ou coisas como erros de OCR.

prepare_dataset.py deve funcionar bem.

# Passo 2: Construir um Tokenizador Personalizado

Execute train_tokenizer.py ou train_tokenizer_hf.py nos dados limpos.
Isso ir√° gerar vocab.json e merges.txt

Esses arquivos definem o vocabul√°rio e as regras de mesclagem para o seu modelo

# Passo 3: Treinar Seu Modelo (nanoGPT)

Consulte o [nanoGPT de Andrej Karpathy](https://github.com/karpathy/nanoGPT) para o processo de treinamento.

Voc√™ pode treinar outro LLM se quiser, mas eu usei o nanoGPT

# FAQ

## O que √© Treinamento Temporal Seletivo?

Treinamento Temporal Seletivo (STT) √© uma metodologia de machine learning onde todos os dados de treinamento s√£o cuidadosamente selecionados para pertencer a um per√≠odo hist√≥rico espec√≠fico. Isso √© feito para modelar a linguagem e o conhecimento daquela √©poca sem influ√™ncia de conceitos modernos. Por exemplo, o modelo atual (v0.5) √© treinado exclusivamente com dados de 1800-1875, n√£o √© fine tuned, mas treinado do zero, resultando em uma sa√≠da que reflete o estilo lingu√≠stico e o contexto hist√≥rico daquele per√≠odo.

## Por que n√£o usar apenas fine-tuning ou LoRA?

Para este projeto, estou tentando criar um modelo de linguagem livre de vieses modernos. Se eu fizer fine-tuning em algo como o GPT-2, ele j√° est√° pr√©-treinado e essa informa√ß√£o n√£o desaparecer√°. Se eu treinar do zero, o modelo de linguagem n√£o vai fingir ser antigo, ele ser√° de fato. O objetivo deste projeto agora √© criar algo que possa raciocinar exclusivamente com o conhecimento de livros de Londres publicados entre 1800 e 1850.

## Que tipo de dados voc√™ usou para o treinamento?

Estou usando livros, documentos legais, jornais e outros escritos de Londres de 1800‚Äì1850. A lista que linkei tem cerca de 200, mas para o primeiro treinamento usei apenas 50 arquivos, cerca de ~187 MB. Voc√™ pode ver uma lista dos documentos:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## Qual o tamanho do modelo da Vers√£o 0?

Este modelo √© bem pequeno no momento, estou fazendo por divers√£o e seguindo uma regra r√≠gida de n√£o usar fontes modernas. Ele tem quase 16 milh√µes de par√¢metros, mas vou come√ßar a reunir mais textos antigos para come√ßar outro treinamento. Vou atualizando conforme avan√ßar.

## Especifica√ß√µes de Treinamento?

GPU: Geforce rtx 4060
CPU: i5-13400F
Ram: 16GB DDR5.



---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-29

---