<div align="right">
  <details>
    <summary >üåê Idioma</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="#" title="Coming soon">ÁπÅÈ´î‰∏≠Êñá (em breve)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="#" title="Coming soon">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä (em breve)</a> |
        | <a href="#" title="Coming soon">‡πÑ‡∏ó‡∏¢ (em breve)</a> |
        | <a href="#" title="Coming soon">Fran√ßais (em breve)</a>
        | <a href="#" title="Coming soon">Deutsch (em breve)</a>
        | <a href="#" title="Coming soon">Espa√±ol (em breve)</a>
        | <a href="#" title="Coming soon">Italiano (em breve)</a>
        | <a href="#" title="Coming soon">–†—É—Å—Å–∫–∏–π (em breve)</a>
        | <a href="#" title="Coming soon">Portugu√™s (em breve)</a>
        | <a href="#" title="Coming soon">Nederlands (em breve)</a>
        | <a href="#" title="Coming soon">Polski (em breve)</a>
        | <a href="#" title="Coming soon">ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (em breve)</a>
        | <a href="#" title="Coming soon">ŸÅÿßÿ±ÿ≥€å (em breve)</a>
        | <a href="#" title="Coming soon">T√ºrk√ße (em breve)</a>
        | <a href="#" title="Coming soon">Ti·∫øng Vi·ªát (em breve)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (em breve)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM
Um LLM treinado apenas com dados de determinados per√≠odos para reduzir o vi√©s moderno.

Imagine se um modelo de IA n√£o apenas fingisse ser hist√≥rico, mas realmente fosse.

Baseado no [nanoGPT de Andrej Karpathy](https://github.com/karpathy/nanoGPT) Os scripts principais de treinamento e a arquitetura do modelo s√£o obra dele. 

# Objetivos do Projeto

TimeCapsule LLM √© um projeto experimental que ser√° treinado somente com textos escritos em certos per√≠odos hist√≥ricos. O objetivo √© simular a vis√£o de mundo e a linguagem de √©pocas espec√≠ficas.

# Por que apenas o fine tuning n√£o √© suficiente

Se voc√™ apenas faz fine tuning em um modelo pr√©-treinado, seu LLM ainda conhecer√° conceitos modernos. Claro que atingir zero vi√©s moderno √© dif√≠cil, mas quero chegar o mais pr√≥ximo poss√≠vel disso. Ter nenhum vi√©s moderno exige treinar um modelo do zero.

# Resultados Esperados

Espero que, quando finalizado, este modelo n√£o conhe√ßa conceitos modernos e n√£o seja capaz de raciocinar al√©m do que foi treinado. Ele n√£o deve reconhecer conceitos/vocabul√°rio modernos e espero que n√£o "alucine" conhecimento atual.

# Atualiza√ß√µes de Progresso

## 9 de julho de 2025

Defini meu per√≠odo para 1800-1850 e regi√£o: Londres

Reuni uma lista de textos, livros, documentos

At√© agora consegui 50 arquivos txt e em breve come√ßarei a treinar o NanoGPT

Vou atualizar isso enquanto houver progresso

## 13 de julho de 2025

Treinei o nanoGPT com 187MB de dados hist√≥ricos em texto.

## 15 de julho de 2025

Comecei a baixar textos para a segunda rodada de treinamento. Estou pegando tudo do Internet Archive e ampliei o per√≠odo para 1800-1875. Para obter uma variedade de textos, voc√™ pode usar filtros de assunto e busca por local de publica√ß√£o, per√≠odo e temas no Internet Archive.

![Filtros de Busca](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## 16 de julho de 2025

Baixei cerca de 500 arquivos txt do Internet Archive e, ap√≥s limp√°-los (apenas removendo espa√ßos em branco, cabe√ßalhos do Gutenberg, etc.), fiquei com cerca de 500MB de dados. √â um conjunto de dados pequeno, mas da √∫ltima vez treinei com 187MB, ent√£o deve haver alguma diferen√ßa percept√≠vel no resultado ap√≥s treinar o segundo modelo. Espero que este modelo consiga ao menos produzir frases mais coerentes que fa√ßam algum sentido. N√£o √© garantia, √© claro, j√° que o conjunto ainda √© min√∫sculo, mas √© mais do que usei da √∫ltima vez.

Isso deve ser poss√≠vel no meu pr√≥prio hardware, e √© bom porque posso ver alguma melhoria antes de partir para um conjunto de dados maior, o que exigiria alugar uma GPU. Mas n√£o se preocupem, ainda pretendo alugar uma GPU em breve, mas antes quero garantir que meu conjunto de dados esteja o mais curado e limpo poss√≠vel. Um dos problemas √© a limpeza, muitos desses txt t√™m trechos sem sentido misturados. Os scripts que usei funcionam, mas n√£o s√£o 100% eficazes.

Vou treinar este conjunto hoje e deve levar cerca de 4-5 horas. Quando terminar e eu testar, darei atualiza√ß√µes. Obrigado a todos que est√£o acompanhando meu projeto, at√© recebi links para recursos de OCR, ent√£o obrigado! Espero que mais pessoas tentem isso e experimentem com seus pr√≥prios conjuntos de dados.

### Atualiza√ß√£o de Treinamento

Comecei a treinar em um corpus de 435MB (108 M tokens), est√° indo bem at√© agora. A perda de treino caiu de 10,9 para 4,9 nas primeiras 2800 itera√ß√µes. Acho que vai levar cerca de 8 ou 9 horas para completar. Postarei outra atualiza√ß√£o quando terminar.

## 17 de julho de 2025 2:13AM

O treinamento terminou para o segundo modelo, levou cerca de 8 horas e 40 minutos no meu 4060 (3.900 iters/h) para 33.000 itera√ß√µes (5 √©pocas). A perda final de treino foi 3,73. Os resultados foram surpreendentemente bons, ele agora gera frases coerentes no estilo do s√©culo XIX.

# Comportamento & Limita√ß√µes do Modelo V0

Os primeiros prompts mostram o modelo respondendo com linguagem e comportamento dos anos 1800. Por exemplo, dei o prompt "Who art Henry?" e ele respondeu "I know that man, I have did not a black, the storm." e sim, essa frase n√£o faz sentido, mas o LLM reconhece que estou perguntando sobre uma pessoa.

![TimeLockLLM Sa√≠da de Exemplo](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

N√£o h√° men√ß√£o de conceitos modernos, as sa√≠das cont√™m principalmente palavras e frases dos anos 1800.

Ainda precisa de muito trabalho, treinar com 187MB n√£o vai te dar um modelo que produza textos com racioc√≠nio complexo.

No momento ele produz frases sem estrutura completa e geralmente sem sentido, mas isso √© normal para esse tamanho de treino.

# Comportamento e Limita√ß√µes do Modelo V0.5

Esta √© uma boa melhoria em compara√ß√£o com o √∫ltimo modelo. O estilo de escrita e o vocabul√°rio s√£o vitorianos e quase todas as frases est√£o gramaticalmente corretas, com pontua√ß√£o adequada. E novamente, este modelo foi treinado do zero, ent√£o ele se mant√©m nos temas do s√©culo XIX.

![Sa√≠da de Amostra do TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

H√° muitas alucina√ß√µes factuais. Muitos (tipo, 100%) dos detalhes (datas, eventos, figuras hist√≥ricas) s√£o inventados. Al√©m disso, as frases realmente n√£o t√™m conex√£o entre si, √†s vezes talvez 2 frases se relacionem, mas al√©m disso n√£o. Outro problema √© que √†s vezes aparece um rodap√© ‚ÄúDigitized by Google‚Äù perdido, ent√£o da pr√≥xima vez que eu treinar preciso garantir que os textos estejam bem limpos. No geral, estou muito satisfeito com os resultados, ainda est√° longe de ser um LLM, mas definitivamente √© um gerador de frases.

Estou aprendendo muito e vou come√ßar a descobrir o que preciso melhorar nas pr√≥ximas semanas. Vou fazer upload dos arquivos em breve!

# Planos Futuros

(Conclu√≠do) Vou come√ßar a trabalhar na vers√£o 0.5, em vez de treinar usando 50 livros, treinarei idealmente usando 500-600. Agora estou treinando o nanoGPT usando livros de 1800-1850 e especificamente de Londres. Existem alguns desafios, como garantir que os livros encontrados n√£o sejam atualizados ou tenham interpreta√ß√µes modernas, mas sim livros intocados publicados dentro do per√≠odo escolhido.

Quero treinar um novo modelo (v1) com um corpus muito maior, talvez 5-10x maior que o que usei no v0.5. Meu objetivo √© ver se consigo que habilidades de racioc√≠nio surjam apenas do Treinamento Temporal Seletivo, isso ser√° uma tarefa mais dif√≠cil e nem tenho certeza se √© poss√≠vel devido √†s limita√ß√µes dos dados hist√≥ricos. Nas pr√≥ximas semanas, tentarei reunir dados suficientes para um corpus de 5-10GB. Acredito que, se eu conseguir dados limpos e de alta qualidade e alugar uma GPU, haver√° progresso.

# Como Usar Este Projeto

Este projeto foca principalmente na curadoria de dados hist√≥ricos, prepara√ß√£o para treinamento e constru√ß√£o de um tokenizador. N√£o vou cobrir o processo completo de treinamento de LLM, para isso consulte o nanoGPT de Andrej Karpathy.

# Passo 1: Recolha e Prepare Textos Hist√≥ricos

Colete arquivos .txt de livros de dom√≠nio p√∫blico, documentos, etc., do per√≠odo escolhido (por exemplo, Londres 1800-1850)

Voc√™ pode usar o download_texts_improved.py para baixar livros para voc√™, se necess√°rio.

Limpe os arquivos de texto usando um script ou remova manualmente cabe√ßalhos/rodap√©s do Project Gutenberg, anota√ß√µes modernas ou erros de OCR.

prepare_dataset.py deve funcionar bem.

# Passo 2: Construa um Tokenizador Personalizado

Execute train_tokenizer.py ou train_tokenizer_hf.py nos dados limpos.
Isso ir√° gerar vocab.json e merges.txt

Esses arquivos definem o vocabul√°rio e as regras de mesclagem para o seu modelo

# Passo 3: Treine Seu Modelo (nanoGPT)

Consulte [nanoGPT de Andrej Karpathy](https://github.com/karpathy/nanoGPT) para o processo de treinamento.

Voc√™ pode treinar outro LLM se quiser, mas eu usei o nanoGPT

# FAQ

## O que √© Treinamento Temporal Seletivo?

Treinamento Temporal Seletivo (STT) √© uma metodologia de aprendizado de m√°quina onde todos os dados de treinamento s√£o especificamente selecionados para estar dentro de um determinado per√≠odo hist√≥rico. Isso √© feito para modelar a linguagem e o conhecimento daquela √©poca sem influ√™ncia de conceitos modernos. Por exemplo, o modelo atual que tenho (v0.5) √© treinado exclusivamente com dados de 1800-1875, n√£o √© fine-tuned, mas treinado do zero, resultando em uma sa√≠da que reflete o estilo lingu√≠stico e contexto hist√≥rico daquele per√≠odo.

## Por que n√£o usar apenas fine-tuning ou LoRA?

Para este projeto, estou tentando criar um modelo de linguagem sem vi√©s moderno. Se eu fizer fine-tuning em algo como o GPT-2, ele j√° est√° pr√©-treinado e essa informa√ß√£o n√£o ser√° removida. Se eu treinar do zero, o modelo de linguagem n√£o vai fingir ser antigo, ele realmente ser√°. O objetivo deste projeto, neste momento, √© criar algo que possa raciocinar exclusivamente usando o conhecimento de livros de Londres publicados entre 1800 e 1850.

## Que tipo de dados voc√™ usou para o treinamento?

Estou usando livros, documentos legais, jornais e outros escritos de Londres, de 1800‚Äì1850. A lista que vinculei tem cerca de 200, mas para o primeiro treinamento usei apenas 50 arquivos, cerca de ~187 MB. Voc√™ pode ver uma lista dos documentos:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## Qual o tamanho do Modelo Vers√£o 0?

Este modelo √© bem pequeno no momento, estou fazendo isso por divers√£o e seguindo uma regra rigorosa de n√£o usar fontes modernas. Ele tem quase 16 milh√µes de par√¢metros, mas vou come√ßar a reunir mais textos antigos para iniciar outro treinamento de modelo. Darei atualiza√ß√µes conforme avan√ßar.

## Especifica√ß√µes do Treinamento?

GPU: Geforce rtx 4060
CPU: i5-13400F
Ram: 16GB DDR5.



---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-21

---