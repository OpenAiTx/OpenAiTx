<div align="right">
  <details>
    <summary >üåê Idioma</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="#" title="Coming soon">ÁπÅÈ´î‰∏≠Êñá (em breve)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="#" title="Coming soon">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä (em breve)</a> |
        | <a href="#" title="Coming soon">‡πÑ‡∏ó‡∏¢ (em breve)</a> |
        | <a href="#" title="Coming soon">Fran√ßais (em breve)</a>
        | <a href="#" title="Coming soon">Deutsch (em breve)</a>
        | <a href="#" title="Coming soon">Espa√±ol (em breve)</a>
        | <a href="#" title="Coming soon">Italiano (em breve)</a>
        | <a href="#" title="Coming soon">–†—É—Å—Å–∫–∏–π (em breve)</a>
        | <a href="#" title="Coming soon">Portugu√™s (em breve)</a>
        | <a href="#" title="Coming soon">Nederlands (em breve)</a>
        | <a href="#" title="Coming soon">Polski (em breve)</a>
        | <a href="#" title="Coming soon">ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (em breve)</a>
        | <a href="#" title="Coming soon">ŸÅÿßÿ±ÿ≥€å (em breve)</a>
        | <a href="#" title="Coming soon">T√ºrk√ße (em breve)</a>
        | <a href="#" title="Coming soon">Ti·∫øng Vi·ªát (em breve)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (em breve)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM
Um LLM treinado apenas com dados de certos per√≠odos de tempo para reduzir o vi√©s moderno.

Imagine se um modelo de IA n√£o apenas fingisse ser hist√≥rico, mas realmente fosse.

Baseado no [nanoGPT de Andrej Karpathy](https://github.com/karpathy/nanoGPT) Os scripts principais de treinamento e a arquitetura do modelo s√£o de sua autoria.

# Objetivos do Projeto

TimeCapsule LLM √© um projeto experimental que ser√° treinado apenas em textos escritos durante certos per√≠odos hist√≥ricos. O objetivo √© simular a vis√£o de mundo e a linguagem de eras hist√≥ricas espec√≠ficas.

# Por que apenas o fine tuning n√£o √© suficiente

Se voc√™ apenas fizer fine tuning em um modelo pr√©-treinado, seu LLM ainda conhecer√° conceitos modernos. Claro que alcan√ßar zero vi√©s moderno √© dif√≠cil, mas quero chegar o mais pr√≥ximo poss√≠vel disso. Para n√£o ter vi√©s moderno, √© preciso treinar um modelo do zero.

# Resultados esperados

Espero que, quando finalizado, este modelo n√£o conhe√ßa conceitos modernos e n√£o consiga raciocinar al√©m do que foi treinado. Ele n√£o deve reconhecer conceitos/vocabul√°rio modernos e espero que n√£o "alucine" conhecimento atual.

# Atualiza√ß√µes de Progresso

## 9 de julho de 2025

Defini meu per√≠odo de tempo para 1800-1850 e regi√£o: Londres

Reuni uma lista de textos, livros, documentos

At√© agora consegui 50 arquivos em txt e vou come√ßar o treinamento do NanoGPT em breve

Vou atualizar aqui √† medida que houver progresso

## 13 de julho de 2025

Treinei o nanoGPT com 187MB de dados hist√≥ricos em texto.

## 15 de julho de 2025

Comecei a baixar textos para a segunda rodada de treinamento. Estou pegando tudo do Internet Archive e expandi o per√≠odo para 1800-1875. Para obter uma variedade maior de textos, voc√™ pode usar filtros de assunto e pesquisa para local de publica√ß√£o, per√≠odo e temas no Internet Archive.

![Filtros de Pesquisa](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## 16 de julho de 2025

Baixei cerca de 500 arquivos txt do Internet Archive e ap√≥s limp√°-los (apenas removendo espa√ßos em branco, cabe√ßalhos do Gutenberg, etc.) fiquei com cerca de 500MB de dados. √â um conjunto de dados pequeno, mas da √∫ltima vez treinei com 187MB, ent√£o deve haver alguma diferen√ßa percept√≠vel na sa√≠da ap√≥s treinar o segundo modelo. Espero que este modelo consiga pelo menos produzir frases mais coerentes que fa√ßam algum sentido. Claro que n√£o √© garantia, pois ainda √© um conjunto de dados bem pequeno, mas √© mais do que usei antes.

Deve ser poss√≠vel treinar no meu pr√≥prio hardware, o que √© bom porque assim posso ver melhorias antes de partir para um conjunto de dados maior, o que exigiria alugar uma GPU. Mas n√£o se preocupem, ainda pretendo alugar uma GPU em breve, s√≥ quero garantir que meu conjunto de dados esteja o mais curado e limpo poss√≠vel antes. Um dos problemas √© a limpeza, muitos desses txt t√™m trechos sem sentido misturados. Os scripts que usei para limpar funcionam, mas n√£o s√£o 100% eficazes.

Vou treinar este conjunto de dados hoje e deve levar cerca de 4-5 horas. Assim que terminar e eu testar, darei atualiza√ß√µes. Obrigado novamente a todos que est√£o acompanhando o projeto, j√° teve gente me enviando links de recursos de OCR, ent√£o obrigado! Espero que mais pessoas tentem isso e experimentem com seus pr√≥prios conjuntos de dados.


### Atualiza√ß√£o de Treinamento

Comecei o treinamento com um corpus de 435MB (108 M tokens), est√° indo bem por enquanto. O train loss caiu de 10,9 para 4,9 nas primeiras 2800 itera√ß√µes. Espero que leve cerca de 8 ou 9 horas para completar. Vou postar outra atualiza√ß√£o quando terminar.

## 17 de julho de 2025 2:13AM

O treinamento do segundo modelo terminou, levou cerca de 8 horas e 40 minutos na minha 4060 (3.900 itera√ß√µes/hora) para 33.000 itera√ß√µes (5 √©pocas). O loss final de treino foi 3,73. Os resultados foram surpreendentemente bons, agora genuinamente gera frases coerentes no estilo do s√©culo XIX.

## 28 de julho de 2025

J√° subi a vers√£o v0.5 no Hugging Face, [Confira aqui](https://huggingface.co/haykgrigorian/TimeCapsuleLLM) se quiser. Agora voc√™ pode baixar meu reposit√≥rio e rodar localmente. Infelizmente o nanoGPT n√£o funciona nativamente com o HuggingFace, ent√£o voc√™ ter√° que baixar e rodar o modelo localmente.

Tamb√©m vou come√ßar a curar dados para meu pr√≥ximo ciclo de treinamento, acredito que precisarei de 5 a 10 vezes mais dados para atingir capacidades de racioc√≠nio.


# Comportamento & Limita√ß√µes do Modelo V0

Os primeiros prompts mostram o modelo respondendo com linguagem e comportamento dos anos 1800. Por exemplo, perguntei "Who art Henry?" e ele respondeu "I know that man, I have did not a black, the storm." e sim, essa frase n√£o faz sentido, mas o LLM est√° reconhecendo que estou perguntando sobre uma pessoa.


![Exemplo de Sa√≠da do TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

N√£o h√° men√ß√£o de conceitos modernos, as sa√≠das cont√™m principalmente palavras e frases do s√©culo XIX.

Ainda precisa de muito trabalho, treinar com 187MB n√£o vai te dar um modelo que produza textos com racioc√≠nio complexo.

Atualmente ele produz senten√ßas sem estrutura frasal completa e, no geral, sem sentido, mas isso √© normal para o tamanho do treinamento.

# Comportamento e Limita√ß√µes do Modelo V0.5

Esta √© uma boa melhoria em compara√ß√£o ao √∫ltimo modelo. O estilo de escrita e o vocabul√°rio s√£o vitorianos e quase todas as frases s√£o gramaticalmente corretas com pontua√ß√£o adequada. E novamente, este foi treinado do zero, ent√£o se mant√©m nos assuntos do s√©culo XIX.

![Exemplo de Sa√≠da do TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

H√° muitas alucina√ß√µes factuais. Muitos (como 100%) dos detalhes (datas, eventos, figuras hist√≥ricas) s√£o inventados. Al√©m disso, as frases realmente n√£o t√™m conex√£o entre si, √†s vezes talvez 2 frases se relacionem, mas al√©m disso n√£o. Outro problema √© que √†s vezes aparece um rodap√© ‚ÄúDigitized by Google‚Äù perdido, ent√£o na pr√≥xima vez que eu treinar realmente preciso garantir que os textos estejam bem limpos. No geral, estou muito satisfeito com os resultados, ainda est√° longe de ser um LLM, mas definitivamente √© um gerador de senten√ßas.

Estou aprendendo muito e vou come√ßar a descobrir o que preciso melhorar nas pr√≥ximas semanas. Em breve vou disponibilizar arquivos!

# Planos Futuros

(Conclu√≠do) Vou come√ßar a trabalhar na vers√£o 0.5, em vez de treinar usando 50 livros, vou treinar idealmente usando 500-600. No momento estou treinando o nanoGPT usando livros de 1800-1850 e especificamente de Londres. Existem alguns desafios como garantir que os livros que encontro n√£o foram atualizados ou tenham interpreta√ß√µes modernas, mas sim livros intactos publicados dentro do per√≠odo escolhido.

Quero treinar um novo modelo (v1) com um corpus muito maior, talvez 5-10x maior que o que usei para o v0.5. Meu objetivo √© ver se consigo fazer habilidades de racioc√≠nio emergirem apenas do Treinamento Temporal Seletivo, o que ser√° uma tarefa mais dif√≠cil e nem tenho certeza se √© poss√≠vel devido √†s limita√ß√µes de dados hist√≥ricos. Nas pr√≥ximas semanas, tentarei reunir dados suficientes para um corpus de 5-10GB. Acredito que se conseguir dados limpos e de alta qualidade e alugar uma GPU, haver√° progresso.

# Como Usar Este Projeto

Este projeto foca principalmente em curar dados hist√≥ricos, prepar√°-los para treinamento e construir um tokenizador. N√£o vou cobrir o processo completo de treinamento de LLM, para isso consulte o nanoGPT do Andrej Karpathy.

# Passo 1: Coletar e Preparar Textos Hist√≥ricos

Colete arquivos .txt de livros de dom√≠nio p√∫blico, documentos, etc, do per√≠odo escolhido (ex: Londres 1800-1850)

Voc√™ pode usar o download_texts_improved.py para baixar livros para voc√™, se precisar.

Limpe os arquivos de texto usando um script ou remova manualmente cabe√ßalhos/rodap√©s do Project Gutenberg, anota√ß√µes modernas ou coisas como erros de OCR.

prepare_dataset.py deve funcionar bem.

# Passo 2: Construir um Tokenizador Personalizado

Execute train_tokenizer.py ou train_tokenizer_hf.py nos dados limpos.
Isso ir√° gerar vocab.json e merges.txt

Esses arquivos definem vocabul√°rio e regras de fus√£o para seu modelo

# Passo 3: Treine Seu Modelo (nanoGPT)

Consulte o [nanoGPT do Andrej Karpathy](https://github.com/karpathy/nanoGPT) para o processo de treinamento.

Voc√™ pode treinar outro LLM se quiser, mas eu usei o nanoGPT

# FAQ

## O que √© Treinamento Temporal Seletivo?

Treinamento Temporal Seletivo (STT) √© uma metodologia de aprendizado de m√°quina onde todos os dados de treinamento s√£o especificamente curados para pertencer a um determinado per√≠odo hist√≥rico. √â feito para modelar a linguagem e o conhecimento daquela √©poca sem influ√™ncia de conceitos modernos. Por exemplo, o modelo que tenho agora (v0.5) foi treinado exclusivamente com dados de 1800-1875, n√£o foi ajustado, mas treinado do zero, resultando em sa√≠das que refletem o estilo lingu√≠stico e o contexto hist√≥rico desse per√≠odo.

## Por que n√£o usar apenas fine-tuning ou LoRA?

Para este projeto, estou tentando criar um modelo de linguagem sem vi√©s moderno. Se eu fizer fine-tuning em algo como o GPT-2, ele j√° est√° pr√©-treinado e essa informa√ß√£o n√£o vai embora. Se eu treinar do zero, o modelo de linguagem n√£o vai fingir ser antigo, ele simplesmente ser√°. O objetivo deste projeto no momento √© criar algo que possa raciocinar exclusivamente usando conhecimento de livros de Londres publicados entre 1800 e 1850.

## Que tipo de dados voc√™ usou para o treinamento?

Estou usando livros, documentos legais, jornais e outros escritos de Londres entre 1800‚Äì1850. A lista que linkei tem cerca de 200, mas para o primeiro treinamento usei apenas 50 arquivos de ~187 MB. Voc√™ pode ver uma lista dos documentos:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## Qual o tamanho do modelo Vers√£o 0?

Este modelo √© muito pequeno no momento, estou fazendo isso por divers√£o e seguindo uma regra rigorosa de treinamento sem fontes modernas. Ele tem quase 16 milh√µes de par√¢metros, mas vou come√ßar a reunir mais textos antigos para iniciar outro treinamento de modelo. Darei atualiza√ß√µes conforme avan√ßar.

## Especifica√ß√µes de Treinamento?

GPU: Geforce rtx 4060
CPU: i5-13400F
Ram: 16GB DDR5.


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-08-02

---