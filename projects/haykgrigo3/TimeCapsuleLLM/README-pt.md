
<div align="right">
  <details>
    <summary >üåê Idioma</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="#" title="Em breve">ÁπÅÈ´î‰∏≠Êñá (em breve)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="#" title="Em breve">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä (em breve)</a> |
        | <a href="#" title="Em breve">‡πÑ‡∏ó‡∏¢ (em breve)</a> |
        | <a href="#" title="Em breve">Fran√ßais (em breve)</a>
        | <a href="#" title="Em breve">Deutsch (em breve)</a>
        | <a href="#" title="Em breve">Espa√±ol (em breve)</a>
        | <a href="#" title="Em breve">Italiano (em breve)</a>
        | <a href="#" title="Em breve">–†—É—Å—Å–∫–∏–π (em breve)</a>
        | <a href="#" title="Em breve">Portugu√™s (em breve)</a>
        | <a href="#" title="Em breve">Nederlands (em breve)</a>
        | <a href="#" title="Em breve">Polski (em breve)</a>
        | <a href="#" title="Em breve">ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (em breve)</a>
        | <a href="#" title="Em breve">ŸÅÿßÿ±ÿ≥€å (em breve)</a>
        | <a href="#" title="Em breve">T√ºrk√ße (em breve)</a>
        | <a href="#" title="Em breve">Ti·∫øng Vi·ªát (em breve)</a>
        | <a href="#" title="Em breve">Bahasa Indonesia (em breve)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM
Um LLM treinado apenas com dados de determinados per√≠odos hist√≥ricos para reduzir o vi√©s moderno.

Imagine se um modelo de IA n√£o apenas fingisse ser hist√≥rico, mas realmente fosse.

Baseado no [nanoGPT de Andrej Karpathy](https://github.com/karpathy/nanoGPT) Os scripts principais de treinamento e a arquitetura do modelo s√£o de autoria dele. 

# Objetivos do Projeto
TimeCapsule LLM √© um projeto experimental que ser√° treinado apenas com textos escritos durante determinados per√≠odos hist√≥ricos. O objetivo √© simular a vis√£o de mundo e a linguagem de eras hist√≥ricas espec√≠ficas.

# Por que o fine tuning n√£o √© suficiente

Se voc√™ apenas fizer o fine tuning de um modelo pr√©-treinado, seu LLM ainda vai conhecer conceitos modernos. Claro que alcan√ßar zero vi√©s moderno √© dif√≠cil, mas quero chegar o mais pr√≥ximo poss√≠vel disso. Para eliminar o vi√©s moderno, √© necess√°rio treinar um modelo do zero.

# Resultados esperados

Espero que, quando finalizado, este modelo n√£o conhe√ßa conceitos modernos e n√£o consiga raciocinar al√©m do que foi treinado. Ele n√£o deve reconhecer vocabul√°rio/conceitos modernos e n√£o deve alucinar conhecimento moderno.

# Atualiza√ß√µes de progresso

## 9 de julho de 2025

Defini meu per√≠odo como 1800-1850 e regi√£o: Londres

Reuni uma lista de textos, livros, documentos

At√© agora consegui 50 arquivos txt e come√ßarei a treinar o NanoGPT em breve

Atualizarei isto enquanto houver progresso

## 13 de julho de 2025

Treinei o nanoGPT com 187MB de dados hist√≥ricos em texto.

## 15 de julho de 2025

Comecei a baixar textos para a segunda rodada de treinamento. Estou pegando tudo do Internet Archive e ampliei o per√≠odo para 1800-1875. Para obter uma gama diversificada de textos, voc√™ pode usar filtros de assunto e pesquisa por local de publica√ß√£o, per√≠odo e temas no Internet Archive.

![Filtros de Pesquisa](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## 16 de julho de 2025

Baixei cerca de 500 arquivos txt do Internet Archive e ap√≥s limp√°-los (removendo espa√ßos em branco, cabe√ßalhos do Gutenberg, etc) fiquei com cerca de 500MB de dados. √â um conjunto de dados pequeno, mas da √∫ltima vez treinei com 187MB ent√£o deve haver pelo menos alguma diferen√ßa not√°vel na sa√≠da ap√≥s treinar o segundo modelo. Espero que este modelo consiga produzir frases mais coerentes que fa√ßam algum sentido. N√£o √© garantido, claro, pois ainda √© um conjunto de dados min√∫sculo, mas √© mais do que usei da √∫ltima vez.

Deve ser poss√≠vel treinar isso no meu pr√≥prio hardware, o que √© bom porque espero ver algum tipo de melhoria antes de migrar para um conjunto maior de dados, o que exigiria alugar uma GPU. Mas n√£o se preocupe, ainda pretendo alugar uma GPU em breve, mas antes disso quero garantir que meu conjunto de dados esteja o mais curado e limpo poss√≠vel. Um dos problemas que tenho √© a limpeza, muitos desses arquivos txt t√™m textos sem sentido misturados. Os scripts que usei para limpeza funcionam, mas n√£o s√£o 100% eficazes.

Vou treinar este conjunto de dados hoje e deve levar cerca de 4-5 horas. Quando terminar e eu testar, darei atualiza√ß√µes. Obrigado novamente a todos que est√£o acompanhando meu projeto, at√© recebi algumas pessoas me enviando links para recursos de OCR ent√£o obrigado! Espero que mais pessoas tentem isso e experimentem com seus pr√≥prios conjuntos de dados.


## 12 de agosto de 2025

A Vers√£o 1 est√° completa, esta vers√£o utiliza Phi 1.5 (700M par√¢metros) e foi treinada em um conjunto de dados com quase 7000 textos. As principais melhorias deste modelo s√£o sua capacidade de √†s vezes referenciar nomes reais e eventos hist√≥ricos. As Vers√µes 0 e 0.5 n√£o conseguiam fazer isso; sempre que eu pedia algo como "No ano de 1821", a sa√≠da era apenas alucina√ß√µes, coisas que nunca aconteceram. Um exemplo disso com a Vers√£o 1:

Meu prompt: Era o ano de nosso Senhor de 1834,

A sa√≠da: "Era o ano de nosso Senhor de 1834 e as ruas de Londres estavam cheias de protestos e peti√ß√µes. A causa, como muitos relataram, n√£o estava vinculada ao privado, mas tendo ocupado o mesmo dia no dia de Lord Palmerston, o p√∫blico receber√° uma breve declara√ß√£o das dificuldades sob as quais o dia da lei nos alcan√ßou. √â motivo de profundo pesar que os atuais eventos na hist√≥ria do mundo sejam claros e, consequentemente, ser√£o conhecidos. N√£o √© verdade que os mesmos homens que primeiro se estabeleceram no Evangelho em Jerusal√©m tenham um registro t√£o extenso e interessante da prosperidade e prosperidade"

A princ√≠pio pensei que fosse coincid√™ncia, mas veja isso: ![1834protest](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)


### Atualiza√ß√£o do Treinamento

Comecei a treinar em um corpus de 435MB (108 M tokens), est√° indo bem at√© agora. A perda de treinamento caiu de 10,9 para 4,9 nas primeiras 2800 itera√ß√µes. Espero que leve cerca de 8 ou 9 horas para completar. Vou postar outra atualiza√ß√£o quando terminar.

## 17 de julho de 2025

O treinamento est√° conclu√≠do para o segundo modelo, minha 4060 levou cerca de 8 horas e 40 minutos (3.900 itera√ß√µes/h) para 33.000 itera√ß√µes (5 √©pocas). A perda final de treinamento foi 3,73. As sa√≠das foram surpreendentemente boas, realmente gera frases coerentes no estilo do s√©culo XIX agora.

## 28 de julho de 2025

Fui em frente e enviei a v0.5 para o Hugging Face, [Confira aqui](https://huggingface.co/haykgrigorian/TimeCapsuleLLM) se quiser. Agora voc√™ pode baixar meu reposit√≥rio e rodar localmente. Infelizmente o nanoGPT n√£o funciona nativamente com o HuggingFace, ent√£o voc√™ ter√° que baixar e rodar o modelo localmente.

Tamb√©m vou come√ßar a curar dados para minha pr√≥xima rodada de treinamento, acredito que vou precisar de talvez 5-10x mais dados para atingir capacidade de racioc√≠nio.

## 2 de agosto de 2025

Vou come√ßar a trabalhar na Vers√£o 1 em breve. Ser√° necess√°rio migrar da arquitetura do nanoGPT para algo mais moderno. Tenho v√°rias arquiteturas LLM de c√≥digo aberto em mente, incluindo: OpenLLaMA v3, Phi-2 e Qwen 1.5B. E para suportar o salto para a V1, precisarei curar cuidadosamente um conjunto de dados muito maior e diverso. Vou precisar de pelo menos 5GB de dados limpos de treinamento.

# Comportamento e Limita√ß√µes do Modelo V0

Prompts iniciais mostram o modelo respondendo com linguagem e comportamento dos anos 1800. Por exemplo, pedi "Quem √© Henry?" e respondeu "Eu conhe√ßo esse homem, eu n√£o fiz um preto, a tempestade." e sim, essa frase n√£o faz sentido, mas o LLM reconhece que estou perguntando sobre uma pessoa.

![Sa√≠da de Amostra do TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

N√£o h√° men√ß√£o a conceitos modernos, as sa√≠das cont√™m principalmente palavras e frases dos anos 1800.

Ainda precisa de muito trabalho, treinar com 187MB n√£o vai te dar um modelo que produza texto com racioc√≠nio complexo.

Atualmente, ele produz senten√ßas que n√£o possuem estrutura completa e, no geral, n√£o fazem sentido, mas isso √© normal para o tamanho do treinamento.

# Comportamento & Limita√ß√µes do Modelo V0.5

Esta √© uma boa melhoria comparada ao √∫ltimo modelo. O estilo de escrita e o vocabul√°rio s√£o vitorianos e quase todas as frases est√£o gramaticalmente corretas, com pontua√ß√£o adequada. E novamente, este foi treinado do zero, ent√£o mant√©m os temas dos anos 1800.

![TimeLockLLM Sample Output](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

H√° muitas alucina√ß√µes factuais. Muitas (como 100%) das informa√ß√µes (datas, eventos, figuras hist√≥ricas) s√£o inventadas. Al√©m disso, as frases n√£o t√™m realmente conex√£o umas com as outras, √†s vezes talvez 2 frases se relacionem, mas al√©m disso n√£o. Outro problema √© que √†s vezes aparece um rodap√© ‚ÄúDigitized by Google‚Äù perdido, ent√£o da pr√≥xima vez que treinar realmente preciso garantir que os textos estejam bem limpos. No geral estou muito satisfeito com os resultados, ainda est√° longe de ser um LLM, mas definitivamente √© um gerador de senten√ßas.

Estou aprendendo muito e come√ßarei a descobrir o que preciso fazer melhor nas pr√≥ximas semanas. Em breve vou fazer upload dos arquivos!

# Comportamento & Limita√ß√µes do Modelo V1

Em breve vou fazer upload de alguns exemplos de sa√≠da e tamb√©m realizar compara√ß√µes entre os 3 modelos com o mesmo prompt. Tamb√©m vou fazer upload do V1 para o Huggingface, como fiz com minha √∫ltima vers√£o; voc√™ pode encontrar minha conta Huggingface aqui: https://huggingface.co/haykgrigorian/TimeCapsuleLLM

# Planos Futuros

(Conclu√≠do) Vou come√ßar a trabalhar na vers√£o 0.5, em vez de treinar usando 50 livros, vou treinar idealmente usando 500-600. Agora estou treinando o nanoGPT usando livros de 1800-1850 e especificamente de Londres. Existem alguns desafios como garantir que os livros que encontro n√£o foram atualizados ou possuem interpreta√ß√µes modernas, mas sim livros intactos publicados dentro do per√≠odo escolhido.

Quero treinar um novo modelo (v1) com um corpus muito maior, talvez 5-10x maior que o que usei para o v0.5. Meu objetivo √© ver se consigo fazer habilidades de racioc√≠nio emergirem apenas do Treinamento Temporal Seletivo, esta ser√° uma tarefa mais dif√≠cil e nem tenho certeza se √© poss√≠vel devido √†s limita√ß√µes dos dados hist√≥ricos. Nas pr√≥ximas semanas tentarei curar dados suficientes para um corpus de 5-10GB. Acredito que se conseguir dados limpos e de alta qualidade e alugar uma GPU, haver√° progresso.

# Como Usar Este Projeto

Este projeto foca principalmente em curar dados hist√≥ricos, prepar√°-los para treinamento e construir um tokenizador. N√£o vou cobrir o processo completo de treinamento de LLM, para isso consulte o nanoGPT de Andrej Karpathy.

# Passo 1: Coletar e Preparar Textos Hist√≥ricos

Colete arquivos .txt de livros de dom√≠nio p√∫blico, documentos, etc, do per√≠odo escolhido (ex: Londres 1800-1850)

Voc√™ pode usar o download_texts_improved.py para baixar livros caso precise.

Limpe os arquivos de texto usando um script ou remova manualmente cabe√ßalhos/rodap√©s do Project Gutenberg, anota√ß√µes modernas ou coisas como erros de OCR.

prepare_dataset.py deve funcionar bem.

# Passo 2: Construir um Tokenizador Personalizado

Execute train_tokenizer.py ou train_tokenizer_hf.py nos dados limpos.
Isso ir√° gerar vocab.json e merges.txt
Esses arquivos definem o vocabul√°rio e as regras de mesclagem para o seu modelo

# Passo 3: Treine Seu Modelo (nanoGPT)

Consulte [nanoGPT por Andrej Karpathy](https://github.com/karpathy/nanoGPT) para o processo de treinamento.

Voc√™ pode treinar um LLM diferente se quiser, mas eu usei o nanoGPT

# FAQ

## O que √© Treinamento Temporal Seletivo?

O Treinamento Temporal Seletivo (STT) √© uma metodologia de aprendizado de m√°quina onde todos os dados de treinamento s√£o especificamente selecionados para se enquadrar em um determinado per√≠odo hist√≥rico. Isso √© feito para modelar a linguagem e o conhecimento daquela era sem influ√™ncia de conceitos modernos. Por exemplo, o modelo que tenho agora (v0.5) foi treinado exclusivamente com dados de 1800‚Äì1875, n√£o foi apenas ajustado, mas treinado do zero, resultando em uma sa√≠da que reflete o estilo lingu√≠stico e o contexto hist√≥rico daquele per√≠odo.

## Por que n√£o usar apenas fine-tuning ou LoRA?

Para este projeto estou tentando criar um modelo de linguagem livre de vieses modernos. Se eu fizer fine-tuning em algo como GPT-2, ele j√° foi pr√©-treinado e essa informa√ß√£o n√£o ser√° eliminada. Se eu treinar do zero, o modelo de linguagem n√£o vai fingir ser antigo, ele realmente ser√°. O objetivo deste projeto agora √© criar algo que possa raciocinar exclusivamente usando o conhecimento de livros publicados em Londres entre 1800 e 1850.

## Que tipo de dados voc√™ usou para o treinamento?

Estou usando livros, documentos legais, jornais e outros escritos de Londres entre 1800‚Äì1850. A lista que eu linkei tem cerca de 200, mas para o primeiro treinamento eu usei apenas 50 arquivos, cerca de ~187 MB. Voc√™ pode ver uma lista dos documentos:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## Qual o tamanho dos modelos?

V0: 16M Par√¢metros

V0.5: 123M Par√¢metros

V1: 700M Par√¢metros

# Especifica√ß√µes de Treinamento?

#V0/V0.5
GPU: Geforce RTX 4060
CPU: i5-13400F
RAM: 16GB DDR5.

#V1

GPU: A100 rented









---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-08-12

---