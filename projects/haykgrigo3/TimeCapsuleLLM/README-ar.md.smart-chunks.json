[
  {
    "Id": 1,
    "Content": "\n<div align=\"right\">\n  <details>\n    <summary >🌐 Language</summary>\n    <div>\n      <div align=\"center\">\n        <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en\">English</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN\">简体中文</a>\n        | <a href=\"#\" title=\"Coming soon\">繁體中文 (coming soon)</a> |\n        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja\">日本語</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko\">한국어</a>\n        | <a href=\"#\" title=\"Coming soon\">हिन्दी (coming soon)</a> |\n        | <a href=\"#\" title=\"Coming soon\">ไทย (coming soon)</a> |\n        | <a href=\"#\" title=\"Coming soon\">Français (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Deutsch (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Español (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Italiano (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Русский (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Português (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Nederlands (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Polski (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">العربية (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">فارسی (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Türkçe (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Tiếng Việt (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Bahasa Indonesia (coming soon)</a>\n\n      </div>\n    </div>\n  </details>\n</div>\n\n# TimeCapsule LLM\n\n*A language model trained **from scratch** exclusively on data from certain places and time periods to reduce modern bias and emulate the voice, vocabulary, and worldview of the era.*\n\nImagine if an AI model didnt just pretend to be historical but actually was.\n\nv0 and v0.5 built on [nanoGPT by Andrej Karpathy](https://github.com/karpathy/nanoGPT) Core training scripts and model architecture are his work. \n",
    "ContentSha": "Nctf/KF+1qyjgomYTtfV5kKwO9Fg9lrGLFemM++WzgU=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "\n<div align=\"right\">\n  <details>\n    <summary >🌐 اللغة</summary>\n    <div>\n      <div align=\"center\">\n        <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en\">English</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN\">简体中文</a>\n        | <a href=\"#\" title=\"قريباً\">繁體中文 (قريباً)</a> |\n        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja\">日本語</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko\">한국어</a>\n        | <a href=\"#\" title=\"قريباً\">हिन्दी (قريباً)</a> |\n        | <a href=\"#\" title=\"قريباً\">ไทย (قريباً)</a> |\n        | <a href=\"#\" title=\"قريباً\">Français (قريباً)</a>\n        | <a href=\"#\" title=\"قريباً\">Deutsch (قريباً)</a>\n        | <a href=\"#\" title=\"قريباً\">Español (قريباً)</a>\n        | <a href=\"#\" title=\"قريباً\">Italiano (قريباً)</a>\n        | <a href=\"#\" title=\"قريباً\">Русский (قريباً)</a>\n        | <a href=\"#\" title=\"قريباً\">Português (قريباً)</a>\n        | <a href=\"#\" title=\"قريباً\">Nederlands (قريباً)</a>\n        | <a href=\"#\" title=\"قريباً\">Polski (قريباً)</a>\n        | <a href=\"#\" title=\"قريباً\">العربية (قريباً)</a>\n        | <a href=\"#\" title=\"قريباً\">فارسی (قريباً)</a>\n        | <a href=\"#\" title=\"قريباً\">Türkçe (قريباً)</a>\n        | <a href=\"#\" title=\"قريباً\">Tiếng Việt (قريباً)</a>\n        | <a href=\"#\" title=\"قريباً\">Bahasa Indonesia (قريباً)</a>\n\n      </div>\n    </div>\n  </details>\n</div>\n\n# تايم كابسول LLM\n\n*نموذج لغوي تم تدريبه **من الصفر** حصرياً على بيانات من أماكن وفترات زمنية محددة بهدف تقليل التحيزات الحديثة ومحاكاة أسلوب ومفردات ورؤية العصر.*\n\nتخيل لو أن نموذج الذكاء الاصطناعي لم يكن فقط يتظاهر بأنه تاريخي بل كان كذلك فعلاً.\n\nالإصدار v0 و v0.5 تم بناؤهما اعتماداً على [nanoGPT بواسطة Andrej Karpathy](https://github.com/karpathy/nanoGPT) نصوص التدريب الأساسية وبنية النموذج هي من عمله. \n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 2,
    "Content": "v1 built on [Phi 1.5 by Microsoft](https://huggingface.co/microsoft/phi-1_5)\n\n[Hugging Face Link](https://huggingface.co/haykgrigorian/TimeCapsuleLLM)\n\n\n##  Model Behavior & Limitations\n\n### **v0**  \n\nEarly prompts show the model responding with 1800's language and behavior. \nExample: Prompt: \"Who art Henry?\" and it replied \"I know that man, I have did not a black, the storm.\" \n\n![TimeLockLLM Sample Output](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)\n\n- No mention of modern concepts  \n- Mostly era-accurate vocabulary  \n- Sentences are mostly incoherent (expected for ~187MB training data)\n\n### **v0.5** \n\nA significant improvement over v0.  \n- Victorian writing style, proper punctuation, mostly grammatical sentences  \n- Still high factual hallucination rate  \n- OCR noise (“Digitized by Google”) still present in outputs\n\n![TimeLockLLM Sample Output](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)\n\n### **v1**\n\nFirst model to recall and connect a real historical event with an actual figure from the dataset.\n\nExample: Prompt: \"It was the year of our Lord 1834\" \n\nThe output: \"It was the year of our Lord 1834 and the streets of London were filled with protest and petition. The cause, as many re counted, was not bound in the way of private, but having taken up the same day in the day of Lord Palmerston, the public will receive a short statement of the difficulties under which the day of law has reached us. It is a matter of deep regret, that the present events in the history of the world are clear, and consequently will be'known. It is not true that the very men who first settled in the Gospel at Jerusalem should have so extensive and so interesting a record of the prosperity and prosperity\" \n\nAt first I assumed that a protest might have coincidentally taken place the same year but take a look at this: ![1834protest](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)\n\n### Why this matters:\n\nThis is the first example of one of my models connecting a year to both a real historical event and a real person tied to that event (Lord Palmerston). Earlier models (v0 and v0.5) could mimic writing styles of the 19th century but would always hallucinate events, people and facts. This shows the model is beggining to remember things from the dataset ",
    "ContentSha": "uiT3rRoD6Qv1qSIL749rEr+c4kH+VDeg6rL8gib2bzs=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "v1 تم بناؤه على [Phi 1.5 بواسطة Microsoft](https://huggingface.co/microsoft/phi-1_5)\n\n[رابط Hugging Face](https://huggingface.co/haykgrigorian/TimeCapsuleLLM)\n\n##  سلوك النموذج والقيود\n\n### **v0**  \n\nالمطالبات المبكرة تظهر النموذج يرد بلغة وسلوك القرن التاسع عشر. \nمثال: الطلب: \"من هو هنري؟\" ورد: \"أنا أعرف ذلك الرجل، لم أفعل شيئًا أسودًا، العاصفة.\"\n\n![مخرج عينة TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)\n\n- لا ذكر للمفاهيم الحديثة  \n- المفردات دقيقة للعصر في الغالب  \n- الجمل غير مترابطة في الغالب (متوقع لحجم بيانات تدريب ~187 ميغابايت)\n\n### **v0.5** \n\nتحسن كبير على الإصدار v0.  \n- أسلوب كتابة فيكتوري، علامات ترقيم صحيحة، جمل نحوية في الغالب  \n- معدل الهلوسة الواقعية لا يزال مرتفعًا  \n- ضوضاء OCR (\"Digitized by Google\") لا تزال تظهر في النتائج\n\n![مخرج عينة TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)\n\n### **v1**\n\nأول نموذج يسترجع ويربط حدث تاريخي حقيقي بشخصية فعلية من مجموعة البيانات.\n\nمثال: الطلب: \"كان ذلك في سنة ربنا 1834\"\n\nالناتج: \"كان ذلك في سنة ربنا 1834 وكانت شوارع لندن مليئة بالاحتجاجات والعرائض. السبب، كما روى الكثيرون، لم يكن مرتبطًا بالخصوصية، بل تم تناوله في نفس اليوم في يوم اللورد بالمرستون، سيتلقى الجمهور بيانًا موجزًا عن الصعوبات التي وصل إليها القانون. ومن المؤسف أن الأحداث الحالية في تاريخ العالم واضحة، وبالتالي ستعرف. ليس صحيحًا أن الرجال الذين أسسوا الإنجيل في القدس كان لديهم سجل واسع ومثير للاهتمام عن الرخاء والازدهار\"\n\nفي البداية افترضت أن احتجاجًا قد حدث بالمصادفة في نفس السنة ولكن انظر لهذا: ![1834protest](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)\n\n### لماذا هذا مهم:\n\nهذا هو أول مثال على أحد نماذجي يربط سنة بحدث تاريخي حقيقي وشخصية حقيقية مرتبطة بذلك الحدث (اللورد بالمرستون). النماذج السابقة (v0 و v0.5) كانت تستطيع تقليد أساليب الكتابة في القرن التاسع عشر ولكنها كانت تهلوس دائمًا الأحداث والأشخاص والحقائق. هذا يُظهر أن النموذج بدأ يتذكر أشياء من مجموعة البيانات\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "qJUbd5u/iOZOtuM+fYFPZOjgtKDl8iwUfAi1n3MVisI=",
        "originContent": "v1 built on [Phi 1.5 by Microsoft](https://huggingface.co/microsoft/phi-1_5)",
        "translatedContent": "v1 تم بناؤه على [Phi 1.5 بواسطة Microsoft](https://huggingface.co/microsoft/phi-1_5)"
      },
      {
        "row": 2,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 3,
        "rowsha": "0kza+J1J1rtVrJmQnk2qdxJsC7lvjDN6mSSj8nyd31Y=",
        "originContent": "[Hugging Face Link](https://huggingface.co/haykgrigorian/TimeCapsuleLLM)",
        "translatedContent": "[رابط Hugging Face](https://huggingface.co/haykgrigorian/TimeCapsuleLLM)"
      },
      {
        "row": 4,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 5,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "##  سلوك النموذج والقيود"
      },
      {
        "row": 6,
        "rowsha": "S8kboQ4LnLUhavjolKrTgNgwVOMJrwpnaY1CWGr6epQ=",
        "originContent": "##  Model Behavior & Limitations",
        "translatedContent": ""
      },
      {
        "row": 7,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "### **v0**  "
      },
      {
        "row": 8,
        "rowsha": "1nqZ1MR/Rxg5du0+tZch+QesCAT+5qMlbhJftBOVqDI=",
        "originContent": "### **v0**  ",
        "translatedContent": ""
      },
      {
        "row": 9,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "المطالبات المبكرة تظهر النموذج يرد بلغة وسلوك القرن التاسع عشر. "
      },
      {
        "row": 10,
        "rowsha": "hlawgmI5oFABN37xYPae7PebQq1TP5c6DMT8b78524g=",
        "originContent": "Early prompts show the model responding with 1800's language and behavior. ",
        "translatedContent": "مثال: الطلب: \"من هو هنري؟\" ورد: \"أنا أعرف ذلك الرجل، لم أفعل شيئًا أسودًا، العاصفة.\""
      },
      {
        "row": 11,
        "rowsha": "5eCkvYiGa3VxONgXmVAvuTajgTbveIO0PKy313G2fos=",
        "originContent": "Example: Prompt: \"Who art Henry?\" and it replied \"I know that man, I have did not a black, the storm.\" ",
        "translatedContent": ""
      },
      {
        "row": 12,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "![مخرج عينة TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)"
      },
      {
        "row": 13,
        "rowsha": "yKIR0teTc66wVDG+jdIyNmAzItXb2JH2ld3D7tm4qnM=",
        "originContent": "![TimeLockLLM Sample Output](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)",
        "translatedContent": ""
      },
      {
        "row": 14,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "- لا ذكر للمفاهيم الحديثة  "
      },
      {
        "row": 15,
        "rowsha": "iCVdbo9EUfepGTS1z1bOwgFn1wDAa9zhHoBLJxEw1w8=",
        "originContent": "- No mention of modern concepts  ",
        "translatedContent": "- المفردات دقيقة للعصر في الغالب  "
      },
      {
        "row": 16,
        "rowsha": "khHeh5VmbIuCnyoNCTDAXLsQOpnqbamm8vyOjvWVmLE=",
        "originContent": "- Mostly era-accurate vocabulary  ",
        "translatedContent": "- الجمل غير مترابطة في الغالب (متوقع لحجم بيانات تدريب ~187 ميغابايت)"
      },
      {
        "row": 17,
        "rowsha": "UP5LBmH8evd8yPger3Fjvs1PXYiVAALQC0JWmcmlN08=",
        "originContent": "- Sentences are mostly incoherent (expected for ~187MB training data)",
        "translatedContent": ""
      },
      {
        "row": 18,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "### **v0.5** "
      },
      {
        "row": 19,
        "rowsha": "5p60D9yLuB40iACJ6apiWJq8lt1AqZAyvFkG70v4xco=",
        "originContent": "### **v0.5** ",
        "translatedContent": ""
      },
      {
        "row": 20,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "تحسن كبير على الإصدار v0.  "
      },
      {
        "row": 21,
        "rowsha": "IPr+C8pVE68OvudYYmIOcbEumcoTmjBiq3HmXPghcvU=",
        "originContent": "A significant improvement over v0.  ",
        "translatedContent": "- أسلوب كتابة فيكتوري، علامات ترقيم صحيحة، جمل نحوية في الغالب  "
      },
      {
        "row": 22,
        "rowsha": "VEokY2Qi826s5lAKKkVoOd5hju8hvOxG21P/L2LPz9k=",
        "originContent": "- Victorian writing style, proper punctuation, mostly grammatical sentences  ",
        "translatedContent": "- معدل الهلوسة الواقعية لا يزال مرتفعًا  "
      },
      {
        "row": 23,
        "rowsha": "hkitRVnW6jxtd+R18O5Olb5eqNIJGCKv4Yt2I+yLrWE=",
        "originContent": "- Still high factual hallucination rate  ",
        "translatedContent": "- ضوضاء OCR (\"Digitized by Google\") لا تزال تظهر في النتائج"
      },
      {
        "row": 24,
        "rowsha": "qlWBqyQVUBsL120iExHvs0PDrN2m0WeMIyRHeHJ1qhM=",
        "originContent": "- OCR noise (“Digitized by Google”) still present in outputs",
        "translatedContent": ""
      },
      {
        "row": 25,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "![مخرج عينة TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)"
      },
      {
        "row": 26,
        "rowsha": "8DhXpgpVtg05XdyplRHf49EFOQNCJVzXA9RpmJQ+y9U=",
        "originContent": "![TimeLockLLM Sample Output](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)",
        "translatedContent": ""
      },
      {
        "row": 27,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "### **v1**"
      },
      {
        "row": 28,
        "rowsha": "mQ9MARMITxOHtqSNIP76nXvTlEM8Ope3qbQ/YG1h6/8=",
        "originContent": "### **v1**",
        "translatedContent": ""
      },
      {
        "row": 29,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "أول نموذج يسترجع ويربط حدث تاريخي حقيقي بشخصية فعلية من مجموعة البيانات."
      },
      {
        "row": 30,
        "rowsha": "sAlnUjWbUBk6czBOHW/IgxyE98WYhX0LrIYmZBr8P2Y=",
        "originContent": "First model to recall and connect a real historical event with an actual figure from the dataset.",
        "translatedContent": ""
      },
      {
        "row": 31,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "مثال: الطلب: \"كان ذلك في سنة ربنا 1834\""
      },
      {
        "row": 32,
        "rowsha": "Cd0DPnkqItUnSk3fHx9ckSSN6MNcZnsEX3S44GZyxTw=",
        "originContent": "Example: Prompt: \"It was the year of our Lord 1834\" ",
        "translatedContent": ""
      },
      {
        "row": 33,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "الناتج: \"كان ذلك في سنة ربنا 1834 وكانت شوارع لندن مليئة بالاحتجاجات والعرائض. السبب، كما روى الكثيرون، لم يكن مرتبطًا بالخصوصية، بل تم تناوله في نفس اليوم في يوم اللورد بالمرستون، سيتلقى الجمهور بيانًا موجزًا عن الصعوبات التي وصل إليها القانون. ومن المؤسف أن الأحداث الحالية في تاريخ العالم واضحة، وبالتالي ستعرف. ليس صحيحًا أن الرجال الذين أسسوا الإنجيل في القدس كان لديهم سجل واسع ومثير للاهتمام عن الرخاء والازدهار\""
      },
      {
        "row": 34,
        "rowsha": "zvm/1tcaMFjcQmuN7y0hczAbbgAwaKRtjBpah5/+kb8=",
        "originContent": "The output: \"It was the year of our Lord 1834 and the streets of London were filled with protest and petition. The cause, as many re counted, was not bound in the way of private, but having taken up the same day in the day of Lord Palmerston, the public will receive a short statement of the difficulties under which the day of law has reached us. It is a matter of deep regret, that the present events in the history of the world are clear, and consequently will be'known. It is not true that the very men who first settled in the Gospel at Jerusalem should have so extensive and so interesting a record of the prosperity and prosperity\" ",
        "translatedContent": ""
      },
      {
        "row": 35,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "في البداية افترضت أن احتجاجًا قد حدث بالمصادفة في نفس السنة ولكن انظر لهذا: ![1834protest](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)"
      },
      {
        "row": 36,
        "rowsha": "WfSfj2g4CByfYQWI59s4C3EwGLZuWfz1t4w49a49e+8=",
        "originContent": "At first I assumed that a protest might have coincidentally taken place the same year but take a look at this: ![1834protest](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)",
        "translatedContent": ""
      },
      {
        "row": 37,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "### لماذا هذا مهم:"
      },
      {
        "row": 38,
        "rowsha": "qlFf3SlVZpptxE7ExYx9wjs1lBnO/bsCjUl1Q4Sw2ds=",
        "originContent": "### Why this matters:",
        "translatedContent": ""
      },
      {
        "row": 39,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "هذا هو أول مثال على أحد نماذجي يربط سنة بحدث تاريخي حقيقي وشخصية حقيقية مرتبطة بذلك الحدث (اللورد بالمرستون). النماذج السابقة (v0 و v0.5) كانت تستطيع تقليد أساليب الكتابة في القرن التاسع عشر ولكنها كانت تهلوس دائمًا الأحداث والأشخاص والحقائق. هذا يُظهر أن النموذج بدأ يتذكر أشياء من مجموعة البيانات"
      },
      {
        "row": 40,
        "rowsha": "WdZfxluSvMeRhCl5u8NEkZI+F2o+eDrUIZyWPgHGv6w=",
        "originContent": "This is the first example of one of my models connecting a year to both a real historical event and a real person tied to that event (Lord Palmerston). Earlier models (v0 and v0.5) could mimic writing styles of the 19th century but would always hallucinate events, people and facts. This shows the model is beggining to remember things from the dataset ",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 3,
    "Content": "\n## Upcoming Plans \n\n- There are nearly 175,000 texts published in London from 1800-1875 on Internet Archive \n- I plan on expanding the corpus and cleaning it more for better reasoning abilities\n- Expanding to different regions and time periods for more historical models\n\n\n## How to Use\n\nThis project focuses mostly on curating historical data, preparing it for training and building a tokenizer. I am not going to cover the full LLM training process, for that refer to nanoGPT by Andrej Karpathy.\n\n### Step 1: Gather and Prepare Historical Texts \n\n- Collect .txt files of public domain books, documents, etc from your chosen time period (e.g., London 1800-1850) \n- Keep them within your chosen time/place window  \n- Clean the text files using a script or manually remove headers/footer from Project Gutenberg, Modern annotations or things like OCR errors.\n\n### Step 2: Build a Custom Tokenizer\n\n- Run train_tokenizer.py or train_tokenizer_hf.py on the cleaned data.\n- This will give you vocab.json and merges.txt\n- Thes files define vocab and merge rules for your model\n\n### Step 3: Train Your Model \n\n- Refer to [nanoGPT by Andrej Karpathy](https://github.com/karpathy/nanoGPT) for the training process or your chosen architecture’s docs.\n\n# FAQ\n\n## What is Selective Temporal Training ?\n\nSelective Temporal Training (STT) is a machine learning methodology where all training data is specifically curated to fall within a specific historical time period. It's done in order to model the language and knowledge of that era without influence from modern concepts. For example, the current model I have now (v0.5) is trained on data exclusively from 1800-1875, it's not fine tuned but trained from scratch resulting in output that reflects the linguistic style and historical context of that time period.\n\n## Why not just use fine-tuning or LoRA?\n\nFor this project I'm trying to create a language model that is unclouded from modern bias. If I fine-tune something like GPT-2, it's already pre-trained and that information won't go away. If I train from scratch the language model won't pretend to be old, it just will be. The Goal for this project right now is to create something can reason exclusively using knowledge from London books published between 1800 and 1875.\n\n## What kind of data did you use for training?\n",
    "ContentSha": "PVmS0gcv9I8wbtUt+qdoQ+UXB+m+tDn+XX5dLnLZNN4=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## الخطط القادمة\n\n- هناك ما يقرب من 175,000 نص منشور في لندن بين عامي 1800-1875 على أرشيف الإنترنت\n- أخطط لتوسيع مجموعة البيانات وتنظيفها بشكل أكبر لتحسين قدرات الاستدلال\n- التوسع إلى مناطق وفترات زمنية مختلفة لنماذج تاريخية أكثر\n\n## كيفية الاستخدام\n\nيركز هذا المشروع في الغالب على تنسيق البيانات التاريخية وتحضيرها للتدريب وبناء معالج الرموز. لن أغطي عملية تدريب النموذج اللغوي الكبير بالكامل، لذلك يُرجى الرجوع إلى nanoGPT بواسطة أندريه كارباتي.\n\n### الخطوة 1: جمع وتحضير النصوص التاريخية\n\n- جمع ملفات .txt من الكتب والمستندات ضمن الملكية العامة من الفترة الزمنية التي تختارها (مثلاً: لندن 1800-1850)\n- احتفظ بها ضمن نافذة الزمان/المكان المختارة\n- نظّف ملفات النصوص باستخدام سكربت أو يدوياً لإزالة رؤوس/ذيول جوتنبرغ، أو التعليقات الحديثة أو أخطاء OCR.\n\n### الخطوة 2: بناء معالج الرموز المخصص\n\n- شغل train_tokenizer.py أو train_tokenizer_hf.py على البيانات المنظفة.\n- سيعطيك هذا vocab.json و merges.txt\n- هذه الملفات تعرف المفردات وقواعد الدمج لنموذجك\n\n### الخطوة 3: درّب نموذجك\n\n- راجع [nanoGPT بواسطة أندريه كارباتي](https://github.com/karpathy/nanoGPT) لعملية التدريب أو وثائق البنية التي اخترتها.\n\n# الأسئلة الشائعة\n\n## ما هو التدريب الزمني الانتقائي؟\n\nالتدريب الزمني الانتقائي (STT) هو منهجية تعلم آلي حيث يتم اختيار كل بيانات التدريب لتكون ضمن فترة زمنية تاريخية محددة. يتم ذلك بهدف نمذجة اللغة والمعرفة لتلك الحقبة دون تأثير من المفاهيم الحديثة. على سبيل المثال، النموذج الحالي الذي أملكه الآن (الإصدار 0.5) مدرب فقط على بيانات بين 1800-1875، لم يتم ضبطه بدقة بل تم تدريبه من الصفر، مما ينتج عنه مخرجات تعكس الأسلوب اللغوي والسياق التاريخي لتلك الفترة.\n\n## لماذا لا أستخدم فقط الضبط الدقيق أو LoRA؟\n\nفي هذا المشروع أهدف إلى إنشاء نموذج لغوي خالٍ من التحيزات الحديثة. إذا قمت بضبط نموذج مثل GPT-2، فهو مدرب مسبقاً وتلك المعلومات لن تختفي. إذا دربت من الصفر، لن يتظاهر النموذج بأنه قديم، بل سيكون كذلك فعلاً. الهدف من هذا المشروع حالياً هو إنشاء شيء يمكنه الاستدلال حصرياً باستخدام معرفة من كتب لندن المنشورة بين 1800 و1875.\n\n## ما نوع البيانات التي استخدمتها في التدريب؟\n\n\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## الخطط القادمة"
      },
      {
        "row": 2,
        "rowsha": "xt81v+AyDswOM8KVPmSYl1lfW4vISpujtQjRWm0mKbA=",
        "originContent": "## Upcoming Plans ",
        "translatedContent": ""
      },
      {
        "row": 3,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "- هناك ما يقرب من 175,000 نص منشور في لندن بين عامي 1800-1875 على أرشيف الإنترنت"
      },
      {
        "row": 4,
        "rowsha": "CulYLHqIw5Hn1cLDvlekSWBe1moOd8QSIirjeM77tlk=",
        "originContent": "- There are nearly 175,000 texts published in London from 1800-1875 on Internet Archive ",
        "translatedContent": "- أخطط لتوسيع مجموعة البيانات وتنظيفها بشكل أكبر لتحسين قدرات الاستدلال"
      },
      {
        "row": 5,
        "rowsha": "VWINtuHshK3Fs4MeBmv24/pZC0iMQ70HtMIowLnrsAI=",
        "originContent": "- I plan on expanding the corpus and cleaning it more for better reasoning abilities",
        "translatedContent": "- التوسع إلى مناطق وفترات زمنية مختلفة لنماذج تاريخية أكثر"
      },
      {
        "row": 6,
        "rowsha": "3b7jQ1J58CCfC/VV9FWJY23SOlvhzx/vIVth3ZBIPog=",
        "originContent": "- Expanding to different regions and time periods for more historical models",
        "translatedContent": ""
      },
      {
        "row": 7,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## كيفية الاستخدام"
      },
      {
        "row": 8,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 9,
        "rowsha": "yJlGQKI3p2nVYC1uqNa6eKPSIzUW38uRiGu0PspqvUU=",
        "originContent": "## How to Use",
        "translatedContent": "يركز هذا المشروع في الغالب على تنسيق البيانات التاريخية وتحضيرها للتدريب وبناء معالج الرموز. لن أغطي عملية تدريب النموذج اللغوي الكبير بالكامل، لذلك يُرجى الرجوع إلى nanoGPT بواسطة أندريه كارباتي."
      },
      {
        "row": 10,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 11,
        "rowsha": "XVoXr9uzZwN09vboETojEQJe057RBzcMUjXmQRCB/jo=",
        "originContent": "This project focuses mostly on curating historical data, preparing it for training and building a tokenizer. I am not going to cover the full LLM training process, for that refer to nanoGPT by Andrej Karpathy.",
        "translatedContent": "### الخطوة 1: جمع وتحضير النصوص التاريخية"
      },
      {
        "row": 12,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 13,
        "rowsha": "h3A3o0uEgrm0Te8o4iW7R0m1bNwx7jj9Q5iUhhV0iTI=",
        "originContent": "### Step 1: Gather and Prepare Historical Texts ",
        "translatedContent": "- جمع ملفات .txt من الكتب والمستندات ضمن الملكية العامة من الفترة الزمنية التي تختارها (مثلاً: لندن 1800-1850)"
      },
      {
        "row": 14,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "- احتفظ بها ضمن نافذة الزمان/المكان المختارة"
      },
      {
        "row": 15,
        "rowsha": "E60jdnnHtQ07Lgqtx5VBWz6jh3ysKu7LMtZHrIYaQiQ=",
        "originContent": "- Collect .txt files of public domain books, documents, etc from your chosen time period (e.g., London 1800-1850) ",
        "translatedContent": "- نظّف ملفات النصوص باستخدام سكربت أو يدوياً لإزالة رؤوس/ذيول جوتنبرغ، أو التعليقات الحديثة أو أخطاء OCR."
      },
      {
        "row": 16,
        "rowsha": "/VflzaebH3p9kM3YtRzEWV0i/lVdD/pGby0wiJoXoco=",
        "originContent": "- Keep them within your chosen time/place window  ",
        "translatedContent": ""
      },
      {
        "row": 17,
        "rowsha": "6P9O4yLTQNPt+lgfziVbtyNP+4+SBhj6UvzpJ+Br7PM=",
        "originContent": "- Clean the text files using a script or manually remove headers/footer from Project Gutenberg, Modern annotations or things like OCR errors.",
        "translatedContent": "### الخطوة 2: بناء معالج الرموز المخصص"
      },
      {
        "row": 18,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 19,
        "rowsha": "QHGXZGXfahA0jwtugXfFPK3RfnoD/MgGK7KZOEu7Yu0=",
        "originContent": "### Step 2: Build a Custom Tokenizer",
        "translatedContent": "- شغل train_tokenizer.py أو train_tokenizer_hf.py على البيانات المنظفة."
      },
      {
        "row": 20,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "- سيعطيك هذا vocab.json و merges.txt"
      },
      {
        "row": 21,
        "rowsha": "hPnDq1u2xQSmYVNea/p6gaqpP4bMQD0lWroth4bud9A=",
        "originContent": "- Run train_tokenizer.py or train_tokenizer_hf.py on the cleaned data.",
        "translatedContent": "- هذه الملفات تعرف المفردات وقواعد الدمج لنموذجك"
      },
      {
        "row": 22,
        "rowsha": "ib4aP51Hu/ktXBqXfyiOtGRLdTUEhyEupPgINhe9JwA=",
        "originContent": "- This will give you vocab.json and merges.txt",
        "translatedContent": ""
      },
      {
        "row": 23,
        "rowsha": "Bit9udkCe0pa2QERa6r17MT8TZN59hctFxW7DQ/7Ka4=",
        "originContent": "- Thes files define vocab and merge rules for your model",
        "translatedContent": "### الخطوة 3: درّب نموذجك"
      },
      {
        "row": 24,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 25,
        "rowsha": "NMTP0o8ite3isz8AN3pbJxnD1y/ULsyqoGpOqqCM9k0=",
        "originContent": "### Step 3: Train Your Model ",
        "translatedContent": "- راجع [nanoGPT بواسطة أندريه كارباتي](https://github.com/karpathy/nanoGPT) لعملية التدريب أو وثائق البنية التي اخترتها."
      },
      {
        "row": 26,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 27,
        "rowsha": "b/wzY4grG20GGDNSiBPVe1mYZhuqNzNEaHz5XhGaZ1I=",
        "originContent": "- Refer to [nanoGPT by Andrej Karpathy](https://github.com/karpathy/nanoGPT) for the training process or your chosen architecture’s docs.",
        "translatedContent": "# الأسئلة الشائعة"
      },
      {
        "row": 28,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 29,
        "rowsha": "OoCxyGfPN5TmdzAkaPphtPx303MJJ7vpfWbKrufGH5g=",
        "originContent": "# FAQ",
        "translatedContent": "## ما هو التدريب الزمني الانتقائي؟"
      },
      {
        "row": 30,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 31,
        "rowsha": "+5dDgPw4ILEotxso4tjjjz1cxwUei16yNQPDUKbgxoo=",
        "originContent": "## What is Selective Temporal Training ?",
        "translatedContent": "التدريب الزمني الانتقائي (STT) هو منهجية تعلم آلي حيث يتم اختيار كل بيانات التدريب لتكون ضمن فترة زمنية تاريخية محددة. يتم ذلك بهدف نمذجة اللغة والمعرفة لتلك الحقبة دون تأثير من المفاهيم الحديثة. على سبيل المثال، النموذج الحالي الذي أملكه الآن (الإصدار 0.5) مدرب فقط على بيانات بين 1800-1875، لم يتم ضبطه بدقة بل تم تدريبه من الصفر، مما ينتج عنه مخرجات تعكس الأسلوب اللغوي والسياق التاريخي لتلك الفترة."
      },
      {
        "row": 32,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 33,
        "rowsha": "hooEARKH4r/sDPh7JUtZAZ6TYMvBkTLZIcfw3g83xos=",
        "originContent": "Selective Temporal Training (STT) is a machine learning methodology where all training data is specifically curated to fall within a specific historical time period. It's done in order to model the language and knowledge of that era without influence from modern concepts. For example, the current model I have now (v0.5) is trained on data exclusively from 1800-1875, it's not fine tuned but trained from scratch resulting in output that reflects the linguistic style and historical context of that time period.",
        "translatedContent": "## لماذا لا أستخدم فقط الضبط الدقيق أو LoRA؟"
      },
      {
        "row": 34,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 35,
        "rowsha": "dVMKQ2mPI1Spc6x6r/jNG0PIR5YKpalU4MXx9JmKp/I=",
        "originContent": "## Why not just use fine-tuning or LoRA?",
        "translatedContent": "في هذا المشروع أهدف إلى إنشاء نموذج لغوي خالٍ من التحيزات الحديثة. إذا قمت بضبط نموذج مثل GPT-2، فهو مدرب مسبقاً وتلك المعلومات لن تختفي. إذا دربت من الصفر، لن يتظاهر النموذج بأنه قديم، بل سيكون كذلك فعلاً. الهدف من هذا المشروع حالياً هو إنشاء شيء يمكنه الاستدلال حصرياً باستخدام معرفة من كتب لندن المنشورة بين 1800 و1875."
      },
      {
        "row": 36,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 37,
        "rowsha": "uUg17WnHfiRHAiHJTO+5TIAJlkny2eH+Ov6Lm0GmrJQ=",
        "originContent": "For this project I'm trying to create a language model that is unclouded from modern bias. If I fine-tune something like GPT-2, it's already pre-trained and that information won't go away. If I train from scratch the language model won't pretend to be old, it just will be. The Goal for this project right now is to create something can reason exclusively using knowledge from London books published between 1800 and 1875.",
        "translatedContent": "## ما نوع البيانات التي استخدمتها في التدريب؟"
      },
      {
        "row": 38,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 39,
        "rowsha": "ByP4WlNmMoG6WIiLJNd6b080/DSciCgWmj9aYSJjAF0=",
        "originContent": "## What kind of data did you use for training?",
        "translatedContent": ""
      },
      {
        "row": 40,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 4,
    "Content": "I'm using books, legal documents, newspapers, and other writings from 1800–1875 London. The list I linked (for v0) has like 200 but for the first training I just used 50 files about ~187 MB. You can view a list of the documents:\nhttps://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt\n\nDataset sizes:\nv0: ~187MB\nv0.5: ~435MB \nv1: ~6.25GB \n\n## How large are the models ?\n\nV0: 16M Parameters\n\nV0.5 123M Parameters\n\nV1: 700M Parameters\n\n# Training Specs ? \n\n# V0/V0.5\nGPU: Geforce rtx 4060\nCPU: i5-13400F \nRam: 16GB DDR5.\n\n# V1\nGPU: A100 rented\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "ContentSha": "FscW2JRCPF/8vYlOatMJlwnmRIDByIlyT+CEgRRkpw0=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "أنا أستخدم كتبًا، وثائق قانونية، صحفًا، وكتابات أخرى من لندن في الفترة 1800–1875. القائمة التي قمتُ بربطها (لإصدار v0) تحتوي على حوالي 200 ملف، لكن في أول تدريب استخدمت فقط 50 ملفًا بحجم حوالي ~187 ميجابايت. يمكنك عرض قائمة الوثائق على الرابط:\nhttps://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt\n\nأحجام مجموعات البيانات:\nv0: ~187 ميجابايت\nv0.5: ~435 ميجابايت\nv1: ~6.25 جيجابايت\n\n## ما حجم النماذج؟\n\nV0: 16 مليون معلمة\n\nV0.5: 123 مليون معلمة\n\nV1: 700 مليون معلمة\n\n# مواصفات التدريب؟\n\n# V0/V0.5\nGPU: Geforce rtx 4060\nCPU: i5-13400F\nالرام: 16GB DDR5.\n\n# V1\nGPU: A100 مستأجرة\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "TiKmhaEUee3SRrWijVMwW3s/qbNf2ziThQdK9PtiW9M=",
        "originContent": "I'm using books, legal documents, newspapers, and other writings from 1800–1875 London. The list I linked (for v0) has like 200 but for the first training I just used 50 files about ~187 MB. You can view a list of the documents:",
        "translatedContent": "أنا أستخدم كتبًا، وثائق قانونية، صحفًا، وكتابات أخرى من لندن في الفترة 1800–1875. القائمة التي قمتُ بربطها (لإصدار v0) تحتوي على حوالي 200 ملف، لكن في أول تدريب استخدمت فقط 50 ملفًا بحجم حوالي ~187 ميجابايت. يمكنك عرض قائمة الوثائق على الرابط:"
      },
      {
        "row": 2,
        "rowsha": "0mxyGiLJxzp9JPCg1oA+nbIwAKJbEC4ei9kSV3Gp84Y=",
        "originContent": "https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt",
        "translatedContent": "https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt"
      },
      {
        "row": 3,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 4,
        "rowsha": "m08SxkUzMWecQCroTF6uw3RhQ50sDm4BaoM0Ql/huS0=",
        "originContent": "Dataset sizes:",
        "translatedContent": "أحجام مجموعات البيانات:"
      },
      {
        "row": 5,
        "rowsha": "uah9doEnpRGciSUsNDHMkbOxtzCOyu4QuAM07AAUp1o=",
        "originContent": "v0: ~187MB",
        "translatedContent": "v0: ~187 ميجابايت"
      },
      {
        "row": 6,
        "rowsha": "4bhyzflpyjgeXMXaoAIxXATaPQqBpKaX945kxvp1ewE=",
        "originContent": "v0.5: ~435MB ",
        "translatedContent": "v0.5: ~435 ميجابايت"
      },
      {
        "row": 7,
        "rowsha": "thwQOIv1UCJQK/UZwcFrVR5tZmLvXp7mbQYGfpiFTN8=",
        "originContent": "v1: ~6.25GB ",
        "translatedContent": "v1: ~6.25 جيجابايت"
      },
      {
        "row": 8,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 9,
        "rowsha": "rrsRMYLHKsjbkzYRKilseYJKKApHSgo9buiBkZFSvlY=",
        "originContent": "## How large are the models ?",
        "translatedContent": "## ما حجم النماذج؟"
      },
      {
        "row": 10,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 11,
        "rowsha": "MVeUG+6lIlH89E699W2WfE+eQmKEul25KNMIY8AtB6M=",
        "originContent": "V0: 16M Parameters",
        "translatedContent": "V0: 16 مليون معلمة"
      },
      {
        "row": 12,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 13,
        "rowsha": "hQIpZXf/UvJi3QFJEM6jrjG7vnb9cvTFg/cvKv0buqM=",
        "originContent": "V0.5 123M Parameters",
        "translatedContent": "V0.5: 123 مليون معلمة"
      },
      {
        "row": 14,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 15,
        "rowsha": "26WDqv9Q/x8glaWZTkjUWL5VNwuuXgUuXs51BhKEskk=",
        "originContent": "V1: 700M Parameters",
        "translatedContent": "V1: 700 مليون معلمة"
      },
      {
        "row": 16,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 17,
        "rowsha": "zaP9HMP5oBcc9MGTkbFGzMO2XxeWWl+QlS9Yz9nfNXc=",
        "originContent": "# Training Specs ? ",
        "translatedContent": "# مواصفات التدريب؟"
      },
      {
        "row": 18,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 19,
        "rowsha": "7TPD+AUCDeUFol0fzwax1DEgBjstPoRauQ1zQAHJdc8=",
        "originContent": "# V0/V0.5",
        "translatedContent": "# V0/V0.5"
      },
      {
        "row": 20,
        "rowsha": "EH8H1HW/C4Tb7LfJgVUnVGsk4pF9l40Rlev8tAkKhjI=",
        "originContent": "GPU: Geforce rtx 4060",
        "translatedContent": "GPU: Geforce rtx 4060"
      },
      {
        "row": 21,
        "rowsha": "vo3FdN37kY6VUB7PruRKfBPJDgsVJyBHIUCn/g8mt68=",
        "originContent": "CPU: i5-13400F ",
        "translatedContent": "CPU: i5-13400F"
      },
      {
        "row": 22,
        "rowsha": "W8fXPiQKUkoNso0PPfTvjYMy0IYo85j+gNXmB0aERO4=",
        "originContent": "Ram: 16GB DDR5.",
        "translatedContent": "الرام: 16GB DDR5."
      },
      {
        "row": 23,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 24,
        "rowsha": "xpPrp95oVxh/XiVWlKlS072VerOoQbJEFU46i3XD2mk=",
        "originContent": "# V1",
        "translatedContent": "# V1"
      },
      {
        "row": 25,
        "rowsha": "cBfLeLi5ORliO9eNHlkjuWbV5U2y03sn/wFBoChMpoc=",
        "originContent": "GPU: A100 rented",
        "translatedContent": "GPU: A100 مستأجرة"
      },
      {
        "row": 26,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 27,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 28,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 29,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 30,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 31,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 32,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 33,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 34,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 35,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 36,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 37,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 38,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 39,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  }
]