[
  {
    "Id": 1,
    "Content": "\n<div align=\"right\">\n  <details>\n    <summary >🌐 Language</summary>\n    <div>\n      <div align=\"center\">\n        <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en\">English</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN\">简体中文</a>\n        | <a href=\"#\" title=\"Coming soon\">繁體中文 (coming soon)</a> |\n        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja\">日本語</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko\">한국어</a>\n        | <a href=\"#\" title=\"Coming soon\">हिन्दी (coming soon)</a> |\n        | <a href=\"#\" title=\"Coming soon\">ไทย (coming soon)</a> |\n        | <a href=\"#\" title=\"Coming soon\">Français (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Deutsch (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Español (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Italiano (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Русский (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Português (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Nederlands (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Polski (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">العربية (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">فارسی (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Türkçe (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Tiếng Việt (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Bahasa Indonesia (coming soon)</a>\n\n      </div>\n    </div>\n  </details>\n</div>\n\n# TimeCapsule LLM\nAn LLM trained only on data from certain time periods to reduce modern bias.\n\nImagine if an AI model didnt just pretend to be historical but actually was.\n\nBuilt on [nanoGPT by Andrej Karpathy](https://github.com/karpathy/nanoGPT) Core training scripts and model architecture are his work. \n\n# Project Goals \n\nTimeCapsule LLM is an expirimental project that will only be trained on texts written during certain time periods. The goal is to simulate the worldview and language of specific historical eras.\n\n# Why fine tuning isn't enough \n\nIf you just fine tune a pre-trained model, your LLM is still gonna know modern concepts. Of course achieving zero modern bias is difficult but I want to get as close as possible to this. Getting no modern bias requires training a model from scratch.\n\n# Expected outcomes \n\nHopefully when finished, this model will not know modern concepts and will not be able to reason beyond what it's been trained on. It shouldnt recognize modern concepts/vocab and I hope it doesn't hallucinate modern knowledge.\n\n# Progress Updates\n\n## July 9th, 2025\n\nI've set my time period for 1800-1850 and region: London \n\nI've gathered a list of texts, books, documents \n\nSo far I've gotten 50 as txt files and will begin training NanoGPT soon \n\nWill update this as long as progress is made\n\n## July 13th, 2025\n\nTrained nanoGPT with 187MB of historial text data. \n\n## July 15th, 2025\n\nI started downloading texts for the second training run. I'm getting everything from Internet Archive and I've expanded the time period to 1800-1875. To get a diverse range of texts, you can use subject and search filters for publication location, time period and subjects on Internet Archive. \n\n![Search Filters](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)\n\n## July 16th, 2025\n\nI downloaded around 500 txt files from Internet Archive and after cleaning them (just deleting whitespaces, Gutenberg headers, etc) I have around 500MB of data. It's a tiny dataset but last time I trained off of 187MB so there should be at least some kind of noticable difference in the output after I train the second model. I'm hoping this model can at least produce more coherent sentences that kind of make sense. It's not a guarantee of course since this is still a tiny tiny dataset, but it's more than what I used last time. \n\nThis should be doable on my own hardware, it's good too because I can hopefully see some kind of improvements before I jump to a bigger dataset which would require me to rent a GPU. But don't worry I still plan on renting a GPU soon, but before I do that I wanna make sure my dataset is as curated and clean as possible. One of the issues I have is cleaning, a lot of these txt files have gibberish mixed in. The scripts I've used for cleaning do work but they're not 100% effective. \n\nI will train this dataset today and it should take around 4-5 hours. Once it's done and I test it, I will give updates. Thank you again to everyone whos checking out my project, I've even had some people even giving me links to OCR resources so Thank you! I hope more people try this out and expirement with they're own datasets. \n\n## July 28th, 2025 \n\nI've gone ahead and uploaded v0.5 to Hugging Face, [Check it out](https://huggingface.co/haykgrigorian/TimeCapsuleLLM) if youd like. You can now download my repo and run it locally. Unfortunately nanoGPT doesn't work natively with HuggingFace, so you'll have to download and run the model locally. \n\nAlso I will begin curating data for my next training run, I believe I'll need maybe 5-10x more data to achieve reasoning capabilities. \n\n### Training Update \n\nI started training on a 435MB (108 M tokens) corpus, it's going pretty smooth right now. Train loss dropped from 10.9 to 4.9 in the first 2800 iterations. I expect it'll take around 8 or 9 hours to complete. I'll post another update once it's done.\n\n## July 17th, 2025 2:13AM\n\nThe training is done for the second model, it took my 4060 around 8 hours and 40 minutes (3,900 iters/hr) for 33,000 iters (5 epochs). Final train loss was 3.73. The outputs were suprisingly good it genuinely generates coherent 19th century style sentences now. \n\n# V0 Model Behavior & Limitations \n\nEarly prompts show the model responding with 1800's language and behavior. For example, I prompted it with \"Who art Henry?\" and it replied \"I know that man, I have did not a black, the storm.\" and yeah that sentence makes no sense but the LLM is recognizing I'm asking about a person. \n\n![TimeLockLLM Sample Output](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)",
    "ContentSha": "RLQDbR0VcjBvqg5kgkBKD3KwC/2mcZW9xrm3vgATFC0=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "<div align=\"right\">\n  <details>\n    <summary >🌐 اللغة</summary>\n    <div>\n      <div align=\"center\">\n        <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en\">English</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN\">简体中文</a>\n        | <a href=\"#\" title=\"قريباً\">繁體中文 (قريباً)</a> |\n        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja\">日本語</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko\">한국어</a>\n        | <a href=\"#\" title=\"قريباً\">हिन्दी (قريباً)</a> |\n        | <a href=\"#\" title=\"قريباً\">ไทย (قريباً)</a> |\n        | <a href=\"#\" title=\"قريباً\">Français (قريباً)</a>\n        | <a href=\"#\" title=\"قريباً\">Deutsch (قريباً)</a>\n        | <a href=\"#\" title=\"قريباً\">Español (قريباً)</a>\n        | <a href=\"#\" title=\"قريباً\">Italiano (قريباً)</a>\n        | <a href=\"#\" title=\"قريباً\">Русский (قريباً)</a>\n        | <a href=\"#\" title=\"قريباً\">Português (قريباً)</a>\n        | <a href=\"#\" title=\"قريباً\">Nederlands (قريباً)</a>\n        | <a href=\"#\" title=\"قريباً\">Polski (قريباً)</a>\n        | <a href=\"#\" title=\"قريباً\">العربية (قريباً)</a>\n        | <a href=\"#\" title=\"قريباً\">فارسی (قريباً)</a>\n        | <a href=\"#\" title=\"قريباً\">Türkçe (قريباً)</a>\n        | <a href=\"#\" title=\"قريباً\">Tiếng Việt (قريباً)</a>\n        | <a href=\"#\" title=\"قريباً\">Bahasa Indonesia (قريباً)</a>\n\n      </div>\n    </div>\n  </details>\n</div>\n\n# تايم كابسول LLM\nنموذج لغوي ضخم (LLM) تم تدريبه فقط على بيانات من فترات زمنية معينة لتقليل الانحياز العصري.\n\nتخيل لو أن نموذج الذكاء الاصطناعي لم يكتفِ بادعاء التاريخية بل كان كذلك فعلاً.\n\nتم بناؤه على [nanoGPT بواسطة أندريه كارباتي](https://github.com/karpathy/nanoGPT) سكريبتات التدريب الأساسية وبنية النموذج هي عمله.\n\n# أهداف المشروع\n\nتايم كابسول LLM هو مشروع تجريبي سيتم تدريبه فقط على نصوص مكتوبة خلال فترات زمنية معينة. الهدف هو محاكاة وجهة نظر ولغة حقب تاريخية محددة.\n\n# لماذا التخصيص وحده لا يكفي\n\nإذا قمت فقط بتخصيص نموذج مدرب مسبقاً، سيظل النموذج يعرف المفاهيم الحديثة. بالطبع تحقيق انعدام الانحياز العصري بالكامل أمر صعب لكني أريد الاقتراب من هذا الهدف قدر الإمكان. الحصول على نموذج بلا انحياز عصري يتطلب التدريب من الصفر.\n\n# النتائج المتوقعة\n\nآمل عند الانتهاء أن هذا النموذج لن يعرف المفاهيم الحديثة ولن يكون قادراً على الاستدلال خارج ما تم تدريبه عليه. يجب ألا يتعرف على المفاهيم أو المفردات الحديثة وأتمنى ألا يهلوس بمعرفة عصرية.\n\n# تحديثات التقدم\n\n## 9 يوليو 2025\n\nلقد حددت فترتي الزمنية بين 1800-1850 والمنطقة: لندن\n\nجمعت قائمة من النصوص والكتب والوثائق\n\nحتى الآن حصلت على 50 ملف نصي وسأبدأ قريباً تدريب NanoGPT\n\nسأواصل تحديث هذا القسم طالما هناك تقدم\n\n## 13 يوليو 2025\n\nتم تدريب nanoGPT على 187 ميغابايت من بيانات نصوص تاريخية.\n\n## 15 يوليو 2025\n\nبدأت في تحميل نصوص للتدريب الثاني. أحصل على كل شيء من Internet Archive وقد وسعت الفترة الزمنية إلى 1800-1875. للحصول على مجموعة متنوعة من النصوص، يمكنك استخدام الفلاتر للموضوع وفترة النشر والموقع في Internet Archive.\n\n![Search Filters](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)\n\n## 16 يوليو 2025\n\nقمت بتحميل حوالي 500 ملف نصي من Internet Archive وبعد تنظيفها (حذف المسافات البيضاء، رؤوس جوتنبرج، إلخ) أصبح لدي حوالي 500 ميغابايت من البيانات. إنها مجموعة بيانات صغيرة لكن آخر مرة تدربت على 187 ميغابايت لذا يجب أن يكون هناك فرق ملحوظ على الأقل في النتائج بعد تدريب النموذج الثاني. آمل أن يتمكن هذا النموذج على الأقل من إنتاج جمل أكثر اتساقاً وتبدو منطقية نوعاً ما. بالطبع هذا ليس مضموناً بعد لأن المجموعة لا تزال صغيرة جداً، لكنها أكثر مما استخدمته في السابق.\n\nيجب أن يكون هذا ممكناً على جهازي الخاص، وهذا أمر جيد لأنني آمل أن أرى بعض التحسينات قبل الانتقال إلى مجموعة بيانات أكبر ستتطلب مني استئجار وحدة معالجة رسومات. لكن لا تقلق لا أزال أخطط لاستئجار GPU قريباً، لكن قبل ذلك أريد التأكد من أن بياناتي نظيفة ومنتقاة قدر الإمكان. إحدى المشكلات التي أواجهها هي التنظيف، فالكثير من هذه الملفات النصية تحتوي على هراء مختلط. السكريبتات التي استخدمتها للتنظيف تعمل لكنها ليست فعالة بنسبة 100%.\n\nسأقوم بتدريب هذه البيانات اليوم ويجب أن يستغرق الأمر حوالي 4-5 ساعات. بمجرد الانتهاء واختبار النتائج سأقدم التحديثات. شكراً مرة أخرى لكل من يتابع مشروعي، حتى أن بعض الأشخاص أرسلوا لي روابط لموارد OCR فشكراً لكم! آمل أن يجرب المزيد من الأشخاص هذا المشروع ويجربوا مجموعاتهم الخاصة من البيانات.\n\n## 28 يوليو 2025\n\nلقد قمت برفع النسخة v0.5 على Hugging Face، [تفضل بالاطلاع](https://huggingface.co/haykgrigorian/TimeCapsuleLLM) إذا أحببت. يمكنك الآن تحميل المستودع وتشغيله محلياً. للأسف nanoGPT لا يعمل مباشرة مع HuggingFace، لذا عليك تحميل وتشغيل النموذج محلياً.\n\nكذلك سأبدأ في تنقية البيانات لجولة التدريب التالية، أعتقد أنني سأحتاج ربما 5-10 أضعاف البيانات لتحقيق قدرات استدلال.\n\n### تحديث التدريب\n\nبدأت التدريب على مجموعة بيانات 435 ميغابايت (108 مليون رمز)، الأمور تسير بسلاسة الآن. انخفضت خسارة التدريب من 10.9 إلى 4.9 في أول 2800 دورة تدريبية. أتوقع أن يستغرق الأمر حوالي 8 أو 9 ساعات حتى يكتمل. سأضع تحديثاً آخر بمجرد الانتهاء.\n\n## 17 يوليو 2025 الساعة 2:13 صباحاً\n\nانتهى تدريب النموذج الثاني، استغرق الأمر حوالي 8 ساعات و40 دقيقة على بطاقتي 4060 (3900 دورة/ساعة) لـ 33000 دورة (5 عصور). الخسارة النهائية كانت 3.73. النتائج كانت مفاجئة إذ ينتج جمل متماسكة بأسلوب القرن التاسع عشر الآن.\n\n# سلوك النموذج V0 والقيود\n\nالعينات المبكرة تظهر استجابة النموذج بلغة وسلوك القرن التاسع عشر. على سبيل المثال، طلبت منه \"Who art Henry?\" فأجاب: \"I know that man, I have did not a black, the storm.\" نعم هذه الجملة ليست منطقية لكن النموذج يتعرف أني أسأل عن شخص.\n\n![TimeLockLLM Sample Output](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "<div align=\"right\">"
      },
      {
        "row": 2,
        "rowsha": "cWgam+tnnXudu7i74+ahMEGk/A9dQS+EwWLAIfi3dHk=",
        "originContent": "<div align=\"right\">",
        "translatedContent": "  <details>"
      },
      {
        "row": 3,
        "rowsha": "orOcu5ARna/hb3RUkj6dBI8pHTM3WHeTvby17l5E0h0=",
        "originContent": "  <details>",
        "translatedContent": "    <summary >🌐 اللغة</summary>"
      },
      {
        "row": 4,
        "rowsha": "TtgkLzblnvP0q9aAIVXt6s2LczXjy5k+QvHKcU0/5Ms=",
        "originContent": "    <summary >🌐 Language</summary>",
        "translatedContent": "    <div>"
      },
      {
        "row": 5,
        "rowsha": "fZtk4rPTAJEEslnbhSVkHEcPlsctYSzAV7CDPL3rJmA=",
        "originContent": "    <div>",
        "translatedContent": "      <div align=\"center\">"
      },
      {
        "row": 6,
        "rowsha": "9KQxOeJSigvTmGWO+mtnl8kZY9zQfueoy8sk4lYm09Q=",
        "originContent": "      <div align=\"center\">",
        "translatedContent": "        <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en\">English</a>"
      },
      {
        "row": 7,
        "rowsha": "CeOhdpchZBoZSEUDtSE417JEcMBSZw18jeJuHJBKB2Y=",
        "originContent": "        <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en\">English</a>",
        "translatedContent": "        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN\">简体中文</a>"
      },
      {
        "row": 8,
        "rowsha": "ToO7MFa3QrNNljdQWIagsnOPxe8cXuuA2m5msIm+Kbs=",
        "originContent": "        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN\">简体中文</a>",
        "translatedContent": "        | <a href=\"#\" title=\"قريباً\">繁體中文 (قريباً)</a> |"
      },
      {
        "row": 9,
        "rowsha": "MRATmWdRMRw0JU4u9h5pMb6GU17lQFgG9v/bpGLr9pM=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">繁體中文 (coming soon)</a> |",
        "translatedContent": "        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja\">日本語</a>"
      },
      {
        "row": 10,
        "rowsha": "GY7LXxG3rk5eFh9itcqM0cTtmHybyjLTf1icB3jN31I=",
        "originContent": "        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja\">日本語</a>",
        "translatedContent": "        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko\">한국어</a>"
      },
      {
        "row": 11,
        "rowsha": "b5TwunGJh+gsAe7aQU3dkfobXF/nknCEta1msDa7XBU=",
        "originContent": "        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko\">한국어</a>",
        "translatedContent": "        | <a href=\"#\" title=\"قريباً\">हिन्दी (قريباً)</a> |"
      },
      {
        "row": 12,
        "rowsha": "1/HCgPsVh2ChqMY+k/VVxEWHPRRmWWCjy5nDRibi3mM=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">हिन्दी (coming soon)</a> |",
        "translatedContent": "        | <a href=\"#\" title=\"قريباً\">ไทย (قريباً)</a> |"
      },
      {
        "row": 13,
        "rowsha": "3lfEHT+5HYFEvbE5cl+xujQPYjtVmzTifT37iqPTWII=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">ไทย (coming soon)</a> |",
        "translatedContent": "        | <a href=\"#\" title=\"قريباً\">Français (قريباً)</a>"
      },
      {
        "row": 14,
        "rowsha": "KmG3P0px2E3bt1lU/w3eGop+zeA1j8xL0k280Zd9m2s=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">Français (coming soon)</a>",
        "translatedContent": "        | <a href=\"#\" title=\"قريباً\">Deutsch (قريباً)</a>"
      },
      {
        "row": 15,
        "rowsha": "CSdHSEXgIs3M2Q/6zIIJ8NbKkZWhydhBqNus94qrPvg=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">Deutsch (coming soon)</a>",
        "translatedContent": "        | <a href=\"#\" title=\"قريباً\">Español (قريباً)</a>"
      },
      {
        "row": 16,
        "rowsha": "8wz7pDuXc3dk+ZcqZ1jmmh8zh6xN3Wb6qWbCjxAj7dA=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">Español (coming soon)</a>",
        "translatedContent": "        | <a href=\"#\" title=\"قريباً\">Italiano (قريباً)</a>"
      },
      {
        "row": 17,
        "rowsha": "op/NqIZs7OjCSpNgpXk8RnqDnTegVPyWUQhuQxvTR7U=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">Italiano (coming soon)</a>",
        "translatedContent": "        | <a href=\"#\" title=\"قريباً\">Русский (قريباً)</a>"
      },
      {
        "row": 18,
        "rowsha": "tAvlfwut/Ad9q1huxc8EREZGv7vYHbrEujzUS8xoaQo=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">Русский (coming soon)</a>",
        "translatedContent": "        | <a href=\"#\" title=\"قريباً\">Português (قريباً)</a>"
      },
      {
        "row": 19,
        "rowsha": "WhhSpeeCUUAqJiVTS4Fvyc6A2c+24Jnj3MW7XLQuIcI=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">Português (coming soon)</a>",
        "translatedContent": "        | <a href=\"#\" title=\"قريباً\">Nederlands (قريباً)</a>"
      },
      {
        "row": 20,
        "rowsha": "0yPXPrWh+Vzc6FBE9iiciw5HwpOSmo05HNe36wfTWCI=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">Nederlands (coming soon)</a>",
        "translatedContent": "        | <a href=\"#\" title=\"قريباً\">Polski (قريباً)</a>"
      },
      {
        "row": 21,
        "rowsha": "mdW6YUUXf5KzI4CwZxrE08ofaLonUOMnJpN3vPR7Y2A=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">Polski (coming soon)</a>",
        "translatedContent": "        | <a href=\"#\" title=\"قريباً\">العربية (قريباً)</a>"
      },
      {
        "row": 22,
        "rowsha": "sw1AXxAGQNvn4eSG9enTWNkwKH0yr6LlVtXBH1j9z8s=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">العربية (coming soon)</a>",
        "translatedContent": "        | <a href=\"#\" title=\"قريباً\">فارسی (قريباً)</a>"
      },
      {
        "row": 23,
        "rowsha": "I8dh9zmXisU0+CpddA55QQgvujH03J/dEnXgj5aFtQM=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">فارسی (coming soon)</a>",
        "translatedContent": "        | <a href=\"#\" title=\"قريباً\">Türkçe (قريباً)</a>"
      },
      {
        "row": 24,
        "rowsha": "7VFv8o6de72ciJrbh3mctfrEgCJhNvuKGWJNOmCaPdM=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">Türkçe (coming soon)</a>",
        "translatedContent": "        | <a href=\"#\" title=\"قريباً\">Tiếng Việt (قريباً)</a>"
      },
      {
        "row": 25,
        "rowsha": "C+XRvFz/D3o9/JyPqwitsxtskFZleJC/oFUr4SEeiHA=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">Tiếng Việt (coming soon)</a>",
        "translatedContent": "        | <a href=\"#\" title=\"قريباً\">Bahasa Indonesia (قريباً)</a>"
      },
      {
        "row": 26,
        "rowsha": "ntGI5B+n9x96pV3ZG5GG83nmocQbxTJjKY7VVwa6Rq8=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">Bahasa Indonesia (coming soon)</a>",
        "translatedContent": ""
      },
      {
        "row": 27,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "      </div>"
      },
      {
        "row": 28,
        "rowsha": "0OM5wNEm0TO56MEBvQzL7AUZM7/3OpgIeqRf2zFre3Q=",
        "originContent": "      </div>",
        "translatedContent": "    </div>"
      },
      {
        "row": 29,
        "rowsha": "fcjTfY+fs8YnY5slBs1sZvWPAqEQR7tzaBDO54skkGQ=",
        "originContent": "    </div>",
        "translatedContent": "  </details>"
      },
      {
        "row": 30,
        "rowsha": "+fQNH2ldI7UM/rqRscP3hUSWAmw1HvQ2wEKDN8JagT0=",
        "originContent": "  </details>",
        "translatedContent": "</div>"
      },
      {
        "row": 31,
        "rowsha": "qsMmUbEPVnxGG5tPJV1vsfpoWbU2jYvZpRr5IKshzyM=",
        "originContent": "</div>",
        "translatedContent": ""
      },
      {
        "row": 32,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "# تايم كابسول LLM"
      },
      {
        "row": 33,
        "rowsha": "VRGjp0FtfvQ89lbX/wJLis2ypCRtNJwe8ViIi29+Rko=",
        "originContent": "# TimeCapsule LLM",
        "translatedContent": "نموذج لغوي ضخم (LLM) تم تدريبه فقط على بيانات من فترات زمنية معينة لتقليل الانحياز العصري."
      },
      {
        "row": 34,
        "rowsha": "XGlykErifWX9oIzV4ZXDc4AUnsuesz8LvpruG76e6uY=",
        "originContent": "An LLM trained only on data from certain time periods to reduce modern bias.",
        "translatedContent": ""
      },
      {
        "row": 35,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "تخيل لو أن نموذج الذكاء الاصطناعي لم يكتفِ بادعاء التاريخية بل كان كذلك فعلاً."
      },
      {
        "row": 36,
        "rowsha": "06wDXO9Un3ot9kUKAGg7CaRsIVSkfS1d2m+EQ6HOFog=",
        "originContent": "Imagine if an AI model didnt just pretend to be historical but actually was.",
        "translatedContent": ""
      },
      {
        "row": 37,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "تم بناؤه على [nanoGPT بواسطة أندريه كارباتي](https://github.com/karpathy/nanoGPT) سكريبتات التدريب الأساسية وبنية النموذج هي عمله."
      },
      {
        "row": 38,
        "rowsha": "2773v/qIXSAsW4pN2HtYcVltfzG1vzgVbHgfjStBQIY=",
        "originContent": "Built on [nanoGPT by Andrej Karpathy](https://github.com/karpathy/nanoGPT) Core training scripts and model architecture are his work. ",
        "translatedContent": ""
      },
      {
        "row": 39,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "# أهداف المشروع"
      },
      {
        "row": 40,
        "rowsha": "wITJJBD/4abiy4E37iMdOcGmifkmz4dALLyk6AhA1kc=",
        "originContent": "# Project Goals ",
        "translatedContent": ""
      },
      {
        "row": 41,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "تايم كابسول LLM هو مشروع تجريبي سيتم تدريبه فقط على نصوص مكتوبة خلال فترات زمنية معينة. الهدف هو محاكاة وجهة نظر ولغة حقب تاريخية محددة."
      },
      {
        "row": 42,
        "rowsha": "LlW7r/H8NhftFgGAHce1f4KThGgsoT8aJ88/IsiLntc=",
        "originContent": "TimeCapsule LLM is an expirimental project that will only be trained on texts written during certain time periods. The goal is to simulate the worldview and language of specific historical eras.",
        "translatedContent": ""
      },
      {
        "row": 43,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "# لماذا التخصيص وحده لا يكفي"
      },
      {
        "row": 44,
        "rowsha": "obYFMCTDj8qHZGo0BQtA2AlwA8JgNcjDK9WlMRI4eq8=",
        "originContent": "# Why fine tuning isn't enough ",
        "translatedContent": ""
      },
      {
        "row": 45,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "إذا قمت فقط بتخصيص نموذج مدرب مسبقاً، سيظل النموذج يعرف المفاهيم الحديثة. بالطبع تحقيق انعدام الانحياز العصري بالكامل أمر صعب لكني أريد الاقتراب من هذا الهدف قدر الإمكان. الحصول على نموذج بلا انحياز عصري يتطلب التدريب من الصفر."
      },
      {
        "row": 46,
        "rowsha": "yNEBOKV/RnG7CvDjiWhkXKK6vqbwki9QKC+Zs+8PzbM=",
        "originContent": "If you just fine tune a pre-trained model, your LLM is still gonna know modern concepts. Of course achieving zero modern bias is difficult but I want to get as close as possible to this. Getting no modern bias requires training a model from scratch.",
        "translatedContent": ""
      },
      {
        "row": 47,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "# النتائج المتوقعة"
      },
      {
        "row": 48,
        "rowsha": "SdJkrN/DUD4+aOCh9lfDM3AAqMxlyukDfye/nzXzxN0=",
        "originContent": "# Expected outcomes ",
        "translatedContent": ""
      },
      {
        "row": 49,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "آمل عند الانتهاء أن هذا النموذج لن يعرف المفاهيم الحديثة ولن يكون قادراً على الاستدلال خارج ما تم تدريبه عليه. يجب ألا يتعرف على المفاهيم أو المفردات الحديثة وأتمنى ألا يهلوس بمعرفة عصرية."
      },
      {
        "row": 50,
        "rowsha": "bsSMnG6qSBf/pVtCQNGFlaKye8GxBKV660amPA/pINE=",
        "originContent": "Hopefully when finished, this model will not know modern concepts and will not be able to reason beyond what it's been trained on. It shouldnt recognize modern concepts/vocab and I hope it doesn't hallucinate modern knowledge.",
        "translatedContent": ""
      },
      {
        "row": 51,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "# تحديثات التقدم"
      },
      {
        "row": 52,
        "rowsha": "8EWRxPaogE2BaXxVJE1VFNAXNdS6KUYPLDFN8xlQ9LE=",
        "originContent": "# Progress Updates",
        "translatedContent": ""
      },
      {
        "row": 53,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## 9 يوليو 2025"
      },
      {
        "row": 54,
        "rowsha": "oq91hnNV5WwmrEF0amya8kSN7gu21MN5nOcR2dPRBZ0=",
        "originContent": "## July 9th, 2025",
        "translatedContent": ""
      },
      {
        "row": 55,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "لقد حددت فترتي الزمنية بين 1800-1850 والمنطقة: لندن"
      },
      {
        "row": 56,
        "rowsha": "yU3u8taDdAaD23tJB9+n/2wmry0GfF+KXqADT4YLuJ8=",
        "originContent": "I've set my time period for 1800-1850 and region: London ",
        "translatedContent": ""
      },
      {
        "row": 57,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "جمعت قائمة من النصوص والكتب والوثائق"
      },
      {
        "row": 58,
        "rowsha": "uQe95shOfOi8NA0M2/CQCXlOjNsiSmGrt5dZbeP4ANs=",
        "originContent": "I've gathered a list of texts, books, documents ",
        "translatedContent": ""
      },
      {
        "row": 59,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "حتى الآن حصلت على 50 ملف نصي وسأبدأ قريباً تدريب NanoGPT"
      },
      {
        "row": 60,
        "rowsha": "i9Kzka7MMa5yKjfdslauZFzKk+gAcnyILwscFoaepYs=",
        "originContent": "So far I've gotten 50 as txt files and will begin training NanoGPT soon ",
        "translatedContent": ""
      },
      {
        "row": 61,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "سأواصل تحديث هذا القسم طالما هناك تقدم"
      },
      {
        "row": 62,
        "rowsha": "Wov5RgnyTA0P0gtJKLL0GTcSf7t8WrgFcAzDufE5Xh4=",
        "originContent": "Will update this as long as progress is made",
        "translatedContent": ""
      },
      {
        "row": 63,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## 13 يوليو 2025"
      },
      {
        "row": 64,
        "rowsha": "FSOOW2G6pyPozg+3u76To6E4Pthd9lRoZE396fwY2I4=",
        "originContent": "## July 13th, 2025",
        "translatedContent": ""
      },
      {
        "row": 65,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "تم تدريب nanoGPT على 187 ميغابايت من بيانات نصوص تاريخية."
      },
      {
        "row": 66,
        "rowsha": "GszU76q+4dgDkDO4uNZpx/9WhQTTCZlq4VYIt0ZdgD0=",
        "originContent": "Trained nanoGPT with 187MB of historial text data. ",
        "translatedContent": ""
      },
      {
        "row": 67,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## 15 يوليو 2025"
      },
      {
        "row": 68,
        "rowsha": "pGPL3z/t2hDtXa67ubNgmRC/+b4O2Yp/0ff3R/9mraE=",
        "originContent": "## July 15th, 2025",
        "translatedContent": ""
      },
      {
        "row": 69,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "بدأت في تحميل نصوص للتدريب الثاني. أحصل على كل شيء من Internet Archive وقد وسعت الفترة الزمنية إلى 1800-1875. للحصول على مجموعة متنوعة من النصوص، يمكنك استخدام الفلاتر للموضوع وفترة النشر والموقع في Internet Archive."
      },
      {
        "row": 70,
        "rowsha": "+jwjqBw9Cr+lRnmxUzCQ0SnVBfXYeJDycuYf/p0JJgg=",
        "originContent": "I started downloading texts for the second training run. I'm getting everything from Internet Archive and I've expanded the time period to 1800-1875. To get a diverse range of texts, you can use subject and search filters for publication location, time period and subjects on Internet Archive. ",
        "translatedContent": ""
      },
      {
        "row": 71,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "![Search Filters](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)"
      },
      {
        "row": 72,
        "rowsha": "XE9Xts6Q8wsZVZHg8uD/1ZXBQ/j2uFLsR9HwsiaqMds=",
        "originContent": "![Search Filters](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)",
        "translatedContent": ""
      },
      {
        "row": 73,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## 16 يوليو 2025"
      },
      {
        "row": 74,
        "rowsha": "iREIjirFxs+ic0QmjQG1FQYKHC5brQaZ5JjPaEto+lU=",
        "originContent": "## July 16th, 2025",
        "translatedContent": ""
      },
      {
        "row": 75,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "قمت بتحميل حوالي 500 ملف نصي من Internet Archive وبعد تنظيفها (حذف المسافات البيضاء، رؤوس جوتنبرج، إلخ) أصبح لدي حوالي 500 ميغابايت من البيانات. إنها مجموعة بيانات صغيرة لكن آخر مرة تدربت على 187 ميغابايت لذا يجب أن يكون هناك فرق ملحوظ على الأقل في النتائج بعد تدريب النموذج الثاني. آمل أن يتمكن هذا النموذج على الأقل من إنتاج جمل أكثر اتساقاً وتبدو منطقية نوعاً ما. بالطبع هذا ليس مضموناً بعد لأن المجموعة لا تزال صغيرة جداً، لكنها أكثر مما استخدمته في السابق."
      },
      {
        "row": 76,
        "rowsha": "c1Ww8CUkqpg5TNm17QY7m130dQycuSFaAia2gfx/uLw=",
        "originContent": "I downloaded around 500 txt files from Internet Archive and after cleaning them (just deleting whitespaces, Gutenberg headers, etc) I have around 500MB of data. It's a tiny dataset but last time I trained off of 187MB so there should be at least some kind of noticable difference in the output after I train the second model. I'm hoping this model can at least produce more coherent sentences that kind of make sense. It's not a guarantee of course since this is still a tiny tiny dataset, but it's more than what I used last time. ",
        "translatedContent": ""
      },
      {
        "row": 77,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "يجب أن يكون هذا ممكناً على جهازي الخاص، وهذا أمر جيد لأنني آمل أن أرى بعض التحسينات قبل الانتقال إلى مجموعة بيانات أكبر ستتطلب مني استئجار وحدة معالجة رسومات. لكن لا تقلق لا أزال أخطط لاستئجار GPU قريباً، لكن قبل ذلك أريد التأكد من أن بياناتي نظيفة ومنتقاة قدر الإمكان. إحدى المشكلات التي أواجهها هي التنظيف، فالكثير من هذه الملفات النصية تحتوي على هراء مختلط. السكريبتات التي استخدمتها للتنظيف تعمل لكنها ليست فعالة بنسبة 100%."
      },
      {
        "row": 78,
        "rowsha": "h/hyxvgOlOm5er9sn3CL2wmktMoq2q+qZi5Vi7upXGI=",
        "originContent": "This should be doable on my own hardware, it's good too because I can hopefully see some kind of improvements before I jump to a bigger dataset which would require me to rent a GPU. But don't worry I still plan on renting a GPU soon, but before I do that I wanna make sure my dataset is as curated and clean as possible. One of the issues I have is cleaning, a lot of these txt files have gibberish mixed in. The scripts I've used for cleaning do work but they're not 100% effective. ",
        "translatedContent": ""
      },
      {
        "row": 79,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "سأقوم بتدريب هذه البيانات اليوم ويجب أن يستغرق الأمر حوالي 4-5 ساعات. بمجرد الانتهاء واختبار النتائج سأقدم التحديثات. شكراً مرة أخرى لكل من يتابع مشروعي، حتى أن بعض الأشخاص أرسلوا لي روابط لموارد OCR فشكراً لكم! آمل أن يجرب المزيد من الأشخاص هذا المشروع ويجربوا مجموعاتهم الخاصة من البيانات."
      },
      {
        "row": 80,
        "rowsha": "8LYxYjHwrXnU59N+13Fd9m653Tgyom3yQQAWGIrSPSc=",
        "originContent": "I will train this dataset today and it should take around 4-5 hours. Once it's done and I test it, I will give updates. Thank you again to everyone whos checking out my project, I've even had some people even giving me links to OCR resources so Thank you! I hope more people try this out and expirement with they're own datasets. ",
        "translatedContent": ""
      },
      {
        "row": 81,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## 28 يوليو 2025"
      },
      {
        "row": 82,
        "rowsha": "13xzxThQO3iEGB5PU51V1kYgK85//yEFbCkwtu07a2U=",
        "originContent": "## July 28th, 2025 ",
        "translatedContent": ""
      },
      {
        "row": 83,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "لقد قمت برفع النسخة v0.5 على Hugging Face، [تفضل بالاطلاع](https://huggingface.co/haykgrigorian/TimeCapsuleLLM) إذا أحببت. يمكنك الآن تحميل المستودع وتشغيله محلياً. للأسف nanoGPT لا يعمل مباشرة مع HuggingFace، لذا عليك تحميل وتشغيل النموذج محلياً."
      },
      {
        "row": 84,
        "rowsha": "R6HmXRpR+7izGCE6kI9wyELs4kYvAtLMWO75nOXAf1M=",
        "originContent": "I've gone ahead and uploaded v0.5 to Hugging Face, [Check it out](https://huggingface.co/haykgrigorian/TimeCapsuleLLM) if youd like. You can now download my repo and run it locally. Unfortunately nanoGPT doesn't work natively with HuggingFace, so you'll have to download and run the model locally. ",
        "translatedContent": ""
      },
      {
        "row": 85,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "كذلك سأبدأ في تنقية البيانات لجولة التدريب التالية، أعتقد أنني سأحتاج ربما 5-10 أضعاف البيانات لتحقيق قدرات استدلال."
      },
      {
        "row": 86,
        "rowsha": "3/uUOOHuCoxdKkQ6c+1J96AfmOsQ59aZxo/FYGK9OlI=",
        "originContent": "Also I will begin curating data for my next training run, I believe I'll need maybe 5-10x more data to achieve reasoning capabilities. ",
        "translatedContent": ""
      },
      {
        "row": 87,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "### تحديث التدريب"
      },
      {
        "row": 88,
        "rowsha": "mNjt3ebwxfwyPM2/AK7E/GDTFM6i95HG8GZ/8TZs9EA=",
        "originContent": "### Training Update ",
        "translatedContent": ""
      },
      {
        "row": 89,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "بدأت التدريب على مجموعة بيانات 435 ميغابايت (108 مليون رمز)، الأمور تسير بسلاسة الآن. انخفضت خسارة التدريب من 10.9 إلى 4.9 في أول 2800 دورة تدريبية. أتوقع أن يستغرق الأمر حوالي 8 أو 9 ساعات حتى يكتمل. سأضع تحديثاً آخر بمجرد الانتهاء."
      },
      {
        "row": 90,
        "rowsha": "RxsWSGTgM12Md9v3+GBLg3PyoUxQ9kl1AglpanRmZqE=",
        "originContent": "I started training on a 435MB (108 M tokens) corpus, it's going pretty smooth right now. Train loss dropped from 10.9 to 4.9 in the first 2800 iterations. I expect it'll take around 8 or 9 hours to complete. I'll post another update once it's done.",
        "translatedContent": ""
      },
      {
        "row": 91,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## 17 يوليو 2025 الساعة 2:13 صباحاً"
      },
      {
        "row": 92,
        "rowsha": "CLW3lPsO6v+tT6iir338rG5+IYXM+JqRoag5w7K8exE=",
        "originContent": "## July 17th, 2025 2:13AM",
        "translatedContent": ""
      },
      {
        "row": 93,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "انتهى تدريب النموذج الثاني، استغرق الأمر حوالي 8 ساعات و40 دقيقة على بطاقتي 4060 (3900 دورة/ساعة) لـ 33000 دورة (5 عصور). الخسارة النهائية كانت 3.73. النتائج كانت مفاجئة إذ ينتج جمل متماسكة بأسلوب القرن التاسع عشر الآن."
      },
      {
        "row": 94,
        "rowsha": "Q0uM34dBNqytALNUZSPxoBQZT3LxqlwyioEi3nTshXQ=",
        "originContent": "The training is done for the second model, it took my 4060 around 8 hours and 40 minutes (3,900 iters/hr) for 33,000 iters (5 epochs). Final train loss was 3.73. The outputs were suprisingly good it genuinely generates coherent 19th century style sentences now. ",
        "translatedContent": ""
      },
      {
        "row": 95,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "# سلوك النموذج V0 والقيود"
      },
      {
        "row": 96,
        "rowsha": "YbYVsndAe75CgxPU/J34HsXOaqcQlIjTCm06o4eVQIg=",
        "originContent": "# V0 Model Behavior & Limitations ",
        "translatedContent": ""
      },
      {
        "row": 97,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "العينات المبكرة تظهر استجابة النموذج بلغة وسلوك القرن التاسع عشر. على سبيل المثال، طلبت منه \"Who art Henry?\" فأجاب: \"I know that man, I have did not a black, the storm.\" نعم هذه الجملة ليست منطقية لكن النموذج يتعرف أني أسأل عن شخص."
      },
      {
        "row": 98,
        "rowsha": "OCh97kyOITXqFpKyZsol6voS+nrzc9n9flpDpNyf35w=",
        "originContent": "Early prompts show the model responding with 1800's language and behavior. For example, I prompted it with \"Who art Henry?\" and it replied \"I know that man, I have did not a black, the storm.\" and yeah that sentence makes no sense but the LLM is recognizing I'm asking about a person. ",
        "translatedContent": ""
      },
      {
        "row": 99,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "![TimeLockLLM Sample Output](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)"
      },
      {
        "row": 100,
        "rowsha": "yKIR0teTc66wVDG+jdIyNmAzItXb2JH2ld3D7tm4qnM=",
        "originContent": "![TimeLockLLM Sample Output](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 2,
    "Content": "\nThere is no mention of modern concetps, outputs contain mostly words and phrasing from the 1800's.\n\nIt still needs alot of work, training off of 187MB will not give you a model that produces text with complex reasoning. \n\nRight now it produces sentences that lack full sentence structure and overall just make no sense but this is normal for the training size. \n\n# V0.5 Model Behavior & Limitations\n\nThis is a nice improvement compared to the last model. The writing style and vocab is Victorian and almost every sentence is grammatically correct with proper punctuation. And again this is trained from scratch so it sticks to 1800's subjects. \n\n![TimeLockLLM Sample Output](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)\n\nThere are a lot of factual hallucinations. A lot (like 100%) of the details (dates, events, historical figures)  are made up. Also the sentences don't really have connections to each other, sometimes maybe 2 sentences will relate to each other but beyond that they dont. Another issue is sometimes a stray “Digitized by Google” footer shows up, so the next time I train I really have to make sure the texts are cleaned well. Overall I'm very happy with the results, it's nowhere near an LLM yet but definitely a sentence generator. \n\nI'm learning a lot and will start figuring out what I need to do better in the coming weeks. I will upload files soon! \n\n# Upcoming Plans \n\n(Completed) I'm going to start work on version 0.5 , instead of training using 50 books, I'll train using ideally 500-600. Right now I'm training nanoGPT using books from 1800-1850 and specifically from London. There is some challeneges like making sure the books I find are not updated or have modern interpretations but untouched books published withtin my chosen time period.\n\nI want to train a new model (v1) with a much larger corpus, maybe 5-10x larger than the one I used for v0.5. My goal is to see if I can get reasoning abilities to emerge from Selective Temporal Training alone, this will be a more difficult task and I'm not even entirely sure if it's possible due to the fact that there is historical data limitations. In the upcoming weeks I will try to curate enough data for a 5-10GB corpus. I believe If I can get clean clean high quality data and rent a GPU, there will be progress.\n\n# How to Use This Project \n\nThis project focuses mostly on curating historical data, preparing it for training and building a tokenizer. I am not going to cover the full LLM training process, for that refer to nanoGPT by Andrej Karpathy.\n\n# Step 1: Gather and Prepare Historical Texts \n\nCollect .txt files of public domain books, documents, etc from your chosen time period (e.g., London 1800-1850)\n\nYou can use download_texts_improved.py to download books for you if you need to.\n\nClean the text files using a script or manually remove headers/footer from Project Gutenberg, Modern annotations or things like OCR errors.\n\nprepare_dataset.py should work fine.\n\n# Step 2: Build a Custom Tokenizer\n\nRun train_tokenizer.py or train_tokenizer_hf.py on the cleaned data.\nThis will give you vocab.json and merges.txt\n\nThes files define vocab and merge rules for your model\n\n# Step 3: Train Your Model (nanoGPT) \n\nRefer to [nanoGPT by Andrej Karpathy](https://github.com/karpathy/nanoGPT) for the training process.\n\nYou can train a different LLM if you want, but I used nanoGPT \n\n# FAQ\n\n## What is Selective Temporal Training ?\n\nSelective Temporal Training (STT) is a machine learning methodology where all training data is specifically curated to fall within a specific historical time period. It's done in order to model the language and knowledge of that era without influence from modern concepts. For example, the current model I have now (v0.5) is trained on data exclusively from 1800-1875, it's not fine tuned but trained from scratch resulting in output that reflects the linguistic style and historical context of that time period.\n\n## Why not just use fine-tuning or LoRA?\n\nFor this project I'm trying to create a language model that is unclouded from modern bias. If I fine-tune something like GPT-2, it's already pre-trained and that information won't go away. If I train from scratch the language model won't pretend to be old, it just will be. The Goal for this project right now is to create something can reason exclusively using knowledge from London books published between 1800 and 1850.\n\n## What kind of data did you use for training?\n\nI'm using books, legal documents, newspapers, and other writings from 1800–1850 London. The list I linked has like 200 but for the first training I just used 50 files about ~187 MB. You can view a list of the documents:\nhttps://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt\n\n## How large is the Version 0 model ?\n\nThis model is very small right now, I'm just doing this for fun and following a strict training rule of no modern sources. It has almost 16 million parameters but I'm gonna start gathering more old texts to begin another model training. Will give updates as I go.\n\n## Training Specs ? \n\nGPU: Geforce rtx 4060\nCPU: i5-13400F \nRam: 16GB DDR5.\n",
    "ContentSha": "KkciZ2KlgM0Mgb6PRUqxDc9F/T8DRKxQX/MKV4NPutA=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "\nلا يوجد ذكر للمفاهيم الحديثة، والمخرجات تحتوي في الغالب على كلمات وتراكيب من القرن التاسع عشر.\n\nلا يزال يحتاج إلى الكثير من العمل، فالتدريب على 187 ميجابايت لن يعطيك نموذجاً ينتج نصوصاً ذات تفكير معقد.\n\nحالياً، ينتج النموذج جمل تفتقر إلى الهيكل الكامل للجملة وبشكل عام لا معنى لها، لكن هذا طبيعي لحجم البيانات المستخدمة في التدريب.\n\n# سلوك النموذج V0.5 والقيود\n\nهذا تحسن جيد مقارنة بالنموذج السابق. أسلوب الكتابة والمفردات فيكتورية وكل جملة تقريباً صحيحة نحوياً وبعلامات ترقيم مناسبة. ومرة أخرى، تم تدريبه من الصفر لذا يلتزم بمواضيع القرن التاسع عشر.\n\n![مخرجات نموذج TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)\n\nهناك الكثير من الهلوسات الواقعية. الكثير (حوالي 100%) من التفاصيل (تواريخ، أحداث، شخصيات تاريخية) مختلقة. أيضاً الجمل لا ترتبط ببعضها، أحياناً ربما جملتان فقط ترتبطان لكن غير ذلك لا يوجد ترابط. مشكلة أخرى أحياناً يظهر تذييل \"Digitized by Google\" بشكل عشوائي، لذا في التدريب القادم يجب أن أتأكد من تنظيف النصوص جيداً. بشكل عام أنا سعيد جداً بالنتائج، فهو ليس نموذج لغة كبير بعد لكنه بالتأكيد مولد للجمل.\n\nأتعلم الكثير وسأبدأ بتحديد ما يجب علي تحسينه في الأسابيع القادمة. سأقوم برفع الملفات قريباً!\n\n# الخطط القادمة\n\n(تم الإنجاز) سأبدأ العمل على النسخة 0.5، وبدلاً من التدريب باستخدام 50 كتاباً، سأقوم بالتدريب باستخدام 500-600 كتاب إن أمكن. حالياً أدرب nanoGPT باستخدام كتب من 1800-1850 ومن لندن تحديداً. هناك تحديات مثل التأكد من أن الكتب التي أجدها ليست محدثة أو تحوي تفسيرات حديثة بل كتب أصلية من الفترة الزمنية المختارة.\n\nأرغب في تدريب نموذج جديد (v1) بقاعدة بيانات أكبر بكثير، ربما أكبر بـ 5-10 مرات من تلك التي استخدمتها في v0.5. هدفي هو معرفة ما إذا كان بإمكاني جعل قدرات الاستدلال تظهر من خلال التدريب الزمني الانتقائي فقط، وهذه مهمة أصعب ولست متأكداً تماماً من إمكانية ذلك بسبب محدودية البيانات التاريخية. في الأسابيع المقبلة سأحاول جمع بيانات كافية لقاعدة بيانات بحجم 5-10 جيجابايت. أعتقد أنه إذا حصلت على بيانات نظيفة وعالية الجودة واستأجرت معالج رسومي، سيكون هناك تقدم.\n\n# كيفية استخدام هذا المشروع\n\nيركز هذا المشروع بشكل أساسي على جمع البيانات التاريخية وتحضيرها للتدريب وبناء محلل ترميز tokenizer. لن أغطي عملية تدريب النموذج اللغوي بالكامل، لهذا راجع nanoGPT من أندريه كارباتي.\n\n# الخطوة 1: جمع وتحضير النصوص التاريخية\n\nاجمع ملفات .txt لكتب أو مستندات من الملكية العامة من الفترة الزمنية التي تختارها (مثلاً لندن 1800-1850)\n\nيمكنك استخدام download_texts_improved.py لتنزيل الكتب إذا كنت بحاجة لذلك.\n\nنظف ملفات النصوص باستخدام سكريبت أو يدوياً لإزالة الرؤوس/التذييلات من Project Gutenberg أو التفسيرات الحديثة أو أخطاء OCR.\n\nprepare_dataset.py يجب أن يعمل بشكل جيد.\n\n# الخطوة 2: بناء محلل ترميز مخصص\n\nشغل train_tokenizer.py أو train_tokenizer_hf.py على البيانات المنظفة.\nسيعطيك هذا vocab.json و merges.txt\n\nهذه الملفات تعرف المفردات وقواعد الدمج لنموذجك\n\n# الخطوة 3: تدريب النموذج الخاص بك (nanoGPT)\n\nراجع [nanoGPT من أندريه كارباتي](https://github.com/karpathy/nanoGPT) لعملية التدريب.\n\nيمكنك تدريب نموذج لغة آخر إذا رغبت، لكنني استخدمت nanoGPT\n\n# الأسئلة الشائعة\n\n## ما هو التدريب الزمني الانتقائي؟\n\nالتدريب الزمني الانتقائي (STT) هو منهجية تعلم آلي حيث يتم اختيار جميع بيانات التدريب لتكون ضمن فترة زمنية تاريخية محددة. يتم ذلك لنمذجة اللغة والمعرفة لتلك الحقبة دون تأثير المفاهيم الحديثة. على سبيل المثال، النموذج الحالي (v0.5) تم تدريبه على بيانات من 1800-1875 فقط، لم يتم تحسينه بل تم تدريبه من الصفر مما جعله يعكس أسلوب اللغة والسياق التاريخي لتلك الفترة.\n\n## لماذا لا أستخدم فقط الضبط الدقيق أو LoRA؟\n\nفي هذا المشروع أحاول إنشاء نموذج لغوي غير متأثر بالتحيز الحديث. إذا قمت بتحسين نموذج مثل GPT-2، فهو مدرب مسبقاً ولن تزول تلك المعلومات. إذا دربت من الصفر، لن يتظاهر النموذج بأنه قديم، بل سيكون كذلك فعلاً. الهدف حالياً هو إنشاء شيء يمكنه الاستدلال باستخدام معرفة مستمدة حصراً من كتب لندن المنشورة بين 1800 و1850.\n\n## ما نوع البيانات التي استخدمتها في التدريب؟\n\nأستخدم كتباً، مستندات قانونية، صحفاً، وكتابات أخرى من لندن بين 1800–1850. القائمة التي أرفقتها تحوي حوالي 200 ملف لكن في التدريب الأول استخدمت فقط 50 ملفاً بحجم ~187 ميجابايت. يمكنك عرض قائمة الوثائق:\nhttps://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt\n\n## ما حجم النموذج الإصدار 0؟\n\nهذا النموذج صغير جداً حالياً، أعمل عليه للمتعة وأتبع قاعدة صارمة بعدم استخدام مصادر حديثة. يحتوي على حوالي 16 مليون معامل وسأبدأ بجمع المزيد من النصوص القديمة لبدء تدريب نموذج آخر. سأقدم التحديثات مع التقدم.\n\n## مواصفات التدريب؟\n\nGPU: Geforce rtx 4060\nCPU: i5-13400F\nالذاكرة: 16 جيجابايت DDR5.\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 2,
        "rowsha": "/GR84OQ/Xp3d+Yv3/vVIIQFyl8ExKQLgjT1Go5rPplE=",
        "originContent": "There is no mention of modern concetps, outputs contain mostly words and phrasing from the 1800's.",
        "translatedContent": "لا يوجد ذكر للمفاهيم الحديثة، والمخرجات تحتوي في الغالب على كلمات وتراكيب من القرن التاسع عشر."
      },
      {
        "row": 3,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 4,
        "rowsha": "aojQk3KjkX8shkLnJSZvUZBo0vk4tKDT9aysMfY9yK4=",
        "originContent": "It still needs alot of work, training off of 187MB will not give you a model that produces text with complex reasoning. ",
        "translatedContent": "لا يزال يحتاج إلى الكثير من العمل، فالتدريب على 187 ميجابايت لن يعطيك نموذجاً ينتج نصوصاً ذات تفكير معقد."
      },
      {
        "row": 5,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 6,
        "rowsha": "SzQ3fs08W+JGBPGw6+aEzLsm/vGfQNDJ6YZnfFE0F4I=",
        "originContent": "Right now it produces sentences that lack full sentence structure and overall just make no sense but this is normal for the training size. ",
        "translatedContent": "حالياً، ينتج النموذج جمل تفتقر إلى الهيكل الكامل للجملة وبشكل عام لا معنى لها، لكن هذا طبيعي لحجم البيانات المستخدمة في التدريب."
      },
      {
        "row": 7,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 8,
        "rowsha": "eeaXlcgOJhwk4liDhaA6/6S4URmAdV8vj9hlqGr3LFw=",
        "originContent": "# V0.5 Model Behavior & Limitations",
        "translatedContent": "# سلوك النموذج V0.5 والقيود"
      },
      {
        "row": 9,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 10,
        "rowsha": "tII9OX/M7KpPoQKPIphmQBWtsSVucUevvTnzVsTXcaI=",
        "originContent": "This is a nice improvement compared to the last model. The writing style and vocab is Victorian and almost every sentence is grammatically correct with proper punctuation. And again this is trained from scratch so it sticks to 1800's subjects. ",
        "translatedContent": "هذا تحسن جيد مقارنة بالنموذج السابق. أسلوب الكتابة والمفردات فيكتورية وكل جملة تقريباً صحيحة نحوياً وبعلامات ترقيم مناسبة. ومرة أخرى، تم تدريبه من الصفر لذا يلتزم بمواضيع القرن التاسع عشر."
      },
      {
        "row": 11,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 12,
        "rowsha": "8DhXpgpVtg05XdyplRHf49EFOQNCJVzXA9RpmJQ+y9U=",
        "originContent": "![TimeLockLLM Sample Output](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)",
        "translatedContent": "![مخرجات نموذج TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)"
      },
      {
        "row": 13,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 14,
        "rowsha": "YgdLunUOAWWHsq+eAfyWcDz1fCc0rCcobnRpEXHoK94=",
        "originContent": "There are a lot of factual hallucinations. A lot (like 100%) of the details (dates, events, historical figures)  are made up. Also the sentences don't really have connections to each other, sometimes maybe 2 sentences will relate to each other but beyond that they dont. Another issue is sometimes a stray “Digitized by Google” footer shows up, so the next time I train I really have to make sure the texts are cleaned well. Overall I'm very happy with the results, it's nowhere near an LLM yet but definitely a sentence generator. ",
        "translatedContent": "هناك الكثير من الهلوسات الواقعية. الكثير (حوالي 100%) من التفاصيل (تواريخ، أحداث، شخصيات تاريخية) مختلقة. أيضاً الجمل لا ترتبط ببعضها، أحياناً ربما جملتان فقط ترتبطان لكن غير ذلك لا يوجد ترابط. مشكلة أخرى أحياناً يظهر تذييل \"Digitized by Google\" بشكل عشوائي، لذا في التدريب القادم يجب أن أتأكد من تنظيف النصوص جيداً. بشكل عام أنا سعيد جداً بالنتائج، فهو ليس نموذج لغة كبير بعد لكنه بالتأكيد مولد للجمل."
      },
      {
        "row": 15,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 16,
        "rowsha": "V44Ne2sN8v7ZVpRZ5vo7B2aUZFjGppYthg0fjsQAqd0=",
        "originContent": "I'm learning a lot and will start figuring out what I need to do better in the coming weeks. I will upload files soon! ",
        "translatedContent": "أتعلم الكثير وسأبدأ بتحديد ما يجب علي تحسينه في الأسابيع القادمة. سأقوم برفع الملفات قريباً!"
      },
      {
        "row": 17,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 18,
        "rowsha": "iMvQyl++GAiLpFMT+58v3e9/hb7zXnIJauchG5p986Y=",
        "originContent": "# Upcoming Plans ",
        "translatedContent": "# الخطط القادمة"
      },
      {
        "row": 19,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 20,
        "rowsha": "OIt/QAbIp2G7kt87dbAIf6Ec0U3HnwjOW6LYRmdf68I=",
        "originContent": "(Completed) I'm going to start work on version 0.5 , instead of training using 50 books, I'll train using ideally 500-600. Right now I'm training nanoGPT using books from 1800-1850 and specifically from London. There is some challeneges like making sure the books I find are not updated or have modern interpretations but untouched books published withtin my chosen time period.",
        "translatedContent": "(تم الإنجاز) سأبدأ العمل على النسخة 0.5، وبدلاً من التدريب باستخدام 50 كتاباً، سأقوم بالتدريب باستخدام 500-600 كتاب إن أمكن. حالياً أدرب nanoGPT باستخدام كتب من 1800-1850 ومن لندن تحديداً. هناك تحديات مثل التأكد من أن الكتب التي أجدها ليست محدثة أو تحوي تفسيرات حديثة بل كتب أصلية من الفترة الزمنية المختارة."
      },
      {
        "row": 21,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 22,
        "rowsha": "13sW7eekt8pSDdOxr/oKukwKEOVmw/M8xiAJC1VFXZc=",
        "originContent": "I want to train a new model (v1) with a much larger corpus, maybe 5-10x larger than the one I used for v0.5. My goal is to see if I can get reasoning abilities to emerge from Selective Temporal Training alone, this will be a more difficult task and I'm not even entirely sure if it's possible due to the fact that there is historical data limitations. In the upcoming weeks I will try to curate enough data for a 5-10GB corpus. I believe If I can get clean clean high quality data and rent a GPU, there will be progress.",
        "translatedContent": "أرغب في تدريب نموذج جديد (v1) بقاعدة بيانات أكبر بكثير، ربما أكبر بـ 5-10 مرات من تلك التي استخدمتها في v0.5. هدفي هو معرفة ما إذا كان بإمكاني جعل قدرات الاستدلال تظهر من خلال التدريب الزمني الانتقائي فقط، وهذه مهمة أصعب ولست متأكداً تماماً من إمكانية ذلك بسبب محدودية البيانات التاريخية. في الأسابيع المقبلة سأحاول جمع بيانات كافية لقاعدة بيانات بحجم 5-10 جيجابايت. أعتقد أنه إذا حصلت على بيانات نظيفة وعالية الجودة واستأجرت معالج رسومي، سيكون هناك تقدم."
      },
      {
        "row": 23,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 24,
        "rowsha": "sO+voevLpUtEbqum6Gntntle+nPVa66c5GATAMLrgf0=",
        "originContent": "# How to Use This Project ",
        "translatedContent": "# كيفية استخدام هذا المشروع"
      },
      {
        "row": 25,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 26,
        "rowsha": "XVoXr9uzZwN09vboETojEQJe057RBzcMUjXmQRCB/jo=",
        "originContent": "This project focuses mostly on curating historical data, preparing it for training and building a tokenizer. I am not going to cover the full LLM training process, for that refer to nanoGPT by Andrej Karpathy.",
        "translatedContent": "يركز هذا المشروع بشكل أساسي على جمع البيانات التاريخية وتحضيرها للتدريب وبناء محلل ترميز tokenizer. لن أغطي عملية تدريب النموذج اللغوي بالكامل، لهذا راجع nanoGPT من أندريه كارباتي."
      },
      {
        "row": 27,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 28,
        "rowsha": "kDK7XtqFkTiZvD804yYG4VEojvLrRbdEhmHEBzDAQz4=",
        "originContent": "# Step 1: Gather and Prepare Historical Texts ",
        "translatedContent": "# الخطوة 1: جمع وتحضير النصوص التاريخية"
      },
      {
        "row": 29,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 30,
        "rowsha": "oRoOYaG+mx3SqTnhGcOKeH7W3W4wSRQmhZS4jPGMH8s=",
        "originContent": "Collect .txt files of public domain books, documents, etc from your chosen time period (e.g., London 1800-1850)",
        "translatedContent": "اجمع ملفات .txt لكتب أو مستندات من الملكية العامة من الفترة الزمنية التي تختارها (مثلاً لندن 1800-1850)"
      },
      {
        "row": 31,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 32,
        "rowsha": "EBDhZvTogrL4hqRuH095O7/tXXoy2OEskLQlang/pOA=",
        "originContent": "You can use download_texts_improved.py to download books for you if you need to.",
        "translatedContent": "يمكنك استخدام download_texts_improved.py لتنزيل الكتب إذا كنت بحاجة لذلك."
      },
      {
        "row": 33,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 34,
        "rowsha": "q5OO8x+9kIbzaQRWimI/Vo9ZowzVBtCX+TodObLsQoY=",
        "originContent": "Clean the text files using a script or manually remove headers/footer from Project Gutenberg, Modern annotations or things like OCR errors.",
        "translatedContent": "نظف ملفات النصوص باستخدام سكريبت أو يدوياً لإزالة الرؤوس/التذييلات من Project Gutenberg أو التفسيرات الحديثة أو أخطاء OCR."
      },
      {
        "row": 35,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 36,
        "rowsha": "N4wsIjC0LRlClodmfCtYX3qswttJnk0psU28/mlCRTw=",
        "originContent": "prepare_dataset.py should work fine.",
        "translatedContent": "prepare_dataset.py يجب أن يعمل بشكل جيد."
      },
      {
        "row": 37,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 38,
        "rowsha": "jDM2lr7pP+MT0pt+L0cd5nBXI83IPoT27NzIgplt7R8=",
        "originContent": "# Step 2: Build a Custom Tokenizer",
        "translatedContent": "# الخطوة 2: بناء محلل ترميز مخصص"
      },
      {
        "row": 39,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 40,
        "rowsha": "+90EorsgO/X2bFK2pYJw+vZIpjIbuMm4QR6W/xfj8C8=",
        "originContent": "Run train_tokenizer.py or train_tokenizer_hf.py on the cleaned data.",
        "translatedContent": "شغل train_tokenizer.py أو train_tokenizer_hf.py على البيانات المنظفة."
      },
      {
        "row": 41,
        "rowsha": "tkP3Eg1rWphTQMNhN2yYg/1+AA1IdcXbGT96aRMpnwc=",
        "originContent": "This will give you vocab.json and merges.txt",
        "translatedContent": "سيعطيك هذا vocab.json و merges.txt"
      },
      {
        "row": 42,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 43,
        "rowsha": "/wqxgtOu72+x3a2xi7q23jDkx+WQv2SHrJddzpvm1Ys=",
        "originContent": "Thes files define vocab and merge rules for your model",
        "translatedContent": "هذه الملفات تعرف المفردات وقواعد الدمج لنموذجك"
      },
      {
        "row": 44,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 45,
        "rowsha": "vKPAEsPxc9uYtzjTX9/rNADSDnxkKwYcYpX/aiAp8Hc=",
        "originContent": "# Step 3: Train Your Model (nanoGPT) ",
        "translatedContent": "# الخطوة 3: تدريب النموذج الخاص بك (nanoGPT)"
      },
      {
        "row": 46,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 47,
        "rowsha": "tCDY5iXt+Z7YYeTPouMSYDX5uuFnGROxZMvHyTOIblY=",
        "originContent": "Refer to [nanoGPT by Andrej Karpathy](https://github.com/karpathy/nanoGPT) for the training process.",
        "translatedContent": "راجع [nanoGPT من أندريه كارباتي](https://github.com/karpathy/nanoGPT) لعملية التدريب."
      },
      {
        "row": 48,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 49,
        "rowsha": "bLpr9snECDSJ5ejrqDWYNT8VXSupCQKQxyLxvAmQ2Kc=",
        "originContent": "You can train a different LLM if you want, but I used nanoGPT ",
        "translatedContent": "يمكنك تدريب نموذج لغة آخر إذا رغبت، لكنني استخدمت nanoGPT"
      },
      {
        "row": 50,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 51,
        "rowsha": "OoCxyGfPN5TmdzAkaPphtPx303MJJ7vpfWbKrufGH5g=",
        "originContent": "# FAQ",
        "translatedContent": "# الأسئلة الشائعة"
      },
      {
        "row": 52,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 53,
        "rowsha": "+5dDgPw4ILEotxso4tjjjz1cxwUei16yNQPDUKbgxoo=",
        "originContent": "## What is Selective Temporal Training ?",
        "translatedContent": "## ما هو التدريب الزمني الانتقائي؟"
      },
      {
        "row": 54,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 55,
        "rowsha": "hooEARKH4r/sDPh7JUtZAZ6TYMvBkTLZIcfw3g83xos=",
        "originContent": "Selective Temporal Training (STT) is a machine learning methodology where all training data is specifically curated to fall within a specific historical time period. It's done in order to model the language and knowledge of that era without influence from modern concepts. For example, the current model I have now (v0.5) is trained on data exclusively from 1800-1875, it's not fine tuned but trained from scratch resulting in output that reflects the linguistic style and historical context of that time period.",
        "translatedContent": "التدريب الزمني الانتقائي (STT) هو منهجية تعلم آلي حيث يتم اختيار جميع بيانات التدريب لتكون ضمن فترة زمنية تاريخية محددة. يتم ذلك لنمذجة اللغة والمعرفة لتلك الحقبة دون تأثير المفاهيم الحديثة. على سبيل المثال، النموذج الحالي (v0.5) تم تدريبه على بيانات من 1800-1875 فقط، لم يتم تحسينه بل تم تدريبه من الصفر مما جعله يعكس أسلوب اللغة والسياق التاريخي لتلك الفترة."
      },
      {
        "row": 56,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 57,
        "rowsha": "dVMKQ2mPI1Spc6x6r/jNG0PIR5YKpalU4MXx9JmKp/I=",
        "originContent": "## Why not just use fine-tuning or LoRA?",
        "translatedContent": "## لماذا لا أستخدم فقط الضبط الدقيق أو LoRA؟"
      },
      {
        "row": 58,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 59,
        "rowsha": "oNvWlJHtQSyq1TwlqJyGtMzk4Z4mBIn8AW2SudzvUYs=",
        "originContent": "For this project I'm trying to create a language model that is unclouded from modern bias. If I fine-tune something like GPT-2, it's already pre-trained and that information won't go away. If I train from scratch the language model won't pretend to be old, it just will be. The Goal for this project right now is to create something can reason exclusively using knowledge from London books published between 1800 and 1850.",
        "translatedContent": "في هذا المشروع أحاول إنشاء نموذج لغوي غير متأثر بالتحيز الحديث. إذا قمت بتحسين نموذج مثل GPT-2، فهو مدرب مسبقاً ولن تزول تلك المعلومات. إذا دربت من الصفر، لن يتظاهر النموذج بأنه قديم، بل سيكون كذلك فعلاً. الهدف حالياً هو إنشاء شيء يمكنه الاستدلال باستخدام معرفة مستمدة حصراً من كتب لندن المنشورة بين 1800 و1850."
      },
      {
        "row": 60,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 61,
        "rowsha": "ByP4WlNmMoG6WIiLJNd6b080/DSciCgWmj9aYSJjAF0=",
        "originContent": "## What kind of data did you use for training?",
        "translatedContent": "## ما نوع البيانات التي استخدمتها في التدريب؟"
      },
      {
        "row": 62,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 63,
        "rowsha": "Kj6EF7wZdUrAFg4ErGmJuh9Q5Xujmb+tunpfssPKXkA=",
        "originContent": "I'm using books, legal documents, newspapers, and other writings from 1800–1850 London. The list I linked has like 200 but for the first training I just used 50 files about ~187 MB. You can view a list of the documents:",
        "translatedContent": "أستخدم كتباً، مستندات قانونية، صحفاً، وكتابات أخرى من لندن بين 1800–1850. القائمة التي أرفقتها تحوي حوالي 200 ملف لكن في التدريب الأول استخدمت فقط 50 ملفاً بحجم ~187 ميجابايت. يمكنك عرض قائمة الوثائق:"
      },
      {
        "row": 64,
        "rowsha": "0mxyGiLJxzp9JPCg1oA+nbIwAKJbEC4ei9kSV3Gp84Y=",
        "originContent": "https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt",
        "translatedContent": "https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt"
      },
      {
        "row": 65,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 66,
        "rowsha": "RiLgksbH2sYLZRyKsFhVyfjC6nNxLsxWsc7XnFS061A=",
        "originContent": "## How large is the Version 0 model ?",
        "translatedContent": "## ما حجم النموذج الإصدار 0؟"
      },
      {
        "row": 67,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 68,
        "rowsha": "55Ce1mBEzOreC728R5HRfuMIC/nZRg3Zcr87GVkjVVY=",
        "originContent": "This model is very small right now, I'm just doing this for fun and following a strict training rule of no modern sources. It has almost 16 million parameters but I'm gonna start gathering more old texts to begin another model training. Will give updates as I go.",
        "translatedContent": "هذا النموذج صغير جداً حالياً، أعمل عليه للمتعة وأتبع قاعدة صارمة بعدم استخدام مصادر حديثة. يحتوي على حوالي 16 مليون معامل وسأبدأ بجمع المزيد من النصوص القديمة لبدء تدريب نموذج آخر. سأقدم التحديثات مع التقدم."
      },
      {
        "row": 69,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 70,
        "rowsha": "A40eQ7ZJiqr+bs9cl0Cb4QKKuS9z7/PA1ZaGn1TSehI=",
        "originContent": "## Training Specs ? ",
        "translatedContent": "## مواصفات التدريب؟"
      },
      {
        "row": 71,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 72,
        "rowsha": "EH8H1HW/C4Tb7LfJgVUnVGsk4pF9l40Rlev8tAkKhjI=",
        "originContent": "GPU: Geforce rtx 4060",
        "translatedContent": "GPU: Geforce rtx 4060"
      },
      {
        "row": 73,
        "rowsha": "vo3FdN37kY6VUB7PruRKfBPJDgsVJyBHIUCn/g8mt68=",
        "originContent": "CPU: i5-13400F ",
        "translatedContent": "CPU: i5-13400F"
      },
      {
        "row": 74,
        "rowsha": "W8fXPiQKUkoNso0PPfTvjYMy0IYo85j+gNXmB0aERO4=",
        "originContent": "Ram: 16GB DDR5.",
        "translatedContent": "الذاكرة: 16 جيجابايت DDR5."
      },
      {
        "row": 75,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  }
]