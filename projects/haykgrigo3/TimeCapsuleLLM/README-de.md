
<div align="right">
  <details>
    <summary >üåê Sprache</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">Englisch</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="#" title="Coming soon">ÁπÅÈ´î‰∏≠Êñá (demn√§chst verf√ºgbar)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">Japanisch</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">Koreanisch</a>
        | <a href="#" title="Coming soon">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä (demn√§chst verf√ºgbar)</a> |
        | <a href="#" title="Coming soon">‡πÑ‡∏ó‡∏¢ (demn√§chst verf√ºgbar)</a> |
        | <a href="#" title="Coming soon">Franz√∂sisch (demn√§chst verf√ºgbar)</a>
        | <a href="#" title="Coming soon">Deutsch (demn√§chst verf√ºgbar)</a>
        | <a href="#" title="Coming soon">Spanisch (demn√§chst verf√ºgbar)</a>
        | <a href="#" title="Coming soon">Italienisch (demn√§chst verf√ºgbar)</a>
        | <a href="#" title="Coming soon">Russisch (demn√§chst verf√ºgbar)</a>
        | <a href="#" title="Coming soon">Portugiesisch (demn√§chst verf√ºgbar)</a>
        | <a href="#" title="Coming soon">Niederl√§ndisch (demn√§chst verf√ºgbar)</a>
        | <a href="#" title="Coming soon">Polnisch (demn√§chst verf√ºgbar)</a>
        | <a href="#" title="Coming soon">ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (demn√§chst verf√ºgbar)</a>
        | <a href="#" title="Coming soon">ŸÅÿßÿ±ÿ≥€å (demn√§chst verf√ºgbar)</a>
        | <a href="#" title="Coming soon">T√ºrkisch (demn√§chst verf√ºgbar)</a>
        | <a href="#" title="Coming soon">Vietnamesisch (demn√§chst verf√ºgbar)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (demn√§chst verf√ºgbar)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM
Ein LLM, der ausschlie√ülich mit Daten aus bestimmten Zeitperioden trainiert wurde, um moderne Verzerrungen zu reduzieren.

Stellen Sie sich vor, ein KI-Modell w√ºrde nicht nur so tun, als w√§re es historisch, sondern w√§re es tats√§chlich.

Basiert auf [nanoGPT von Andrej Karpathy](https://github.com/karpathy/nanoGPT). Die zentralen Trainingsskripte und die Modellarchitektur stammen von ihm.

# Projektziele

TimeCapsule LLM ist ein experimentelles Projekt, das ausschlie√ülich mit Texten aus bestimmten Zeitabschnitten trainiert wird. Ziel ist es, die Weltsicht und Sprache spezifischer historischer Epochen zu simulieren.

# Warum Fine-Tuning nicht ausreicht

Wenn man nur ein vortrainiertes Modell feinabstimmt, kennt das LLM trotzdem moderne Konzepte. Nat√ºrlich ist es schwierig, einen v√∂llig modernen Bias zu vermeiden, aber ich m√∂chte diesem Ziel so nahe wie m√∂glich kommen. Um keinen modernen Bias zu haben, muss ein Modell von Grund auf neu trainiert werden.

# Erwartete Ergebnisse

Hoffentlich wei√ü das Modell am Ende nichts von modernen Konzepten und kann nicht √ºber das hinaus argumentieren, worauf es trainiert wurde. Es sollte moderne Begriffe/Vokabeln nicht erkennen und keine modernen Kenntnisse halluzinieren.

# Fortschrittsupdates

## 9. Juli 2025

Ich habe meinen Zeitraum auf 1800-1850 und die Region: London festgelegt

Ich habe eine Liste von Texten, B√ºchern, Dokumenten zusammengestellt

Bisher habe ich 50 als txt-Dateien und werde bald mit dem Training von NanoGPT beginnen

Ich werde dies aktualisieren, solange Fortschritte gemacht werden

## 13. Juli 2025

NanoGPT mit 187MB historischer Textdaten trainiert.

## 15. Juli 2025

Ich habe begonnen, Texte f√ºr den zweiten Trainingslauf herunterzuladen. Ich bekomme alles vom Internet Archive und habe den Zeitraum auf 1800-1875 erweitert. Um eine breite Palette von Texten zu erhalten, kann man auf Internet Archive nach Ver√∂ffentlichungsort, Zeitraum und Themen filtern. 

![Suchfilter](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## 16. Juli 2025

Ich habe etwa 500 txt-Dateien vom Internet Archive heruntergeladen und nach dem Bereinigen (nur Leerzeichen l√∂schen, Gutenberg-Header etc.) habe ich etwa 500MB Daten. Es ist ein winziger Datensatz, aber letztes Mal habe ich mit 187MB trainiert, also sollte es zumindest einen bemerkbaren Unterschied im Output nach dem Training des zweiten Modells geben. Ich hoffe, dieses Modell kann zumindest koh√§rentere S√§tze produzieren, die einigerma√üen Sinn ergeben. Das ist nat√ºrlich keine Garantie, denn der Datensatz ist immer noch winzig, aber es ist mehr als beim letzten Mal.

Das sollte auf meiner eigenen Hardware machbar sein, was gut ist, weil ich hoffentlich einige Verbesserungen sehe, bevor ich zu einem gr√∂√üeren Datensatz springe, f√ºr den ich eine GPU mieten m√ºsste. Aber keine Sorge, ich plane weiterhin, bald eine GPU zu mieten, aber bevor ich das tue, m√∂chte ich sicherstellen, dass mein Datensatz so kuratiert und sauber wie m√∂glich ist. Eines der Probleme ist das Bereinigen, viele dieser txt-Dateien enthalten Kauderwelsch. Die Skripte, die ich zum Bereinigen verwendet habe, funktionieren, sind aber nicht zu 100% effektiv.

Ich werde diesen Datensatz heute trainieren und es sollte etwa 4-5 Stunden dauern. Sobald es fertig ist und ich es getestet habe, gebe ich Updates. Vielen Dank nochmals an alle, die sich mein Projekt ansehen, manche haben mir sogar Links zu OCR-Ressourcen geschickt, also Danke! Ich hoffe, dass mehr Leute das ausprobieren und mit eigenen Datens√§tzen experimentieren.

## 12. August 2025

Version 1 ist fertiggestellt, diese Version verwendet Phi 1.5 (700M Parameter) und wurde mit einem Datensatz von fast 7000 Texten trainiert. Die Hauptverbesserung bei diesem Modell ist seine F√§higkeit, gelegentlich echte Namen und historische Ereignisse zu referenzieren. Version 0 und 0.5 konnten dies nicht, wenn ich diese Modelle mit etwas wie ‚ÄûIm Jahr 1821‚Äú aufforderte, waren die Ausgaben nur Halluzinationen, Dinge, die nie passiert sind. Ein Beispiel daf√ºr mit Version 1:

Mein Prompt: Es war das Jahr unseres Herrn 1834,

Die Ausgabe: ‚ÄûEs war das Jahr unseres Herrn 1834 und die Stra√üen Londons waren voller Protest und Petition. Der Grund, wie viele berichteten, lag nicht im Privaten, sondern wurde am selben Tag im Hause von Lord Palmerston aufgenommen, die √ñffentlichkeit wird eine kurze Darstellung der Schwierigkeiten erhalten, unter denen das Gesetz uns erreicht hat. Es ist sehr bedauerlich, dass die aktuellen Ereignisse der Weltgeschichte klar sind und folglich bekannt sein werden. Es ist nicht wahr, dass gerade die M√§nner, die zuerst im Evangelium zu Jerusalem t√§tig wurden, so umfassende und interessante Aufzeichnungen √ºber den Wohlstand und das Gedeihen besitzen.‚Äú

Zuerst dachte ich, das sei Zufall, aber schaut euch das an: ![1834protest](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)


### Trainings-Update

Ich habe mit dem Training auf einem 435MB (108 Mio. Token) Korpus begonnen, es l√§uft gerade ziemlich reibungslos. Der Trainingsverlust fiel von 10,9 auf 4,9 in den ersten 2800 Iterationen. Ich rechne damit, dass es etwa 8 oder 9 Stunden dauern wird. Ich poste ein weiteres Update, sobald es fertig ist.

## 17. Juli 2025

Das Training f√ºr das zweite Modell ist abgeschlossen, meine 4060 hat etwa 8 Stunden und 40 Minuten gebraucht (3.900 Iterationen/Stunde) f√ºr 33.000 Iterationen (5 Epochen). Der finale Trainingsverlust lag bei 3,73. Die Ausgaben waren √ºberraschend gut, es generiert jetzt tats√§chlich zusammenh√§ngende S√§tze im Stil des 19. Jahrhunderts.

## 28. Juli 2025

Ich habe v0.5 auf Hugging Face hochgeladen, [Schaut es euch an](https://huggingface.co/haykgrigorian/TimeCapsuleLLM), wenn ihr m√∂chtet. Ihr k√∂nnt jetzt mein Repo herunterladen und lokal ausf√ºhren. Leider funktioniert nanoGPT nicht nativ mit HuggingFace, daher m√ºsst ihr das Modell lokal herunterladen und ausf√ºhren.

Au√üerdem beginne ich mit der Auswahl von Daten f√ºr meinen n√§chsten Trainingslauf, ich glaube, ich werde 5-10x mehr Daten ben√∂tigen, um F√§higkeiten zum logischen Schlussfolgern zu erreichen.

## 2. August 2025

Ich werde bald mit der Arbeit an Version 1 beginnen. Ich muss von der Architektur von nanoGPT zu etwas Modernerem wechseln. Ich habe mehrere Open-Source-LLM-Architekturen im Blick, darunter: OpenLLaMA v3, Phi-2 und Qwen 1.5B. Und um den Sprung zu V1 zu unterst√ºtzen, muss ich einen viel gr√∂√üeren und vielf√§ltigeren Datensatz sorgf√§ltig zusammenstellen. Ich brauche mindestens 5GB saubere Trainingsdaten.

# V0 Modellverhalten & Einschr√§nkungen

Fr√ºhe Prompts zeigen, dass das Modell mit Sprache und Verhalten der 1800er reagiert. Zum Beispiel habe ich es mit ‚ÄûWho art Henry?‚Äú aufgefordert und es antwortete ‚ÄûI know that man, I have did not a black, the storm.‚Äú und ja, dieser Satz ergibt keinen Sinn, aber das LLM erkennt, dass ich nach einer Person frage.

![TimeLockLLM Beispielausgabe](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

Es gibt keine Erw√§hnung moderner Konzepte, die Ausgaben enthalten haupts√§chlich W√∂rter und Formulierungen aus den 1800er Jahren.

Es braucht noch viel Arbeit, das Training mit 187MB reicht nicht aus, um ein Modell zu bekommen, das Texte mit komplexer Argumentation erzeugt.

Im Moment erzeugt das Modell S√§tze, die keine vollst√§ndige Satzstruktur aufweisen und insgesamt einfach keinen Sinn ergeben, aber das ist bei der Trainingsgr√∂√üe normal.

# V0.5 Modellverhalten & Einschr√§nkungen

Dies ist eine sch√∂ne Verbesserung im Vergleich zum letzten Modell. Der Schreibstil und Wortschatz ist viktorianisch und fast jeder Satz ist grammatikalisch korrekt mit richtiger Zeichensetzung. Und wieder wurde das Modell von Grund auf neu trainiert, daher bezieht es sich auf Themen aus dem 19. Jahrhundert.

![TimeLockLLM Beispielausgabe](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

Es gibt viele faktische Halluzinationen. Viele (etwa 100%) der Details (Daten, Ereignisse, historische Figuren) sind erfunden. Au√üerdem haben die S√§tze untereinander keine wirklichen Verbindungen, manchmal beziehen sich vielleicht 2 S√§tze aufeinander, aber dar√ºber hinaus nicht. Ein weiteres Problem ist, dass manchmal ein fremder ‚ÄûDigitized by Google‚Äú-Footer auftaucht, daher muss ich beim n√§chsten Training unbedingt darauf achten, dass die Texte gut bereinigt sind. Insgesamt bin ich sehr zufrieden mit den Ergebnissen, es ist zwar noch lange kein LLM, aber definitiv ein Satzgenerator.

Ich lerne viel und werde in den kommenden Wochen herausfinden, was ich besser machen muss. Ich werde bald Dateien hochladen!

# V1 Modellverhalten & Einschr√§nkungen

Ich werde bald einige Beispielausgaben hochladen und auch Vergleiche zwischen den 3 Modellen mit dem gleichen Prompt machen. Ich werde V1 auch auf Huggingface hochladen, wie ich es bei meiner letzten Version gemacht habe, meinen Huggingface-Account findet ihr hier: https://huggingface.co/haykgrigorian/TimeCapsuleLLM

# Kommende Pl√§ne

(Abgeschlossen) Ich werde mit der Arbeit an Version 0.5 beginnen. Statt mit 50 B√ºchern zu trainieren, werde ich idealerweise mit 500-600 B√ºchern trainieren. Im Moment trainiere ich nanoGPT mit B√ºchern aus 1800-1850 und speziell aus London. Es gibt einige Herausforderungen, wie sicherzustellen, dass die gefundenen B√ºcher nicht aktualisiert oder modern interpretiert wurden, sondern unver√§nderte B√ºcher, die innerhalb meines gew√§hlten Zeitraums ver√∂ffentlicht wurden.

Ich m√∂chte ein neues Modell (v1) mit einem viel gr√∂√üeren Korpus trainieren, vielleicht 5-10x gr√∂√üer als der f√ºr v0.5 verwendete. Mein Ziel ist es zu sehen, ob durch Selective Temporal Training allein F√§higkeiten zum logischen Denken entstehen k√∂nnen. Dies wird eine schwierigere Aufgabe und ich bin mir nicht einmal sicher, ob es m√∂glich ist, da es Einschr√§nkungen bei historischen Daten gibt. In den n√§chsten Wochen werde ich versuchen, genug Daten f√ºr einen Korpus von 5-10GB zusammenzustellen. Ich glaube, wenn ich saubere, hochwertige Daten bekomme und eine GPU miete, wird es Fortschritte geben.

# Wie man dieses Projekt verwendet

Dieses Projekt konzentriert sich haupts√§chlich auf das Kuratieren historischer Daten, deren Vorbereitung f√ºr das Training und den Aufbau eines Tokenizers. Ich werde den vollst√§ndigen LLM-Trainingsprozess nicht abdecken, daf√ºr verweise ich auf nanoGPT von Andrej Karpathy.

# Schritt 1: Historische Texte sammeln und vorbereiten

Sammle .txt-Dateien von gemeinfreien B√ºchern, Dokumenten usw. aus deinem gew√§hlten Zeitraum (z.B. London 1800-1850)

Du kannst download_texts_improved.py verwenden, um B√ºcher f√ºr dich herunterzuladen, falls du das ben√∂tigst.

Bereinige die Textdateien mit einem Skript oder entferne manuell Header/Footer von Project Gutenberg, moderne Anmerkungen oder Dinge wie OCR-Fehler.

prepare_dataset.py sollte einwandfrei funktionieren.

# Schritt 2: Einen eigenen Tokenizer bauen

F√ºhre train_tokenizer.py oder train_tokenizer_hf.py auf den bereinigten Daten aus.
Dadurch erh√§ltst du vocab.json und merges.txt

Diese Dateien definieren Vokabular und Merge-Regeln f√ºr Ihr Modell

# Schritt 3: Trainieren Sie Ihr Modell (nanoGPT)

Beziehen Sie sich auf [nanoGPT von Andrej Karpathy](https://github.com/karpathy/nanoGPT) f√ºr den Trainingsprozess.

Sie k√∂nnen ein anderes LLM trainieren, wenn Sie m√∂chten, aber ich habe nanoGPT verwendet

# FAQ

## Was ist Selektives Temporales Training?

Selektives Temporales Training (STT) ist eine Methode des maschinellen Lernens, bei der alle Trainingsdaten gezielt auf einen bestimmten historischen Zeitraum beschr√§nkt werden. Dies dient dazu, die Sprache und das Wissen jener Epoche ohne Einfluss moderner Konzepte zu modellieren. Zum Beispiel wurde das aktuelle Modell (v0.5) ausschlie√ülich mit Daten von 1800‚Äì1875 trainiert, nicht feinabgestimmt, sondern von Grund auf neu, wodurch die Ausgabe den Sprachstil und Kontext jener Zeit widerspiegelt.

## Warum nicht einfach Fine-Tuning oder LoRA verwenden?

F√ºr dieses Projekt m√∂chte ich ein Sprachmodell erschaffen, das frei von modernen Einfl√ºssen ist. Wenn ich etwa GPT-2 feinabstimme, ist es bereits vortrainiert, und diese Informationen bleiben erhalten. Wenn ich von Grund auf trainiere, tut das Sprachmodell nicht so, als w√§re es alt, sondern es ist es wirklich. Das Ziel des Projekts ist aktuell, etwas zu schaffen, das ausschlie√ülich mit Wissen aus Londoner B√ºchern zwischen 1800 und 1850 argumentieren kann.

## Welche Daten wurden f√ºr das Training verwendet?

Ich verwende B√ºcher, juristische Dokumente, Zeitungen und andere Schriften aus London von 1800‚Äì1850. Die verlinkte Liste enth√§lt etwa 200, aber f√ºr das erste Training habe ich nur 50 Dateien mit ca. 187 MB genutzt. Eine Liste der Dokumente finden Sie unter:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## Wie gro√ü sind die Modelle?

V0: 16M Parameter

V0.5: 123M Parameter

V1: 700M Parameter

# Trainingsspezifikationen?

#V0/V0.5
GPU: Geforce RTX 4060
CPU: i5-13400F
RAM: 16GB DDR5.

#V1
GPU: A100 rented









---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-08-12

---