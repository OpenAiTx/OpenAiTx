<div align="right">
  <details>
    <summary >üåê Sprache</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">Englisch</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="#" title="Coming soon">ÁπÅÈ´î‰∏≠Êñá (coming soon)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">Japanisch</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">Koreanisch</a>
        | <a href="#" title="Coming soon">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä (coming soon)</a> |
        | <a href="#" title="Coming soon">‡πÑ‡∏ó‡∏¢ (coming soon)</a> |
        | <a href="#" title="Coming soon">Franz√∂sisch (coming soon)</a>
        | <a href="#" title="Coming soon">Deutsch (coming soon)</a>
        | <a href="#" title="Coming soon">Spanisch (coming soon)</a>
        | <a href="#" title="Coming soon">Italienisch (coming soon)</a>
        | <a href="#" title="Coming soon">Russisch (coming soon)</a>
        | <a href="#" title="Coming soon">Portugiesisch (coming soon)</a>
        | <a href="#" title="Coming soon">Niederl√§ndisch (coming soon)</a>
        | <a href="#" title="Coming soon">Polnisch (coming soon)</a>
        | <a href="#" title="Coming soon">ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (coming soon)</a>
        | <a href="#" title="Coming soon">ŸÅÿßÿ±ÿ≥€å (coming soon)</a>
        | <a href="#" title="Coming soon">T√ºrkisch (coming soon)</a>
        | <a href="#" title="Coming soon">Vietnamesisch (coming soon)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (coming soon)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM
Ein LLM, der ausschlie√ülich mit Daten aus bestimmten Zeitr√§umen trainiert wurde, um moderne Verzerrungen zu reduzieren.

Stellen Sie sich vor, ein KI-Modell w√ºrde nicht nur so tun, als w√§re es historisch, sondern es w√§re es tats√§chlich.

Basiert auf [nanoGPT von Andrej Karpathy](https://github.com/karpathy/nanoGPT). Die Kern-Trainingsskripte und die Modellarchitektur stammen von ihm.

# Projektziele

TimeCapsule LLM ist ein experimentelles Projekt, das ausschlie√ülich mit Texten aus bestimmten Zeitperioden trainiert wird. Ziel ist es, Weltanschauung und Sprache spezifischer historischer Epochen zu simulieren.

# Warum Fine-Tuning nicht ausreicht

Wenn man ein vortrainiertes Modell nur feinabstimmt, kennt das LLM trotzdem moderne Konzepte. Nat√ºrlich ist es schwierig, eine v√∂llig moderne Verzerrung zu vermeiden, aber ich m√∂chte dem so nahe wie m√∂glich kommen. Um keine moderne Verzerrung zu haben, muss man ein Modell von Grund auf neu trainieren.

# Erwartete Ergebnisse

Hoffentlich wird das fertige Modell keine modernen Konzepte kennen und nicht √ºber das hinaus denken k√∂nnen, was es gelernt hat. Es sollte moderne Begriffe/Vokabeln nicht erkennen, und ich hoffe, es halluziniert kein modernes Wissen.

# Fortschritts-Updates

## 9. Juli 2025

Ich habe meinen Zeitraum auf 1800‚Äì1850 und die Region: London festgelegt.

Ich habe eine Liste von Texten, B√ºchern, Dokumenten gesammelt.

Bisher habe ich 50 als txt-Dateien und werde bald mit dem Training von NanoGPT beginnen.

Ich werde dies aktualisieren, solange Fortschritte gemacht werden.

## 13. Juli 2025

NanoGPT mit 187MB historischen Textdaten trainiert.

## 15. Juli 2025

Ich habe begonnen, Texte f√ºr den zweiten Trainingslauf herunterzuladen. Ich bekomme alles vom Internet Archive und habe den Zeitraum auf 1800‚Äì1875 erweitert. Um eine breite Vielfalt an Texten zu erhalten, kann man im Internet Archive nach Ver√∂ffentlichungsort, Zeitraum und Themen filtern.

![Suchfilter](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## 16. Juli 2025

Ich habe etwa 500 txt-Dateien vom Internet Archive heruntergeladen und nach dem Bereinigen (nur Leerzeichen l√∂schen, Gutenberg-Kopfzeilen usw.) habe ich etwa 500MB Daten. Es ist ein winziges Datenset, aber das letzte Mal habe ich mit 187MB trainiert, also sollte es zumindest irgendeinen sp√ºrbaren Unterschied beim Output nach dem Training des zweiten Modells geben. Ich hoffe, dieses Modell kann zumindest zusammenh√§ngendere S√§tze erzeugen, die einigerma√üen Sinn ergeben. Das ist nat√ºrlich keine Garantie, denn das Datenset ist immer noch winzig klein, aber es ist mehr als beim letzten Mal.

Das sollte auf meiner eigenen Hardware machbar sein, was auch gut ist, weil ich hoffentlich Verbesserungen sehe, bevor ich zu einem gr√∂√üeren Datensatz springe, f√ºr den ich eine GPU mieten m√ºsste. Aber keine Sorge, ich plane trotzdem bald eine GPU zu mieten, aber vorher will ich sicherstellen, dass mein Datensatz so kuratiert und sauber wie m√∂glich ist. Ein Problem ist das Bereinigen, viele dieser txt-Dateien enthalten Kauderwelsch. Die Skripte, die ich zum Bereinigen verwendet habe, funktionieren, aber sie sind nicht 100% effektiv.

Ich werde dieses Datenset heute trainieren, das sollte etwa 4‚Äì5 Stunden dauern. Sobald es fertig ist und ich es getestet habe, gebe ich Updates. Vielen Dank an alle, die sich mein Projekt ansehen, einige haben mir sogar Links zu OCR-Ressourcen geschickt, also danke! Ich hoffe, mehr Leute probieren das aus und experimentieren mit eigenen Datens√§tzen.

### Trainings-Update

Ich habe mit dem Training eines 435MB (108 Mio. Tokens) Korpus begonnen, bisher l√§uft alles ziemlich reibungslos. Der Trainingsverlust fiel von 10,9 auf 4,9 in den ersten 2800 Iterationen. Ich sch√§tze, es wird etwa 8 oder 9 Stunden dauern. Ich poste ein weiteres Update, sobald es fertig ist.

## 17. Juli 2025, 2:13 Uhr

Das Training f√ºr das zweite Modell ist abgeschlossen, meine 4060 hat etwa 8 Stunden und 40 Minuten gebraucht (3.900 Iterationen/Stunde) f√ºr 33.000 Iterationen (5 Epochen). Der finale Trainingsverlust lag bei 3,73. Die Ausgaben waren √ºberraschend gut, es generiert jetzt tats√§chlich zusammenh√§ngende S√§tze im Stil des 19. Jahrhunderts.

# V0 Modellverhalten & Einschr√§nkungen

Erste Prompts zeigen, dass das Modell mit Sprache und Verhalten der 1800er antwortet. Zum Beispiel habe ich mit "Who art Henry?" getestet, und es antwortete: "I know that man, I have did not a black, the storm." ‚Äì ja, dieser Satz ergibt keinen Sinn, aber das LLM erkennt, dass ich nach einer Person frage.

![TimeLockLLM Beispielausgabe](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

Es gibt keine Erw√§hnung moderner Konzepte, die Ausgaben enthalten meist W√∂rter und Formulierungen aus den 1800ern.

Es ist noch viel Arbeit n√∂tig, mit 187MB Training erh√§lt man kein Modell, das Texte mit komplexer Argumentation erzeugt.

Momentan produziert es S√§tze, denen vollst√§ndige Satzstruktur fehlt und die insgesamt wenig Sinn ergeben, aber das ist bei der Trainingsgr√∂√üe normal.

# V0.5 Modellverhalten & Einschr√§nkungen

Dies ist eine sch√∂ne Verbesserung im Vergleich zum letzten Modell. Der Schreibstil und der Wortschatz sind viktorianisch und fast jeder Satz ist grammatikalisch korrekt mit richtiger Zeichensetzung. Und wieder wurde das Modell von Grund auf trainiert, sodass es bei Themen des 19. Jahrhunderts bleibt.

![TimeLockLLM Beispielausgabe](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

Es gibt viele sachliche Halluzinationen. Viele (etwa 100%) der Details (Daten, Ereignisse, historische Pers√∂nlichkeiten) sind erfunden. Au√üerdem stehen die S√§tze oft nicht wirklich im Zusammenhang, manchmal beziehen sich vielleicht zwei S√§tze aufeinander, aber dar√ºber hinaus nicht. Ein weiteres Problem ist, dass manchmal ein zuf√§lliger ‚ÄûDigitized by Google‚Äú-Footer auftaucht, also muss ich beim n√§chsten Training wirklich sicherstellen, dass die Texte gut bereinigt sind. Insgesamt bin ich sehr zufrieden mit den Ergebnissen, es ist noch lange kein LLM, aber definitiv ein Satzgenerator.

Ich lerne viel und werde in den kommenden Wochen herausfinden, was ich besser machen muss. Dateien werden bald hochgeladen!

# Geplante Vorhaben

(Abgeschlossen) Ich werde mit der Arbeit an Version 0.5 beginnen. Statt mit 50 B√ºchern zu trainieren, werde ich idealerweise mit 500‚Äì600 trainieren. Im Moment trainiere ich nanoGPT mit B√ºchern aus den Jahren 1800‚Äì1850, speziell aus London. Es gibt einige Herausforderungen, wie sicherzustellen, dass die gefundenen B√ºcher nicht aktualisiert oder modern interpretiert wurden, sondern im gew√§hlten Zeitraum unver√§ndert ver√∂ffentlicht wurden.

Ich m√∂chte ein neues Modell (v1) mit einem viel gr√∂√üeren Korpus trainieren, vielleicht 5‚Äì10 Mal gr√∂√üer als der, den ich f√ºr v0.5 verwendet habe. Mein Ziel ist zu sehen, ob aus rein selektiv-temporalem Training bereits logische F√§higkeiten entstehen k√∂nnen; das wird eine schwierigere Aufgabe und ich bin mir nicht sicher, ob das m√∂glich ist, da es historische Datenbeschr√§nkungen gibt. In den n√§chsten Wochen werde ich versuchen, gen√ºgend Daten f√ºr einen 5‚Äì10 GB Korpus zusammenzustellen. Ich glaube, wenn ich saubere, hochwertige Daten bekomme und eine GPU miete, wird es Fortschritte geben.

# Wie man dieses Projekt nutzt

Dieses Projekt konzentriert sich haupts√§chlich auf das Kuratieren historischer Daten, deren Vorbereitung f√ºr das Training und das Erstellen eines Tokenizers. Ich werde den kompletten LLM-Trainingsprozess nicht abdecken; dazu siehe nanoGPT von Andrej Karpathy.

# Schritt 1: Historische Texte sammeln und vorbereiten

Sammle .txt-Dateien von B√ºchern, Dokumenten usw. aus dem gew√ºnschten Zeitraum (z. B. London 1800‚Äì1850), die gemeinfrei sind.

Du kannst download_texts_improved.py verwenden, um f√ºr dich B√ºcher herunterzuladen, falls du das brauchst.

Bereinige die Textdateien mit einem Skript oder entferne manuell Header/Footer von Project Gutenberg, moderne Anmerkungen oder Dinge wie OCR-Fehler.

prepare_dataset.py sollte problemlos funktionieren.

# Schritt 2: Einen eigenen Tokenizer bauen

F√ºhre train_tokenizer.py oder train_tokenizer_hf.py auf den bereinigten Daten aus.
Das ergibt vocab.json und merges.txt

Diese Dateien definieren den Wortschatz und die Zusammenf√ºhrungsregeln f√ºr dein Modell

# Schritt 3: Trainiere dein Modell (nanoGPT)

Siehe [nanoGPT von Andrej Karpathy](https://github.com/karpathy/nanoGPT) f√ºr den Trainingsprozess.

Du kannst auch ein anderes LLM trainieren, aber ich habe nanoGPT verwendet

# FAQ

## Was ist Selective Temporal Training?

Selective Temporal Training (STT) ist eine Machine-Learning-Methodik, bei der alle Trainingsdaten gezielt so ausgew√§hlt werden, dass sie in einen bestimmten historischen Zeitraum fallen. Ziel ist es, die Sprache und das Wissen dieser Epoche ohne Einfluss moderner Konzepte zu modellieren. Zum Beispiel wurde mein aktuelles Modell (v0.5) ausschlie√ülich mit Daten von 1800‚Äì1875 trainiert, nicht feinabgestimmt, sondern von Grund auf, sodass die Ausgaben den Sprachstil und historischen Kontext dieser Zeit widerspiegeln.

## Warum nicht einfach Fine-Tuning oder LoRA verwenden?

In diesem Projekt versuche ich, ein Sprachmodell zu erstellen, das frei von modernem Bias ist. Wenn ich z. B. GPT-2 feinabstimme, ist es bereits vortrainiert und diese Information bleibt erhalten. Wenn ich von Grund auf trainiere, tut das Modell nicht so, als w√§re es alt ‚Äì es ist es einfach. Ziel des Projekts ist es momentan, etwas zu schaffen, das ausschlie√ülich mit dem Wissen aus Londoner B√ºchern, die zwischen 1800 und 1850 ver√∂ffentlicht wurden, argumentieren kann.

## Welche Daten wurden f√ºr das Training verwendet?

Ich verwende B√ºcher, juristische Dokumente, Zeitungen und andere Schriften aus London von 1800‚Äì1850. Die verlinkte Liste hat etwa 200, aber f√ºr das erste Training habe ich nur 50 Dateien mit insgesamt ca. 187 MB verwendet. Eine Liste der Dokumente findest du hier:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## Wie gro√ü ist das Version-0-Modell?

Dieses Modell ist derzeit sehr klein, ich mache das nur zum Spa√ü und halte mich strikt an die Regel, keine modernen Quellen zu verwenden. Es hat fast 16 Millionen Parameter, aber ich werde mehr alte Texte sammeln, um ein weiteres Modelltraining zu beginnen. Updates folgen.

## Trainingsspezifikationen?

GPU: Geforce rtx 4060
CPU: i5-13400F
Ram: 16GB DDR5.



---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-21

---