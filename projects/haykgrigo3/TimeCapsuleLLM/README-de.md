<div align="right">
  <details>
    <summary >üåê Sprache</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-TW">ÁπÅÈ´î‰∏≠Êñá</a> 
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=as">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä</a> 
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=th">‡πÑ‡∏ó‡∏¢</a> 
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=fr">Fran√ßais</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=es">Espa√±ol</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=it">Italiano</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=ru">–†—É—Å—Å–∫–∏–π</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=pt">Portugu√™s</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=ar">ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</a>
        | <a href="#" title="Coming soon">ŸÅÿßÿ±ÿ≥€å (coming soon)</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=tr">T√ºrk√ße</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=vi">Ti·∫øng Vi·ªát</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (coming soon)</a>

      </div>
    </div>
  </details>
</div>


# TimeCapsule LLM

*Ein Sprachmodell, das **von Grund auf** ausschlie√ülich mit Daten aus bestimmten Orten und Zeitperioden trainiert wurde, um moderne Verzerrungen zu minimieren und die Stimme, den Wortschatz und die Weltanschauung der jeweiligen Epoche nachzuahmen.*

Stellen Sie sich vor, ein KI-Modell w√ºrde nicht nur so tun, als w√§re es historisch, sondern w√§re es wirklich.

v0 und v0.5 basieren auf [nanoGPT von Andrej Karpathy](https://github.com/karpathy/nanoGPT). Die Kern-Trainingsskripte und die Modellarchitektur sind sein Werk.

v1 basiert auf [Phi 1.5 von Microsoft](https://huggingface.co/microsoft/phi-1_5)

v2 basiert auf llamaforcausallm

[Hugging Face Link](https://huggingface.co/haykgrigorian/TimeCapsuleLLM)


##  Modellverhalten & Einschr√§nkungen

### **v0**  

Fr√ºhe Prompts zeigen, dass das Modell mit Sprache und Verhalten aus dem 19. Jahrhundert antwortet.
Beispiel: Prompt: "Who art Henry?" und es antwortete: "I know that man, I have did not a black, the storm." 

![TimeLockLLM Beispielausgabe](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

- Keine Erw√§hnung moderner Konzepte  
- Meistens epochengetreues Vokabular  
- S√§tze sind gr√∂√ütenteils unzusammenh√§ngend (erwartet bei ~187MB Trainingsdaten)

### **v0.5** 

Eine deutliche Verbesserung gegen√ºber v0.  
- Viktorianischer Schreibstil, korrekte Zeichensetzung, meist grammatikalisch richtige S√§tze  
- Immer noch hohe Rate an faktischen Halluzinationen  
- OCR-Rauschen (‚ÄûDigitized by Google‚Äú) weiterhin in Ausgaben vorhanden

![TimeLockLLM Beispielausgabe](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

### **v1**

Erstes Modell, das ein echtes historisches Ereignis mit einer tats√§chlichen Figur aus dem Datensatz verkn√ºpft.

Beispiel: Prompt: "It was the year of our Lord 1834" 

Die Ausgabe: "It was the year of our Lord 1834 and the streets of London were filled with protest and petition. The cause, as many re counted, was not bound in the way of private, but having taken up the same day in the day of Lord Palmerston, the public will receive a short statement of the difficulties under which the day of law has reached us. It is a matter of deep regret, that the present events in the history of the world are clear, and consequently will be'known. It is not true that the very men who first settled in the Gospel at Jerusalem should have so extensive and so interesting a record of the prosperity and prosperity" 

Zun√§chst nahm ich an, dass ein Protest zuf√§llig im selben Jahr stattfand, aber sehen Sie sich das an: ![1834protest](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)

### **v2mini-eval1**

Trainiert mit einer 15GB gro√üen Stichprobe aus dem 90GB-Datensatz von v2.
Modell nur bis zu 10.000 Schritten trainiert.

Es gab ein Problem mit der Tokenisierung, das dazu f√ºhrt, dass die Ausgabe so aussieht:

PROMPT:
Wer ist Charles Dickens?


W ho is Charles D ic ens ? D oes that work more of h ise x cell ent st ir ring , in his pl ays , int he G reat C omp any 's f arm ? What I have y et to qu ote from J ack P ick ett ? D oy oun ot know th att hey were as pe cies of g all oping , ors aw ing oft heir bre asts , or what w ast ob ed one about the t im ew hen J acks on was looking ont hew ind ow ? What ist hes uccess of an English man , and which h isson has not be end is cover ing t ome , where by to acc omp lish such a we ight ? D idy ou ever m ake ap ass age int othe old road way , ort o an anch or - b ree ze att he foot of our boat , which you m ust le ave us ? The fact is , that whe ther the wind would rise up from the pl ain on S atur day night orn o ont ide , ort ill then orth , or other wise , wes hall be com pelled t od esc ribe af orm idable bar rier , with t hes ame e ffect s ast he p resent . In th iss itu ation , at le ast , itis not to omuch t os ay that we have le ft that room . I bel ieve there are three c op ies int he " F ive H undred - fold ," to bere fer red to , ast he first n umber of our read ers wh ow is ht

Korrigiert:

"Wer ist Charles Dickens? Wirkt das mehr von seinem ausgezeichneten, bewegenden Schaffen, in seinen Theaterst√ºcken, auf dem Hof der Gro√üen Gesellschaft? Was habe ich noch von Jack Pickett zu zitieren? Wissen Sie nicht, dass sie eine Art von Galoppieren waren, oder S√§gen ihrer Brust, oder was in der Zeit zu tun war, als Jackson am Fenster stand? Was ist der Erfolg eines Engl√§nders, und was hat sein Sohn mir noch nicht entdeckt, wodurch ein solches Gewicht erreicht werden kann? Haben Sie jemals einen Durchgang auf die alte Stra√üe gemacht, oder zu einer Ankerbrise am Fu√ü unseres Bootes, die Sie uns verlassen m√ºssen? Tatsache ist, dass unabh√§ngig davon, ob der Wind am Samstagabend oder am Mittag vom Feld aufsteigt oder nach Norden zieht, oder anders, wir gezwungen sein werden, eine gewaltige Barriere zu beschreiben, mit denselben Auswirkungen wie derzeit. In dieser Situation ist es zumindest nicht zu viel zu sagen, dass wir diesen Raum verlassen haben. Ich glaube, es gibt drei Exemplare im 'F√ºnfhundertfachen', auf die verwiesen werden kann, als die erste Nummer unserer Leser, die t[o]..."

### **v2mini-eval2**

Trainiert mit demselben 15GB Sample, das f√ºr die Schulung von eval1 verwendet wurde.
Das Modell wurde erneut nur bis zu 10.000 Schritten trainiert.

PROMPT:Charles Darwin

Charles DarwinECCEMACY. Sehr geehrter Herr, ‚Äî Der folgende Fall ist f√ºr mich interessant: ‚Äî Ich war zwei Wochen in London und litt stark unter einem Rheumaanfall. Der erste Rheumaanfall war eine Woche bevor ich Sie sah, und der zweite als ich Sie sah, und der dritte als ich Sie sah, und der dritte zur gleichen Zeit. Der zweite Gichtanfall war jedoch nicht von Fiebersymptomen begleitet, sondern ging mit erh√∂hter Urinausscheidung und einer reichlicheren Entleerung einher. Der dritte Anfall trat eine Stunde nach unserem Treffen auf und wurde von einer R√ºckkehr eines Gichtanfalls und einer rascheren Wiederkehr der Gicht gefolgt. Der vierte Anfall ging ebenfalls mit Fieber einher, jedoch nicht immer mit Fiebersymptomen. Der dritte Gichtanfall war zwei Wochen nach Ihrer Erkrankung, und der vierte wurde von einem Gichtanfall gefolgt. Der vierte Anfall war zwei Wochen nach Ihrem Angriff und ging mit einem Gef√ºhl einher


##  Datens√§tze

### **v2**

- 90GB an London-Texten von 1800-1875
- 136.344 Dokumente
- Das vollst√§ndige 90GB-Datenset ist noch nicht verf√ºgbar, da es noch nicht tokenisiert ist, aber eine 15GB-Probe finden Sie hier: https://huggingface.co/datasets/haykgrigorian/TimeCapsuleLLM-London-1800-1875-v2-15GB

 ### Bias-Statistiken 
  ![Pronomen-Bias](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v2mini_eval1/pronoun_bias.png)

  ![Geografischer Bias](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v2mini_eval1/geographic_bias.png)

  ![Zeitlicher Bias](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v2mini_eval1/temporal_bias.png)

Weitere Informationen finden Sie im [v2 Bias Report](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v2mini_eval1/v2_bias_report.json).


## Verwendung

Dieses Projekt konzentriert sich haupts√§chlich auf die Zusammenstellung historischer Daten, deren Aufbereitung f√ºr das Training und den Bau eines Tokenizers. Der vollst√§ndige LLM-Trainingsprozess wird hier nicht behandelt, daf√ºr verweise ich auf nanoGPT von Andrej Karpathy.

### Schritt 1: Sammeln und Vorbereiten historischer Texte 

- Sammeln Sie .txt-Dateien von gemeinfreien B√ºchern, Dokumenten usw. aus Ihrer gew√§hlten Zeitperiode (z.B. London 1800-1850) 
- Halten Sie sie innerhalb Ihres gew√§hlten Zeit-/Ortsfensters  
- Bereinigen Sie die Textdateien mit einem Skript oder entfernen Sie manuell Kopf-/Fu√üzeilen von Project Gutenberg, moderne Anmerkungen oder Dinge wie OCR-Fehler.

### Schritt 2: Eigener Tokenizer bauen

- F√ºhren Sie train_tokenizer.py oder train_tokenizer_hf.py auf den bereinigten Daten aus.
- Das ergibt vocab.json und merges.txt
- Diese Dateien definieren Vokabular und Merge-Regeln f√ºr Ihr Modell

### Schritt 3: Trainieren Sie Ihr Modell 

- Orientieren Sie sich f√ºr den Trainingsprozess an [nanoGPT von Andrej Karpathy](https://github.com/karpathy/nanoGPT) oder an den Dokumentationen Ihrer gew√§hlten Architektur.

# FAQ

## Was ist Selective Temporal Training?

Selective Temporal Training (STT) ist eine Machine-Learning-Methodik, bei der alle Trainingsdaten gezielt so ausgew√§hlt werden, dass sie in einen bestimmten historischen Zeitraum fallen. Dies geschieht, um die Sprache und das Wissen jener Epoche ohne Einfluss moderner Konzepte zu modellieren. Zum Beispiel ist das aktuelle Modell (v0.5) ausschlie√ülich mit Daten von 1800‚Äì1875 trainiert, es ist nicht feinabgestimmt, sondern von Grund auf neu trainiert, sodass die Ausgabe den Sprachstil und historischen Kontext dieser Zeit widerspiegelt.

## Warum nicht einfach Fine-Tuning oder LoRA verwenden?

F√ºr dieses Projekt versuche ich, ein Sprachmodell zu erstellen, das nicht von modernen Verzerrungen beeinflusst ist. Wenn ich z. B. GPT-2 feinabstimme, ist es bereits vortrainiert und diese Informationen verschwinden nicht. Wenn ich von Grund auf neu trainiere, tut das Sprachmodell nicht so, als w√§re es alt, sondern es ist es einfach. Das Ziel f√ºr dieses Projekt ist derzeit, etwas zu schaffen, das ausschlie√ülich mit Wissen aus Londoner B√ºchern, die zwischen 1800 und 1875 erschienen sind, argumentieren kann.

## Welche Art von Daten wurde f√ºr das Training verwendet?

Ich verwende B√ºcher, Rechtsdokumente, Zeitungen und andere Schriften aus London von 1800‚Äì1875. Die von mir verlinkte Liste (f√ºr v0) enth√§lt etwa 200, aber f√ºr das erste Training habe ich nur 50 Dateien mit insgesamt ca. 187 MB verwendet. Eine Liste der Dokumente finden Sie hier:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

Datensatzgr√∂√üen:
- v0: ~187MB
- v0.5: ~435MB 
- v1: ~6,25GB 
- v2mini-eval1: 15GB

## Wie gro√ü sind die Modelle?

v0: 16M Parameter

v0.5 123M Parameter

v1: 700M Parameters

v2mini-eval1: 300M Parameters

# Training Specs ? 

# v0/v0.5
GPU: Geforce rtx 4060
CPU: i5-13400F 
Ram: 16GB DDR5.

# v1
GPU: A100 SXM rented

# v2mini-eval1

GPU: A100 SXM rented






























---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2026-01-13

---