<div align="right">
  <details>
    <summary >üåê Sprache</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="#" title="Coming soon">ÁπÅÈ´î‰∏≠Êñá (coming soon)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="#" title="Coming soon">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä (coming soon)</a> |
        | <a href="#" title="Coming soon">‡πÑ‡∏ó‡∏¢ (coming soon)</a> |
        | <a href="#" title="Coming soon">Fran√ßais (coming soon)</a>
        | <a href="#" title="Coming soon">Deutsch (coming soon)</a>
        | <a href="#" title="Coming soon">Espa√±ol (coming soon)</a>
        | <a href="#" title="Coming soon">Italiano (coming soon)</a>
        | <a href="#" title="Coming soon">–†—É—Å—Å–∫–∏–π (coming soon)</a>
        | <a href="#" title="Coming soon">Portugu√™s (coming soon)</a>
        | <a href="#" title="Coming soon">Nederlands (coming soon)</a>
        | <a href="#" title="Coming soon">Polski (coming soon)</a>
        | <a href="#" title="Coming soon">ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (coming soon)</a>
        | <a href="#" title="Coming soon">ŸÅÿßÿ±ÿ≥€å (coming soon)</a>
        | <a href="#" title="Coming soon">T√ºrk√ße (coming soon)</a>
        | <a href="#" title="Coming soon">Ti·∫øng Vi·ªát (coming soon)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (coming soon)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM
Ein LLM, das nur mit Daten aus bestimmten Zeitperioden trainiert wurde, um moderne Verzerrungen zu reduzieren.

Stell dir vor, ein KI-Modell w√ºrde nicht nur so tun, als w√§re es historisch, sondern w√§re es tats√§chlich.

Basierend auf [nanoGPT von Andrej Karpathy](https://github.com/karpathy/nanoGPT) Die zentralen Trainingsskripte und die Modellarchitektur stammen von ihm.

# Projektziele

TimeCapsule LLM ist ein experimentelles Projekt, das ausschlie√ülich mit Texten aus bestimmten Zeitr√§umen trainiert wird. Ziel ist es, die Weltanschauung und Sprache bestimmter historischer Epochen zu simulieren.

# Warum Feintuning nicht ausreicht

Wenn man nur ein vortrainiertes Modell finetuned, kennt das LLM trotzdem moderne Konzepte. Nat√ºrlich ist es schwierig, eine Verzerrung durch moderne Einfl√ºsse vollst√§ndig zu vermeiden, aber ich m√∂chte so nah wie m√∂glich daran kommen. Keine moderne Verzerrung zu erreichen, erfordert ein Training des Modells von Grund auf.

# Erwartete Ergebnisse

Hoffentlich wird dieses Modell, wenn es fertig ist, keine modernen Konzepte kennen und nicht √ºber das hinaus denken k√∂nnen, was es gelernt hat. Es sollte keine modernen Begriffe oder Vokabeln erkennen und ich hoffe, es halluziniert kein modernes Wissen.

# Fortschritts-Updates

## 9. Juli 2025

Ich habe meinen Zeitraum auf 1800‚Äì1850 und die Region London festgelegt.

Ich habe eine Liste von Texten, B√ºchern, Dokumenten zusammengestellt.

Bisher habe ich 50 als txt-Dateien erhalten und werde NanoGPT bald mit dem Training beginnen.

Ich werde dies aktualisieren, solange Fortschritte gemacht werden.

## 13. Juli 2025

NanoGPT mit 187MB historischen Textdaten trainiert.

## 15. Juli 2025

Ich habe begonnen, Texte f√ºr den zweiten Trainingslauf herunterzuladen. Ich bekomme alles aus dem Internet Archive und habe den Zeitraum auf 1800‚Äì1875 erweitert. Um eine breite Palette an Texten zu erhalten, kann man im Internet Archive die Filter f√ºr Ver√∂ffentlichungsort, Zeitraum und Themen verwenden.

![Suchfilter](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## 16. Juli 2025

Ich habe etwa 500 txt-Dateien aus dem Internet Archive heruntergeladen und nach dem Bereinigen (nur Leerzeichen l√∂schen, Gutenberg-Kopfzeilen usw.) habe ich etwa 500MB Daten. Es ist ein winziger Datensatz, aber letztes Mal habe ich mit 187MB trainiert, also sollte es zumindest einen merklichen Unterschied im Output geben, nachdem ich das zweite Modell trainiert habe. Ich hoffe, dieses Modell kann zumindest zusammenh√§ngendere S√§tze produzieren, die einigerma√üen Sinn ergeben. Das ist nat√ºrlich keine Garantie, da es immer noch ein sehr kleiner Datensatz ist, aber mehr als beim letzten Mal.

Das sollte auf meiner eigenen Hardware machbar sein, was gut ist, weil ich hoffentlich Verbesserungen sehe, bevor ich zu einem gr√∂√üeren Datensatz wechsle, f√ºr den ich eine GPU mieten m√ºsste. Keine Sorge, ich plane trotzdem bald eine GPU zu mieten, aber vorher m√∂chte ich sicherstellen, dass mein Datensatz so kuratiert und sauber wie m√∂glich ist. Eines der Probleme ist das Bereinigen ‚Äì viele dieser txt-Dateien enthalten Unsinn. Die Skripte, die ich f√ºr das Bereinigen verwendet habe, funktionieren, sind aber nicht 100% effektiv.

Ich werde diesen Datensatz heute trainieren und es sollte etwa 4‚Äì5 Stunden dauern. Sobald es fertig ist und ich es getestet habe, gebe ich Updates. Danke nochmal an alle, die sich mein Projekt ansehen, einige haben mir sogar Links zu OCR-Ressourcen geschickt ‚Äì vielen Dank! Ich hoffe, mehr Leute probieren das aus und experimentieren mit eigenen Datens√§tzen.


### Trainings-Update

Ich habe mit dem Training auf einem 435MB (108 Mio. Tokens) Korpus begonnen, es l√§uft aktuell ziemlich reibungslos. Der Trainingsverlust ist von 10,9 auf 4,9 in den ersten 2800 Iterationen gefallen. Ich rechne damit, dass es etwa 8 oder 9 Stunden dauern wird. Ich poste ein weiteres Update, sobald es fertig ist.

## 17. Juli 2025 2:13 Uhr

Das Training f√ºr das zweite Modell ist abgeschlossen, meine 4060 hat daf√ºr etwa 8 Stunden und 40 Minuten gebraucht (3.900 Iterationen/Stunde) f√ºr 33.000 Iterationen (5 Epochen). Der endg√ºltige Trainingsverlust lag bei 3,73. Die Ergebnisse waren √ºberraschend gut, es generiert jetzt tats√§chlich zusammenh√§ngende S√§tze im Stil des 19. Jahrhunderts.

## 28. Juli 2025

Ich habe v0.5 bei Hugging Face hochgeladen, [schau es dir an](https://huggingface.co/haykgrigorian/TimeCapsuleLLM), wenn du m√∂chtest. Du kannst jetzt mein Repo herunterladen und lokal ausf√ºhren. Leider funktioniert nanoGPT nicht nativ mit HuggingFace, daher musst du das Modell lokal herunterladen und ausf√ºhren.

Au√üerdem werde ich jetzt Daten f√ºr meinen n√§chsten Trainingslauf kuratieren. Ich denke, ich brauche vielleicht 5- bis 10-mal mehr Daten, um echte Argumentationsf√§higkeiten zu erreichen.


# V0 Modellverhalten & Einschr√§nkungen

Fr√ºhe Prompts zeigen, dass das Modell mit Sprache und Verhalten der 1800er reagiert. Zum Beispiel fragte ich: "Who art Henry?" und es antwortete: "I know that man, I have did not a black, the storm." Und ja, dieser Satz ergibt keinen Sinn, aber das LLM erkennt immerhin, dass ich nach einer Person frage.


![TimeLockLLM Beispielausgabe](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

Es gibt keine Erw√§hnung moderner Konzepte, die Ausgaben enthalten haupts√§chlich W√∂rter und Formulierungen aus dem 19. Jahrhundert.

Es muss noch viel Arbeit geleistet werden, das Training mit 187MB reicht nicht aus, um ein Modell zu erhalten, das Texte mit komplexem Denken produziert.

Momentan erzeugt es S√§tze, die keine vollst√§ndige Satzstruktur haben und insgesamt einfach keinen Sinn ergeben, aber das ist bei dieser Trainingsgr√∂√üe normal.

# V0.5 Modellverhalten & Einschr√§nkungen

Dies ist eine sch√∂ne Verbesserung im Vergleich zum letzten Modell. Der Schreibstil und der Wortschatz sind viktorianisch und fast jeder Satz ist grammatikalisch korrekt mit richtiger Zeichensetzung. Und erneut: Das Modell wurde von Grund auf trainiert, sodass es bei Themen des 19. Jahrhunderts bleibt.

![TimeLockLLM Beispielausgabe](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

Es gibt viele faktische Halluzinationen. Sehr viele (fast 100%) der Details (Daten, Ereignisse, historische Figuren) sind erfunden. Au√üerdem stehen die S√§tze nicht wirklich im Zusammenhang zueinander, manchmal beziehen sich vielleicht 2 S√§tze aufeinander, aber dar√ºber hinaus nicht. Ein weiteres Problem ist, dass manchmal eine zuf√§llige ‚ÄûDigitized by Google‚Äú-Fu√üzeile auftaucht, deshalb muss ich beim n√§chsten Training wirklich darauf achten, dass die Texte gut ges√§ubert sind. Insgesamt bin ich mit den Ergebnissen sehr zufrieden, es ist zwar noch kein LLM, aber definitiv ein Satzgenerator.

Ich lerne viel und werde in den kommenden Wochen herausfinden, was ich besser machen muss. Ich werde bald Dateien hochladen!

# Zuk√ºnftige Pl√§ne

(Abgeschlossen) Ich beginne mit der Arbeit an Version 0.5. Anstatt 50 B√ºcher zu verwenden, werde ich idealerweise 500‚Äì600 verwenden. Momentan trainiere ich nanoGPT mit B√ºchern aus den Jahren 1800‚Äì1850, speziell aus London. Es gibt Herausforderungen, wie sicherzustellen, dass die gefundenen B√ºcher nicht √ºberarbeitet oder modern interpretiert sind, sondern unber√ºhrte B√ºcher, die innerhalb meines gew√§hlten Zeitraums ver√∂ffentlicht wurden.

Ich m√∂chte ein neues Modell (v1) mit einem viel gr√∂√üeren Korpus trainieren, vielleicht 5‚Äì10x so gro√ü wie der f√ºr v0.5 verwendete. Mein Ziel ist es zu sehen, ob sich allein durch Selective Temporal Training Denkf√§higkeiten herausbilden k√∂nnen. Das wird schwieriger und ich bin nicht ganz sicher, ob es wegen der historischen Datenbegrenzungen √ºberhaupt m√∂glich ist. In den n√§chsten Wochen werde ich versuchen, genug Daten f√ºr einen 5‚Äì10GB gro√üen Korpus zusammenzustellen. Ich glaube, wenn ich saubere, hochwertige Daten bekomme und eine GPU miete, wird es Fortschritte geben.

# So benutzt du dieses Projekt

Dieses Projekt konzentriert sich haupts√§chlich auf das Sammeln historischer Daten, deren Aufbereitung f√ºr das Training und den Bau eines Tokenizers. Ich werde nicht den gesamten LLM-Trainingsprozess abdecken, daf√ºr verweise ich auf nanoGPT von Andrej Karpathy.

# Schritt 1: Historische Texte sammeln und vorbereiten

Sammle .txt-Dateien von gemeinfreien B√ºchern, Dokumenten usw. aus deinem gew√§hlten Zeitraum (z. B. London 1800‚Äì1850).

Du kannst download_texts_improved.py verwenden, um B√ºcher herunterzuladen, falls du welche brauchst.

Bereinige die Textdateien mit einem Skript oder entferne manuell Header/Footers von Project Gutenberg, moderne Anmerkungen oder Fehler wie OCR-Fehler.

prepare_dataset.py sollte gut funktionieren.

# Schritt 2: Einen eigenen Tokenizer bauen

F√ºhre train_tokenizer.py oder train_tokenizer_hf.py auf den ges√§uberten Daten aus.
Dadurch erh√§ltst du vocab.json und merges.txt

Diese Dateien definieren Wortschatz und Merge-Regeln f√ºr dein Modell.

# Schritt 3: Dein Modell trainieren (nanoGPT)

Siehe [nanoGPT von Andrej Karpathy](https://github.com/karpathy/nanoGPT) f√ºr den Trainingsprozess.

Du kannst auch ein anderes LLM trainieren, aber ich habe nanoGPT verwendet.

# FAQ

## Was ist Selective Temporal Training?

Selective Temporal Training (STT) ist eine Machine Learning-Methode, bei der alle Trainingsdaten gezielt aus einem bestimmten historischen Zeitraum stammen. Das geschieht, um Sprache und Wissen jener Epoche ohne Einfluss moderner Konzepte zu modellieren. Das aktuelle Modell (v0.5) wurde beispielsweise ausschlie√ülich mit Daten von 1800‚Äì1875 trainiert, nicht feinabgestimmt, sondern von Grund auf. Das Ergebnis spiegelt den Sprachstil und den historischen Kontext jener Zeit wider.

## Warum nicht einfach Fine-Tuning oder LoRA verwenden?

F√ºr dieses Projekt versuche ich, ein Sprachmodell zu erschaffen, das nicht von modernen Einfl√ºssen getr√ºbt ist. Wenn ich z. B. GPT-2 feinabstimme, ist es bereits vortrainiert und dieses Wissen bleibt erhalten. Wenn ich von Grund auf trainiere, tut das Sprachmodell nicht so, als w√§re es alt ‚Äì es ist es einfach. Das Ziel ist, ein Modell zu bauen, das ausschlie√ülich mit Wissen aus Londoner B√ºchern von 1800 bis 1850 argumentieren kann.

## Welche Daten hast du zum Training verwendet?

Ich verwende B√ºcher, juristische Dokumente, Zeitungen und andere Schriften aus dem London der Jahre 1800‚Äì1850. Die verlinkte Liste enth√§lt etwa 200, aber f√ºr das erste Training habe ich nur 50 Dateien mit rund 187 MB verwendet. Du kannst die Dokumentenliste hier ansehen:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## Wie gro√ü ist das Version 0 Modell?

Dieses Modell ist momentan sehr klein, ich mache das nur zum Spa√ü und folge strikt der Regel, keine modernen Quellen zu verwenden. Es hat fast 16 Millionen Parameter, aber ich beginne bald, mehr alte Texte zu sammeln, um ein weiteres Modell zu trainieren. Updates folgen!

## Trainingsspezifikationen?

GPU: Geforce RTX 4060
CPU: i5-13400F
RAM: 16GB DDR5.


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-08-02

---