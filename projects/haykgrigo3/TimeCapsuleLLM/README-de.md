<div align="right">
  <details>
    <summary >üåê Sprache</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="#" title="Coming soon">ÁπÅÈ´î‰∏≠Êñá (coming soon)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="#" title="Coming soon">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä (coming soon)</a> |
        | <a href="#" title="Coming soon">‡πÑ‡∏ó‡∏¢ (coming soon)</a> |
        | <a href="#" title="Coming soon">Fran√ßais (coming soon)</a>
        | <a href="#" title="Coming soon">Deutsch (coming soon)</a>
        | <a href="#" title="Coming soon">Espa√±ol (coming soon)</a>
        | <a href="#" title="Coming soon">Italiano (coming soon)</a>
        | <a href="#" title="Coming soon">–†—É—Å—Å–∫–∏–π (coming soon)</a>
        | <a href="#" title="Coming soon">Portugu√™s (coming soon)</a>
        | <a href="#" title="Coming soon">Nederlands (coming soon)</a>
        | <a href="#" title="Coming soon">Polski (coming soon)</a>
        | <a href="#" title="Coming soon">ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (coming soon)</a>
        | <a href="#" title="Coming soon">ŸÅÿßÿ±ÿ≥€å (coming soon)</a>
        | <a href="#" title="Coming soon">T√ºrk√ße (coming soon)</a>
        | <a href="#" title="Coming soon">Ti·∫øng Vi·ªát (coming soon)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (coming soon)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM
Ein LLM, das nur mit Daten aus bestimmten Zeitr√§umen trainiert wurde, um moderne Verzerrungen zu reduzieren.

Stellen Sie sich vor, ein KI-Modell w√ºrde nicht nur vorgeben, historisch zu sein, sondern es tats√§chlich w√§re.

Basiert auf [nanoGPT von Andrej Karpathy](https://github.com/karpathy/nanoGPT). Die wichtigsten Trainingsskripte und die Modellarchitektur stammen von ihm.

# Projektziele 

TimeCapsule LLM ist ein experimentelles Projekt, das ausschlie√ülich mit Texten aus bestimmten historischen Zeitr√§umen trainiert wird. Ziel ist es, die Weltanschauung und Sprache spezifischer Epochen zu simulieren.

# Warum Fine-Tuning nicht ausreicht 

Wenn man nur ein vortrainiertes Modell feinjustiert, kennt das LLM trotzdem moderne Konzepte. Nat√ºrlich ist es schwierig, v√∂llige Neutralit√§t gegen√ºber der Moderne zu erreichen, aber ich m√∂chte diesem Ziel so nahe wie m√∂glich kommen. Keine moderne Voreingenommenheit zu haben, erfordert ein Modell, das von Grund auf neu trainiert wird.

# Erwartete Ergebnisse 

Hoffentlich kennt dieses Modell nach Fertigstellung keine modernen Konzepte und kann nicht √ºber das hinaus argumentieren, was ihm beigebracht wurde. Es sollte moderne Begriffe/Vokabeln nicht erkennen und ich hoffe, dass es kein modernes Wissen halluziniert.

# Fortschritts-Updates

## 9. Juli 2025

Ich habe meinen Zeitraum auf 1800-1850 und die Region: London festgelegt.

Ich habe eine Liste von Texten, B√ºchern, Dokumenten zusammengestellt.

Bisher habe ich 50 als txt-Dateien erhalten und werde bald mit dem Training von NanoGPT beginnen.

Ich werde dies aktualisieren, solange Fortschritte gemacht werden.

## 13. Juli 2025

NanoGPT wurde mit 187MB historischen Textdaten trainiert.

## 15. Juli 2025

Ich habe begonnen, Texte f√ºr den zweiten Trainingslauf herunterzuladen. Ich beziehe alles aus dem Internet Archive und habe den Zeitraum auf 1800-1875 ausgeweitet. Um eine vielf√§ltige Auswahl an Texten zu bekommen, kann man im Internet Archive die Filter f√ºr Ver√∂ffentlichungsort, Zeitraum und Themen nutzen.

![Suchfilter](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## 16. Juli 2025

Ich habe etwa 500 txt-Dateien aus dem Internet Archive heruntergeladen und nach dem S√§ubern (nur Leerzeichen, Gutenberg-Header usw. gel√∂scht) habe ich etwa 500MB Daten. Es ist ein winziger Datensatz, aber beim letzten Mal habe ich mit 187MB trainiert, also sollte es zumindest einen merklichen Unterschied im Output nach dem Training des zweiten Modells geben. Ich hoffe, dass dieses Modell zumindest zusammenh√§ngendere S√§tze erzeugen kann, die einigerma√üen Sinn ergeben. Das ist nat√ºrlich keine Garantie, da es immer noch ein winziger Datensatz ist, aber mehr als beim letzten Mal.

Das sollte mit meiner eigenen Hardware machbar sein. Das ist auch gut, denn hoffentlich sehe ich so Verbesserungen, bevor ich auf einen gr√∂√üeren Datensatz umsteige, f√ºr den ich eine GPU mieten m√ºsste. Aber keine Sorge, ich plane immer noch, bald eine GPU zu mieten, aber bevor ich das tue, will ich sicherstellen, dass mein Datensatz so kuratiert und sauber wie m√∂glich ist. Eines der Probleme ist das S√§ubern, viele dieser txt-Dateien enthalten Kauderwelsch. Die Skripte, die ich zum S√§ubern verwendet habe, funktionieren, sind aber nicht 100% effektiv.

Ich werde diesen Datensatz heute trainieren, das sollte etwa 4-5 Stunden dauern. Sobald es fertig ist und ich es getestet habe, gebe ich Updates. Nochmals vielen Dank an alle, die sich mein Projekt anschauen, einige haben mir sogar Links zu OCR-Ressourcen geschickt, also danke! Ich hoffe, mehr Leute probieren das aus und experimentieren mit eigenen Datens√§tzen.

## 28. Juli 2025

Ich habe v0.5 bei Hugging Face hochgeladen, [Schau es dir an](https://huggingface.co/haykgrigorian/TimeCapsuleLLM) wenn du m√∂chtest. Du kannst jetzt mein Repository herunterladen und lokal ausf√ºhren. Leider funktioniert nanoGPT nicht nativ mit HuggingFace, daher musst du das Modell lokal herunterladen und ausf√ºhren.

Au√üerdem beginne ich nun damit, Daten f√ºr meinen n√§chsten Trainingslauf zu kuratieren. Ich glaube, ich brauche vielleicht 5-10x mehr Daten, um logische F√§higkeiten zu erreichen.

### Trainings-Update

Ich habe mit einem 435MB (108 Mio. Tokens) Korpus mit dem Training begonnen, es l√§uft derzeit ziemlich reibungslos. Der Trainingsverlust fiel in den ersten 2800 Iterationen von 10,9 auf 4,9. Ich erwarte, dass es etwa 8 oder 9 Stunden dauert, bis es abgeschlossen ist. Ich werde ein weiteres Update posten, sobald es fertig ist.

## 17. Juli 2025, 2:13 Uhr

Das Training f√ºr das zweite Modell ist abgeschlossen, meine 4060 hat etwa 8 Stunden und 40 Minuten gebraucht (3.900 Iterationen/Stunde) f√ºr 33.000 Iterationen (5 Epochen). Der finale Trainingsverlust lag bei 3,73. Die Ergebnisse waren √ºberraschend gut, es generiert jetzt wirklich zusammenh√§ngende S√§tze im Stil des 19. Jahrhunderts.

# V0 Modellverhalten & Einschr√§nkungen

Erste Prompts zeigen, dass das Modell mit Sprache und Verhalten der 1800er reagiert. Zum Beispiel habe ich es mit "Who art Henry?" aufgefordert und es antwortete "I know that man, I have did not a black, the storm." und ja, dieser Satz ergibt keinen Sinn, aber das LLM erkennt, dass ich nach einer Person frage.

![TimeLockLLM Beispielausgabe](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)


Es gibt keine Erw√§hnung moderner Konzepte, die Ausgaben enthalten gr√∂√ütenteils W√∂rter und Formulierungen aus dem 19. Jahrhundert.

Es ist noch viel Arbeit n√∂tig, das Training mit 187MB wird kein Modell hervorbringen, das Texte mit komplexem logischem Denken erzeugt.

Im Moment produziert es S√§tze, die keine vollst√§ndige Satzstruktur haben und insgesamt einfach keinen Sinn ergeben, aber das ist bei der Trainingsgr√∂√üe normal.

# V0.5 Modellverhalten & Einschr√§nkungen

Dies ist eine sch√∂ne Verbesserung im Vergleich zum letzten Modell. Der Schreibstil und der Wortschatz sind viktorianisch und fast jeder Satz ist grammatikalisch korrekt mit richtiger Zeichensetzung. Und erneut: Das Modell wurde von Grund auf trainiert, daher bezieht es sich nur auf Themen des 19. Jahrhunderts.

![TimeLockLLM Beispielausgabe](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

Es gibt viele faktische Halluzinationen. Viele (nahezu 100%) der Details (Daten, Ereignisse, historische Pers√∂nlichkeiten) sind erfunden. Au√üerdem stehen die S√§tze nicht wirklich miteinander in Verbindung, manchmal beziehen sich vielleicht 2 S√§tze aufeinander, aber dar√ºber hinaus nicht. Ein weiteres Problem ist, dass manchmal eine ‚ÄûDigitized by Google‚Äú-Fu√üzeile auftaucht, daher muss ich beim n√§chsten Training wirklich sicherstellen, dass die Texte gut bereinigt sind. Insgesamt bin ich mit den Ergebnissen sehr zufrieden, es ist noch kein LLM, aber definitiv ein Satzgenerator.

Ich lerne viel und werde in den kommenden Wochen herausfinden, was ich besser machen muss. Ich werde die Dateien bald hochladen!

# Geplante Vorhaben

(Abgeschlossen) Ich werde mit Version 0.5 beginnen, anstatt mit 50 B√ºchern zu trainieren, werde ich idealerweise 500‚Äì600 verwenden. Aktuell trainiere ich nanoGPT mit B√ºchern aus den Jahren 1800‚Äì1850 und speziell aus London. Es gibt einige Herausforderungen, wie sicherzustellen, dass die gefundenen B√ºcher nicht aktualisiert oder modern interpretiert sind, sondern unbearbeitete B√ºcher, die innerhalb meines gew√§hlten Zeitraums ver√∂ffentlicht wurden.

Ich m√∂chte ein neues Modell (v1) mit einem viel gr√∂√üeren Korpus trainieren, vielleicht 5‚Äì10 Mal gr√∂√üer als der, den ich f√ºr v0.5 verwendet habe. Mein Ziel ist es zu sehen, ob sich durch ausschlie√ülich selektives temporales Training logische F√§higkeiten entwickeln k√∂nnen ‚Äì das wird eine schwierigere Aufgabe und ich bin mir nicht einmal sicher, ob es m√∂glich ist, da es historische Datenbeschr√§nkungen gibt. In den kommenden Wochen werde ich versuchen, genug Daten f√ºr einen 5‚Äì10GB-Korpus zusammenzustellen. Ich glaube, wenn ich saubere, hochwertige Daten bekomme und eine GPU miete, wird es Fortschritte geben.

# Wie man dieses Projekt benutzt

Dieses Projekt konzentriert sich haupts√§chlich auf das Kuratieren historischer Daten, deren Aufbereitung f√ºr das Training und den Aufbau eines Tokenizers. Den vollst√§ndigen LLM-Trainingsprozess werde ich nicht abdecken, siehe dazu nanoGPT von Andrej Karpathy.

# Schritt 1: Historische Texte sammeln und vorbereiten

Sammle .txt-Dateien von gemeinfreien B√ºchern, Dokumenten usw. aus deinem gew√§hlten Zeitraum (z. B. London 1800‚Äì1850)

Du kannst download_texts_improved.py verwenden, um B√ºcher herunterzuladen, falls du welche brauchst.

Bereinige die Textdateien mit einem Skript oder entferne manuell Header/Footers von Project Gutenberg, moderne Anmerkungen oder Dinge wie OCR-Fehler.

prepare_dataset.py sollte problemlos funktionieren.

# Schritt 2: Einen eigenen Tokenizer bauen

F√ºhre train_tokenizer.py oder train_tokenizer_hf.py auf den bereinigten Daten aus.
Das gibt dir vocab.json und merges.txt

Diese Dateien definieren Vokabular und Merge-Regeln f√ºr dein Modell

# Schritt 3: Trainiere dein Modell (nanoGPT)

Siehe [nanoGPT von Andrej Karpathy](https://github.com/karpathy/nanoGPT) f√ºr den Trainingsprozess.

Du kannst auch ein anderes LLM trainieren, aber ich habe nanoGPT verwendet

# FAQ

## Was ist selektives temporales Training?

Selektives temporales Training (STT) ist eine Machine-Learning-Methodik, bei der alle Trainingsdaten gezielt aus einem bestimmten historischen Zeitraum stammen. Ziel ist es, die Sprache und das Wissen dieser √Ñra ohne Einfluss moderner Konzepte zu modellieren. Das aktuelle Modell (v0.5) ist z.B. ausschlie√ülich auf Daten von 1800‚Äì1875 trainiert, nicht feinabgestimmt, sondern von Grund auf, was zu Ausgaben f√ºhrt, die den Sprachstil und Kontext jener Zeit widerspiegeln.

## Warum nicht einfach Fine-Tuning oder LoRA verwenden?

F√ºr dieses Projekt m√∂chte ich ein Sprachmodell schaffen, das nicht von modernen Einfl√ºssen gepr√§gt ist. Wenn ich z. B. GPT-2 feinabstimme, ist es schon vortrainiert und dieses Wissen verschwindet nicht. Wenn ich von Grund auf trainiere, wird das Modell die alte Sprache nicht nur imitieren, sondern sie tats√§chlich verwenden. Das Ziel ist, ein Modell zu schaffen, das ausschlie√ülich mit Wissen aus Londoner B√ºchern von 1800 bis 1850 logische Schl√ºsse ziehen kann.

## Welche Daten wurden f√ºr das Training verwendet?

Ich nutze B√ºcher, juristische Dokumente, Zeitungen und andere Schriften aus London von 1800‚Äì1850. Die verlinkte Liste enth√§lt etwa 200, aber f√ºr das erste Training habe ich nur 50 Dateien mit insgesamt ca. 187 MB verwendet. Eine Liste der Dokumente findest du hier:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## Wie gro√ü ist das Version-0-Modell?

Das Modell ist aktuell sehr klein, ich mache das nur zum Spa√ü und folge der strengen Trainingsregel ohne moderne Quellen. Es hat fast 16 Millionen Parameter, aber ich werde bald mehr alte Texte sammeln, um ein weiteres Modelltraining zu starten. Updates folgen.

## Trainingsspezifikationen?

GPU: Geforce rtx 4060
CPU: i5-13400F
Ram: 16GB DDR5.


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-29

---