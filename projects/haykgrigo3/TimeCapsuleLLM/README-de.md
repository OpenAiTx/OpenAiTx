
<div align="right">
  <details>
    <summary >üåê Sprache</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">Englisch</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="#" title="Coming soon">ÁπÅÈ´î‰∏≠Êñá (bald verf√ºgbar)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">Japanisch</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">Koreanisch</a>
        | <a href="#" title="Coming soon">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä (bald verf√ºgbar)</a> |
        | <a href="#" title="Coming soon">‡πÑ‡∏ó‡∏¢ (bald verf√ºgbar)</a> |
        | <a href="#" title="Coming soon">Franz√∂sisch (bald verf√ºgbar)</a>
        | <a href="#" title="Coming soon">Deutsch (bald verf√ºgbar)</a>
        | <a href="#" title="Coming soon">Spanisch (bald verf√ºgbar)</a>
        | <a href="#" title="Coming soon">Italienisch (bald verf√ºgbar)</a>
        | <a href="#" title="Coming soon">Russisch (bald verf√ºgbar)</a>
        | <a href="#" title="Coming soon">Portugiesisch (bald verf√ºgbar)</a>
        | <a href="#" title="Coming soon">Niederl√§ndisch (bald verf√ºgbar)</a>
        | <a href="#" title="Coming soon">Polnisch (bald verf√ºgbar)</a>
        | <a href="#" title="Coming soon">ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (bald verf√ºgbar)</a>
        | <a href="#" title="Coming soon">ŸÅÿßÿ±ÿ≥€å (bald verf√ºgbar)</a>
        | <a href="#" title="Coming soon">T√ºrkisch (bald verf√ºgbar)</a>
        | <a href="#" title="Coming soon">Vietnamesisch (bald verf√ºgbar)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (bald verf√ºgbar)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM

*Ein Sprachmodell, das **von Grund auf** ausschlie√ülich mit Daten aus bestimmten Orten und Zeitperioden trainiert wurde, um moderne Verzerrungen zu reduzieren und die Stimme, den Wortschatz und die Weltanschauung der jeweiligen Epoche zu verk√∂rpern.*

Stellen Sie sich vor, ein KI-Modell w√ºrde nicht nur historisch erscheinen, sondern tats√§chlich historisch sein.

v0 und v0.5 basieren auf [nanoGPT von Andrej Karpathy](https://github.com/karpathy/nanoGPT). Die zentralen Trainingsskripte und die Modellarchitektur stammen von ihm. 

v1 basiert auf [Phi 1.5 von Microsoft](https://huggingface.co/microsoft/phi-1_5)

##  Modellverhalten & Einschr√§nkungen

### **v0**  

Fr√ºhe Eingaben zeigen, dass das Modell mit Sprache und Verhalten aus dem 19. Jahrhundert antwortet. 
Beispiel: Eingabe: ‚ÄûWho art Henry?‚Äú und es antwortete: ‚ÄûI know that man, I have did not a black, the storm.‚Äú

![TimeLockLLM Beispielausgabe](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

- Keine Erw√§hnung moderner Konzepte  
- Meistens zeitgen√∂ssischer Wortschatz  
- S√§tze sind gr√∂√ütenteils unzusammenh√§ngend (zu erwarten bei ~187MB Trainingsdaten)

### **v0.5** 

Eine deutliche Verbesserung gegen√ºber v0.  
- Viktorianischer Schreibstil, korrekte Interpunktion, meist grammatikalisch korrekte S√§tze  
- Immer noch hohe Rate an faktischen Halluzinationen  
- OCR-Rauschen (‚ÄûDigitized by Google‚Äú) weiterhin in den Ausgaben enthalten

![TimeLockLLM Beispielausgabe](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

### **v1**

Erstes Modell, das ein tats√§chliches historisches Ereignis mit einer echten Person aus dem Datensatz verkn√ºpft.

Beispiel: Eingabe: ‚ÄûIt was the year of our Lord 1834‚Äú

Die Ausgabe: ‚ÄûIt was the year of our Lord 1834 and the streets of London were filled with protest and petition. The cause, as many re counted, was not bound in the way of private, but having taken up the same day in the day of Lord Palmerston, the public will receive a short statement of the difficulties under which the day of law has reached us. It is a matter of deep regret, that the present events in the history of the world are clear, and consequently will be'known. It is not true that the very men who first settled in the Gospel at Jerusalem should have so extensive and so interesting a record of the prosperity and prosperity‚Äú

Zun√§chst nahm ich an, dass zuf√§llig im selben Jahr ein Protest stattgefunden haben k√∂nnte, aber sehen Sie sich das an: ![1834protest](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)

### Warum das wichtig ist:

Dies ist das erste Beispiel, bei dem eines meiner Modelle ein Jahr sowohl mit einem realen historischen Ereignis als auch mit einer realen Person, die mit diesem Ereignis verbunden ist (Lord Palmerston), verkn√ºpft. Fr√ºhere Modelle (v0 und v0.5) konnten zwar Schreibstile des 19. Jahrhunderts nachahmen, halluzinierten jedoch immer Ereignisse, Personen und Fakten. Dies zeigt, dass das Modell beginnt, sich an Inhalte aus dem Datensatz zu erinnern.

## Kommende Pl√§ne


- Es gibt fast 175.000 Texte, die zwischen 1800-1875 in London ver√∂ffentlicht wurden und im Internet Archive verf√ºgbar sind
- Ich plane, das Korpus zu erweitern und weiter zu bereinigen, um bessere Schlussfolgerungsf√§higkeiten zu erm√∂glichen
- Erweiterung auf verschiedene Regionen und Zeitr√§ume f√ºr historischere Modelle


## Verwendung

Dieses Projekt konzentriert sich haupts√§chlich auf die Kuratierung historischer Daten, deren Vorbereitung f√ºr das Training und den Aufbau eines Tokenizers. Ich werde nicht den vollst√§ndigen LLM-Trainingsprozess abdecken, daf√ºr siehe nanoGPT von Andrej Karpathy.

### Schritt 1: Historische Texte sammeln und vorbereiten

- Sammle .txt-Dateien von gemeinfreien B√ºchern, Dokumenten usw. aus deinem gew√§hlten Zeitraum (z. B. London 1800-1850)
- Halte dich an das gew√§hlte Zeit-/Ortsfenster
- Bereinige die Textdateien mit einem Skript oder entferne manuell Kopf-/Fu√üzeilen von Project Gutenberg, moderne Anmerkungen oder Fehler wie OCR-Fehler.

### Schritt 2: Eigenen Tokenizer bauen

- Starte train_tokenizer.py oder train_tokenizer_hf.py auf den bereinigten Daten.
- Dies liefert dir vocab.json und merges.txt
- Diese Dateien definieren Vokabular und Zusammenf√ºhrungsregeln f√ºr dein Modell

### Schritt 3: Trainiere dein Modell

- Siehe [nanoGPT von Andrej Karpathy](https://github.com/karpathy/nanoGPT) f√ºr den Trainingsprozess oder die Dokumentation deiner gew√§hlten Architektur.

# FAQ

## Was ist Selective Temporal Training?

Selective Temporal Training (STT) ist eine Machine-Learning-Methodik, bei der alle Trainingsdaten gezielt so ausgew√§hlt werden, dass sie aus einem bestimmten historischen Zeitraum stammen. Dies dient dazu, die Sprache und das Wissen jener √Ñra zu modellieren, ohne von modernen Konzepten beeinflusst zu werden. Zum Beispiel: Das aktuelle Modell (v0.5) wurde ausschlie√ülich mit Daten von 1800-1875 trainiert, nicht feinabgestimmt, sondern von Grund auf, wodurch die Ausgabe den Sprachstil und historischen Kontext jener Zeit widerspiegelt.

## Warum nicht einfach Fine-Tuning oder LoRA verwenden?

F√ºr dieses Projekt versuche ich, ein Sprachmodell zu erstellen, das nicht von modernem Bias beeinflusst ist. Wenn ich etwa GPT-2 feinabstimme, ist es bereits vortrainiert und diese Information verschwindet nicht. Wenn ich von Grund auf trainiere, tut das Sprachmodell nicht so, als w√§re es alt ‚Äì es ist es einfach. Das Ziel dieses Projekts ist aktuell, etwas zu erschaffen, das ausschlie√ülich mit Wissen aus Londoner B√ºchern von 1800 bis 1875 schlussfolgern kann.

## Welche Daten wurden f√ºr das Training verwendet?

Ich verwende B√ºcher, juristische Dokumente, Zeitungen und andere Schriften aus London von 1800‚Äì1875. Die von mir verlinkte Liste (f√ºr v0) enth√§lt etwa 200, aber f√ºr das erste Training habe ich nur 50 Dateien mit insgesamt ca. 187 MB verwendet. Eine Liste der Dokumente findest du hier:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

Datensatzgr√∂√üen:
v0: ~187MB
v0.5: ~435MB
v1: ~6,25GB

## Wie gro√ü sind die Modelle?

V0: 16M Parameter

V0.5: 123M Parameter

V1: 700M Parameter

# Trainingsspezifikationen?

# V0/V0.5
GPU: Geforce RTX 4060
CPU: i5-13400F
Ram: 16GB DDR5.

# V1
GPU: A100 gemietet














---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-08-19

---