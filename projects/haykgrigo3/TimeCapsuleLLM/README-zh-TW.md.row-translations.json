[
  {
    "row": 1,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 2,
    "rowsha": "cWgam+tnnXudu7i74+ahMEGk/A9dQS+EwWLAIfi3dHk=",
    "originContent": "<div align=\"right\">",
    "translatedContent": "<div align=\"right\">"
  },
  {
    "row": 3,
    "rowsha": "orOcu5ARna/hb3RUkj6dBI8pHTM3WHeTvby17l5E0h0=",
    "originContent": "  <details>",
    "translatedContent": "  <details>"
  },
  {
    "row": 4,
    "rowsha": "TtgkLzblnvP0q9aAIVXt6s2LczXjy5k+QvHKcU0/5Ms=",
    "originContent": "    <summary >🌐 Language</summary>",
    "translatedContent": "    <summary >🌐 語言</summary>"
  },
  {
    "row": 5,
    "rowsha": "fZtk4rPTAJEEslnbhSVkHEcPlsctYSzAV7CDPL3rJmA=",
    "originContent": "    <div>",
    "translatedContent": "    <div>"
  },
  {
    "row": 6,
    "rowsha": "9KQxOeJSigvTmGWO+mtnl8kZY9zQfueoy8sk4lYm09Q=",
    "originContent": "      <div align=\"center\">",
    "translatedContent": "      <div align=\"center\">"
  },
  {
    "row": 7,
    "rowsha": "CeOhdpchZBoZSEUDtSE417JEcMBSZw18jeJuHJBKB2Y=",
    "originContent": "        <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en\">English</a>",
    "translatedContent": "        <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en\">English</a>"
  },
  {
    "row": 8,
    "rowsha": "ToO7MFa3QrNNljdQWIagsnOPxe8cXuuA2m5msIm+Kbs=",
    "originContent": "        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN\">简体中文</a>",
    "translatedContent": "        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN\">简体中文</a>"
  },
  {
    "row": 9,
    "rowsha": "MRATmWdRMRw0JU4u9h5pMb6GU17lQFgG9v/bpGLr9pM=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">繁體中文 (coming soon)</a> |",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">繁體中文 (即將推出)</a> |"
  },
  {
    "row": 10,
    "rowsha": "GY7LXxG3rk5eFh9itcqM0cTtmHybyjLTf1icB3jN31I=",
    "originContent": "        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja\">日本語</a>",
    "translatedContent": "        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja\">日本語</a>"
  },
  {
    "row": 11,
    "rowsha": "b5TwunGJh+gsAe7aQU3dkfobXF/nknCEta1msDa7XBU=",
    "originContent": "        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko\">한국어</a>",
    "translatedContent": "        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko\">한국어</a>"
  },
  {
    "row": 12,
    "rowsha": "1/HCgPsVh2ChqMY+k/VVxEWHPRRmWWCjy5nDRibi3mM=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">हिन्दी (coming soon)</a> |",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">हिन्दी (即將推出)</a> |"
  },
  {
    "row": 13,
    "rowsha": "3lfEHT+5HYFEvbE5cl+xujQPYjtVmzTifT37iqPTWII=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">ไทย (coming soon)</a> |",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">ไทย (即將推出)</a> |"
  },
  {
    "row": 14,
    "rowsha": "KmG3P0px2E3bt1lU/w3eGop+zeA1j8xL0k280Zd9m2s=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">Français (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Français (即將推出)</a>"
  },
  {
    "row": 15,
    "rowsha": "CSdHSEXgIs3M2Q/6zIIJ8NbKkZWhydhBqNus94qrPvg=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">Deutsch (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Deutsch (即將推出)</a>"
  },
  {
    "row": 16,
    "rowsha": "8wz7pDuXc3dk+ZcqZ1jmmh8zh6xN3Wb6qWbCjxAj7dA=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">Español (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Español (即將推出)</a>"
  },
  {
    "row": 17,
    "rowsha": "op/NqIZs7OjCSpNgpXk8RnqDnTegVPyWUQhuQxvTR7U=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">Italiano (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Italiano (即將推出)</a>"
  },
  {
    "row": 18,
    "rowsha": "tAvlfwut/Ad9q1huxc8EREZGv7vYHbrEujzUS8xoaQo=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">Русский (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Русский (即將推出)</a>"
  },
  {
    "row": 19,
    "rowsha": "WhhSpeeCUUAqJiVTS4Fvyc6A2c+24Jnj3MW7XLQuIcI=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">Português (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Português (即將推出)</a>"
  },
  {
    "row": 20,
    "rowsha": "0yPXPrWh+Vzc6FBE9iiciw5HwpOSmo05HNe36wfTWCI=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">Nederlands (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Nederlands (即將推出)</a>"
  },
  {
    "row": 21,
    "rowsha": "mdW6YUUXf5KzI4CwZxrE08ofaLonUOMnJpN3vPR7Y2A=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">Polski (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Polski (即將推出)</a>"
  },
  {
    "row": 22,
    "rowsha": "sw1AXxAGQNvn4eSG9enTWNkwKH0yr6LlVtXBH1j9z8s=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">العربية (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">العربية (即將推出)</a>"
  },
  {
    "row": 23,
    "rowsha": "I8dh9zmXisU0+CpddA55QQgvujH03J/dEnXgj5aFtQM=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">فارسی (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">فارسی (即將推出)</a>"
  },
  {
    "row": 24,
    "rowsha": "7VFv8o6de72ciJrbh3mctfrEgCJhNvuKGWJNOmCaPdM=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">Türkçe (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Türkçe (即將推出)</a>"
  },
  {
    "row": 25,
    "rowsha": "C+XRvFz/D3o9/JyPqwitsxtskFZleJC/oFUr4SEeiHA=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">Tiếng Việt (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Tiếng Việt (即將推出)</a>"
  },
  {
    "row": 26,
    "rowsha": "ntGI5B+n9x96pV3ZG5GG83nmocQbxTJjKY7VVwa6Rq8=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">Bahasa Indonesia (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Bahasa Indonesia (即將推出)</a>"
  },
  {
    "row": 27,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 28,
    "rowsha": "0OM5wNEm0TO56MEBvQzL7AUZM7/3OpgIeqRf2zFre3Q=",
    "originContent": "      </div>",
    "translatedContent": "      </div>"
  },
  {
    "row": 29,
    "rowsha": "fcjTfY+fs8YnY5slBs1sZvWPAqEQR7tzaBDO54skkGQ=",
    "originContent": "    </div>",
    "translatedContent": "    </div>"
  },
  {
    "row": 30,
    "rowsha": "+fQNH2ldI7UM/rqRscP3hUSWAmw1HvQ2wEKDN8JagT0=",
    "originContent": "  </details>",
    "translatedContent": "  </details>"
  },
  {
    "row": 31,
    "rowsha": "qsMmUbEPVnxGG5tPJV1vsfpoWbU2jYvZpRr5IKshzyM=",
    "originContent": "</div>",
    "translatedContent": "</div>"
  },
  {
    "row": 32,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 33,
    "rowsha": "VRGjp0FtfvQ89lbX/wJLis2ypCRtNJwe8ViIi29+Rko=",
    "originContent": "# TimeCapsule LLM",
    "translatedContent": "# TimeCapsule LLM"
  },
  {
    "row": 34,
    "rowsha": "XGlykErifWX9oIzV4ZXDc4AUnsuesz8LvpruG76e6uY=",
    "originContent": "An LLM trained only on data from certain time periods to reduce modern bias.",
    "translatedContent": "一個僅以特定時期資料訓練的 LLM，以降低現代偏見。"
  },
  {
    "row": 35,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 36,
    "rowsha": "06wDXO9Un3ot9kUKAGg7CaRsIVSkfS1d2m+EQ6HOFog=",
    "originContent": "Imagine if an AI model didnt just pretend to be historical but actually was.",
    "translatedContent": "想像一下，如果 AI 模型不只是「假裝」是歷史性的，而是真正如此。"
  },
  {
    "row": 37,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 38,
    "rowsha": "2773v/qIXSAsW4pN2HtYcVltfzG1vzgVbHgfjStBQIY=",
    "originContent": "Built on [nanoGPT by Andrej Karpathy](https://github.com/karpathy/nanoGPT) Core training scripts and model architecture are his work. ",
    "translatedContent": "基於 [Andrej Karpathy 的 nanoGPT](https://github.com/karpathy/nanoGPT) 開發，核心訓練腳本與模型架構皆源自其作品。"
  },
  {
    "row": 39,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 40,
    "rowsha": "wITJJBD/4abiy4E37iMdOcGmifkmz4dALLyk6AhA1kc=",
    "originContent": "# Project Goals ",
    "translatedContent": "# 專案目標"
  },
  {
    "row": 41,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 42,
    "rowsha": "LlW7r/H8NhftFgGAHce1f4KThGgsoT8aJ88/IsiLntc=",
    "originContent": "TimeCapsule LLM is an expirimental project that will only be trained on texts written during certain time periods. The goal is to simulate the worldview and language of specific historical eras.",
    "translatedContent": "TimeCapsule LLM 是一個實驗性專案，僅以特定時期的文本進行訓練。目標是模擬特定歷史時代的世界觀與語言。"
  },
  {
    "row": 43,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 44,
    "rowsha": "obYFMCTDj8qHZGo0BQtA2AlwA8JgNcjDK9WlMRI4eq8=",
    "originContent": "# Why fine tuning isn't enough ",
    "translatedContent": "# 為什麼微調還不夠"
  },
  {
    "row": 45,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 46,
    "rowsha": "yNEBOKV/RnG7CvDjiWhkXKK6vqbwki9QKC+Zs+8PzbM=",
    "originContent": "If you just fine tune a pre-trained model, your LLM is still gonna know modern concepts. Of course achieving zero modern bias is difficult but I want to get as close as possible to this. Getting no modern bias requires training a model from scratch.",
    "translatedContent": "如果你只是對預訓練模型進行微調，你的 LLM 仍然會知道現代的概念。當然要完全消除現代偏見非常困難，但我希望能盡可能接近這個目標。要做到完全沒有現代偏見，必須從零開始訓練模型。"
  },
  {
    "row": 47,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 48,
    "rowsha": "SdJkrN/DUD4+aOCh9lfDM3AAqMxlyukDfye/nzXzxN0=",
    "originContent": "# Expected outcomes ",
    "translatedContent": "# 預期成果"
  },
  {
    "row": 49,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 50,
    "rowsha": "bsSMnG6qSBf/pVtCQNGFlaKye8GxBKV660amPA/pINE=",
    "originContent": "Hopefully when finished, this model will not know modern concepts and will not be able to reason beyond what it's been trained on. It shouldnt recognize modern concepts/vocab and I hope it doesn't hallucinate modern knowledge.",
    "translatedContent": "希望完成後，這個模型將不會知道現代概念，也無法推理超出訓練內容的知識。它不應該辨識現代詞彙／概念，也希望不會幻想出現代知識。"
  },
  {
    "row": 51,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 52,
    "rowsha": "8EWRxPaogE2BaXxVJE1VFNAXNdS6KUYPLDFN8xlQ9LE=",
    "originContent": "# Progress Updates",
    "translatedContent": "# 進度更新"
  },
  {
    "row": 53,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 54,
    "rowsha": "oq91hnNV5WwmrEF0amya8kSN7gu21MN5nOcR2dPRBZ0=",
    "originContent": "## July 9th, 2025",
    "translatedContent": "## 2025 年 7 月 9 日"
  },
  {
    "row": 55,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 56,
    "rowsha": "yU3u8taDdAaD23tJB9+n/2wmry0GfF+KXqADT4YLuJ8=",
    "originContent": "I've set my time period for 1800-1850 and region: London ",
    "translatedContent": "我將時間範圍設定為 1800-1850 年，地區為倫敦"
  },
  {
    "row": 57,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 58,
    "rowsha": "uQe95shOfOi8NA0M2/CQCXlOjNsiSmGrt5dZbeP4ANs=",
    "originContent": "I've gathered a list of texts, books, documents ",
    "translatedContent": "已整理出一份文本、書籍、文件清單"
  },
  {
    "row": 59,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 60,
    "rowsha": "i9Kzka7MMa5yKjfdslauZFzKk+gAcnyILwscFoaepYs=",
    "originContent": "So far I've gotten 50 as txt files and will begin training NanoGPT soon ",
    "translatedContent": "目前已取得 50 份 txt 檔案，將很快開始訓練 NanoGPT"
  },
  {
    "row": 61,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 62,
    "rowsha": "Wov5RgnyTA0P0gtJKLL0GTcSf7t8WrgFcAzDufE5Xh4=",
    "originContent": "Will update this as long as progress is made",
    "translatedContent": "只要有進展會隨時更新"
  },
  {
    "row": 63,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 64,
    "rowsha": "FSOOW2G6pyPozg+3u76To6E4Pthd9lRoZE396fwY2I4=",
    "originContent": "## July 13th, 2025",
    "translatedContent": "## 2025 年 7 月 13 日"
  },
  {
    "row": 65,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 66,
    "rowsha": "GszU76q+4dgDkDO4uNZpx/9WhQTTCZlq4VYIt0ZdgD0=",
    "originContent": "Trained nanoGPT with 187MB of historial text data. ",
    "translatedContent": "已用 187MB 的歷史文本資料訓練 nanoGPT。"
  },
  {
    "row": 67,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 68,
    "rowsha": "pGPL3z/t2hDtXa67ubNgmRC/+b4O2Yp/0ff3R/9mraE=",
    "originContent": "## July 15th, 2025",
    "translatedContent": "## 2025 年 7 月 15 日"
  },
  {
    "row": 69,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 70,
    "rowsha": "+jwjqBw9Cr+lRnmxUzCQ0SnVBfXYeJDycuYf/p0JJgg=",
    "originContent": "I started downloading texts for the second training run. I'm getting everything from Internet Archive and I've expanded the time period to 1800-1875. To get a diverse range of texts, you can use subject and search filters for publication location, time period and subjects on Internet Archive. ",
    "translatedContent": "我已開始下載第二次訓練所需的文本。全部都來自 Internet Archive，並將時間範圍擴大到 1800-1875 年。為了獲得多樣化文本，你可以使用 Internet Archive 的主題與搜尋篩選功能，按出版地點、時期及主題篩選。"
  },
  {
    "row": 71,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 72,
    "rowsha": "XE9Xts6Q8wsZVZHg8uD/1ZXBQ/j2uFLsR9HwsiaqMds=",
    "originContent": "![Search Filters](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)",
    "translatedContent": "![搜尋篩選器](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)"
  },
  {
    "row": 73,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 74,
    "rowsha": "iREIjirFxs+ic0QmjQG1FQYKHC5brQaZ5JjPaEto+lU=",
    "originContent": "## July 16th, 2025",
    "translatedContent": "## 2025 年 7 月 16 日"
  },
  {
    "row": 75,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 76,
    "rowsha": "c1Ww8CUkqpg5TNm17QY7m130dQycuSFaAia2gfx/uLw=",
    "originContent": "I downloaded around 500 txt files from Internet Archive and after cleaning them (just deleting whitespaces, Gutenberg headers, etc) I have around 500MB of data. It's a tiny dataset but last time I trained off of 187MB so there should be at least some kind of noticable difference in the output after I train the second model. I'm hoping this model can at least produce more coherent sentences that kind of make sense. It's not a guarantee of course since this is still a tiny tiny dataset, but it's more than what I used last time. ",
    "translatedContent": "我從 Internet Archive 下載了約 500 個 txt 檔案，經過清理（只刪除空白、Gutenberg 標頭等）後，約有 500MB 資料。資料集很小，但上次只用 187MB 訓練，所以這次訓練後輸出應該會有明顯差異。我希望這次的模型至少能產生較通順且有意義的句子。當然這沒有保證，畢竟資料集還是很小，但總比上次多了。"
  },
  {
    "row": 77,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 78,
    "rowsha": "h/hyxvgOlOm5er9sn3CL2wmktMoq2q+qZi5Vi7upXGI=",
    "originContent": "This should be doable on my own hardware, it's good too because I can hopefully see some kind of improvements before I jump to a bigger dataset which would require me to rent a GPU. But don't worry I still plan on renting a GPU soon, but before I do that I wanna make sure my dataset is as curated and clean as possible. One of the issues I have is cleaning, a lot of these txt files have gibberish mixed in. The scripts I've used for cleaning do work but they're not 100% effective. ",
    "translatedContent": "這應該能在我自己的硬體上完成訓練，也好，因為希望在跳到更大資料集、需要租用 GPU 之前，先看到一些進步。但別擔心，我仍然計畫很快租用 GPU，只是在此之前要確保我的資料集夠精選、夠乾淨。其中一個問題就是清理，很多 txt 檔案裡有亂碼。用過的清理腳本雖然有效，但不是 100% 有效。"
  },
  {
    "row": 79,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 80,
    "rowsha": "8LYxYjHwrXnU59N+13Fd9m653Tgyom3yQQAWGIrSPSc=",
    "originContent": "I will train this dataset today and it should take around 4-5 hours. Once it's done and I test it, I will give updates. Thank you again to everyone whos checking out my project, I've even had some people even giving me links to OCR resources so Thank you! I hope more people try this out and expirement with they're own datasets. ",
    "translatedContent": "我今天會訓練這個資料集，預計需要 4-5 小時。完成並測試後會再更新。再次感謝所有關注我專案的人，甚至有人提供 OCR 資源連結，感謝！希望有更多人嘗試並實驗自己的資料集。"
  },
  {
    "row": 81,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 82,
    "rowsha": "13xzxThQO3iEGB5PU51V1kYgK85//yEFbCkwtu07a2U=",
    "originContent": "## July 28th, 2025 ",
    "translatedContent": "## 2025 年 7 月 28 日"
  },
  {
    "row": 83,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 84,
    "rowsha": "R6HmXRpR+7izGCE6kI9wyELs4kYvAtLMWO75nOXAf1M=",
    "originContent": "I've gone ahead and uploaded v0.5 to Hugging Face, [Check it out](https://huggingface.co/haykgrigorian/TimeCapsuleLLM) if youd like. You can now download my repo and run it locally. Unfortunately nanoGPT doesn't work natively with HuggingFace, so you'll have to download and run the model locally. ",
    "translatedContent": "我已將 v0.5 上傳至 Hugging Face，[點此查看](https://huggingface.co/haykgrigorian/TimeCapsuleLLM)。你現在可以下載我的 repo 並在本地運行。不幸的是 nanoGPT 無法直接與 HuggingFace 整合，所以必須下載模型到本地執行。"
  },
  {
    "row": 85,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 86,
    "rowsha": "3/uUOOHuCoxdKkQ6c+1J96AfmOsQ59aZxo/FYGK9OlI=",
    "originContent": "Also I will begin curating data for my next training run, I believe I'll need maybe 5-10x more data to achieve reasoning capabilities. ",
    "translatedContent": "同時我會開始整理下一輪訓練所需的資料，我認為需要 5-10 倍的資料量才能讓模型有推理能力。"
  },
  {
    "row": 87,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 88,
    "rowsha": "mNjt3ebwxfwyPM2/AK7E/GDTFM6i95HG8GZ/8TZs9EA=",
    "originContent": "### Training Update ",
    "translatedContent": "### 訓練進度更新"
  },
  {
    "row": 89,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 90,
    "rowsha": "RxsWSGTgM12Md9v3+GBLg3PyoUxQ9kl1AglpanRmZqE=",
    "originContent": "I started training on a 435MB (108 M tokens) corpus, it's going pretty smooth right now. Train loss dropped from 10.9 to 4.9 in the first 2800 iterations. I expect it'll take around 8 or 9 hours to complete. I'll post another update once it's done.",
    "translatedContent": "我已開始用 435MB（1.08 億詞元）的語料庫訓練，目前進展順利。訓練損失從 10.9 降到 4.9，僅 2800 次迭代。我預計總共要花 8~9 小時才能完成。等完成會再發布更新。"
  },
  {
    "row": 91,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 92,
    "rowsha": "CLW3lPsO6v+tT6iir338rG5+IYXM+JqRoag5w7K8exE=",
    "originContent": "## July 17th, 2025 2:13AM",
    "translatedContent": "## 2025 年 7 月 17 日 2:13AM"
  },
  {
    "row": 93,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 94,
    "rowsha": "Q0uM34dBNqytALNUZSPxoBQZT3LxqlwyioEi3nTshXQ=",
    "originContent": "The training is done for the second model, it took my 4060 around 8 hours and 40 minutes (3,900 iters/hr) for 33,000 iters (5 epochs). Final train loss was 3.73. The outputs were suprisingly good it genuinely generates coherent 19th century style sentences now. ",
    "translatedContent": "第二個模型訓練完成，我的 4060 共花了約 8 小時 40 分鐘（每小時 3,900 次迭代），共 33,000 次迭代（5 輪）。最終訓練損失為 3.73。結果讓我驚訝，現在真的能產生通順的 19 世紀風格句子。"
  },
  {
    "row": 95,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 96,
    "rowsha": "YbYVsndAe75CgxPU/J34HsXOaqcQlIjTCm06o4eVQIg=",
    "originContent": "# V0 Model Behavior & Limitations ",
    "translatedContent": "# V0 模型行為與限制"
  },
  {
    "row": 97,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 98,
    "rowsha": "OCh97kyOITXqFpKyZsol6voS+nrzc9n9flpDpNyf35w=",
    "originContent": "Early prompts show the model responding with 1800's language and behavior. For example, I prompted it with \"Who art Henry?\" and it replied \"I know that man, I have did not a black, the storm.\" and yeah that sentence makes no sense but the LLM is recognizing I'm asking about a person. ",
    "translatedContent": "早期提示顯示模型會用 1800 年代的語言與行為回應。例如，我以 \"Who art Henry?\" 提示，它回覆 \"I know that man, I have did not a black, the storm.\"——雖然語句不通，但 LLM 已能辨識我在詢問某個人。"
  },
  {
    "row": 99,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 100,
    "rowsha": "yKIR0teTc66wVDG+jdIyNmAzItXb2JH2ld3D7tm4qnM=",
    "originContent": "![TimeLockLLM Sample Output](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)",
    "translatedContent": "![TimeLockLLM 範例輸出](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)"
  },
  {
    "row": 101,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 102,
    "rowsha": "/GR84OQ/Xp3d+Yv3/vVIIQFyl8ExKQLgjT1Go5rPplE=",
    "originContent": "There is no mention of modern concetps, outputs contain mostly words and phrasing from the 1800's.",
    "translatedContent": "沒有提及現代概念，輸出內容大多為十九世紀的詞彙與用語。"
  },
  {
    "row": 103,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 104,
    "rowsha": "aojQk3KjkX8shkLnJSZvUZBo0vk4tKDT9aysMfY9yK4=",
    "originContent": "It still needs alot of work, training off of 187MB will not give you a model that produces text with complex reasoning. ",
    "translatedContent": "目前還需要大量改進，僅以187MB的資料進行訓練，無法產生具有複雜推理能力的模型。"
  },
  {
    "row": 105,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 106,
    "rowsha": "SzQ3fs08W+JGBPGw6+aEzLsm/vGfQNDJ6YZnfFE0F4I=",
    "originContent": "Right now it produces sentences that lack full sentence structure and overall just make no sense but this is normal for the training size. ",
    "translatedContent": "現在產出的句子缺乏完整句子結構，整體也不合邏輯，但這對於這樣的訓練規模來說是正常的。"
  },
  {
    "row": 107,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 108,
    "rowsha": "eeaXlcgOJhwk4liDhaA6/6S4URmAdV8vj9hlqGr3LFw=",
    "originContent": "# V0.5 Model Behavior & Limitations",
    "translatedContent": "# V0.5 模型行為與限制"
  },
  {
    "row": 109,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 110,
    "rowsha": "tII9OX/M7KpPoQKPIphmQBWtsSVucUevvTnzVsTXcaI=",
    "originContent": "This is a nice improvement compared to the last model. The writing style and vocab is Victorian and almost every sentence is grammatically correct with proper punctuation. And again this is trained from scratch so it sticks to 1800's subjects. ",
    "translatedContent": "這比上一個模型有明顯改進。寫作風格與詞彙屬於維多利亞時代，幾乎每個句子都文法正確且標點合宜。由於完全從零訓練，因此內容緊扣十九世紀主題。"
  },
  {
    "row": 111,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 112,
    "rowsha": "8DhXpgpVtg05XdyplRHf49EFOQNCJVzXA9RpmJQ+y9U=",
    "originContent": "![TimeLockLLM Sample Output](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)",
    "translatedContent": "![TimeLockLLM Sample Output](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)"
  },
  {
    "row": 113,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 114,
    "rowsha": "YgdLunUOAWWHsq+eAfyWcDz1fCc0rCcobnRpEXHoK94=",
    "originContent": "There are a lot of factual hallucinations. A lot (like 100%) of the details (dates, events, historical figures)  are made up. Also the sentences don't really have connections to each other, sometimes maybe 2 sentences will relate to each other but beyond that they dont. Another issue is sometimes a stray “Digitized by Google” footer shows up, so the next time I train I really have to make sure the texts are cleaned well. Overall I'm very happy with the results, it's nowhere near an LLM yet but definitely a sentence generator. ",
    "translatedContent": "存在大量事實性幻覺。大多數細節（日期、事件、歷史人物）幾乎都是虛構的。另外句子之間沒有真正的連貫性，頂多兩句會有關聯，超過就沒有了。另一個問題是偶爾會有“Digitized by Google”的頁腳出現，下次訓練時必須確實清理文本。總體來說我對結果很滿意，雖然離LLM還很遠，但確實可以生成句子。"
  },
  {
    "row": 115,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 116,
    "rowsha": "V44Ne2sN8v7ZVpRZ5vo7B2aUZFjGppYthg0fjsQAqd0=",
    "originContent": "I'm learning a lot and will start figuring out what I need to do better in the coming weeks. I will upload files soon! ",
    "translatedContent": "我學到了很多，接下來幾週會開始思考該如何改進。我很快會上傳檔案！"
  },
  {
    "row": 117,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 118,
    "rowsha": "iMvQyl++GAiLpFMT+58v3e9/hb7zXnIJauchG5p986Y=",
    "originContent": "# Upcoming Plans ",
    "translatedContent": "# 未來計畫"
  },
  {
    "row": 119,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 120,
    "rowsha": "OIt/QAbIp2G7kt87dbAIf6Ec0U3HnwjOW6LYRmdf68I=",
    "originContent": "(Completed) I'm going to start work on version 0.5 , instead of training using 50 books, I'll train using ideally 500-600. Right now I'm training nanoGPT using books from 1800-1850 and specifically from London. There is some challeneges like making sure the books I find are not updated or have modern interpretations but untouched books published withtin my chosen time period.",
    "translatedContent": "（已完成）我將開始製作0.5版，這次不再只用50本書訓練，而是目標500-600本。目前我正使用來自1800-1850年、特別是倫敦的書籍來訓練nanoGPT。挑戰包括確保找到的書沒有被修改或有現代詮釋，而是要選擇在目標時期內出版且未經更動的原始書籍。"
  },
  {
    "row": 121,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 122,
    "rowsha": "13sW7eekt8pSDdOxr/oKukwKEOVmw/M8xiAJC1VFXZc=",
    "originContent": "I want to train a new model (v1) with a much larger corpus, maybe 5-10x larger than the one I used for v0.5. My goal is to see if I can get reasoning abilities to emerge from Selective Temporal Training alone, this will be a more difficult task and I'm not even entirely sure if it's possible due to the fact that there is historical data limitations. In the upcoming weeks I will try to curate enough data for a 5-10GB corpus. I believe If I can get clean clean high quality data and rent a GPU, there will be progress.",
    "translatedContent": "我希望訓練一個（v1）更大的模型，語料庫規模可能會是v0.5的5-10倍。我的目標是觀察是否僅靠選擇性時期訓練能產生推理能力，這會更具挑戰性，甚至不確定可不可行，畢竟歷史資料有限。接下來幾週我會盡量整理出5-10GB的語料庫。我相信若能取得乾淨高品質的資料並租用GPU，應該會有進展。"
  },
  {
    "row": 123,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 124,
    "rowsha": "sO+voevLpUtEbqum6Gntntle+nPVa66c5GATAMLrgf0=",
    "originContent": "# How to Use This Project ",
    "translatedContent": "# 如何使用本專案"
  },
  {
    "row": 125,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 126,
    "rowsha": "XVoXr9uzZwN09vboETojEQJe057RBzcMUjXmQRCB/jo=",
    "originContent": "This project focuses mostly on curating historical data, preparing it for training and building a tokenizer. I am not going to cover the full LLM training process, for that refer to nanoGPT by Andrej Karpathy.",
    "translatedContent": "本專案主要聚焦於整理歷史資料、準備訓練用數據及建立分詞器。不會涵蓋完整的LLM訓練流程，相關細節請參考Andrej Karpathy的nanoGPT。"
  },
  {
    "row": 127,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 128,
    "rowsha": "kDK7XtqFkTiZvD804yYG4VEojvLrRbdEhmHEBzDAQz4=",
    "originContent": "# Step 1: Gather and Prepare Historical Texts ",
    "translatedContent": "# 步驟一：收集與準備歷史文本"
  },
  {
    "row": 129,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 130,
    "rowsha": "oRoOYaG+mx3SqTnhGcOKeH7W3W4wSRQmhZS4jPGMH8s=",
    "originContent": "Collect .txt files of public domain books, documents, etc from your chosen time period (e.g., London 1800-1850)",
    "translatedContent": "蒐集你所選時期的公有領域書籍、文件等.txt檔（如倫敦1800-1850）"
  },
  {
    "row": 131,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 132,
    "rowsha": "EBDhZvTogrL4hqRuH095O7/tXXoy2OEskLQlang/pOA=",
    "originContent": "You can use download_texts_improved.py to download books for you if you need to.",
    "translatedContent": "如需自動下載書籍，可使用 download_texts_improved.py。"
  },
  {
    "row": 133,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 134,
    "rowsha": "q5OO8x+9kIbzaQRWimI/Vo9ZowzVBtCX+TodObLsQoY=",
    "originContent": "Clean the text files using a script or manually remove headers/footer from Project Gutenberg, Modern annotations or things like OCR errors.",
    "translatedContent": "使用腳本或手動清理文本檔，去除Gutenberg專案的標頭/頁腳、現代註釋或OCR錯誤等內容。"
  },
  {
    "row": 135,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 136,
    "rowsha": "N4wsIjC0LRlClodmfCtYX3qswttJnk0psU28/mlCRTw=",
    "originContent": "prepare_dataset.py should work fine.",
    "translatedContent": "prepare_dataset.py 應該可以正常運作。"
  },
  {
    "row": 137,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 138,
    "rowsha": "jDM2lr7pP+MT0pt+L0cd5nBXI83IPoT27NzIgplt7R8=",
    "originContent": "# Step 2: Build a Custom Tokenizer",
    "translatedContent": "# 步驟二：建立自訂分詞器"
  },
  {
    "row": 139,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 140,
    "rowsha": "+90EorsgO/X2bFK2pYJw+vZIpjIbuMm4QR6W/xfj8C8=",
    "originContent": "Run train_tokenizer.py or train_tokenizer_hf.py on the cleaned data.",
    "translatedContent": "在清理過的資料上運行 train_tokenizer.py 或 train_tokenizer_hf.py。"
  },
  {
    "row": 141,
    "rowsha": "tkP3Eg1rWphTQMNhN2yYg/1+AA1IdcXbGT96aRMpnwc=",
    "originContent": "This will give you vocab.json and merges.txt",
    "translatedContent": "這將生成 vocab.json 與 merges.txt"
  },
  {
    "row": 142,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 143,
    "rowsha": "/wqxgtOu72+x3a2xi7q23jDkx+WQv2SHrJddzpvm1Ys=",
    "originContent": "Thes files define vocab and merge rules for your model",
    "translatedContent": "這些檔案定義了模型的詞彙與合併規則。"
  },
  {
    "row": 144,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 145,
    "rowsha": "vKPAEsPxc9uYtzjTX9/rNADSDnxkKwYcYpX/aiAp8Hc=",
    "originContent": "# Step 3: Train Your Model (nanoGPT) ",
    "translatedContent": "# 步驟三：訓練你的模型（nanoGPT）"
  },
  {
    "row": 146,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 147,
    "rowsha": "tCDY5iXt+Z7YYeTPouMSYDX5uuFnGROxZMvHyTOIblY=",
    "originContent": "Refer to [nanoGPT by Andrej Karpathy](https://github.com/karpathy/nanoGPT) for the training process.",
    "translatedContent": "請參考 [Andrej Karpathy 的 nanoGPT](https://github.com/karpathy/nanoGPT) 的訓練流程。"
  },
  {
    "row": 148,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 149,
    "rowsha": "bLpr9snECDSJ5ejrqDWYNT8VXSupCQKQxyLxvAmQ2Kc=",
    "originContent": "You can train a different LLM if you want, but I used nanoGPT ",
    "translatedContent": "你也可以訓練其他LLM，但我這裡使用的是 nanoGPT。"
  },
  {
    "row": 150,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 151,
    "rowsha": "OoCxyGfPN5TmdzAkaPphtPx303MJJ7vpfWbKrufGH5g=",
    "originContent": "# FAQ",
    "translatedContent": "# 常見問答"
  },
  {
    "row": 152,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 153,
    "rowsha": "+5dDgPw4ILEotxso4tjjjz1cxwUei16yNQPDUKbgxoo=",
    "originContent": "## What is Selective Temporal Training ?",
    "translatedContent": "## 什麼是 Selective Temporal Training ？"
  },
  {
    "row": 154,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 155,
    "rowsha": "hooEARKH4r/sDPh7JUtZAZ6TYMvBkTLZIcfw3g83xos=",
    "originContent": "Selective Temporal Training (STT) is a machine learning methodology where all training data is specifically curated to fall within a specific historical time period. It's done in order to model the language and knowledge of that era without influence from modern concepts. For example, the current model I have now (v0.5) is trained on data exclusively from 1800-1875, it's not fine tuned but trained from scratch resulting in output that reflects the linguistic style and historical context of that time period.",
    "translatedContent": "Selective Temporal Training（STT，選擇性時期訓練）是一種機器學習方法，所有訓練資料都經過特別挑選，僅限於特定歷史時期。這樣做是為了建構該時代的語言和知識模型，不受現代概念影響。例如，目前的模型（v0.5）只用1800-1875年的資料訓練，非微調，而是從零開始訓練，因此輸出能反映當時的語言風格與歷史脈絡。"
  },
  {
    "row": 156,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 157,
    "rowsha": "dVMKQ2mPI1Spc6x6r/jNG0PIR5YKpalU4MXx9JmKp/I=",
    "originContent": "## Why not just use fine-tuning or LoRA?",
    "translatedContent": "## 為什麼不用微調或LoRA？"
  },
  {
    "row": 158,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 159,
    "rowsha": "oNvWlJHtQSyq1TwlqJyGtMzk4Z4mBIn8AW2SudzvUYs=",
    "originContent": "For this project I'm trying to create a language model that is unclouded from modern bias. If I fine-tune something like GPT-2, it's already pre-trained and that information won't go away. If I train from scratch the language model won't pretend to be old, it just will be. The Goal for this project right now is to create something can reason exclusively using knowledge from London books published between 1800 and 1850.",
    "translatedContent": "這個專案希望打造一個不受現代偏見影響的語言模型。如果用GPT-2之類的模型做微調，它原本的訓練資料依然存在，無法去除。如果從零開始訓練，語言模型就不會“假裝”是舊的，而是真的只會用舊知識。目前的目標是做出一個能只用1800到1850年倫敦書籍知識進行推理的模型。"
  },
  {
    "row": 160,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 161,
    "rowsha": "ByP4WlNmMoG6WIiLJNd6b080/DSciCgWmj9aYSJjAF0=",
    "originContent": "## What kind of data did you use for training?",
    "translatedContent": "## 你用什麼資料來訓練？"
  },
  {
    "row": 162,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 163,
    "rowsha": "Kj6EF7wZdUrAFg4ErGmJuh9Q5Xujmb+tunpfssPKXkA=",
    "originContent": "I'm using books, legal documents, newspapers, and other writings from 1800–1850 London. The list I linked has like 200 but for the first training I just used 50 files about ~187 MB. You can view a list of the documents:",
    "translatedContent": "我使用了1800-1850年倫敦的書籍、法律文件、報紙和其他著作。我連結的清單有約200本，但第一次訓練只用了約50個檔案，總計約187MB。你可以在這裡查看文件清單："
  },
  {
    "row": 164,
    "rowsha": "0mxyGiLJxzp9JPCg1oA+nbIwAKJbEC4ei9kSV3Gp84Y=",
    "originContent": "https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt",
    "translatedContent": "https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt"
  },
  {
    "row": 165,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 166,
    "rowsha": "RiLgksbH2sYLZRyKsFhVyfjC6nNxLsxWsc7XnFS061A=",
    "originContent": "## How large is the Version 0 model ?",
    "translatedContent": "## 0版模型有多大？"
  },
  {
    "row": 167,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 168,
    "rowsha": "55Ce1mBEzOreC728R5HRfuMIC/nZRg3Zcr87GVkjVVY=",
    "originContent": "This model is very small right now, I'm just doing this for fun and following a strict training rule of no modern sources. It has almost 16 million parameters but I'm gonna start gathering more old texts to begin another model training. Will give updates as I go.",
    "translatedContent": "這個模型目前非常小，純粹是娛樂性質並嚴格遵守不使用現代資料的原則。參數接近1600萬個。我會開始收集更多舊文本，進行新一輪模型訓練。進度會隨時更新。"
  },
  {
    "row": 169,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 170,
    "rowsha": "A40eQ7ZJiqr+bs9cl0Cb4QKKuS9z7/PA1ZaGn1TSehI=",
    "originContent": "## Training Specs ? ",
    "translatedContent": "## 訓練規格？"
  },
  {
    "row": 171,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 172,
    "rowsha": "EH8H1HW/C4Tb7LfJgVUnVGsk4pF9l40Rlev8tAkKhjI=",
    "originContent": "GPU: Geforce rtx 4060",
    "translatedContent": "GPU：Geforce rtx 4060"
  },
  {
    "row": 173,
    "rowsha": "vo3FdN37kY6VUB7PruRKfBPJDgsVJyBHIUCn/g8mt68=",
    "originContent": "CPU: i5-13400F ",
    "translatedContent": "CPU：i5-13400F"
  },
  {
    "row": 174,
    "rowsha": "W8fXPiQKUkoNso0PPfTvjYMy0IYo85j+gNXmB0aERO4=",
    "originContent": "Ram: 16GB DDR5.",
    "translatedContent": "記憶體：16GB DDR5。"
  },
  {
    "row": 175,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  }
]