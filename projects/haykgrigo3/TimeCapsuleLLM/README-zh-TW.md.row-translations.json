[
  {
    "row": 1,
    "rowsha": "qJUbd5u/iOZOtuM+fYFPZOjgtKDl8iwUfAi1n3MVisI=",
    "originContent": "v1 built on [Phi 1.5 by Microsoft](https://huggingface.co/microsoft/phi-1_5)",
    "translatedContent": "v1 建立於 [微軟的 Phi 1.5](https://huggingface.co/microsoft/phi-1_5)"
  },
  {
    "row": 2,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 3,
    "rowsha": "0kza+J1J1rtVrJmQnk2qdxJsC7lvjDN6mSSj8nyd31Y=",
    "originContent": "[Hugging Face Link](https://huggingface.co/haykgrigorian/TimeCapsuleLLM)",
    "translatedContent": "[Hugging Face 連結](https://huggingface.co/haykgrigorian/TimeCapsuleLLM)"
  },
  {
    "row": 4,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 5,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "##  模型行為與限制"
  },
  {
    "row": 6,
    "rowsha": "S8kboQ4LnLUhavjolKrTgNgwVOMJrwpnaY1CWGr6epQ=",
    "originContent": "##  Model Behavior & Limitations",
    "translatedContent": ""
  },
  {
    "row": 7,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "### **v0**  "
  },
  {
    "row": 8,
    "rowsha": "1nqZ1MR/Rxg5du0+tZch+QesCAT+5qMlbhJftBOVqDI=",
    "originContent": "### **v0**  ",
    "translatedContent": ""
  },
  {
    "row": 9,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "早期提示讓模型以 1800 年代的語言和行為回應。"
  },
  {
    "row": 10,
    "rowsha": "hlawgmI5oFABN37xYPae7PebQq1TP5c6DMT8b78524g=",
    "originContent": "Early prompts show the model responding with 1800's language and behavior. ",
    "translatedContent": "範例：提示：\"Who art Henry?\"，它回覆 \"I know that man, I have did not a black, the storm.\""
  },
  {
    "row": 11,
    "rowsha": "5eCkvYiGa3VxONgXmVAvuTajgTbveIO0PKy313G2fos=",
    "originContent": "Example: Prompt: \"Who art Henry?\" and it replied \"I know that man, I have did not a black, the storm.\" ",
    "translatedContent": ""
  },
  {
    "row": 12,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "![TimeLockLLM 範例輸出](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)"
  },
  {
    "row": 13,
    "rowsha": "yKIR0teTc66wVDG+jdIyNmAzItXb2JH2ld3D7tm4qnM=",
    "originContent": "![TimeLockLLM Sample Output](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)",
    "translatedContent": ""
  },
  {
    "row": 14,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "- 未提及現代概念  "
  },
  {
    "row": 15,
    "rowsha": "iCVdbo9EUfepGTS1z1bOwgFn1wDAa9zhHoBLJxEw1w8=",
    "originContent": "- No mention of modern concepts  ",
    "translatedContent": "- 大多使用符合時代的詞彙  "
  },
  {
    "row": 16,
    "rowsha": "khHeh5VmbIuCnyoNCTDAXLsQOpnqbamm8vyOjvWVmLE=",
    "originContent": "- Mostly era-accurate vocabulary  ",
    "translatedContent": "- 句子大多不通順（約 187MB 訓練資料時預期現象）"
  },
  {
    "row": 17,
    "rowsha": "UP5LBmH8evd8yPger3Fjvs1PXYiVAALQC0JWmcmlN08=",
    "originContent": "- Sentences are mostly incoherent (expected for ~187MB training data)",
    "translatedContent": ""
  },
  {
    "row": 18,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "### **v0.5** "
  },
  {
    "row": 19,
    "rowsha": "5p60D9yLuB40iACJ6apiWJq8lt1AqZAyvFkG70v4xco=",
    "originContent": "### **v0.5** ",
    "translatedContent": ""
  },
  {
    "row": 20,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "相較於 v0 有顯著提升。  "
  },
  {
    "row": 21,
    "rowsha": "IPr+C8pVE68OvudYYmIOcbEumcoTmjBiq3HmXPghcvU=",
    "originContent": "A significant improvement over v0.  ",
    "translatedContent": "- 維多利亞時代寫作風格，標點正確，句子多數語法正確  "
  },
  {
    "row": 22,
    "rowsha": "VEokY2Qi826s5lAKKkVoOd5hju8hvOxG21P/L2LPz9k=",
    "originContent": "- Victorian writing style, proper punctuation, mostly grammatical sentences  ",
    "translatedContent": "- 事實幻覺率仍高  "
  },
  {
    "row": 23,
    "rowsha": "hkitRVnW6jxtd+R18O5Olb5eqNIJGCKv4Yt2I+yLrWE=",
    "originContent": "- Still high factual hallucination rate  ",
    "translatedContent": "- 輸出中仍有 OCR 噪音（如 \"Digitized by Google\"）"
  },
  {
    "row": 24,
    "rowsha": "qlWBqyQVUBsL120iExHvs0PDrN2m0WeMIyRHeHJ1qhM=",
    "originContent": "- OCR noise (“Digitized by Google”) still present in outputs",
    "translatedContent": ""
  },
  {
    "row": 25,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "![TimeLockLLM 範例輸出](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)"
  },
  {
    "row": 26,
    "rowsha": "8DhXpgpVtg05XdyplRHf49EFOQNCJVzXA9RpmJQ+y9U=",
    "originContent": "![TimeLockLLM Sample Output](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)",
    "translatedContent": ""
  },
  {
    "row": 27,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "### **v1**"
  },
  {
    "row": 28,
    "rowsha": "mQ9MARMITxOHtqSNIP76nXvTlEM8Ope3qbQ/YG1h6/8=",
    "originContent": "### **v1**",
    "translatedContent": ""
  },
  {
    "row": 29,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "首個能回想並連結真實歷史事件與資料集真實人物的模型。"
  },
  {
    "row": 30,
    "rowsha": "sAlnUjWbUBk6czBOHW/IgxyE98WYhX0LrIYmZBr8P2Y=",
    "originContent": "First model to recall and connect a real historical event with an actual figure from the dataset.",
    "translatedContent": ""
  },
  {
    "row": 31,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "範例：提示：\"It was the year of our Lord 1834\""
  },
  {
    "row": 32,
    "rowsha": "Cd0DPnkqItUnSk3fHx9ckSSN6MNcZnsEX3S44GZyxTw=",
    "originContent": "Example: Prompt: \"It was the year of our Lord 1834\" ",
    "translatedContent": ""
  },
  {
    "row": 33,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "輸出：\"It was the year of our Lord 1834 and the streets of London were filled with protest and petition. The cause, as many re counted, was not bound in the way of private, but having taken up the same day in the day of Lord Palmerston, the public will receive a short statement of the difficulties under which the day of law has reached us. It is a matter of deep regret, that the present events in the history of the world are clear, and consequently will be'known. It is not true that the very men who first settled in the Gospel at Jerusalem should have so extensive and so interesting a record of the prosperity and prosperity\""
  },
  {
    "row": 34,
    "rowsha": "zvm/1tcaMFjcQmuN7y0hczAbbgAwaKRtjBpah5/+kb8=",
    "originContent": "The output: \"It was the year of our Lord 1834 and the streets of London were filled with protest and petition. The cause, as many re counted, was not bound in the way of private, but having taken up the same day in the day of Lord Palmerston, the public will receive a short statement of the difficulties under which the day of law has reached us. It is a matter of deep regret, that the present events in the history of the world are clear, and consequently will be'known. It is not true that the very men who first settled in the Gospel at Jerusalem should have so extensive and so interesting a record of the prosperity and prosperity\" ",
    "translatedContent": ""
  },
  {
    "row": 35,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "起初我以為抗議活動可能只是巧合地發生在同一年，但請看這裡：![1834protest](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)"
  },
  {
    "row": 36,
    "rowsha": "WfSfj2g4CByfYQWI59s4C3EwGLZuWfz1t4w49a49e+8=",
    "originContent": "At first I assumed that a protest might have coincidentally taken place the same year but take a look at this: ![1834protest](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)",
    "translatedContent": ""
  },
  {
    "row": 37,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "### 為何這很重要："
  },
  {
    "row": 38,
    "rowsha": "qlFf3SlVZpptxE7ExYx9wjs1lBnO/bsCjUl1Q4Sw2ds=",
    "originContent": "### Why this matters:",
    "translatedContent": ""
  },
  {
    "row": 39,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "這是我模型首次將年份與真實歷史事件及與該事件有關的真實人物（Palmerston 勳爵）相連結的例子。早期模型（v0 和 v0.5）雖能模仿十九世紀的寫作風格，但總是捏造事件、人物和事實。這顯示模型開始能記住資料集中的內容。"
  },
  {
    "row": 40,
    "rowsha": "WdZfxluSvMeRhCl5u8NEkZI+F2o+eDrUIZyWPgHGv6w=",
    "originContent": "This is the first example of one of my models connecting a year to both a real historical event and a real person tied to that event (Lord Palmerston). Earlier models (v0 and v0.5) could mimic writing styles of the 19th century but would always hallucinate events, people and facts. This shows the model is beggining to remember things from the dataset ",
    "translatedContent": ""
  },
  {
    "row": 41,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 42,
    "rowsha": "xt81v+AyDswOM8KVPmSYl1lfW4vISpujtQjRWm0mKbA=",
    "originContent": "## Upcoming Plans ",
    "translatedContent": "## 未來計畫"
  },
  {
    "row": 43,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 44,
    "rowsha": "CulYLHqIw5Hn1cLDvlekSWBe1moOd8QSIirjeM77tlk=",
    "originContent": "- There are nearly 175,000 texts published in London from 1800-1875 on Internet Archive ",
    "translatedContent": "- 在 Internet Archive 上，倫敦於1800-1875年間出版的文本有近175,000份"
  },
  {
    "row": 45,
    "rowsha": "VWINtuHshK3Fs4MeBmv24/pZC0iMQ70HtMIowLnrsAI=",
    "originContent": "- I plan on expanding the corpus and cleaning it more for better reasoning abilities",
    "translatedContent": "- 我計畫擴展語料庫並進一步清理，以提升推理能力"
  },
  {
    "row": 46,
    "rowsha": "3b7jQ1J58CCfC/VV9FWJY23SOlvhzx/vIVth3ZBIPog=",
    "originContent": "- Expanding to different regions and time periods for more historical models",
    "translatedContent": "- 擴展到不同地區和時期，以建立更多的歷史模型"
  },
  {
    "row": 47,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 48,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "## 如何使用"
  },
  {
    "row": 49,
    "rowsha": "yJlGQKI3p2nVYC1uqNa6eKPSIzUW38uRiGu0PspqvUU=",
    "originContent": "## How to Use",
    "translatedContent": ""
  },
  {
    "row": 50,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "本專案主要聚焦於整理歷史資料、準備訓練資料並建立分詞器。不會涵蓋完整的 LLM 訓練流程，相關內容請參考 Andrej Karpathy 的 nanoGPT。"
  },
  {
    "row": 51,
    "rowsha": "XVoXr9uzZwN09vboETojEQJe057RBzcMUjXmQRCB/jo=",
    "originContent": "This project focuses mostly on curating historical data, preparing it for training and building a tokenizer. I am not going to cover the full LLM training process, for that refer to nanoGPT by Andrej Karpathy.",
    "translatedContent": ""
  },
  {
    "row": 52,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "### 步驟一：收集並準備歷史文本"
  },
  {
    "row": 53,
    "rowsha": "h3A3o0uEgrm0Te8o4iW7R0m1bNwx7jj9Q5iUhhV0iTI=",
    "originContent": "### Step 1: Gather and Prepare Historical Texts ",
    "translatedContent": ""
  },
  {
    "row": 54,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "- 收集你選定時期的公有領域書籍、文件等 .txt 檔（例如：倫敦 1800-1850）"
  },
  {
    "row": 55,
    "rowsha": "E60jdnnHtQ07Lgqtx5VBWz6jh3ysKu7LMtZHrIYaQiQ=",
    "originContent": "- Collect .txt files of public domain books, documents, etc from your chosen time period (e.g., London 1800-1850) ",
    "translatedContent": "- 請確保資料都在你選定的時間/地點範圍內"
  },
  {
    "row": 56,
    "rowsha": "/VflzaebH3p9kM3YtRzEWV0i/lVdD/pGby0wiJoXoco=",
    "originContent": "- Keep them within your chosen time/place window  ",
    "translatedContent": "- 使用腳本清理文本檔，或手動移除如 Project Gutenberg 的標頭/頁尾、現代註解或 OCR 錯誤等雜訊。"
  },
  {
    "row": 57,
    "rowsha": "6P9O4yLTQNPt+lgfziVbtyNP+4+SBhj6UvzpJ+Br7PM=",
    "originContent": "- Clean the text files using a script or manually remove headers/footer from Project Gutenberg, Modern annotations or things like OCR errors.",
    "translatedContent": ""
  },
  {
    "row": 58,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "### 步驟二：建立自訂分詞器"
  },
  {
    "row": 59,
    "rowsha": "QHGXZGXfahA0jwtugXfFPK3RfnoD/MgGK7KZOEu7Yu0=",
    "originContent": "### Step 2: Build a Custom Tokenizer",
    "translatedContent": ""
  },
  {
    "row": 60,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "- 對清理過的資料執行 train_tokenizer.py 或 train_tokenizer_hf.py"
  },
  {
    "row": 61,
    "rowsha": "hPnDq1u2xQSmYVNea/p6gaqpP4bMQD0lWroth4bud9A=",
    "originContent": "- Run train_tokenizer.py or train_tokenizer_hf.py on the cleaned data.",
    "translatedContent": "- 這將產生 vocab.json 與 merges.txt"
  },
  {
    "row": 62,
    "rowsha": "ib4aP51Hu/ktXBqXfyiOtGRLdTUEhyEupPgINhe9JwA=",
    "originContent": "- This will give you vocab.json and merges.txt",
    "translatedContent": "- 這些檔案定義了模型的詞彙表和合併規則"
  },
  {
    "row": 63,
    "rowsha": "Bit9udkCe0pa2QERa6r17MT8TZN59hctFxW7DQ/7Ka4=",
    "originContent": "- Thes files define vocab and merge rules for your model",
    "translatedContent": ""
  },
  {
    "row": 64,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "### 步驟三：訓練你的模型"
  },
  {
    "row": 65,
    "rowsha": "NMTP0o8ite3isz8AN3pbJxnD1y/ULsyqoGpOqqCM9k0=",
    "originContent": "### Step 3: Train Your Model ",
    "translatedContent": ""
  },
  {
    "row": 66,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "- 有關訓練流程，請參考 [Andrej Karpathy 的 nanoGPT](https://github.com/karpathy/nanoGPT) 或你選用架構的官方文件。"
  },
  {
    "row": 67,
    "rowsha": "b/wzY4grG20GGDNSiBPVe1mYZhuqNzNEaHz5XhGaZ1I=",
    "originContent": "- Refer to [nanoGPT by Andrej Karpathy](https://github.com/karpathy/nanoGPT) for the training process or your chosen architecture’s docs.",
    "translatedContent": ""
  },
  {
    "row": 68,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "# 常見問答"
  },
  {
    "row": 69,
    "rowsha": "OoCxyGfPN5TmdzAkaPphtPx303MJJ7vpfWbKrufGH5g=",
    "originContent": "# FAQ",
    "translatedContent": ""
  },
  {
    "row": 70,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "## 什麼是選擇性時期訓練（Selective Temporal Training）？"
  },
  {
    "row": 71,
    "rowsha": "+5dDgPw4ILEotxso4tjjjz1cxwUei16yNQPDUKbgxoo=",
    "originContent": "## What is Selective Temporal Training ?",
    "translatedContent": ""
  },
  {
    "row": 72,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "選擇性時期訓練（STT）是一種機器學習方法，所有訓練資料都特別整理，僅限於某個歷史時期。這麼做是為了建模該時代的語言與知識，不受現代概念影響。例如，目前的模型（v0.5）只用1800-1875年的資料訓練，並非微調，而是從零開始訓練，因此輸出能反映該時期的語言風格與歷史背景。"
  },
  {
    "row": 73,
    "rowsha": "hooEARKH4r/sDPh7JUtZAZ6TYMvBkTLZIcfw3g83xos=",
    "originContent": "Selective Temporal Training (STT) is a machine learning methodology where all training data is specifically curated to fall within a specific historical time period. It's done in order to model the language and knowledge of that era without influence from modern concepts. For example, the current model I have now (v0.5) is trained on data exclusively from 1800-1875, it's not fine tuned but trained from scratch resulting in output that reflects the linguistic style and historical context of that time period.",
    "translatedContent": ""
  },
  {
    "row": 74,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "## 為什麼不直接用微調或 LoRA？"
  },
  {
    "row": 75,
    "rowsha": "dVMKQ2mPI1Spc6x6r/jNG0PIR5YKpalU4MXx9JmKp/I=",
    "originContent": "## Why not just use fine-tuning or LoRA?",
    "translatedContent": ""
  },
  {
    "row": 76,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "本專案目標是建立一個不受現代偏見影響的語言模型。如果用 GPT-2 這類模型微調，原本的預訓練資訊無法完全消除。從零開始訓練的語言模型不會假裝是舊的，而是本身就是。現階段目標是建立一個僅能用1800-1875年倫敦出版書籍知識推理的模型。"
  },
  {
    "row": 77,
    "rowsha": "uUg17WnHfiRHAiHJTO+5TIAJlkny2eH+Ov6Lm0GmrJQ=",
    "originContent": "For this project I'm trying to create a language model that is unclouded from modern bias. If I fine-tune something like GPT-2, it's already pre-trained and that information won't go away. If I train from scratch the language model won't pretend to be old, it just will be. The Goal for this project right now is to create something can reason exclusively using knowledge from London books published between 1800 and 1875.",
    "translatedContent": ""
  },
  {
    "row": 78,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "## 你用什麼資料來訓練？"
  },
  {
    "row": 79,
    "rowsha": "ByP4WlNmMoG6WIiLJNd6b080/DSciCgWmj9aYSJjAF0=",
    "originContent": "## What kind of data did you use for training?",
    "translatedContent": ""
  },
  {
    "row": 80,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 81,
    "rowsha": "TiKmhaEUee3SRrWijVMwW3s/qbNf2ziThQdK9PtiW9M=",
    "originContent": "I'm using books, legal documents, newspapers, and other writings from 1800–1875 London. The list I linked (for v0) has like 200 but for the first training I just used 50 files about ~187 MB. You can view a list of the documents:",
    "translatedContent": "我正在使用1800至1875年倫敦的書籍、法律文件、報紙以及其他著作。你可以查看我提供的清單（v0版本），裡面大約有200份文件，但在第一次訓練時我只用了50個檔案，約187MB。你可以在以下連結查看文件清單："
  },
  {
    "row": 82,
    "rowsha": "0mxyGiLJxzp9JPCg1oA+nbIwAKJbEC4ei9kSV3Gp84Y=",
    "originContent": "https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt",
    "translatedContent": "https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt"
  },
  {
    "row": 83,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 84,
    "rowsha": "m08SxkUzMWecQCroTF6uw3RhQ50sDm4BaoM0Ql/huS0=",
    "originContent": "Dataset sizes:",
    "translatedContent": "資料集大小："
  },
  {
    "row": 85,
    "rowsha": "uah9doEnpRGciSUsNDHMkbOxtzCOyu4QuAM07AAUp1o=",
    "originContent": "v0: ~187MB",
    "translatedContent": "v0：約187MB"
  },
  {
    "row": 86,
    "rowsha": "4bhyzflpyjgeXMXaoAIxXATaPQqBpKaX945kxvp1ewE=",
    "originContent": "v0.5: ~435MB ",
    "translatedContent": "v0.5：約435MB"
  },
  {
    "row": 87,
    "rowsha": "thwQOIv1UCJQK/UZwcFrVR5tZmLvXp7mbQYGfpiFTN8=",
    "originContent": "v1: ~6.25GB ",
    "translatedContent": "v1：約6.25GB"
  },
  {
    "row": 88,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 89,
    "rowsha": "rrsRMYLHKsjbkzYRKilseYJKKApHSgo9buiBkZFSvlY=",
    "originContent": "## How large are the models ?",
    "translatedContent": "## 模型有多大？"
  },
  {
    "row": 90,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 91,
    "rowsha": "MVeUG+6lIlH89E699W2WfE+eQmKEul25KNMIY8AtB6M=",
    "originContent": "V0: 16M Parameters",
    "translatedContent": "V0：1600萬參數"
  },
  {
    "row": 92,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 93,
    "rowsha": "hQIpZXf/UvJi3QFJEM6jrjG7vnb9cvTFg/cvKv0buqM=",
    "originContent": "V0.5 123M Parameters",
    "translatedContent": "V0.5：1億2300萬參數"
  },
  {
    "row": 94,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 95,
    "rowsha": "26WDqv9Q/x8glaWZTkjUWL5VNwuuXgUuXs51BhKEskk=",
    "originContent": "V1: 700M Parameters",
    "translatedContent": "V1：7億參數"
  },
  {
    "row": 96,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 97,
    "rowsha": "zaP9HMP5oBcc9MGTkbFGzMO2XxeWWl+QlS9Yz9nfNXc=",
    "originContent": "# Training Specs ? ",
    "translatedContent": "# 訓練規格？"
  },
  {
    "row": 98,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 99,
    "rowsha": "7TPD+AUCDeUFol0fzwax1DEgBjstPoRauQ1zQAHJdc8=",
    "originContent": "# V0/V0.5",
    "translatedContent": "# V0/V0.5"
  },
  {
    "row": 100,
    "rowsha": "EH8H1HW/C4Tb7LfJgVUnVGsk4pF9l40Rlev8tAkKhjI=",
    "originContent": "GPU: Geforce rtx 4060",
    "translatedContent": "GPU：Geforce RTX 4060"
  },
  {
    "row": 101,
    "rowsha": "vo3FdN37kY6VUB7PruRKfBPJDgsVJyBHIUCn/g8mt68=",
    "originContent": "CPU: i5-13400F ",
    "translatedContent": "CPU：i5-13400F"
  },
  {
    "row": 102,
    "rowsha": "W8fXPiQKUkoNso0PPfTvjYMy0IYo85j+gNXmB0aERO4=",
    "originContent": "Ram: 16GB DDR5.",
    "translatedContent": "記憶體：16GB DDR5。"
  },
  {
    "row": 103,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 104,
    "rowsha": "xpPrp95oVxh/XiVWlKlS072VerOoQbJEFU46i3XD2mk=",
    "originContent": "# V1",
    "translatedContent": "# V1"
  },
  {
    "row": 105,
    "rowsha": "cBfLeLi5ORliO9eNHlkjuWbV5U2y03sn/wFBoChMpoc=",
    "originContent": "GPU: A100 rented",
    "translatedContent": "GPU：租用A100"
  },
  {
    "row": 106,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 107,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 108,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 109,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 110,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 111,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 112,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 113,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 114,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 115,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 116,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 117,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 118,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 119,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  }
]