[
  {
    "row": 1,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 2,
    "rowsha": "cWgam+tnnXudu7i74+ahMEGk/A9dQS+EwWLAIfi3dHk=",
    "originContent": "<div align=\"right\">",
    "translatedContent": "<div align=\"right\">"
  },
  {
    "row": 3,
    "rowsha": "orOcu5ARna/hb3RUkj6dBI8pHTM3WHeTvby17l5E0h0=",
    "originContent": "  <details>",
    "translatedContent": "  <details>"
  },
  {
    "row": 4,
    "rowsha": "TtgkLzblnvP0q9aAIVXt6s2LczXjy5k+QvHKcU0/5Ms=",
    "originContent": "    <summary >🌐 Language</summary>",
    "translatedContent": "    <summary >🌐 語言</summary>"
  },
  {
    "row": 5,
    "rowsha": "fZtk4rPTAJEEslnbhSVkHEcPlsctYSzAV7CDPL3rJmA=",
    "originContent": "    <div>",
    "translatedContent": "    <div>"
  },
  {
    "row": 6,
    "rowsha": "9KQxOeJSigvTmGWO+mtnl8kZY9zQfueoy8sk4lYm09Q=",
    "originContent": "      <div align=\"center\">",
    "translatedContent": "      <div align=\"center\">"
  },
  {
    "row": 7,
    "rowsha": "CeOhdpchZBoZSEUDtSE417JEcMBSZw18jeJuHJBKB2Y=",
    "originContent": "        <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en\">English</a>",
    "translatedContent": "        <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en\">English</a>"
  },
  {
    "row": 8,
    "rowsha": "ToO7MFa3QrNNljdQWIagsnOPxe8cXuuA2m5msIm+Kbs=",
    "originContent": "        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN\">简体中文</a>",
    "translatedContent": "        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN\">簡體中文</a>"
  },
  {
    "row": 9,
    "rowsha": "MRATmWdRMRw0JU4u9h5pMb6GU17lQFgG9v/bpGLr9pM=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">繁體中文 (coming soon)</a> |",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">繁體中文（即將推出）</a> |"
  },
  {
    "row": 10,
    "rowsha": "GY7LXxG3rk5eFh9itcqM0cTtmHybyjLTf1icB3jN31I=",
    "originContent": "        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja\">日本語</a>",
    "translatedContent": "        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja\">日本語</a>"
  },
  {
    "row": 11,
    "rowsha": "b5TwunGJh+gsAe7aQU3dkfobXF/nknCEta1msDa7XBU=",
    "originContent": "        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko\">한국어</a>",
    "translatedContent": "        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko\">韓國語</a>"
  },
  {
    "row": 12,
    "rowsha": "1/HCgPsVh2ChqMY+k/VVxEWHPRRmWWCjy5nDRibi3mM=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">हिन्दी (coming soon)</a> |",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">हिन्दी（即將推出）</a> |"
  },
  {
    "row": 13,
    "rowsha": "3lfEHT+5HYFEvbE5cl+xujQPYjtVmzTifT37iqPTWII=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">ไทย (coming soon)</a> |",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">ไทย（即將推出）</a> |"
  },
  {
    "row": 14,
    "rowsha": "KmG3P0px2E3bt1lU/w3eGop+zeA1j8xL0k280Zd9m2s=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">Français (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Français（即將推出）</a>"
  },
  {
    "row": 15,
    "rowsha": "CSdHSEXgIs3M2Q/6zIIJ8NbKkZWhydhBqNus94qrPvg=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">Deutsch (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Deutsch（即將推出）</a>"
  },
  {
    "row": 16,
    "rowsha": "8wz7pDuXc3dk+ZcqZ1jmmh8zh6xN3Wb6qWbCjxAj7dA=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">Español (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Español（即將推出）</a>"
  },
  {
    "row": 17,
    "rowsha": "op/NqIZs7OjCSpNgpXk8RnqDnTegVPyWUQhuQxvTR7U=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">Italiano (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Italiano（即將推出）</a>"
  },
  {
    "row": 18,
    "rowsha": "tAvlfwut/Ad9q1huxc8EREZGv7vYHbrEujzUS8xoaQo=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">Русский (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Русский（即將推出）</a>"
  },
  {
    "row": 19,
    "rowsha": "WhhSpeeCUUAqJiVTS4Fvyc6A2c+24Jnj3MW7XLQuIcI=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">Português (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Português（即將推出）</a>"
  },
  {
    "row": 20,
    "rowsha": "0yPXPrWh+Vzc6FBE9iiciw5HwpOSmo05HNe36wfTWCI=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">Nederlands (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Nederlands（即將推出）</a>"
  },
  {
    "row": 21,
    "rowsha": "mdW6YUUXf5KzI4CwZxrE08ofaLonUOMnJpN3vPR7Y2A=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">Polski (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Polski（即將推出）</a>"
  },
  {
    "row": 22,
    "rowsha": "sw1AXxAGQNvn4eSG9enTWNkwKH0yr6LlVtXBH1j9z8s=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">العربية (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">العربية（即將推出）</a>"
  },
  {
    "row": 23,
    "rowsha": "I8dh9zmXisU0+CpddA55QQgvujH03J/dEnXgj5aFtQM=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">فارسی (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">فارسی（即將推出）</a>"
  },
  {
    "row": 24,
    "rowsha": "7VFv8o6de72ciJrbh3mctfrEgCJhNvuKGWJNOmCaPdM=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">Türkçe (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Türkçe（即將推出）</a>"
  },
  {
    "row": 25,
    "rowsha": "C+XRvFz/D3o9/JyPqwitsxtskFZleJC/oFUr4SEeiHA=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">Tiếng Việt (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Tiếng Việt（即將推出）</a>"
  },
  {
    "row": 26,
    "rowsha": "ntGI5B+n9x96pV3ZG5GG83nmocQbxTJjKY7VVwa6Rq8=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">Bahasa Indonesia (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Bahasa Indonesia（即將推出）</a>"
  },
  {
    "row": 27,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 28,
    "rowsha": "0OM5wNEm0TO56MEBvQzL7AUZM7/3OpgIeqRf2zFre3Q=",
    "originContent": "      </div>",
    "translatedContent": "      </div>"
  },
  {
    "row": 29,
    "rowsha": "fcjTfY+fs8YnY5slBs1sZvWPAqEQR7tzaBDO54skkGQ=",
    "originContent": "    </div>",
    "translatedContent": "    </div>"
  },
  {
    "row": 30,
    "rowsha": "+fQNH2ldI7UM/rqRscP3hUSWAmw1HvQ2wEKDN8JagT0=",
    "originContent": "  </details>",
    "translatedContent": "  </details>"
  },
  {
    "row": 31,
    "rowsha": "qsMmUbEPVnxGG5tPJV1vsfpoWbU2jYvZpRr5IKshzyM=",
    "originContent": "</div>",
    "translatedContent": "</div>"
  },
  {
    "row": 32,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 33,
    "rowsha": "VRGjp0FtfvQ89lbX/wJLis2ypCRtNJwe8ViIi29+Rko=",
    "originContent": "# TimeCapsule LLM",
    "translatedContent": "# TimeCapsule LLM"
  },
  {
    "row": 34,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 35,
    "rowsha": "yIU5euPsbVrUOG7MDKF9kStW2M4Bn4BSjmDAEYjCGgg=",
    "originContent": "*A language model trained **from scratch** exclusively on data from certain places and time periods to reduce modern bias and emulate the voice, vocabulary, and worldview of the era.*",
    "translatedContent": "*一個語言模型，從零開始**專門**在特定地區和時期的資料上訓練，以減少現代偏見並模擬該時代的語音、詞彙和世界觀。*"
  },
  {
    "row": 36,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 37,
    "rowsha": "06wDXO9Un3ot9kUKAGg7CaRsIVSkfS1d2m+EQ6HOFog=",
    "originContent": "Imagine if an AI model didnt just pretend to be historical but actually was.",
    "translatedContent": "想像一下，如果一個 AI 模型不只是假裝是歷史的，而是真的如此。"
  },
  {
    "row": 38,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 39,
    "rowsha": "FM0UQgFCFucUCjhd6pW/S6IBVnMSJbFjDx5+BItXuUc=",
    "originContent": "v0 and v0.5 built on [nanoGPT by Andrej Karpathy](https://github.com/karpathy/nanoGPT) Core training scripts and model architecture are his work. ",
    "translatedContent": "v0 和 v0.5 基於 [nanoGPT by Andrej Karpathy](https://github.com/karpathy/nanoGPT) 核心訓練腳本及模型架構皆來自於他。"
  },
  {
    "row": 40,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 41,
    "rowsha": "qJUbd5u/iOZOtuM+fYFPZOjgtKDl8iwUfAi1n3MVisI=",
    "originContent": "v1 built on [Phi 1.5 by Microsoft](https://huggingface.co/microsoft/phi-1_5)",
    "translatedContent": "v1 建立於 [微軟 Phi 1.5](https://huggingface.co/microsoft/phi-1_5)"
  },
  {
    "row": 42,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 43,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 44,
    "rowsha": "S8kboQ4LnLUhavjolKrTgNgwVOMJrwpnaY1CWGr6epQ=",
    "originContent": "##  Model Behavior & Limitations",
    "translatedContent": "##  模型行為與限制"
  },
  {
    "row": 45,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 46,
    "rowsha": "1nqZ1MR/Rxg5du0+tZch+QesCAT+5qMlbhJftBOVqDI=",
    "originContent": "### **v0**  ",
    "translatedContent": "### **v0**  "
  },
  {
    "row": 47,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 48,
    "rowsha": "hlawgmI5oFABN37xYPae7PebQq1TP5c6DMT8b78524g=",
    "originContent": "Early prompts show the model responding with 1800's language and behavior. ",
    "translatedContent": "早期提示顯示模型以十九世紀語言及行為回應。"
  },
  {
    "row": 49,
    "rowsha": "5eCkvYiGa3VxONgXmVAvuTajgTbveIO0PKy313G2fos=",
    "originContent": "Example: Prompt: \"Who art Henry?\" and it replied \"I know that man, I have did not a black, the storm.\" ",
    "translatedContent": "範例：提示：「Who art Henry?」模型回覆：「I know that man, I have did not a black, the storm.」"
  },
  {
    "row": 50,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 51,
    "rowsha": "yKIR0teTc66wVDG+jdIyNmAzItXb2JH2ld3D7tm4qnM=",
    "originContent": "![TimeLockLLM Sample Output](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)",
    "translatedContent": "![TimeLockLLM 範例輸出](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)"
  },
  {
    "row": 52,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 53,
    "rowsha": "iCVdbo9EUfepGTS1z1bOwgFn1wDAa9zhHoBLJxEw1w8=",
    "originContent": "- No mention of modern concepts  ",
    "translatedContent": "- 未提及現代概念  "
  },
  {
    "row": 54,
    "rowsha": "khHeh5VmbIuCnyoNCTDAXLsQOpnqbamm8vyOjvWVmLE=",
    "originContent": "- Mostly era-accurate vocabulary  ",
    "translatedContent": "- 主要使用當代詞彙  "
  },
  {
    "row": 55,
    "rowsha": "UP5LBmH8evd8yPger3Fjvs1PXYiVAALQC0JWmcmlN08=",
    "originContent": "- Sentences are mostly incoherent (expected for ~187MB training data)",
    "translatedContent": "- 句子大多不連貫（約 187MB 訓練資料，屬預期現象）"
  },
  {
    "row": 56,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 57,
    "rowsha": "5p60D9yLuB40iACJ6apiWJq8lt1AqZAyvFkG70v4xco=",
    "originContent": "### **v0.5** ",
    "translatedContent": "### **v0.5** "
  },
  {
    "row": 58,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 59,
    "rowsha": "IPr+C8pVE68OvudYYmIOcbEumcoTmjBiq3HmXPghcvU=",
    "originContent": "A significant improvement over v0.  ",
    "translatedContent": "相較 v0 有顯著進步。  "
  },
  {
    "row": 60,
    "rowsha": "VEokY2Qi826s5lAKKkVoOd5hju8hvOxG21P/L2LPz9k=",
    "originContent": "- Victorian writing style, proper punctuation, mostly grammatical sentences  ",
    "translatedContent": "- 維多利亞時期寫作風格，標點正確，句子語法大致正確  "
  },
  {
    "row": 61,
    "rowsha": "hkitRVnW6jxtd+R18O5Olb5eqNIJGCKv4Yt2I+yLrWE=",
    "originContent": "- Still high factual hallucination rate  ",
    "translatedContent": "- 事實虛構率仍高  "
  },
  {
    "row": 62,
    "rowsha": "qlWBqyQVUBsL120iExHvs0PDrN2m0WeMIyRHeHJ1qhM=",
    "originContent": "- OCR noise (“Digitized by Google”) still present in outputs",
    "translatedContent": "- 輸出仍有 OCR 雜訊（如「Digitized by Google」）"
  },
  {
    "row": 63,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 64,
    "rowsha": "8DhXpgpVtg05XdyplRHf49EFOQNCJVzXA9RpmJQ+y9U=",
    "originContent": "![TimeLockLLM Sample Output](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)",
    "translatedContent": "![TimeLockLLM 範例輸出](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)"
  },
  {
    "row": 65,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 66,
    "rowsha": "mQ9MARMITxOHtqSNIP76nXvTlEM8Ope3qbQ/YG1h6/8=",
    "originContent": "### **v1**",
    "translatedContent": "### **v1**"
  },
  {
    "row": 67,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 68,
    "rowsha": "sAlnUjWbUBk6czBOHW/IgxyE98WYhX0LrIYmZBr8P2Y=",
    "originContent": "First model to recall and connect a real historical event with an actual figure from the dataset.",
    "translatedContent": "首個能夠回憶並連結真實歷史事件及資料集中真實人物的模型。"
  },
  {
    "row": 69,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 70,
    "rowsha": "Cd0DPnkqItUnSk3fHx9ckSSN6MNcZnsEX3S44GZyxTw=",
    "originContent": "Example: Prompt: \"It was the year of our Lord 1834\" ",
    "translatedContent": "範例：提示：「It was the year of our Lord 1834」 "
  },
  {
    "row": 71,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 72,
    "rowsha": "zvm/1tcaMFjcQmuN7y0hczAbbgAwaKRtjBpah5/+kb8=",
    "originContent": "The output: \"It was the year of our Lord 1834 and the streets of London were filled with protest and petition. The cause, as many re counted, was not bound in the way of private, but having taken up the same day in the day of Lord Palmerston, the public will receive a short statement of the difficulties under which the day of law has reached us. It is a matter of deep regret, that the present events in the history of the world are clear, and consequently will be'known. It is not true that the very men who first settled in the Gospel at Jerusalem should have so extensive and so interesting a record of the prosperity and prosperity\" ",
    "translatedContent": "模型輸出：「It was the year of our Lord 1834 and the streets of London were filled with protest and petition. The cause, as many re counted, was not bound in the way of private, but having taken up the same day in the day of Lord Palmerston, the public will receive a short statement of the difficulties under which the day of law has reached us. It is a matter of deep regret, that the present events in the history of the world are clear, and consequently will be'known. It is not true that the very men who first settled in the Gospel at Jerusalem should have so extensive and so interesting a record of the prosperity and prosperity」 "
  },
  {
    "row": 73,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 74,
    "rowsha": "WfSfj2g4CByfYQWI59s4C3EwGLZuWfz1t4w49a49e+8=",
    "originContent": "At first I assumed that a protest might have coincidentally taken place the same year but take a look at this: ![1834protest](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)",
    "translatedContent": "我原以為抗議活動可能只是巧合地發生於該年，但請看這張圖：![1834protest](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)"
  },
  {
    "row": 75,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 76,
    "rowsha": "qlFf3SlVZpptxE7ExYx9wjs1lBnO/bsCjUl1Q4Sw2ds=",
    "originContent": "### Why this matters:",
    "translatedContent": "### 為何這很重要："
  },
  {
    "row": 77,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 78,
    "rowsha": "WdZfxluSvMeRhCl5u8NEkZI+F2o+eDrUIZyWPgHGv6w=",
    "originContent": "This is the first example of one of my models connecting a year to both a real historical event and a real person tied to that event (Lord Palmerston). Earlier models (v0 and v0.5) could mimic writing styles of the 19th century but would always hallucinate events, people and facts. This shows the model is beggining to remember things from the dataset ",
    "translatedContent": "這是我模型首次將年份與真實歷史事件及相關人物（Lord Palmerston）連結。早期模型（v0 及 v0.5）雖可模仿十九世紀寫作風格，但總是虛構事件、人物及事實。這顯示模型開始記住資料集中的內容。"
  },
  {
    "row": 79,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 80,
    "rowsha": "xt81v+AyDswOM8KVPmSYl1lfW4vISpujtQjRWm0mKbA=",
    "originContent": "## Upcoming Plans ",
    "translatedContent": "## 未來計畫 "
  },
  {
    "row": 81,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "- 在網際網路檔案館（Internet Archive）上，倫敦於1800-1875年間出版的文本近175,000份"
  },
  {
    "row": 82,
    "rowsha": "CulYLHqIw5Hn1cLDvlekSWBe1moOd8QSIirjeM77tlk=",
    "originContent": "- There are nearly 175,000 texts published in London from 1800-1875 on Internet Archive ",
    "translatedContent": "- 我計劃擴展語料庫並進行更完善的清理，以提升推理能力"
  },
  {
    "row": 83,
    "rowsha": "VWINtuHshK3Fs4MeBmv24/pZC0iMQ70HtMIowLnrsAI=",
    "originContent": "- I plan on expanding the corpus and cleaning it more for better reasoning abilities",
    "translatedContent": "- 擴展至不同地區和時期，以建立更多歷史模型"
  },
  {
    "row": 84,
    "rowsha": "3b7jQ1J58CCfC/VV9FWJY23SOlvhzx/vIVth3ZBIPog=",
    "originContent": "- Expanding to different regions and time periods for more historical models",
    "translatedContent": ""
  },
  {
    "row": 85,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "## 如何使用"
  },
  {
    "row": 86,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 87,
    "rowsha": "yJlGQKI3p2nVYC1uqNa6eKPSIzUW38uRiGu0PspqvUU=",
    "originContent": "## How to Use",
    "translatedContent": "本專案主要聚焦於蒐集歷史資料，準備訓練並建立分詞器。完整的LLM訓練流程將不在此涵蓋，詳細請參考Andrej Karpathy的nanoGPT。"
  },
  {
    "row": 88,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 89,
    "rowsha": "XVoXr9uzZwN09vboETojEQJe057RBzcMUjXmQRCB/jo=",
    "originContent": "This project focuses mostly on curating historical data, preparing it for training and building a tokenizer. I am not going to cover the full LLM training process, for that refer to nanoGPT by Andrej Karpathy.",
    "translatedContent": "### 步驟1：蒐集並準備歷史文本"
  },
  {
    "row": 90,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 91,
    "rowsha": "h3A3o0uEgrm0Te8o4iW7R0m1bNwx7jj9Q5iUhhV0iTI=",
    "originContent": "### Step 1: Gather and Prepare Historical Texts ",
    "translatedContent": "- 收集所選時期的公有領域書籍、文件等的.txt檔案（如：倫敦1800-1850）"
  },
  {
    "row": 92,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "- 保持在你選定的時間與地點範圍內"
  },
  {
    "row": 93,
    "rowsha": "E60jdnnHtQ07Lgqtx5VBWz6jh3ysKu7LMtZHrIYaQiQ=",
    "originContent": "- Collect .txt files of public domain books, documents, etc from your chosen time period (e.g., London 1800-1850) ",
    "translatedContent": "- 使用腳本清理文本檔案，或手動移除古騰堡計畫的頁首/頁尾、現代註釋或OCR錯誤等內容。"
  },
  {
    "row": 94,
    "rowsha": "/VflzaebH3p9kM3YtRzEWV0i/lVdD/pGby0wiJoXoco=",
    "originContent": "- Keep them within your chosen time/place window  ",
    "translatedContent": ""
  },
  {
    "row": 95,
    "rowsha": "6P9O4yLTQNPt+lgfziVbtyNP+4+SBhj6UvzpJ+Br7PM=",
    "originContent": "- Clean the text files using a script or manually remove headers/footer from Project Gutenberg, Modern annotations or things like OCR errors.",
    "translatedContent": "### 步驟2：建立自訂分詞器"
  },
  {
    "row": 96,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 97,
    "rowsha": "QHGXZGXfahA0jwtugXfFPK3RfnoD/MgGK7KZOEu7Yu0=",
    "originContent": "### Step 2: Build a Custom Tokenizer",
    "translatedContent": "- 在清理過的資料上執行train_tokenizer.py或train_tokenizer_hf.py"
  },
  {
    "row": 98,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "- 這會產生vocab.json和merges.txt"
  },
  {
    "row": 99,
    "rowsha": "hPnDq1u2xQSmYVNea/p6gaqpP4bMQD0lWroth4bud9A=",
    "originContent": "- Run train_tokenizer.py or train_tokenizer_hf.py on the cleaned data.",
    "translatedContent": "- 這些檔案定義了你的模型的詞彙及合併規則"
  },
  {
    "row": 100,
    "rowsha": "ib4aP51Hu/ktXBqXfyiOtGRLdTUEhyEupPgINhe9JwA=",
    "originContent": "- This will give you vocab.json and merges.txt",
    "translatedContent": ""
  },
  {
    "row": 101,
    "rowsha": "Bit9udkCe0pa2QERa6r17MT8TZN59hctFxW7DQ/7Ka4=",
    "originContent": "- Thes files define vocab and merge rules for your model",
    "translatedContent": "### 步驟3：訓練你的模型"
  },
  {
    "row": 102,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 103,
    "rowsha": "NMTP0o8ite3isz8AN3pbJxnD1y/ULsyqoGpOqqCM9k0=",
    "originContent": "### Step 3: Train Your Model ",
    "translatedContent": "- 訓練流程請參考[Andrej Karpathy的nanoGPT](https://github.com/karpathy/nanoGPT)或你選用架構的官方文件。"
  },
  {
    "row": 104,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 105,
    "rowsha": "b/wzY4grG20GGDNSiBPVe1mYZhuqNzNEaHz5XhGaZ1I=",
    "originContent": "- Refer to [nanoGPT by Andrej Karpathy](https://github.com/karpathy/nanoGPT) for the training process or your chosen architecture’s docs.",
    "translatedContent": "# 常見問答"
  },
  {
    "row": 106,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 107,
    "rowsha": "OoCxyGfPN5TmdzAkaPphtPx303MJJ7vpfWbKrufGH5g=",
    "originContent": "# FAQ",
    "translatedContent": "## 什麼是選擇性時代訓練（Selective Temporal Training）？"
  },
  {
    "row": 108,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 109,
    "rowsha": "+5dDgPw4ILEotxso4tjjjz1cxwUei16yNQPDUKbgxoo=",
    "originContent": "## What is Selective Temporal Training ?",
    "translatedContent": "選擇性時代訓練（STT）是一種機器學習方法，所有訓練資料都特別挑選在特定歷史時期之內。這麼做是為了建模該時代的語言和知識，避免受現代概念影響。例如，我目前的模型（v0.5）完全以1800-1875年的資料訓練，並非微調，而是從零開始訓練，因此輸出能反映那個時期的語言風格和歷史語境。"
  },
  {
    "row": 110,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 111,
    "rowsha": "hooEARKH4r/sDPh7JUtZAZ6TYMvBkTLZIcfw3g83xos=",
    "originContent": "Selective Temporal Training (STT) is a machine learning methodology where all training data is specifically curated to fall within a specific historical time period. It's done in order to model the language and knowledge of that era without influence from modern concepts. For example, the current model I have now (v0.5) is trained on data exclusively from 1800-1875, it's not fine tuned but trained from scratch resulting in output that reflects the linguistic style and historical context of that time period.",
    "translatedContent": "## 為什麼不直接用微調或LoRA？"
  },
  {
    "row": 112,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 113,
    "rowsha": "dVMKQ2mPI1Spc6x6r/jNG0PIR5YKpalU4MXx9JmKp/I=",
    "originContent": "## Why not just use fine-tuning or LoRA?",
    "translatedContent": "這個專案目標是打造不受現代偏見影響的語言模型。如果我微調如GPT-2，該模型已經預先訓練過，舊知識無法被完全替代。如果從零開始訓練，語言模型不會「假裝」古老，而是本身就具有那個時代的特質。現階段目標是建立一個能只用1800到1875年倫敦出版書籍的知識來推理的模型。"
  },
  {
    "row": 114,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 115,
    "rowsha": "uUg17WnHfiRHAiHJTO+5TIAJlkny2eH+Ov6Lm0GmrJQ=",
    "originContent": "For this project I'm trying to create a language model that is unclouded from modern bias. If I fine-tune something like GPT-2, it's already pre-trained and that information won't go away. If I train from scratch the language model won't pretend to be old, it just will be. The Goal for this project right now is to create something can reason exclusively using knowledge from London books published between 1800 and 1875.",
    "translatedContent": "## 你用什麼資料訓練的？"
  },
  {
    "row": 116,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 117,
    "rowsha": "ByP4WlNmMoG6WIiLJNd6b080/DSciCgWmj9aYSJjAF0=",
    "originContent": "## What kind of data did you use for training?",
    "translatedContent": "我使用1800–1875年倫敦的書籍、法律文件、報紙與其他著作。我連結的清單（v0）約200份，但首次訓練只用50個檔案，約187 MB。你可以查閱文件清單："
  },
  {
    "row": 118,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt"
  },
  {
    "row": 119,
    "rowsha": "TiKmhaEUee3SRrWijVMwW3s/qbNf2ziThQdK9PtiW9M=",
    "originContent": "I'm using books, legal documents, newspapers, and other writings from 1800–1875 London. The list I linked (for v0) has like 200 but for the first training I just used 50 files about ~187 MB. You can view a list of the documents:",
    "translatedContent": ""
  },
  {
    "row": 120,
    "rowsha": "0mxyGiLJxzp9JPCg1oA+nbIwAKJbEC4ei9kSV3Gp84Y=",
    "originContent": "https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt",
    "translatedContent": ""
  },
  {
    "row": 121,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 122,
    "rowsha": "m08SxkUzMWecQCroTF6uw3RhQ50sDm4BaoM0Ql/huS0=",
    "originContent": "Dataset sizes:",
    "translatedContent": "資料集大小："
  },
  {
    "row": 123,
    "rowsha": "uah9doEnpRGciSUsNDHMkbOxtzCOyu4QuAM07AAUp1o=",
    "originContent": "v0: ~187MB",
    "translatedContent": "v0：約 187MB"
  },
  {
    "row": 124,
    "rowsha": "4bhyzflpyjgeXMXaoAIxXATaPQqBpKaX945kxvp1ewE=",
    "originContent": "v0.5: ~435MB ",
    "translatedContent": "v0.5：約 435MB"
  },
  {
    "row": 125,
    "rowsha": "thwQOIv1UCJQK/UZwcFrVR5tZmLvXp7mbQYGfpiFTN8=",
    "originContent": "v1: ~6.25GB ",
    "translatedContent": "v1：約 6.25GB"
  },
  {
    "row": 126,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 127,
    "rowsha": "rrsRMYLHKsjbkzYRKilseYJKKApHSgo9buiBkZFSvlY=",
    "originContent": "## How large are the models ?",
    "translatedContent": "## 模型有多大？"
  },
  {
    "row": 128,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 129,
    "rowsha": "MVeUG+6lIlH89E699W2WfE+eQmKEul25KNMIY8AtB6M=",
    "originContent": "V0: 16M Parameters",
    "translatedContent": "V0：16M 參數"
  },
  {
    "row": 130,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 131,
    "rowsha": "hQIpZXf/UvJi3QFJEM6jrjG7vnb9cvTFg/cvKv0buqM=",
    "originContent": "V0.5 123M Parameters",
    "translatedContent": "V0.5：123M 參數"
  },
  {
    "row": 132,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 133,
    "rowsha": "26WDqv9Q/x8glaWZTkjUWL5VNwuuXgUuXs51BhKEskk=",
    "originContent": "V1: 700M Parameters",
    "translatedContent": "V1：700M 參數"
  },
  {
    "row": 134,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 135,
    "rowsha": "zaP9HMP5oBcc9MGTkbFGzMO2XxeWWl+QlS9Yz9nfNXc=",
    "originContent": "# Training Specs ? ",
    "translatedContent": "# 訓練規格？"
  },
  {
    "row": 136,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 137,
    "rowsha": "7TPD+AUCDeUFol0fzwax1DEgBjstPoRauQ1zQAHJdc8=",
    "originContent": "# V0/V0.5",
    "translatedContent": "# V0/V0.5"
  },
  {
    "row": 138,
    "rowsha": "EH8H1HW/C4Tb7LfJgVUnVGsk4pF9l40Rlev8tAkKhjI=",
    "originContent": "GPU: Geforce rtx 4060",
    "translatedContent": "GPU：Geforce RTX 4060"
  },
  {
    "row": 139,
    "rowsha": "vo3FdN37kY6VUB7PruRKfBPJDgsVJyBHIUCn/g8mt68=",
    "originContent": "CPU: i5-13400F ",
    "translatedContent": "CPU：i5-13400F"
  },
  {
    "row": 140,
    "rowsha": "W8fXPiQKUkoNso0PPfTvjYMy0IYo85j+gNXmB0aERO4=",
    "originContent": "Ram: 16GB DDR5.",
    "translatedContent": "記憶體：16GB DDR5。"
  },
  {
    "row": 141,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 142,
    "rowsha": "xpPrp95oVxh/XiVWlKlS072VerOoQbJEFU46i3XD2mk=",
    "originContent": "# V1",
    "translatedContent": "# V1"
  },
  {
    "row": 143,
    "rowsha": "cBfLeLi5ORliO9eNHlkjuWbV5U2y03sn/wFBoChMpoc=",
    "originContent": "GPU: A100 rented",
    "translatedContent": "GPU：租用的 A100"
  },
  {
    "row": 144,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 145,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 146,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 147,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 148,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 149,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 150,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 151,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 152,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 153,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 154,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 155,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 156,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  }
]