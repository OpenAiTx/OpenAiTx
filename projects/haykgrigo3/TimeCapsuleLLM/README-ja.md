
<div align="right">
  <details>
    <summary >🌐 言語</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">简体中文</a>
        | <a href="#" title="Coming soon">繁體中文 (近日公開)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">한국어</a>
        | <a href="#" title="Coming soon">हिन्दी (近日公開)</a> |
        | <a href="#" title="Coming soon">ไทย (近日公開)</a> |
        | <a href="#" title="Coming soon">Français (近日公開)</a>
        | <a href="#" title="Coming soon">Deutsch (近日公開)</a>
        | <a href="#" title="Coming soon">Español (近日公開)</a>
        | <a href="#" title="Coming soon">Italiano (近日公開)</a>
        | <a href="#" title="Coming soon">Русский (近日公開)</a>
        | <a href="#" title="Coming soon">Português (近日公開)</a>
        | <a href="#" title="Coming soon">Nederlands (近日公開)</a>
        | <a href="#" title="Coming soon">Polski (近日公開)</a>
        | <a href="#" title="Coming soon">العربية (近日公開)</a>
        | <a href="#" title="Coming soon">فارسی (近日公開)</a>
        | <a href="#" title="Coming soon">Türkçe (近日公開)</a>
        | <a href="#" title="Coming soon">Tiếng Việt (近日公開)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (近日公開)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM
特定の時代のデータのみで訓練されたLLMで、現代的なバイアスを低減します。

AIモデルが単に歴史的なふりをするのではなく、本当に歴史的な存在だったらどうでしょう。

[Andrej Karpathy氏のnanoGPT](https://github.com/karpathy/nanoGPT) を基盤としています。コアのトレーニングスクリプトとモデルアーキテクチャは彼の作品です。

# プロジェクト目標
TimeCapsule LLMは、特定の時代に書かれたテキストのみで訓練される実験的プロジェクトです。目的は、特定の歴史的時代の世界観と言語をシミュレートすることです。

# 微調整だけでは不十分な理由

既存のモデルを微調整するだけでは、LLMは現代の概念を知ってしまいます。もちろん、現代バイアスを完全にゼロにするのは難しいですが、できる限りそれに近づきたいです。現代バイアスをなくすには、モデルをゼロから訓練する必要があります。

# 期待される成果

完成時には、このモデルは現代の概念を知らず、訓練された範囲を超えて推論できないはずです。現代の概念や語彙を認識せず、現代の知識を幻覚することもありません。

# 進捗状況

## 2025年7月9日

1800〜1850年の期間、地域はロンドンに設定しました。

テキスト、書籍、文書のリストを集めました。

現在、50件のテキストファイルを取得し、NanoGPTの訓練を開始する予定です。

進捗があれば随時更新します。

## 2025年7月13日

nanoGPTを187MBの歴史的テキストデータで訓練しました。

## 2025年7月15日

2回目の訓練用にテキストのダウンロードを始めました。Internet Archiveからすべて取得しており、期間を1800〜1875年に拡大しました。多様なテキストを得るために、Internet Archiveの検索フィルターで出版場所、期間、主題で絞り込むことができます。

![Search Filters](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## 2025年7月16日

Internet Archiveから約500件のテキストファイルをダウンロードし、（空白やグーテンベルクのヘッダーなどを削除するだけですが）クリーニングした結果、約500MBのデータになりました。これはごく小規模なデータセットですが、前回は187MBで訓練したので、2回目のモデル訓練後には何らかの違いが出るはずです。このモデルが、少なくともより一貫性のある、意味が通る文章を生成できることを期待しています。もちろん、これは保証ではありません。まだ非常に小さなデータセットですが、前回よりは多いです。

これなら自分のハードウェアでも実行できそうです。これは良いことで、GPUをレンタルする前に何らかの改善が見られることを期待しています。もちろんGPUのレンタルもすぐに予定していますが、その前にデータセットをできる限り精査し、クリーンにしておきたいです。クリーニングの問題の一つは、多くのテキストファイルに意味不明な文字列が混在していることです。これまで使ったクリーニングスクリプトは機能しますが、100％の効果はありません。

このデータセットで本日訓練を行い、4〜5時間ほどかかる見込みです。訓練が終わりテストしたら、また進捗を報告します。プロジェクトを見てくれている皆さん、本当にありがとうございます。OCRリソースへのリンクを教えてくれる方もいて感謝しています！もっと多くの人が自分のデータセットで試してみてほしいです。


## 2025年8月12日

バージョン1が完成しました。このバージョンはPhi 1.5（7億パラメータ）を使用し、約7000件のテキストからなるデータセットでトレーニングされています。このモデルの主な改善点は、実際の名前や歴史的出来事を参照できる場合があることです。バージョン0および0.5ではこれができず、「1821年に」といったプロンプトを与えても、出力は現実には存在しない空想の内容ばかりでした。バージョン1での例：

私のプロンプト：「我らが主の1834年だった。」

出力：「我らが主の1834年であり、ロンドンの街は抗議と嘆願で溢れていた。その原因は多くの人が語ったように私的なものではなく、同じ日にパーマストン卿の日に採られたことで、公衆は法の日が我々に到達した困難について簡単な声明を受け取ることになる。世界史の現状が明白であり、したがって知られることになるのは非常に遺憾である。エルサレムで福音を最初に広めたまさにその人々が、繁栄の興味深く広範な記録を持つべきではないというのは真実ではない。」

最初は偶然かと思いましたが、これをご覧ください: ![1834protest](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)


### トレーニングアップデート

435MB（1億800万トークン）のコーパスでトレーニングを開始しましたが、今のところ順調です。トレインロスは最初の2800イテレーションで10.9から4.9に下がりました。完了までに8〜9時間ほどかかると予想しています。完了次第、またアップデートを投稿します。

## 2025年7月17日

2番目のモデルのトレーニングが完了しました。4060で約8時間40分（3,900イテレーション/時）、合計33,000イテレーション（5エポック）かかりました。最終トレインロスは3.73でした。出力は驚くほど良く、本当に一貫した19世紀風の文を生成できるようになっています。

## 2025年7月28日

v0.5をHugging Faceにアップロードしました。[こちらからご覧ください](https://huggingface.co/haykgrigorian/TimeCapsuleLLM)。リポジトリをダウンロードしてローカルで実行できます。ただしnanoGPTはHuggingFaceでネイティブには動作しないため、モデルをダウンロードしてローカルで実行する必要があります。

また、次のトレーニング用データのキュレーションも開始します。推論能力を得るには5〜10倍のデータが必要だと考えています。

## 2025年8月2日

まもなくバージョン1の開発を開始します。nanoGPTのアーキテクチャからよりモダンなものへ移行する必要があります。検討中のオープンソースLLMアーキテクチャには、OpenLLaMA v3、Phi-2、Qwen 1.5Bなどがあります。そしてV1への移行をサポートするために、より大規模かつ多様なデータセットを厳選する必要があります。少なくとも5GBのクリーンなトレーニングデータが必要です。

# V0モデルの挙動と制限

初期のプロンプトでは、モデルが1800年代の言葉遣いや行動で応答していました。例えば「Who art Henry?」とプロンプトを与えると、「I know that man, I have did not a black, the storm.」と返し、文としては意味不明ですが、LLMが人物について尋ねられていると認識していることがわかります。

![TimeLockLLM サンプル出力](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

現代の概念にはまったく言及しておらず、出力はほとんど1800年代の単語や言い回しを含んでいます。

まだ多くの改良が必要であり、187MBのデータで学習しても複雑な推論を行うテキストを生成できるモデルにはなりません。

現在は、完全な文構造が欠如しており、全体的に意味をなさない文が生成されますが、これはトレーニングサイズとしては正常です。

# V0.5 モデルの挙動と制限事項

これは前回のモデルと比べて大きな改善です。文体と語彙はヴィクトリア朝風で、ほぼすべての文が正しい句読点を伴い文法的にも正確です。そして繰り返しますが、これはゼロからトレーニングしたので、1800年代の主題に固執しています。

![TimeLockLLM サンプル出力](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

事実に基づかない幻覚が多く見られます。詳細（日時、出来事、歴史上の人物）はほぼ100％作り話です。また、文同士のつながりもあまりなく、せいぜい2文ほどが関連する程度で、それ以上の繋がりはありません。もう一つの問題として、ときどき「Digitized by Google」というフッターが紛れ込むことがあるので、次回のトレーニングではテキストのクリーンアップを徹底する必要があります。全体としては結果に非常に満足しており、まだLLMにはほど遠いですが、確実に文生成器にはなっています。

多くのことを学んでおり、今後数週間でより良くするために何をすべきかを模索し始めます。ファイルはまもなくアップロードします！

# V1 モデルの挙動と制限事項

まもなくいくつかのサンプル出力をアップロードし、同じプロンプトで3つのモデルを比較します。また、前回と同様にV1もhuggingfaceにアップロードする予定です。私のhuggingfaceアカウントはこちらです：https://huggingface.co/haykgrigorian/TimeCapsuleLLM

# 今後の計画

（完了済み）バージョン0.5の作業を開始します。今回は50冊ではなく、理想的には500～600冊を使用してトレーニングします。現在は1800～1850年のロンドンの本を使ってnanoGPTをトレーニング中です。見つけた本が更新されていないか、現代的な解釈が含まれていないか、選択した期間内に出版された無改訂の本であることを確認することが課題となっています。

より大きなコーパス（v0.5の5～10倍）で新しいモデル（v1）をトレーニングしたいと考えています。私の目標は、選択的時系列トレーニングだけで推論能力が現れるかどうかを確認することです。これはより難しい課題であり、歴史的データの制限があるため、実現可能かどうかも完全には分かりません。今後数週間で5～10GBのコーパスのために十分なデータをキュレートするつもりです。クリーンで高品質なデータを揃えてGPUをレンタルできれば、進展があると信じています。

# このプロジェクトの使い方

このプロジェクトは主に歴史的データの収集、トレーニング用の準備、およびトークナイザーの構築に焦点を当てています。完全なLLMトレーニングプロセスについては触れませんので、Andrej Karpathy氏のnanoGPTを参照してください。

# ステップ1：歴史的テキストを収集・準備

選択した期間（例：ロンドン1800～1850年）のパブリックドメインの本やドキュメント等の.txtファイルを集めます。

必要であれば、download_texts_improved.py を使って本をダウンロードできます。

テキストファイルをスクリプトまたは手動でクリーンアップし、プロジェクト・グーテンベルクのヘッダー/フッター、現代の注釈、OCRエラー等を削除します。

prepare_dataset.pyは問題なく動作するはずです。

# ステップ2：カスタムトークナイザーの構築

クリーンなデータで train_tokenizer.py または train_tokenizer_hf.py を実行します。
これで vocab.json と merges.txt が得られます。
これらのファイルは、モデルの語彙とマージルールを定義します。

# ステップ3: モデルのトレーニング (nanoGPT)

トレーニングプロセスについては [Andrej KarpathyのnanoGPT](https://github.com/karpathy/nanoGPT) を参照してください。

別のLLMをトレーニングすることもできますが、私はnanoGPTを使用しました。

# FAQ

## Selective Temporal Trainingとは何ですか？

Selective Temporal Training（STT）は、すべてのトレーニングデータが特定の歴史的時代内に厳選される機械学習手法です。その時代の言語と知識を現代の概念から影響を受けずにモデル化するために行われます。例えば、現在私が持っているモデル（v0.5）は1800～1875年のデータのみでトレーニングされており、ファインチューニングはされていません。最初からトレーニングすることで、その時代の言語スタイルや歴史的文脈が反映された出力となります。

## なぜファインチューニングやLoRAを使わないのですか？

このプロジェクトでは、現代のバイアスに影響されない言語モデルを作りたいと考えています。GPT-2のようなものをファインチューニングしても、すでに事前学習されている情報は消えません。最初からトレーニングすれば、言語モデルは古いふりをするのではなく、本当に古いものになります。現時点での目標は、1800～1850年のロンドンで出版された本の知識のみで推論できるものを作ることです。

## トレーニングにはどんなデータを使いましたか？

私は1800～1850年ロンドンの本、法的文書、新聞、その他の文書を使用しています。リンクしたリストには約200ありますが、最初のトレーニングには約50ファイル（約187MB）だけ使用しました。文書のリストはこちらで見られます：
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## モデルのサイズはどれくらいですか？

V0: 16M パラメータ

V0.5: 123M パラメータ

V1: 700M パラメータ

# トレーニングスペックは？

#V0/V0.5
GPU: Geforce RTX 4060
CPU: i5-13400F
RAM: 16GB DDR5.

#V1

GPU: A100 rented









---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-08-12

---