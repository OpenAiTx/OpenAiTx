
<div align="right">
  <details>
    <summary >🌐 言語</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">简体中文</a>
        | <a href="#" title="Coming soon">繁體中文 (近日公開)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">한국어</a>
        | <a href="#" title="Coming soon">हिन्दी (近日公開)</a> |
        | <a href="#" title="Coming soon">ไทย (近日公開)</a> |
        | <a href="#" title="Coming soon">Français (近日公開)</a>
        | <a href="#" title="Coming soon">Deutsch (近日公開)</a>
        | <a href="#" title="Coming soon">Español (近日公開)</a>
        | <a href="#" title="Coming soon">Italiano (近日公開)</a>
        | <a href="#" title="Coming soon">Русский (近日公開)</a>
        | <a href="#" title="Coming soon">Português (近日公開)</a>
        | <a href="#" title="Coming soon">Nederlands (近日公開)</a>
        | <a href="#" title="Coming soon">Polski (近日公開)</a>
        | <a href="#" title="Coming soon">العربية (近日公開)</a>
        | <a href="#" title="Coming soon">فارسی (近日公開)</a>
        | <a href="#" title="Coming soon">Türkçe (近日公開)</a>
        | <a href="#" title="Coming soon">Tiếng Việt (近日公開)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (近日公開)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM
特定の時代のデータのみで訓練されたLLMで、現代的なバイアスを低減します。

もしAIモデルが歴史的に振る舞うだけでなく、実際にそうだったとしたらどうでしょう。

[Andrej KarpathyによるnanoGPT](https://github.com/karpathy/nanoGPT)をベースに構築しています。コアのトレーニングスクリプトとモデルアーキテクチャは彼の仕事です。

# プロジェクトの目的

TimeCapsule LLMは、特定の時代に書かれたテキストのみで訓練される実験的なプロジェクトです。目的は、特定の歴史的時代の世界観と言語をシミュレートすることです。

# ファインチューニングだけでは不十分な理由

事前学習済みモデルをファインチューニングするだけでは、LLMは現代的な概念を依然として知っています。もちろん現代バイアスを完全になくすのは難しいですが、私はこれにできるだけ近づきたいと考えています。現代バイアスをなくすには、モデルをゼロから訓練する必要があります。

# 期待される成果

完成時には、このモデルは現代の概念を知らず、訓練された範囲を超えて推論できないことを期待しています。現代の概念や語彙を認識すべきでなく、現代の知識を幻覚することもないことを願っています。

# 進捗状況

## 2025年7月9日

1800年から1850年のロンドン地域を対象期間に設定しました。

テキスト、書籍、文書のリストを収集しました。

現時点で50本のtxtファイルを入手し、まもなくNanoGPTでの訓練を開始します。

進捗があれば、随時更新します。

## 2025年7月13日

nanoGPTで187MBの歴史的テキストデータを用いて訓練しました。

## 2025年7月15日

2回目の訓練用にテキストのダウンロードを始めました。Internet Archiveから全て取得しており、期間を1800年から1875年に拡大しました。多様なテキストを得るため、Internet Archiveでは出版場所、時代、主題で検索フィルターを使うと良いです。

![検索フィルター](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## 2025年7月16日

Internet Archiveから約500本のtxtファイルをダウンロードし、空白やGutenbergヘッダーなどを削除してクリーンアップした後、約500MBのデータになりました。これはごく小さなデータセットですが、前回は187MBで訓練したので、2回目のモデルでは少なくとも何らかの顕著な違いが出るはずです。今回のモデルでは、もう少し意味の通る一貫した文が生成できることを期待しています。もちろんこれは保証ではなく、まだまだ小さなデータセットですが、前回よりは多いです。

これは自分のハードウェアでも対応できる範囲なので、より大きなデータセットに移行する前に何らかの改善が確認できればと思います。いずれGPUをレンタルして大規模な訓練も計画していますが、その前にデータセットをできるだけ厳選・クリーンにしたいと考えています。一つの課題はクリーンアップで、多くのtxtファイルに意味不明な文字が混ざっています。クリーンアップ用スクリプトは機能していますが、100%有効とは言えません。

本日このデータセットで訓練を行い、4～5時間ほどかかる見込みです。終わったらテストして、進捗を報告します。プロジェクトに興味を持っていただいた皆様、本当にありがとうございます。OCRリソースのリンクを教えてくれた方もいて感謝しています！より多くの人が自分のデータセットで挑戦し、実験してくれることを願っています。


### 訓練の進捗

435MB（1億800万トークン）のコーパスで訓練を開始し、今のところ順調です。トレインロスは最初の2800イテレーションで10.9から4.9に下がりました。完了まで約8～9時間かかると予想しています。終了次第、また更新します。

## 2025年7月17日

2回目のモデルの訓練が完了しました。私の4060では約8時間40分（3,900イテレーション/時）で33,000イテレーション（5エポック）を回しました。最終的なトレインロスは3.73でした。出力も驚くほど良く、実際に19世紀風の一貫した文を生成できるようになりました。

## 2025年7月28日

v0.5をHugging Faceにアップロードしました。[ぜひご覧ください](https://huggingface.co/haykgrigorian/TimeCapsuleLLM)。リポジトリをダウンロードしてローカルで実行できます。残念ながらnanoGPTはHuggingFaceとネイティブ互換ではないので、モデルをダウンロードしてローカルで動かす必要があります。

また、次回訓練用データの厳選も始めます。推論能力を得るには5～10倍のデータが必要だと思います。

## 2025年8月2日

まもなくバージョン1の開発を開始します。nanoGPTのアーキテクチャからより現代的なものに移行する必要があります。OpenLLaMA v3、Phi-2、Qwen 1.5Bなど、いくつかのオープンソースLLMアーキテクチャを検討中です。そしてV1への移行に備え、より大規模かつ多様なデータセットを厳選する必要があります。最低でも5GBのクリーンな訓練データが必要です。

# V0モデルの挙動と制限事項

初期のプロンプトでは、モデルが1800年代の言葉遣いや振る舞いで応答することが示されています。例えば、「Who art Henry?」と入力すると、「I know that man, I have did not a black, the storm.」と返されました。この文は意味が通じませんが、LLMが人について尋ねられていることを認識していることが分かります。

![TimeLockLLM サンプル出力](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

現代の概念についての言及はなく、出力には主に1800年代の単語や言い回しが含まれています。

まだ多くの作業が必要です。187MBのデータだけでトレーニングしても、複雑な推論ができるモデルにはなりません。

現時点では、文法構造が完全でない文や、全く意味の通じない文を生成しますが、これはトレーニングデータ量が少ないために普通のことです。

# V0.5モデルの挙動と制限事項

前回のモデルに比べて大きな進歩です。文体や語彙はヴィクトリア朝風で、ほぼ全ての文が文法的に正しく、適切な句読点も使われています。これもゼロからトレーニングしているため、1800年代の話題に限定されています。

![TimeLockLLM サンプル出力](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

事実の幻覚（ファクトハルシネーション）が多いです。詳細（日時、出来事、歴史上の人物など）のほとんど（ほぼ100%）が創作です。また、文同士のつながりがほとんどなく、せいぜい2つの文が関連する程度です。さらに「Digitized by Google」のようなフッターが時々混入することもあるので、次回のトレーニングではテキストをきちんとクリーンアップする必要があります。全体的には結果に満足しています。まだLLMには及びませんが、確かに文生成器にはなっています。

多くのことを学んでおり、今後数週間で改善点を見つけていく予定です。ファイルも近日中にアップロードします！

# 今後の予定

（完了）バージョン0.5の作業を開始します。50冊の本でトレーニングするのではなく、理想的には500～600冊で行います。現在は1800～1850年のロンドンの本を使ってnanoGPTでトレーニングしています。課題として、見つけた本が改訂されていないか、現代的な解釈が入っていないかを確認する必要がありますが、選んだ時代に出版された手つかずの本を探しています。

より大規模なコーパス（v0.5の5～10倍）で新しいモデル（v1）をトレーニングしたいと考えています。選択的時代別トレーニング（Selective Temporal Training）だけで推論能力が現れるかを見てみたいのですが、これはより難しいタスクであり、歴史データの制約から実現可能かはまだ分かりません。今後数週間で5～10GBのコーパス用データを収集する予定です。高品質なデータが十分に集まり、GPUを借りられれば、前進できると信じています。

# このプロジェクトの使い方

このプロジェクトは主に歴史データの収集、トレーニング用の準備、トークナイザーの構築に焦点を当てています。完全なLLMのトレーニング過程については触れませんので、Andrej Karpathy氏のnanoGPTを参照してください。

# ステップ1：歴史テキストの収集と準備

選んだ時代のパブリックドメインの本や文書などの.txtファイルを収集します（例：ロンドン1800-1850年）

必要であればdownload_texts_improved.pyを使って本をダウンロードできます。

スクリプトや手動でテキストファイルをクリーンアップし、Project Gutenbergのヘッダー/フッター、現代の注釈、OCRエラーなどを除去します。

prepare_dataset.pyは問題なく動作するはずです。

# ステップ2：カスタムトークナイザーの構築

クリーンなデータに対してtrain_tokenizer.pyまたはtrain_tokenizer_hf.pyを実行します。
これによりvocab.jsonとmerges.txtが得られます。

これらのファイルがモデルの語彙とマージルールを定義します。

# ステップ3：モデルのトレーニング（nanoGPT）

トレーニング手順は[Andrej Karpathy氏のnanoGPT](https://github.com/karpathy/nanoGPT)を参照してください。

他のLLMでもトレーニングできますが、私はnanoGPTを使用しました。

# FAQ

## Selective Temporal Trainingとは？

Selective Temporal Training（STT）とは、全てのトレーニングデータを特定の歴史的時代に限定して厳選する機械学習手法です。その時代特有の言語や知識を、現代的な概念の影響なしにモデリングするために行います。例えば、現行のモデル（v0.5）は1800～1875年のデータのみでトレーニングされており、ファインチューニングではなくゼロからの学習なので、その時代の言語様式や歴史的文脈が反映された出力となります。

## なぜファインチューニングやLoRAを使わないのですか？

本プロジェクトでは、現代的なバイアスのない言語モデルを作りたいからです。GPT-2のような既存モデルをファインチューニングしても、すでに事前学習された情報は消えません。ゼロからトレーニングすれば、モデルは「古風なふり」をするのではなく、本当にそうなります。今の目標は、1800～1850年にロンドンで出版された本の知識だけで推論できるものを作ることです。

## トレーニングにはどのようなデータを使いましたか？

1800～1850年ロンドンの本、法的文書、新聞、その他の著作物を使用しています。リンク先には約200件ありますが、最初のトレーニングでは50ファイル（約187MB）だけ使いました。ドキュメント一覧はこちらで見られます：
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## バージョン0モデルのサイズは？

このモデルは非常に小さく、現在は趣味の範囲で、現代のソースを使わない厳格なルールでトレーニングしています。パラメータは約1600万ですが、今後さらに古い文献を集めて新たなモデルをトレーニングする予定です。進捗があれば随時お知らせします。

## トレーニング環境は？

GPU: Geforce rtx 4060
CPU: i5-13400F 
RAM: 16GB DDR5.



---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-08-07

---