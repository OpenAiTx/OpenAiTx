
<div align="right">
  <details>
    <summary >üåê Idioma</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="#" title="Pr√≥ximamente">ÁπÅÈ´î‰∏≠Êñá (pr√≥ximamente)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="#" title="Pr√≥ximamente">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä (pr√≥ximamente)</a> |
        | <a href="#" title="Pr√≥ximamente">‡πÑ‡∏ó‡∏¢ (pr√≥ximamente)</a> |
        | <a href="#" title="Pr√≥ximamente">Fran√ßais (pr√≥ximamente)</a>
        | <a href="#" title="Pr√≥ximamente">Deutsch (pr√≥ximamente)</a>
        | <a href="#" title="Pr√≥ximamente">Espa√±ol (pr√≥ximamente)</a>
        | <a href="#" title="Pr√≥ximamente">Italiano (pr√≥ximamente)</a>
        | <a href="#" title="Pr√≥ximamente">–†—É—Å—Å–∫–∏–π (pr√≥ximamente)</a>
        | <a href="#" title="Pr√≥ximamente">Portugu√™s (pr√≥ximamente)</a>
        | <a href="#" title="Pr√≥ximamente">Nederlands (pr√≥ximamente)</a>
        | <a href="#" title="Pr√≥ximamente">Polski (pr√≥ximamente)</a>
        | <a href="#" title="Pr√≥ximamente">ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (pr√≥ximamente)</a>
        | <a href="#" title="Pr√≥ximamente">ŸÅÿßÿ±ÿ≥€å (pr√≥ximamente)</a>
        | <a href="#" title="Pr√≥ximamente">T√ºrk√ße (pr√≥ximamente)</a>
        | <a href="#" title="Pr√≥ximamente">Ti·∫øng Vi·ªát (pr√≥ximamente)</a>
        | <a href="#" title="Pr√≥ximamente">Bahasa Indonesia (pr√≥ximamente)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM

*Un modelo de lenguaje entrenado **desde cero** exclusivamente con datos de ciertos lugares y per√≠odos de tiempo para reducir el sesgo moderno y emular la voz, el vocabulario y la visi√≥n del mundo de la √©poca.*

Imagina si un modelo de IA no solo pretendiera ser hist√≥rico, sino que realmente lo fuera.

v0 y v0.5 construidos sobre [nanoGPT por Andrej Karpathy](https://github.com/karpathy/nanoGPT) Los scripts principales de entrenamiento y la arquitectura del modelo son de su autor√≠a. 

v1 construido sobre [Phi 1.5 de Microsoft](https://huggingface.co/microsoft/phi-1_5)


##  Comportamiento del modelo y limitaciones

### **v0**  

Las primeras pruebas muestran al modelo respondiendo con lenguaje y comportamiento de los a√±os 1800.
Ejemplo: Prompt: "¬øQui√©n es Henry?" y respondi√≥ "Conozco a ese hombre, no he hecho un negro, la tormenta."

![Salida de muestra de TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

- Sin menci√≥n de conceptos modernos  
- Mayormente vocabulario preciso de la √©poca  
- Las oraciones son en su mayor√≠a incoherentes (lo esperado para ~187MB de datos de entrenamiento)

### **v0.5** 

Una mejora significativa sobre v0.  
- Estilo de escritura victoriano, puntuaci√≥n adecuada, oraciones mayormente gramaticales  
- A√∫n alta tasa de alucinaci√≥n factual  
- Ruido de OCR (‚ÄúDigitized by Google‚Äù) todav√≠a presente en las salidas

![Salida de muestra de TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

### **v1**

Primer modelo en recordar y conectar un evento hist√≥rico real con una figura real del conjunto de datos.

Ejemplo: Prompt: "Era el a√±o de nuestro Se√±or 1834"

La salida: "Era el a√±o de nuestro Se√±or 1834 y las calles de Londres estaban llenas de protesta y petici√≥n. La causa, como muchos relataron, no estaba ligada a lo privado, sino que se hab√≠a tomado el mismo d√≠a en el d√≠a de Lord Palmerston, el p√∫blico recibir√° una breve declaraci√≥n de las dificultades bajo las cuales el d√≠a de la ley nos ha alcanzado. Es motivo de profundo pesar, que los presentes acontecimientos en la historia del mundo sean claros, y por consiguiente ser√°n conocidos. No es cierto que los mismos hombres que primero se asentaron en el Evangelio en Jerusal√©n debieran tener un registro tan extenso e interesante de la prosperidad y prosperidad"

Al principio asum√≠ que una protesta pudo haber ocurrido casualmente ese mismo a√±o pero mira esto: ![1834protesta](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)

### Por qu√© esto es importante:

Este es el primer ejemplo de uno de mis modelos conectando un a√±o con un evento hist√≥rico real y una persona real vinculada a ese evento (Lord Palmerston). Los modelos anteriores (v0 y v0.5) pod√≠an imitar estilos de escritura del siglo XIX pero siempre alucinaban eventos, personas y hechos. Esto demuestra que el modelo est√° comenzando a recordar cosas del conjunto de datos

## Pr√≥ximos planes
- Hay cerca de 175,000 textos publicados en Londres entre 1800-1875 en Internet Archive 
- Planeo expandir el corpus y limpiarlo m√°s para mejorar las capacidades de razonamiento
- Ampliando a diferentes regiones y per√≠odos para crear modelos hist√≥ricos m√°s variados


## C√≥mo usar

Este proyecto se enfoca principalmente en curar datos hist√≥ricos, prepararlos para el entrenamiento y construir un tokenizador. No voy a cubrir el proceso completo de entrenamiento de LLM, para eso consulta nanoGPT de Andrej Karpathy.

### Paso 1: Recolecta y prepara textos hist√≥ricos 

- Recopila archivos .txt de libros, documentos, etc. de dominio p√∫blico de tu per√≠odo elegido (por ejemplo, Londres 1800-1850) 
- Mantenlos dentro de tu ventana de tiempo/lugar elegida  
- Limpia los archivos de texto usando un script o elimina manualmente encabezados/pies de p√°gina de Project Gutenberg, anotaciones modernas o errores de OCR.

### Paso 2: Construye un tokenizador personalizado

- Ejecuta train_tokenizer.py o train_tokenizer_hf.py en los datos limpios.
- Esto te dar√° vocab.json y merges.txt
- Estos archivos definen el vocabulario y reglas de combinaci√≥n para tu modelo

### Paso 3: Entrena tu modelo 

- Consulta [nanoGPT de Andrej Karpathy](https://github.com/karpathy/nanoGPT) para el proceso de entrenamiento o la documentaci√≥n de la arquitectura que elijas.

# FAQ

## ¬øQu√© es el Entrenamiento Temporal Selectivo?

El Entrenamiento Temporal Selectivo (STT) es una metodolog√≠a de aprendizaje autom√°tico donde todos los datos de entrenamiento se seleccionan espec√≠ficamente para que pertenezcan a un per√≠odo hist√≥rico determinado. Se realiza para modelar el lenguaje y el conocimiento de esa √©poca sin influencia de conceptos modernos. Por ejemplo, el modelo actual que tengo (v0.5) est√° entrenado exclusivamente con datos de 1800-1875, no est√° ajustado finamente sino entrenado desde cero, lo que resulta en una salida que refleja el estilo ling√º√≠stico y el contexto hist√≥rico de ese per√≠odo.

## ¬øPor qu√© no usar fine-tuning o LoRA?

Para este proyecto intento crear un modelo de lenguaje libre de sesgos modernos. Si hago fine-tuning a algo como GPT-2, ya est√° preentrenado y esa informaci√≥n no desaparece. Si entreno desde cero el modelo no va a pretender ser antiguo, simplemente lo ser√°. El objetivo de este proyecto por ahora es crear algo que pueda razonar exclusivamente usando conocimiento de libros de Londres publicados entre 1800 y 1875.

## ¬øQu√© tipo de datos usaste para el entrenamiento?

Estoy utilizando libros, documentos legales, peri√≥dicos y otros escritos de Londres entre 1800‚Äì1875. La lista que enlac√© (para v0) tiene como 200, pero para el primer entrenamiento solo us√© 50 archivos, aproximadamente ~187 MB. Puedes ver una lista de los documentos:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt


Tama√±os de los conjuntos de datos:
v0: ~187MB
v0.5: ~435MB 
v1: ~6.25GB 

## ¬øQu√© tan grandes son los modelos?

V0: 16M Par√°metros

V0.5 123M Par√°metros

V1: 700M Par√°metros

# ¬øEspecificaciones de entrenamiento?

# V0/V0.5
GPU: Geforce rtx 4060
CPU: i5-13400F 
Ram: 16GB DDR5.

# V1
GPU: A100 alquilada














---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-08-19

---