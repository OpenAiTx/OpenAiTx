
<div align="right">
  <details>
    <summary >üåê Idioma</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="#" title="Coming soon">ÁπÅÈ´î‰∏≠Êñá (pr√≥ximamente)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="#" title="Coming soon">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä (pr√≥ximamente)</a> |
        | <a href="#" title="Coming soon">‡πÑ‡∏ó‡∏¢ (pr√≥ximamente)</a> |
        | <a href="#" title="Coming soon">Fran√ßais (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">Deutsch (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">Espa√±ol (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">Italiano (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">–†—É—Å—Å–∫–∏–π (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">Portugu√™s (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">Nederlands (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">Polski (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">ŸÅÿßÿ±ÿ≥€å (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">T√ºrk√ße (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">Ti·∫øng Vi·ªát (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (pr√≥ximamente)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM
Un LLM entrenado solo con datos de ciertos periodos de tiempo para reducir el sesgo moderno.

Imagina si un modelo de IA no solo pretendiera ser hist√≥rico, sino que realmente lo fuera.

Basado en [nanoGPT de Andrej Karpathy](https://github.com/karpathy/nanoGPT). Los scripts principales de entrenamiento y la arquitectura del modelo son obra suya.

# Objetivos del Proyecto

TimeCapsule LLM es un proyecto experimental que solo ser√° entrenado con textos escritos durante ciertos periodos hist√≥ricos. El objetivo es simular la cosmovisi√≥n y el lenguaje de √©pocas hist√≥ricas espec√≠ficas.

# Por qu√© el fine tuning no es suficiente

Si solo haces fine tuning de un modelo preentrenado, tu LLM seguir√° conociendo conceptos modernos. Por supuesto, lograr cero sesgo moderno es dif√≠cil, pero quiero acercarme lo m√°ximo posible a esto. Obtener ausencia de sesgo moderno requiere entrenar un modelo desde cero.

# Resultados esperados

Espero que, cuando est√© terminado, este modelo no conozca conceptos modernos y no sea capaz de razonar m√°s all√° de lo que ha sido entrenado. No deber√≠a reconocer conceptos/vocabulario modernos y espero que no alucine conocimientos actuales.

# Actualizaciones de Progreso

## 9 de julio de 2025

He establecido mi periodo de tiempo en 1800-1850 y regi√≥n: Londres

He recopilado una lista de textos, libros, documentos

Hasta ahora he conseguido 50 archivos txt y comenzar√© el entrenamiento de NanoGPT pronto

Actualizar√© esto mientras se siga avanzando

## 13 de julio de 2025

Entren√© nanoGPT con 187MB de datos de texto hist√≥ricos.

## 15 de julio de 2025

Comenc√© a descargar textos para la segunda ronda de entrenamiento. Estoy obteniendo todo del Internet Archive y ampli√© el periodo de tiempo a 1800-1875. Para obtener una gama diversa de textos, puedes usar los filtros de tema y b√∫squeda por lugar de publicaci√≥n, periodo de tiempo y temas en Internet Archive.

![Filtros de b√∫squeda](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## 16 de julio de 2025

Descargu√© alrededor de 500 archivos txt de Internet Archive y despu√©s de limpiarlos (solo eliminando espacios en blanco, cabeceras de Gutenberg, etc.) tengo alrededor de 500MB de datos. Es un conjunto de datos peque√±o pero la √∫ltima vez entren√© con 187MB as√≠ que deber√≠a haber al menos alguna diferencia notable en la salida despu√©s de entrenar el segundo modelo. Espero que este modelo pueda al menos producir oraciones m√°s coherentes que tengan algo de sentido. No es una garant√≠a, por supuesto, ya que sigue siendo un conjunto de datos muy peque√±o, pero es m√°s de lo que us√© la √∫ltima vez.

Esto deber√≠a poder hacerse en mi propio hardware, lo cual es bueno porque espero ver alguna mejora antes de pasar a un conjunto de datos mayor, lo que requerir√≠a alquilar una GPU. Pero no te preocupes, todav√≠a planeo alquilar una GPU pronto, pero antes de eso quiero asegurarme de que mi conjunto de datos est√© lo m√°s curado y limpio posible. Uno de los problemas que tengo es la limpieza, muchos de estos archivos txt tienen basura mezclada. Los scripts que he usado para limpiar funcionan pero no son 100% efectivos.

Entrenar√© este conjunto de datos hoy y deber√≠a tomar unas 4-5 horas. Una vez que termine y lo pruebe, dar√© actualizaciones. ¬°Gracias nuevamente a todos los que est√°n revisando mi proyecto, incluso ha habido personas que me han dado enlaces a recursos OCR, as√≠ que gracias! Espero que m√°s personas prueben esto y experimenten con sus propios conjuntos de datos.


### Actualizaci√≥n de Entrenamiento

Comenc√© el entrenamiento con un corpus de 435MB (108 M tokens), va bastante bien hasta ahora. La p√©rdida de entrenamiento baj√≥ de 10.9 a 4.9 en las primeras 2800 iteraciones. Espero que tome unas 8 o 9 horas completar. Publicar√© otra actualizaci√≥n una vez que termine.

## 17 de julio de 2025

El entrenamiento termin√≥ para el segundo modelo, mi 4060 tard√≥ unas 8 horas y 40 minutos (3.900 iteraciones/hora) para 33.000 iteraciones (5 √©pocas). La p√©rdida final de entrenamiento fue 3.73. Los resultados fueron sorprendentemente buenos; ahora realmente genera oraciones coherentes al estilo del siglo XIX.

## 28 de julio de 2025

He subido la versi√≥n v0.5 a Hugging Face, [√âchale un vistazo](https://huggingface.co/haykgrigorian/TimeCapsuleLLM) si quieres. Ahora puedes descargar mi repositorio y ejecutarlo localmente. Desafortunadamente nanoGPT no funciona de forma nativa con HuggingFace, as√≠ que tendr√°s que descargar y ejecutar el modelo localmente.

Tambi√©n comenzar√© a curar datos para mi pr√≥xima ronda de entrenamiento, creo que necesitar√© quiz√° de 5 a 10 veces m√°s datos para lograr capacidades de razonamiento.

## 2 de agosto de 2025

Pronto comenzar√© a trabajar en la Versi√≥n 1. Necesitar√© pasar de la arquitectura de nanoGPT a algo m√°s moderno. Tengo en mente varias arquitecturas LLM de c√≥digo abierto, incluidas: OpenLLaMA v3, Phi-2 y Qwen 1.5B. Y para soportar el salto a la V1, tendr√© que curar cuidadosamente un conjunto de datos mucho m√°s grande y diverso. Necesitar√© al menos 5GB de datos de entrenamiento limpios.

# Comportamiento y Limitaciones del Modelo V0

Los primeros prompts muestran que el modelo responde con lenguaje y comportamiento de los a√±os 1800. Por ejemplo, le pregunt√© "Who art Henry?" y respondi√≥ "I know that man, I have did not a black, the storm." y s√≠, esa frase no tiene sentido pero el LLM reconoce que estoy preguntando por una persona.

![Salida de muestra de TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

No hay menci√≥n de conceptos modernos, las salidas contienen mayormente palabras y frases propias de los a√±os 1800.

Todav√≠a necesita mucho trabajo, entrenar con solo 187MB no te dar√° un modelo que produzca texto con razonamiento complejo.

Por ahora produce frases que carecen de estructura de oraci√≥n completa y en general no tienen sentido, pero esto es normal para el tama√±o de entrenamiento.

# Comportamiento y Limitaciones del Modelo V0.5

Esto es una mejora considerable comparado con el modelo anterior. El estilo de escritura y vocabulario es victoriano y casi todas las frases son gramaticalmente correctas con la puntuaci√≥n adecuada. Y nuevamente esto est√° entrenado desde cero as√≠ que se apega a temas de los a√±os 1800.

![Salida de muestra de TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

Hay muchas alucinaciones de hechos. Muchas (como el 100%) de los detalles (fechas, eventos, personajes hist√≥ricos) son inventados. Adem√°s, las frases realmente no tienen conexi√≥n entre s√≠, a veces tal vez 2 frases se relacionan pero m√°s all√° de eso no. Otro problema es que a veces aparece un pie de p√°gina errante de ‚ÄúDigitized by Google‚Äù, as√≠ que la pr√≥xima vez que entrene tengo que asegurarme de limpiar bien los textos. En general estoy muy contento con los resultados, a√∫n est√° lejos de ser un LLM pero definitivamente es un generador de frases.

Estoy aprendiendo mucho y comenzar√© a descubrir qu√© debo mejorar en las pr√≥ximas semanas. ¬°Pronto subir√© archivos!

# Planes Pr√≥ximos

(Completado) Voy a empezar a trabajar en la versi√≥n 0.5, en vez de entrenar usando 50 libros, entrenar√© idealmente usando 500-600. Ahora mismo estoy entrenando nanoGPT usando libros de 1800-1850 y espec√≠ficamente de Londres. Hay algunos desaf√≠os como asegurarme de que los libros que encuentre no est√©n actualizados o tengan interpretaciones modernas sino que sean libros intactos publicados dentro del per√≠odo que eleg√≠.

Quiero entrenar un nuevo modelo (v1) con un corpus mucho m√°s grande, tal vez 5-10 veces m√°s grande que el que us√© para v0.5. Mi objetivo es ver si puedo lograr que emerjan habilidades de razonamiento solo a partir del Entrenamiento Temporal Selectivo, esto ser√° una tarea m√°s dif√≠cil y ni siquiera estoy seguro de si es posible debido a las limitaciones de datos hist√≥ricos. En las pr√≥ximas semanas tratar√© de curar suficientes datos para un corpus de 5-10GB. Creo que si puedo obtener datos limpios y de alta calidad y alquilar una GPU, habr√° progreso.

# C√≥mo Usar Este Proyecto

Este proyecto se centra principalmente en curar datos hist√≥ricos, prepararlos para el entrenamiento y construir un tokenizador. No voy a cubrir el proceso completo de entrenamiento de LLM, para eso consulta nanoGPT de Andrej Karpathy.

# Paso 1: Reunir y Preparar Textos Hist√≥ricos

Recoge archivos .txt de libros de dominio p√∫blico, documentos, etc. de tu per√≠odo de tiempo elegido (por ejemplo, Londres 1800-1850)

Puedes usar download_texts_improved.py para descargar libros si lo necesitas.

Limpia los archivos de texto usando un script o manualmente elimina encabezados/pies de p√°gina de Project Gutenberg, anotaciones modernas o errores de OCR.

prepare_dataset.py deber√≠a funcionar bien.

# Paso 2: Construir un Tokenizador Personalizado

Ejecuta train_tokenizer.py o train_tokenizer_hf.py en los datos limpios.
Esto te dar√° vocab.json y merges.txt

Estos archivos definen el vocabulario y las reglas de combinaci√≥n para tu modelo.

# Paso 3: Entrena tu Modelo (nanoGPT)

Consulta [nanoGPT de Andrej Karpathy](https://github.com/karpathy/nanoGPT) para el proceso de entrenamiento.

Puedes entrenar un LLM diferente si lo deseas, pero yo us√© nanoGPT

# Preguntas Frecuentes

## ¬øQu√© es el Entrenamiento Temporal Selectivo?

El Entrenamiento Temporal Selectivo (STT) es una metodolog√≠a de aprendizaje autom√°tico donde todos los datos de entrenamiento se curan espec√≠ficamente para que pertenezcan a un per√≠odo hist√≥rico concreto. Se hace para modelar el lenguaje y conocimiento de esa √©poca sin influencia de conceptos modernos. Por ejemplo, el modelo actual que tengo (v0.5) est√° entrenado en datos exclusivamente de 1800-1875, no est√° ajustado sino entrenado desde cero, dando como resultado una salida que refleja el estilo ling√º√≠stico y contexto hist√≥rico de ese per√≠odo.

## ¬øPor qu√© no simplemente usar fine-tuning o LoRA?

Para este proyecto estoy tratando de crear un modelo de lenguaje que no est√© sesgado por lo moderno. Si hago fine-tuning a algo como GPT-2, ya est√° preentrenado y esa informaci√≥n no se ir√°. Si entreno desde cero el modelo de lenguaje no va a pretender ser antiguo, simplemente lo ser√°. El objetivo de este proyecto por ahora es crear algo que pueda razonar exclusivamente usando el conocimiento de libros de Londres publicados entre 1800 y 1850.

## ¬øQu√© tipo de datos usaste para el entrenamiento?

Estoy usando libros, documentos legales, peri√≥dicos y otros escritos de Londres de 1800‚Äì1850. La lista que enlac√© tiene como 200, pero para el primer entrenamiento solo us√© 50 archivos, unos ~187 MB. Puedes ver una lista de los documentos:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## ¬øQu√© tan grande es el modelo Versi√≥n 0?

Este modelo es muy peque√±o por ahora, solo lo hago por diversi√≥n y sigo una regla estricta de entrenamiento sin fuentes modernas. Tiene casi 16 millones de par√°metros pero voy a empezar a reunir m√°s textos antiguos para comenzar otro entrenamiento de modelo. Ir√© dando actualizaciones a medida que avance.

## ¬øEspecificaciones de Entrenamiento?

GPU: Geforce rtx 4060
CPU: i5-13400F
Ram: 16GB DDR5.



---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-08-07

---