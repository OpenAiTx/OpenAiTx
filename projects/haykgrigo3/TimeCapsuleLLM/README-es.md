<div align="right">
  <details>
    <summary >üåê Idioma</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="#" title="Coming soon">ÁπÅÈ´î‰∏≠Êñá (pr√≥ximamente)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="#" title="Coming soon">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä (pr√≥ximamente)</a> |
        | <a href="#" title="Coming soon">‡πÑ‡∏ó‡∏¢ (pr√≥ximamente)</a> |
        | <a href="#" title="Coming soon">Fran√ßais (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">Deutsch (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">Espa√±ol (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">Italiano (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">–†—É—Å—Å–∫–∏–π (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">Portugu√™s (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">Nederlands (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">Polski (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">ŸÅÿßÿ±ÿ≥€å (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">T√ºrk√ße (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">Ti·∫øng Vi·ªát (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (pr√≥ximamente)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM
Un LLM entrenado solo con datos de ciertos periodos de tiempo para reducir el sesgo moderno.

Imagina si un modelo de IA no solo pretendiera ser hist√≥rico sino que realmente lo fuera.

Construido sobre [nanoGPT por Andrej Karpathy](https://github.com/karpathy/nanoGPT) Los scripts de entrenamiento principales y la arquitectura del modelo son su trabajo.

# Objetivos del Proyecto

TimeCapsule LLM es un proyecto experimental que solo ser√° entrenado con textos escritos durante ciertos periodos hist√≥ricos. El objetivo es simular la visi√≥n del mundo y el lenguaje de √©pocas hist√≥ricas espec√≠ficas.

# Por qu√© el ajuste fino no es suficiente

Si solo ajustas un modelo preentrenado, tu LLM todav√≠a conocer√° conceptos modernos. Por supuesto, lograr cero sesgo moderno es dif√≠cil, pero quiero acercarme lo m√°s posible a esto. Eliminar el sesgo moderno requiere entrenar un modelo desde cero.

# Resultados esperados

Con suerte, cuando est√© terminado, este modelo no conocer√° conceptos modernos y no podr√° razonar m√°s all√° de lo que ha sido entrenado. No deber√≠a reconocer conceptos/vocabulario modernos y espero que no alucine conocimiento moderno.

# Actualizaciones de Progreso

## 9 de julio de 2025

He establecido mi periodo de tiempo en 1800-1850 y regi√≥n: Londres

He reunido una lista de textos, libros, documentos

Hasta ahora he conseguido 50 en archivos txt y pronto comenzar√© a entrenar NanoGPT

Actualizar√© esto siempre que haya progreso

## 13 de julio de 2025

Entren√© nanoGPT con 187 MB de datos hist√≥ricos en texto.

## 15 de julio de 2025

Comenc√© a descargar textos para la segunda ronda de entrenamiento. Estoy obteniendo todo del Internet Archive y he ampliado el periodo de tiempo a 1800-1875. Para obtener una gama diversa de textos, puedes usar filtros de tema y b√∫squeda por ubicaci√≥n de publicaci√≥n, periodo de tiempo y temas en Internet Archive.

![Filtros de B√∫squeda](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## 16 de julio de 2025

Descargu√© alrededor de 500 archivos txt de Internet Archive y despu√©s de limpiarlos (solo eliminando espacios en blanco, encabezados de Gutenberg, etc.) tengo alrededor de 500 MB de datos. Es un conjunto de datos peque√±o pero la √∫ltima vez entren√© con 187 MB as√≠ que deber√≠a haber al menos alguna diferencia notable en la salida despu√©s de entrenar el segundo modelo. Espero que este modelo pueda al menos producir oraciones m√°s coherentes que tengan algo de sentido. Por supuesto, no es una garant√≠a ya que sigue siendo un conjunto de datos muy peque√±o, pero es m√°s de lo que us√© la vez pasada.

Esto deber√≠a ser posible con mi propio hardware, lo cual es bueno porque espero ver alg√∫n tipo de mejora antes de pasar a un conjunto de datos m√°s grande que requerir√≠a alquilar una GPU. Pero no te preocupes, todav√≠a planeo alquilar una GPU pronto, pero antes de hacerlo quiero asegurarme de que mi conjunto de datos est√© lo m√°s curado y limpio posible. Uno de los problemas que tengo es la limpieza, muchos de estos archivos txt tienen textos sin sentido mezclados. Los scripts que he usado para limpiar funcionan pero no son 100% efectivos.

Entrenar√© este conjunto de datos hoy y deber√≠a tomar alrededor de 4-5 horas. Una vez que termine y lo pruebe, dar√© actualizaciones. ¬°Gracias de nuevo a todos los que est√°n viendo mi proyecto, incluso ha habido personas que me han enviado enlaces a recursos de OCR as√≠ que gracias! Espero que m√°s gente pruebe esto y experimente con sus propios conjuntos de datos.

### Actualizaci√≥n de Entrenamiento

Comenc√© a entrenar con un corpus de 435 MB (108 M de tokens), va bastante bien hasta ahora. La p√©rdida de entrenamiento baj√≥ de 10.9 a 4.9 en las primeras 2800 iteraciones. Espero que tome alrededor de 8 o 9 horas completar. Publicar√© otra actualizaci√≥n una vez que termine.

## 17 de julio de 2025 2:13AM

El entrenamiento termin√≥ para el segundo modelo, a mi 4060 le tom√≥ alrededor de 8 horas y 40 minutos (3,900 iteraciones/hora) para 33,000 iteraciones (5 √©pocas). La p√©rdida final de entrenamiento fue de 3.73. Las salidas fueron sorprendentemente buenas, ahora realmente genera oraciones coherentes al estilo del siglo XIX.

# Comportamiento y Limitaciones del Modelo V0

Las primeras pruebas muestran que el modelo responde con lenguaje y comportamiento de los a√±os 1800. Por ejemplo, lo prob√© con "Who art Henry?" y respondi√≥ "I know that man, I have did not a black, the storm." y s√≠, esa oraci√≥n no tiene sentido pero el LLM est√° reconociendo que le estoy preguntando por una persona.

![Salida de Muestra de TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

No hay menci√≥n de conceptos modernos, las salidas contienen principalmente palabras y frases de los a√±os 1800.

A√∫n necesita mucho trabajo, entrenar con 187 MB no te dar√° un modelo que produzca texto con razonamiento complejo.

Ahora mismo produce oraciones que carecen de estructura completa y en general simplemente no tienen sentido, pero esto es normal para este tama√±o de entrenamiento.

# Comportamiento y Limitaciones del Modelo V0.5

Esta es una mejora notable en comparaci√≥n con el modelo anterior. El estilo de escritura y el vocabulario son victorianos y casi todas las oraciones son gramaticalmente correctas y con la puntuaci√≥n adecuada. Y de nuevo, este modelo est√° entrenado desde cero, por lo que se ci√±e a temas de los a√±os 1800.

![Salida de muestra de TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

Hay muchas alucinaciones factuales. Muchas (como el 100%) de los detalles (fechas, eventos, figuras hist√≥ricas) son inventados. Adem√°s, las oraciones realmente no tienen conexiones entre s√≠, a veces tal vez 2 oraciones se relacionan, pero m√°s all√° de eso no lo hacen. Otro problema es que a veces aparece un pie de p√°gina errante de ‚ÄúDigitized by Google‚Äù, as√≠ que la pr√≥xima vez que entrene realmente tengo que asegurarme de limpiar bien los textos. En general, estoy muy contento con los resultados; todav√≠a no es un LLM pero definitivamente es un generador de oraciones.

Estoy aprendiendo mucho y en las pr√≥ximas semanas empezar√© a averiguar qu√© necesito mejorar. ¬°Pronto subir√© archivos!

# Planes Futuros

(Completado) Voy a empezar a trabajar en la versi√≥n 0.5, en lugar de entrenar usando 50 libros, entrenar√© usando idealmente 500-600. Ahora mismo estoy entrenando nanoGPT usando libros de 1800-1850 y espec√≠ficamente de Londres. Hay algunos desaf√≠os, como asegurarme de que los libros que encuentro no hayan sido actualizados o tengan interpretaciones modernas, sino libros intactos publicados dentro del periodo de tiempo que eleg√≠.

Quiero entrenar un nuevo modelo (v1) con un corpus mucho m√°s grande, tal vez de 5 a 10 veces mayor que el que us√© para v0.5. Mi objetivo es ver si puedo lograr que emerjan habilidades de razonamiento s√≥lo a partir del Entrenamiento Temporal Selectivo, ser√° una tarea m√°s dif√≠cil y ni siquiera estoy seguro de si es posible debido a las limitaciones de datos hist√≥ricos. En las pr√≥ximas semanas intentar√© recopilar suficiente informaci√≥n para un corpus de 5-10GB. Creo que si puedo obtener datos limpios y de alta calidad y alquilar una GPU, habr√° avances.

# C√≥mo Usar Este Proyecto

Este proyecto se centra principalmente en recopilar datos hist√≥ricos, prepararlos para el entrenamiento y construir un tokenizador. No voy a cubrir todo el proceso de entrenamiento de un LLM; para eso, consulta nanoGPT de Andrej Karpathy.

# Paso 1: Reunir y Preparar Textos Hist√≥ricos

Re√∫ne archivos .txt de libros de dominio p√∫blico, documentos, etc. del periodo de tiempo elegido (por ejemplo, Londres 1800-1850)

Puedes usar download_texts_improved.py para descargar libros si lo necesitas.

Limpia los archivos de texto usando un script o elimina manualmente encabezados/pies de p√°gina de Project Gutenberg, anotaciones modernas o errores de OCR.

prepare_dataset.py deber√≠a funcionar bien.

# Paso 2: Construir un Tokenizador Personalizado

Ejecuta train_tokenizer.py o train_tokenizer_hf.py sobre los datos limpios.
Esto te dar√° vocab.json y merges.txt

Estos archivos definen el vocabulario y las reglas de fusi√≥n para tu modelo

# Paso 3: Entrena Tu Modelo (nanoGPT)

Consulta [nanoGPT de Andrej Karpathy](https://github.com/karpathy/nanoGPT) para el proceso de entrenamiento.

Puedes entrenar un LLM diferente si quieres, pero yo us√© nanoGPT

# Preguntas Frecuentes

## ¬øQu√© es el Entrenamiento Temporal Selectivo?

El Entrenamiento Temporal Selectivo (STT) es una metodolog√≠a de aprendizaje autom√°tico donde todos los datos de entrenamiento son seleccionados espec√≠ficamente para que caigan dentro de un periodo hist√≥rico concreto. Se hace para modelar el lenguaje y el conocimiento de esa √©poca sin la influencia de conceptos modernos. Por ejemplo, el modelo que tengo ahora (v0.5) est√° entrenado con datos exclusivamente de 1800-1875; no est√° afinado, sino entrenado desde cero, lo que da como resultado una salida que refleja el estilo ling√º√≠stico y el contexto hist√≥rico de ese periodo.

## ¬øPor qu√© no simplemente usar fine-tuning o LoRA?

Para este proyecto intento crear un modelo de lenguaje sin sesgos modernos. Si hago fine-tuning a algo como GPT-2, ya est√° preentrenado y esa informaci√≥n no se elimina. Si entreno desde cero, el modelo de lenguaje no fingir√° ser antiguo, simplemente lo ser√°. El objetivo de este proyecto ahora es crear algo que pueda razonar exclusivamente usando conocimiento de libros de Londres publicados entre 1800 y 1850.

## ¬øQu√© tipo de datos usaste para el entrenamiento?

Estoy usando libros, documentos legales, peri√≥dicos y otros escritos de Londres entre 1800 y 1850. La lista que enlac√© tiene como 200, pero para el primer entrenamiento s√≥lo us√© 50 archivos de unos ~187 MB. Puedes ver una lista de los documentos:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## ¬øQu√© tama√±o tiene el modelo Versi√≥n 0?

Este modelo es muy peque√±o por ahora, s√≥lo lo hago por diversi√≥n y siguiendo una estricta regla de entrenamiento sin fuentes modernas. Tiene casi 16 millones de par√°metros, pero voy a empezar a recopilar m√°s textos antiguos para iniciar otro entrenamiento de modelo. Ir√© dando actualizaciones a medida que avance.

## ¬øEspecificaciones de entrenamiento?

GPU: Geforce rtx 4060
CPU: i5-13400F
Ram: 16GB DDR5.



---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-21

---