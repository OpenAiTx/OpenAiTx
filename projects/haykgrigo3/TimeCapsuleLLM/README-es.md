
<div align="right">
  <details>
    <summary >üåê Idioma</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="#" title="Pr√≥ximamente">ÁπÅÈ´î‰∏≠Êñá (pr√≥ximamente)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="#" title="Pr√≥ximamente">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä (pr√≥ximamente)</a> |
        | <a href="#" title="Pr√≥ximamente">‡πÑ‡∏ó‡∏¢ (pr√≥ximamente)</a> |
        | <a href="#" title="Pr√≥ximamente">Fran√ßais (pr√≥ximamente)</a>
        | <a href="#" title="Pr√≥ximamente">Deutsch (pr√≥ximamente)</a>
        | <a href="#" title="Pr√≥ximamente">Espa√±ol (pr√≥ximamente)</a>
        | <a href="#" title="Pr√≥ximamente">Italiano (pr√≥ximamente)</a>
        | <a href="#" title="Pr√≥ximamente">–†—É—Å—Å–∫–∏–π (pr√≥ximamente)</a>
        | <a href="#" title="Pr√≥ximamente">Portugu√™s (pr√≥ximamente)</a>
        | <a href="#" title="Pr√≥ximamente">Nederlands (pr√≥ximamente)</a>
        | <a href="#" title="Pr√≥ximamente">Polski (pr√≥ximamente)</a>
        | <a href="#" title="Pr√≥ximamente">ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (pr√≥ximamente)</a>
        | <a href="#" title="Pr√≥ximamente">ŸÅÿßÿ±ÿ≥€å (pr√≥ximamente)</a>
        | <a href="#" title="Pr√≥ximamente">T√ºrk√ße (pr√≥ximamente)</a>
        | <a href="#" title="Pr√≥ximamente">Ti·∫øng Vi·ªát (pr√≥ximamente)</a>
        | <a href="#" title="Pr√≥ximamente">Bahasa Indonesia (pr√≥ximamente)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM

*Un modelo de lenguaje entrenado **desde cero** exclusivamente con datos de ciertos lugares y per√≠odos de tiempo para reducir el sesgo moderno y emular la voz, el vocabulario y la visi√≥n del mundo de la √©poca.*

Imagina si un modelo de IA no solo pretendiera ser hist√≥rico, sino que realmente lo fuera.

v0 y v0.5 construidos sobre [nanoGPT por Andrej Karpathy](https://github.com/karpathy/nanoGPT) Los scripts principales de entrenamiento y la arquitectura del modelo son de su autor√≠a. 

v1 construido sobre [Phi 1.5 de Microsoft](https://huggingface.co/microsoft/phi-1_5)

[Enlace a Hugging Face](https://huggingface.co/haykgrigorian/TimeCapsuleLLM)

##  Comportamiento y Limitaciones del Modelo

### **v0**  

Las primeras pruebas muestran que el modelo responde con lenguaje y comportamiento de los a√±os 1800.  
Ejemplo: Prompt: "Who art Henry?" y respondi√≥ "I know that man, I have did not a black, the storm."  

![Salida de ejemplo de TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

- Sin menci√≥n de conceptos modernos  
- Vocabulario mayormente preciso de la √©poca  
- Las oraciones son mayormente incoherentes (esperado por ~187MB de datos de entrenamiento)

### **v0.5** 

Una mejora significativa sobre v0.  
- Estilo victoriano de escritura, puntuaci√≥n adecuada, oraciones mayormente gramaticales  
- A√∫n alta tasa de alucinaci√≥n factual  
- Ruido de OCR (‚ÄúDigitized by Google‚Äù) a√∫n presente en las salidas

![Salida de ejemplo de TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

### **v1**

Primer modelo en recordar y conectar un evento hist√≥rico real con una figura real del conjunto de datos.

Ejemplo: Prompt: "It was the year of our Lord 1834"  

La salida: "It was the year of our Lord 1834 and the streets of London were filled with protest and petition. The cause, as many re counted, was not bound in the way of private, but having taken up the same day in the day of Lord Palmerston, the public will receive a short statement of the difficulties under which the day of law has reached us. It is a matter of deep regret, that the present events in the history of the world are clear, and consequently will be'known. It is not true that the very men who first settled in the Gospel at Jerusalem should have so extensive and so interesting a record of the prosperity and prosperity"  

Al principio asum√≠ que una protesta podr√≠a haber ocurrido casualmente ese mismo a√±o, pero mira esto: ![1834protest](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)

### Por qu√© esto importa:

Este es el primer ejemplo de uno de mis modelos conectando un a√±o tanto con un evento hist√≥rico real como con una persona real vinculada a ese evento (Lord Palmerston). Los modelos anteriores (v0 y v0.5) pod√≠an imitar los estilos de escritura del siglo XIX pero siempre alucinaban eventos, personas y hechos. Esto muestra que el modelo est√° comenzando a recordar cosas del conjunto de datos.


## Planes futuros

- Hay casi 175,000 textos publicados en Londres de 1800 a 1875 en Internet Archive
- Planeo expandir el corpus y limpiarlo m√°s para mejorar las habilidades de razonamiento
- Expandirse a diferentes regiones y per√≠odos de tiempo para modelos hist√≥ricos m√°s variados

## C√≥mo usar

Este proyecto se centra principalmente en la curaci√≥n de datos hist√≥ricos, prepar√°ndolos para el entrenamiento y construyendo un tokenizador. No voy a cubrir el proceso completo de entrenamiento de LLM, para eso rem√≠tase a nanoGPT de Andrej Karpathy.

### Paso 1: Reunir y preparar textos hist√≥ricos

- Recoge archivos .txt de libros de dominio p√∫blico, documentos, etc., de tu per√≠odo elegido (por ejemplo, Londres 1800-1850)
- Mant√©n los textos dentro de tu ventana temporal/geogr√°fica seleccionada
- Limpia los archivos de texto usando un script o elimina manualmente encabezados/pies de p√°gina de Project Gutenberg, anotaciones modernas o errores de OCR.

### Paso 2: Construir un tokenizador personalizado

- Ejecuta train_tokenizer.py o train_tokenizer_hf.py en los datos limpios.
- Esto te dar√° vocab.json y merges.txt
- Estos archivos definen el vocabulario y las reglas de combinaci√≥n para tu modelo

### Paso 3: Entrena tu modelo

- Consulta [nanoGPT de Andrej Karpathy](https://github.com/karpathy/nanoGPT) para el proceso de entrenamiento o la documentaci√≥n de la arquitectura que elijas.

# FAQ

## ¬øQu√© es el entrenamiento temporal selectivo?

El Entrenamiento Temporal Selectivo (STT) es una metodolog√≠a de aprendizaje autom√°tico donde todos los datos de entrenamiento se seleccionan espec√≠ficamente para caer dentro de un per√≠odo hist√≥rico definido. Se realiza para modelar el lenguaje y conocimiento de esa √©poca sin influencia de conceptos modernos. Por ejemplo, el modelo actual (v0.5) est√° entrenado exclusivamente con datos de 1800-1875, no est√° ajustado finamente sino entrenado desde cero, lo que resulta en una salida que refleja el estilo ling√º√≠stico y contexto hist√≥rico de ese per√≠odo.

## ¬øPor qu√© no simplemente usar ajuste fino o LoRA?

Para este proyecto estoy tratando de crear un modelo de lenguaje sin sesgos modernos. Si ajusto finamente algo como GPT-2, ya est√° preentrenado y esa informaci√≥n no se elimina. Si entreno desde cero, el modelo de lenguaje no simular√° ser antiguo, simplemente lo ser√°. El objetivo de este proyecto actualmente es crear algo que pueda razonar exclusivamente usando conocimientos de libros publicados en Londres entre 1800 y 1875.

## ¬øQu√© tipo de datos usaste para el entrenamiento?


Estoy utilizando libros, documentos legales, peri√≥dicos y otros escritos de Londres entre 1800 y 1875. La lista que enlac√© (para v0) tiene como 200, pero para el primer entrenamiento solo us√© 50 archivos de aproximadamente ~187 MB. Puedes ver una lista de los documentos:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

Tama√±os del conjunto de datos:
v0: ~187MB
v0.5: ~435MB 
v1: ~6.25GB 

## ¬øQu√© tan grandes son los modelos?

V0: 16M Par√°metros

V0.5 123M Par√°metros

V1: 700M Par√°metros

# ¬øEspecificaciones de entrenamiento?

# V0/V0.5
GPU: Geforce rtx 4060
CPU: i5-13400F 
Ram: 16GB DDR5.

# V1
GPU: A100 alquilada















---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-09-30

---