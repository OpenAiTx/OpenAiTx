<div align="right">
  <details>
    <summary >üåê Idioma</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-TW">ÁπÅÈ´î‰∏≠Êñá</a> 
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=as">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä</a> 
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=th">‡πÑ‡∏ó‡∏¢</a> 
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=fr">Fran√ßais</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=es">Espa√±ol</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=it">Italiano</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=ru">–†—É—Å—Å–∫–∏–π</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=pt">Portugu√™s</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=ar">ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</a>
        | <a href="#" title="Coming soon">ŸÅÿßÿ±ÿ≥€å (pr√≥ximamente)</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=tr">T√ºrk√ße</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=vi">Ti·∫øng Vi·ªát</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (pr√≥ximamente)</a>

      </div>
    </div>
  </details>
</div>


# TimeCapsule LLM

*Un modelo de lenguaje entrenado **desde cero** exclusivamente con datos de ciertos lugares y per√≠odos de tiempo para reducir el sesgo moderno y emular la voz, vocabulario y visi√≥n del mundo de la √©poca.*

Imagina si un modelo de IA no solo fingiera ser hist√≥rico, sino que realmente lo fuera.

v0 y v0.5 construidos sobre [nanoGPT por Andrej Karpathy](https://github.com/karpathy/nanoGPT) Los scripts de entrenamiento principales y la arquitectura del modelo son obra suya. 

v1 construido sobre [Phi 1.5 de Microsoft](https://huggingface.co/microsoft/phi-1_5)

[Enlace a Hugging Face](https://huggingface.co/haykgrigorian/TimeCapsuleLLM)

##  Comportamiento del Modelo y Limitaciones

### **v0**  

Las primeras indicaciones muestran que el modelo responde con lenguaje y comportamiento de los a√±os 1800.
Ejemplo: Indicaci√≥n: "Who art Henry?" y respondi√≥ "I know that man, I have did not a black, the storm."

![Salida de muestra de TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

- Sin menci√≥n de conceptos modernos  
- Vocabulario mayormente preciso para la √©poca  
- Las oraciones son en su mayor√≠a incoherentes (esperado para ~187MB de datos de entrenamiento)

### **v0.5** 

Una mejora significativa respecto a v0.  
- Estilo de escritura victoriano, puntuaci√≥n adecuada, oraciones mayormente gramaticales  
- A√∫n alta tasa de alucinaci√≥n factual  
- Ruido de OCR (‚ÄúDigitized by Google‚Äù) a√∫n presente en las salidas

![Salida de muestra de TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

### **v1**

Primer modelo que recuerda y conecta un evento hist√≥rico real con una figura aut√©ntica del conjunto de datos.

Ejemplo: Indicaci√≥n: "It was the year of our Lord 1834"

La salida: "It was the year of our Lord 1834 and the streets of London were filled with protest and petition. The cause, as many re counted, was not bound in the way of private, but having taken up the same day in the day of Lord Palmerston, the public will receive a short statement of the difficulties under which the day of law has reached us. It is a matter of deep regret, that the present events in the history of the world are clear, and consequently will be'known. It is not true that the very men who first settled in the Gospel at Jerusalem should have so extensive and so interesting a record of the prosperity and prosperity"

Al principio supuse que una protesta podr√≠a haber ocurrido casualmente ese mismo a√±o, pero mira esto: ![1834protesta](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)

### **v2mini-eval1**

Entrenado usando una muestra de 15GB del conjunto de datos de 90GB de v2.

Modelo entrenado solo hasta 10K pasos.

Hubo un problema con la tokenizaci√≥n que hace que la salida se vea as√≠:

INDICACI√ìN:
¬øQui√©n es Charles Dickens?


W ho is Charles D ic ens ? D oes that work more of h ise x cell ent st ir ring , in his pl ays , int he G reat C omp any 's f arm ? What I have y et to qu ote from J ack P ick ett ? D oy oun ot know th att hey were as pe cies of g all oping , ors aw ing oft heir bre asts , or what w ast ob ed one about the t im ew hen J acks on was looking ont hew ind ow ? What ist hes uccess of an English man , and which h isson has not be end is cover ing t ome , where by to acc omp lish such a we ight ? D idy ou ever m ake ap ass age int othe old road way , ort o an anch or - b ree ze att he foot of our boat , which you m ust le ave us ? The fact is , that whe ther the wind would rise up from the pl ain on S atur day night orn o ont ide , ort ill then orth , or other wise , wes hall be com pelled t od esc ribe af orm idable bar rier , with t hes ame e ffect s ast he p resent . In th iss itu ation , at le ast , itis not to omuch t os ay that we have le ft that room . I bel ieve there are three c op ies int he " F ive H undred - fold ," to bere fer red to , ast he first n umber of our read ers wh ow is ht
Corregido:

"¬øQui√©n es Charles Dickens? ¬øHace ese trabajo m√°s de su excelente agitaci√≥n, en sus obras, en la gran granja de la Compa√±√≠a? ¬øQu√© tengo a√∫n que citar de Jack Pickett? ¬øNo sabes que eran una especie de galopar, o de serrar sus pechos, o qu√© se deb√≠a hacer en la √©poca en que Jackson miraba por la ventana? ¬øCu√°l es el √©xito de un ingl√©s, y cu√°l su hijo no ha descubierto para m√≠, por lo que ha de lograr tal peso? ¬øAlguna vez has hecho un pasaje hacia el viejo camino, o hacia una brisa de ancla al pie de nuestro bote, que debes dejarnos? El hecho es que, tanto si el viento se levanta desde la llanura el s√°bado por la noche o al mediod√≠a, o hacia el norte, o de otra manera, nos veremos obligados a describir una barrera formidable, con los mismos efectos que los actuales. En esta situaci√≥n, al menos, no es exagerado decir que hemos dejado esa habitaci√≥n. Creo que hay tres copias en el 'Quinientos veces', a las que se debe referir, como el primer n√∫mero de nuestros lectores que deseen t[o]..."

### **v2mini-eval2**

Entrenado usando la misma muestra de 15GB utilizada para entrenar eval1.
El modelo se entrena nuevamente solo hasta 10K pasos.

PROMPT:Charles Darwin

Charles DarwinECCEMACY. Se√±or, ‚Äî El siguiente caso me resulta interesante: ‚Äî Estuve en Londres dos semanas y me afect√≥ mucho un ataque de reumatismo. El primer ataque de reumatismo fue una semana antes de verlo, y el segundo cuando lo vi, y el tercero cuando lo vi, y el tercero en el mismo tiempo. El segundo ataque de gota, sin embargo, no estuvo acompa√±ado de ning√∫n s√≠ntoma febril, pero s√≠ de un aumento en el flujo de orina, y una descarga m√°s copiosa de orina. El tercer ataque fue una hora despu√©s de verlo, y fue seguido por el regreso de un paroxismo de gota, y un retorno m√°s r√°pido de la gota. El cuarto ataque tambi√©n estuvo acompa√±ado de fiebre, pero no siempre con s√≠ntomas febriles. El tercer ataque de gota fue dos semanas despu√©s de que usted estuvo enfermo, y el cuarto fue seguido por un paroxismo de gota. El cuarto ataque fue dos semanas despu√©s de que usted fue atacado, y estuvo acompa√±ado por una sensaci√≥n


##  Conjuntos de datos

### **v2**

- 90GB de textos londinenses de 1800-1875
- 136,344 documentos
- El total de 90GB a√∫n no est√° disponible ya que no se ha tokenizado, pero puede encontrar una muestra de 15GB aqu√≠: https://huggingface.co/datasets/haykgrigorian/TimeCapsuleLLM-London-1800-1875-v2-15GB

 ### Estad√≠sticas de sesgo 
  ![Sesgo de pronombres](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v2mini_eval1/pronoun_bias.png)

  ![Sesgo geogr√°fico](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v2mini_eval1/geographic_bias.png)

  ![Sesgo temporal](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v2mini_eval1/temporal_bias.png)

Consulte el [informe de sesgo v2](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v2mini_eval1/v2_bias_report.json) para m√°s informaci√≥n.


## C√≥mo usar

Este proyecto se centra principalmente en la curaci√≥n de datos hist√≥ricos, prepararlos para entrenamiento y construir un tokenizador. No voy a cubrir el proceso completo de entrenamiento LLM, para eso consulte nanoGPT de Andrej Karpathy.

### Paso 1: Recolectar y preparar textos hist√≥ricos 

- Re√∫na archivos .txt de libros de dominio p√∫blico, documentos, etc., de su periodo elegido (por ejemplo, Londres 1800-1850)

- Mant√©ngalos dentro de la ventana de tiempo/lugar que haya elegido  
- Limpie los archivos de texto usando un script o elimine manualmente encabezados/pies de p√°gina de Project Gutenberg, anotaciones modernas o errores de OCR.

### Paso 2: Construya un Tokenizador Personalizado

- Ejecute train_tokenizer.py o train_tokenizer_hf.py en los datos limpios.
- Esto le dar√° vocab.json y merges.txt
- Estos archivos definen el vocabulario y las reglas de combinaci√≥n para su modelo

### Paso 3: Entrene Su Modelo 

- Consulte [nanoGPT de Andrej Karpathy](https://github.com/karpathy/nanoGPT) para el proceso de entrenamiento o la documentaci√≥n de la arquitectura elegida.

# FAQ

## ¬øQu√© es el Entrenamiento Temporal Selectivo?

El Entrenamiento Temporal Selectivo (STT) es una metodolog√≠a de aprendizaje autom√°tico donde todos los datos de entrenamiento se curan espec√≠ficamente para que pertenezcan a un periodo hist√≥rico espec√≠fico. Se hace para modelar el lenguaje y el conocimiento de esa √©poca sin influencia de conceptos modernos. Por ejemplo, el modelo actual que tengo (v0.5) est√° entrenado exclusivamente con datos de 1800-1875, no est√° afinado sino entrenado desde cero, lo que resulta en una salida que refleja el estilo ling√º√≠stico y el contexto hist√≥rico de ese periodo.

## ¬øPor qu√© no simplemente usar fine-tuning o LoRA?

Para este proyecto estoy intentando crear un modelo de lenguaje que no est√© contaminado por sesgos modernos. Si hago fine-tuning a algo como GPT-2, ya est√° pre-entrenado y esa informaci√≥n no desaparecer√°. Si entreno desde cero, el modelo de lenguaje no fingir√° ser antiguo, simplemente lo ser√°. El objetivo de este proyecto ahora mismo es crear algo que pueda razonar exclusivamente usando conocimiento de libros de Londres publicados entre 1800 y 1875.

## ¬øQu√© tipo de datos utiliz√≥ para el entrenamiento?

Estoy usando libros, documentos legales, peri√≥dicos y otros escritos de Londres entre 1800 y 1875. La lista que enlac√© (para v0) tiene como 200, pero para el primer entrenamiento solo us√© 50 archivos de aproximadamente ~187 MB. Puede ver una lista de los documentos:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

Tama√±os de los conjuntos de datos:
- v0: ~187MB
- v0.5: ~435MB 
- v1: ~6.25GB 
- v2mini-eval1: 15GB

## ¬øQu√© tama√±o tienen los modelos?

v0: 16M Par√°metros

v0.5 123M Par√°metros

v1: 700M Parameters

v2mini-eval1: 300M Parameters

# Training Specs ? 

# v0/v0.5
GPU: Geforce rtx 4060
CPU: i5-13400F 
Ram: 16GB DDR5.

# v1
GPU: A100 SXM rented

# v2mini-eval1

GPU: A100 SXM rented





























---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2026-01-09

---