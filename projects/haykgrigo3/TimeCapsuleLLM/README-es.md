
<div align="right">
  <details>
    <summary >üåê Idioma</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="#" title="Coming soon">ÁπÅÈ´î‰∏≠Êñá (pr√≥ximamente)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="#" title="Coming soon">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä (pr√≥ximamente)</a> |
        | <a href="#" title="Coming soon">‡πÑ‡∏ó‡∏¢ (pr√≥ximamente)</a> |
        | <a href="#" title="Coming soon">Fran√ßais (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">Deutsch (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">Espa√±ol (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">Italiano (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">–†—É—Å—Å–∫–∏–π (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">Portugu√™s (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">Nederlands (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">Polski (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">ŸÅÿßÿ±ÿ≥€å (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">T√ºrk√ße (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">Ti·∫øng Vi·ªát (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (pr√≥ximamente)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM
Un LLM entrenado solo con datos de ciertos periodos de tiempo para reducir el sesgo moderno.

Imagina que un modelo de IA no solo pretende ser hist√≥rico, sino que realmente lo es.

Basado en [nanoGPT de Andrej Karpathy](https://github.com/karpathy/nanoGPT). Los scripts principales de entrenamiento y la arquitectura del modelo son obra suya.

# Objetivos del Proyecto

TimeCapsule LLM es un proyecto experimental que solo ser√° entrenado con textos escritos durante ciertos periodos hist√≥ricos. El objetivo es simular la visi√≥n del mundo y el lenguaje de √©pocas hist√≥ricas espec√≠ficas.

# Por qu√© el fine tuning no es suficiente

Si solo haces fine tuning a un modelo pre-entrenado, tu LLM seguir√° conociendo conceptos modernos. Por supuesto, lograr cero sesgo moderno es dif√≠cil, pero quiero acercarme lo m√°ximo posible a esto. Obtener ning√∫n sesgo moderno requiere entrenar un modelo desde cero.

# Resultados esperados

Espero que, una vez terminado, este modelo no conozca conceptos modernos y no pueda razonar m√°s all√° de lo que ha sido entrenado. No deber√≠a reconocer conceptos/vocabulario modernos y espero que no alucine conocimientos actuales.

# Actualizaciones de Progreso

## 9 de julio de 2025

He establecido mi periodo de tiempo entre 1800-1850 y regi√≥n: Londres

He reunido una lista de textos, libros y documentos

Hasta ahora he conseguido 50 archivos txt y pronto comenzar√© a entrenar NanoGPT

Actualizar√© esto mientras siga avanzando

## 13 de julio de 2025

Entren√© nanoGPT con 187MB de datos de texto hist√≥ricos.

## 15 de julio de 2025

Empec√© a descargar textos para la segunda ronda de entrenamiento. Estoy obteniendo todo del Internet Archive y he ampliado el periodo a 1800-1875. Para conseguir una gama diversa de textos, puedes usar los filtros de b√∫squeda por ubicaci√≥n de publicaci√≥n, periodo y temas en Internet Archive.

![Filtros de B√∫squeda](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## 16 de julio de 2025

Descargu√© unos 500 archivos txt de Internet Archive y tras limpiarlos (solo borrando espacios en blanco, cabeceras de Gutenberg, etc.) tengo unos 500MB de datos. Es un conjunto de datos peque√±o pero la √∫ltima vez entren√© con 187MB as√≠ que deber√≠a haber al menos alguna diferencia notable en la salida tras entrenar el segundo modelo. Espero que este modelo pueda al menos producir oraciones m√°s coherentes que tengan sentido. No es una garant√≠a, por supuesto, ya que sigue siendo un conjunto de datos muy peque√±o, pero es m√°s de lo que us√© la vez pasada.

Esto deber√≠a poder hacerse con mi propio hardware, lo cual es bueno porque espero ver mejoras antes de pasar a un conjunto de datos m√°s grande que requerir√≠a alquilar una GPU. Pero no te preocupes, a√∫n planeo alquilar una GPU pronto, pero antes quiero asegurarme de que mi conjunto de datos est√© lo m√°s curado y limpio posible. Uno de los problemas que tengo es la limpieza, muchos de estos txt tienen galimat√≠as mezclados. Los scripts de limpieza que he usado funcionan pero no son 100% efectivos.

Voy a entrenar este conjunto de datos hoy y deber√≠a tomar unas 4-5 horas. Cuando termine y lo pruebe, dar√© actualizaciones. ¬°Gracias de nuevo a quienes revisan mi proyecto, incluso me han pasado enlaces a recursos de OCR as√≠ que gracias! Espero que m√°s personas prueben esto y experimenten con sus propios conjuntos de datos.

## 28 de julio de 2025

He subido la v0.5 a Hugging Face, [√©chale un vistazo](https://huggingface.co/haykgrigorian/TimeCapsuleLLM) si quieres. Ahora puedes descargar mi repositorio y ejecutarlo localmente. Desafortunadamente nanoGPT no funciona de forma nativa con HuggingFace, as√≠ que tendr√°s que descargar y ejecutar el modelo localmente.

Tambi√©n comenzar√© a curar datos para mi siguiente ronda de entrenamiento, creo que necesitar√© quiz√° 5-10 veces m√°s datos para lograr capacidades de razonamiento.

### Actualizaci√≥n de Entrenamiento

Comenc√© a entrenar con un corpus de 435MB (108 M tokens), va bastante bien por ahora. La p√©rdida de entrenamiento baj√≥ de 10.9 a 4.9 en las primeras 2800 iteraciones. Espero que tome unas 8 o 9 horas completarlo. Publicar√© otra actualizaci√≥n cuando termine.

## 17 de julio de 2025 2:13AM

El entrenamiento ha terminado para el segundo modelo, mi 4060 tard√≥ unas 8 horas y 40 minutos (3,900 iter/h) para 33,000 iteraciones (5 √©pocas). La p√©rdida final de entrenamiento fue 3.73. Los resultados fueron sorprendentemente buenos, ahora realmente genera frases coherentes al estilo del siglo XIX.

# Comportamiento y Limitaciones del Modelo V0

Las primeras pruebas muestran que el modelo responde con lenguaje y comportamiento de los a√±os 1800. Por ejemplo, lo estimul√© con "Who art Henry?" y respondi√≥ "I know that man, I have did not a black, the storm." y s√≠, esa frase no tiene sentido pero el LLM reconoce que pregunto por una persona.

![Salida de Ejemplo de TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

No hay menci√≥n de conceptos modernos, las salidas contienen mayormente palabras y frases propias de los a√±os 1800.

Todav√≠a necesita mucho trabajo, entrenar con 187MB no te dar√° un modelo que produzca texto con razonamiento complejo.

Por ahora produce oraciones que carecen de una estructura completa y en general no tienen sentido, pero esto es normal para el tama√±o de entrenamiento.

# Comportamiento y Limitaciones del Modelo V0.5

Esto es una buena mejora comparado con el √∫ltimo modelo. El estilo de escritura y vocabulario es victoriano y casi cada frase es gramaticalmente correcta con la puntuaci√≥n adecuada. Y nuevamente, esto se entrena desde cero, por lo que se apega a temas de los a√±os 1800.

![Salida de muestra de TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

Hay muchas alucinaciones f√°cticas. Muchos (como el 100%) de los detalles (fechas, eventos, figuras hist√≥ricas) son inventados. Adem√°s, las oraciones realmente no tienen conexi√≥n entre s√≠, a veces tal vez 2 frases se relacionan pero m√°s all√° de eso no. Otro problema es que a veces aparece un pie de p√°gina errante de ‚ÄúDigitized by Google‚Äù, as√≠ que la pr√≥xima vez que entrene tengo que asegurarme de limpiar bien los textos. En general, estoy muy feliz con los resultados, todav√≠a est√° lejos de ser un LLM pero definitivamente es un generador de frases.

Estoy aprendiendo mucho y empezar√© a averiguar qu√© debo mejorar en las pr√≥ximas semanas. ¬°Pronto subir√© archivos!

# Planes Futuros

(Completado) Voy a empezar a trabajar en la versi√≥n 0.5, en vez de entrenar usando 50 libros, entrenar√© usando idealmente 500-600. Ahora mismo estoy entrenando nanoGPT usando libros de 1800-1850 y espec√≠ficamente de Londres. Hay algunos desaf√≠os como asegurarme de que los libros que encuentro no est√©n actualizados o tengan interpretaciones modernas, sino libros intactos publicados dentro de mi periodo elegido.

Quiero entrenar un nuevo modelo (v1) con un corpus mucho m√°s grande, tal vez 5-10 veces mayor que el que us√© para v0.5. Mi objetivo es ver si puedo lograr que emerjan habilidades de razonamiento solo con Entrenamiento Temporal Selectivo, ser√° una tarea m√°s dif√≠cil y ni siquiera estoy seguro de si es posible debido a las limitaciones de datos hist√≥ricos. En las pr√≥ximas semanas tratar√© de reunir suficientes datos para un corpus de 5-10GB. Creo que si consigo datos limpios y de alta calidad y alquilo una GPU, habr√° progreso.

# C√≥mo Usar Este Proyecto

Este proyecto se centra principalmente en recopilar datos hist√≥ricos, prepararlos para el entrenamiento y construir un tokenizador. No voy a cubrir el proceso completo de entrenamiento de un LLM, para eso consulta nanoGPT por Andrej Karpathy.

# Paso 1: Recopilar y Preparar Textos Hist√≥ricos

Re√∫ne archivos .txt de libros de dominio p√∫blico, documentos, etc. del periodo que elijas (por ejemplo, Londres 1800-1850)

Puedes usar download_texts_improved.py para descargar libros si lo necesitas.

Limpia los archivos de texto usando un script o elimina manualmente encabezados/pies de p√°gina de Project Gutenberg, anotaciones modernas o errores de OCR.

prepare_dataset.py deber√≠a funcionar bien.

# Paso 2: Construir un Tokenizador Personalizado

Ejecuta train_tokenizer.py o train_tokenizer_hf.py en los datos limpios.
Esto te dar√° vocab.json y merges.txt

Estos archivos definen el vocabulario y las reglas de combinaci√≥n para tu modelo

# Paso 3: Entrena Tu Modelo (nanoGPT)

Consulta [nanoGPT por Andrej Karpathy](https://github.com/karpathy/nanoGPT) para el proceso de entrenamiento.

Puedes entrenar otro LLM si lo deseas, pero yo us√© nanoGPT

# FAQ

## ¬øQu√© es el Entrenamiento Temporal Selectivo?

El Entrenamiento Temporal Selectivo (STT) es una metodolog√≠a de aprendizaje autom√°tico donde todos los datos de entrenamiento se seleccionan espec√≠ficamente para que pertenezcan a un periodo hist√≥rico concreto. Se hace para modelar el lenguaje y conocimiento de esa √©poca sin influencia de conceptos modernos. Por ejemplo, el modelo actual que tengo (v0.5) est√° entrenado con datos exclusivamente de 1800-1875, no est√° afinado sino entrenado desde cero, resultando en salidas que reflejan el estilo ling√º√≠stico y contexto hist√≥rico de ese periodo.

## ¬øPor qu√© no usar simplemente fine-tuning o LoRA?

Para este proyecto estoy intentando crear un modelo de lenguaje sin sesgo moderno. Si afino algo como GPT-2, ya est√° pre-entrenado y esa informaci√≥n no desaparecer√°. Si entreno desde cero el modelo no fingir√° ser antiguo, simplemente lo ser√°. El objetivo de este proyecto por ahora es crear algo que razone exclusivamente usando conocimientos de libros de Londres publicados entre 1800 y 1850.

## ¬øQu√© tipo de datos usaste para entrenar?

Estoy usando libros, documentos legales, peri√≥dicos y otros escritos de Londres entre 1800 y 1850. La lista que enlac√© tiene como 200 pero para el primer entrenamiento solo us√© 50 archivos de unos ~187 MB. Puedes ver la lista de documentos:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## ¬øQu√© tama√±o tiene el modelo Versi√≥n 0?

Este modelo es muy peque√±o por ahora, solo lo hago por diversi√≥n y siguiendo una regla estricta de no usar fuentes modernas. Tiene casi 16 millones de par√°metros pero voy a empezar a recopilar m√°s textos antiguos para iniciar otro entrenamiento de modelo. Dar√© actualizaciones en el camino.

## ¬øEspecificaciones de Entrenamiento?

GPU: Geforce rtx 4060
CPU: i5-13400F
Ram: 16GB DDR5.


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-29

---