
<div align="right">
  <details>
    <summary >üåê Idioma</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="#" title="Pr√≥ximamente">ÁπÅÈ´î‰∏≠Êñá (pr√≥ximamente)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="#" title="Pr√≥ximamente">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä (pr√≥ximamente)</a> |
        | <a href="#" title="Pr√≥ximamente">‡πÑ‡∏ó‡∏¢ (pr√≥ximamente)</a> |
        | <a href="#" title="Pr√≥ximamente">Fran√ßais (pr√≥ximamente)</a>
        | <a href="#" title="Pr√≥ximamente">Deutsch (pr√≥ximamente)</a>
        | <a href="#" title="Pr√≥ximamente">Espa√±ol (pr√≥ximamente)</a>
        | <a href="#" title="Pr√≥ximamente">Italiano (pr√≥ximamente)</a>
        | <a href="#" title="Pr√≥ximamente">–†—É—Å—Å–∫–∏–π (pr√≥ximamente)</a>
        | <a href="#" title="Pr√≥ximamente">Portugu√™s (pr√≥ximamente)</a>
        | <a href="#" title="Pr√≥ximamente">Nederlands (pr√≥ximamente)</a>
        | <a href="#" title="Pr√≥ximamente">Polski (pr√≥ximamente)</a>
        | <a href="#" title="Pr√≥ximamente">ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (pr√≥ximamente)</a>
        | <a href="#" title="Pr√≥ximamente">ŸÅÿßÿ±ÿ≥€å (pr√≥ximamente)</a>
        | <a href="#" title="Pr√≥ximamente">T√ºrk√ße (pr√≥ximamente)</a>
        | <a href="#" title="Pr√≥ximamente">Ti·∫øng Vi·ªát (pr√≥ximamente)</a>
        | <a href="#" title="Pr√≥ximamente">Bahasa Indonesia (pr√≥ximamente)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM
Un LLM entrenado solo con datos de ciertos per√≠odos de tiempo para reducir el sesgo moderno.

Imagina si un modelo de IA no solo pretendiera ser hist√≥rico, sino que realmente lo fuera.

Construido sobre [nanoGPT por Andrej Karpathy](https://github.com/karpathy/nanoGPT) Los scripts principales de entrenamiento y la arquitectura del modelo son obra suya.

# Objetivos del proyecto
TimeCapsule LLM es un proyecto experimental que solo ser√° entrenado con textos escritos durante ciertos per√≠odos de tiempo. El objetivo es simular la visi√≥n del mundo y el lenguaje de eras hist√≥ricas espec√≠ficas.

# Por qu√© el ajuste fino no es suficiente

Si solo ajustas un modelo preentrenado, tu LLM todav√≠a conocer√° conceptos modernos. Por supuesto, lograr cero sesgo moderno es dif√≠cil, pero quiero acercarme lo m√°s posible a esto. No tener sesgo moderno requiere entrenar un modelo desde cero.

# Resultados esperados

Con suerte, cuando termine, este modelo no conocer√° conceptos modernos y no podr√° razonar m√°s all√° de lo que ha sido entrenado. No deber√≠a reconocer conceptos/vocabulario modernos y no deber√≠a alucinar conocimiento moderno.

# Actualizaciones de progreso

## 9 de julio de 2025

He establecido mi per√≠odo de tiempo en 1800-1850 y regi√≥n: Londres

He reunido una lista de textos, libros, documentos

Hasta ahora he conseguido 50 archivos txt y pronto comenzar√© a entrenar NanoGPT

Actualizar√© esto mientras se haga progreso

## 13 de julio de 2025

Entren√© nanoGPT con 187MB de datos hist√≥ricos en texto.

## 15 de julio de 2025

Empec√© a descargar textos para la segunda ronda de entrenamiento. Estoy obteniendo todo del Internet Archive y he ampliado el per√≠odo de tiempo a 1800-1875. Para obtener una variedad diversa de textos, puedes usar filtros de b√∫squeda y de tema por ubicaci√≥n de publicaci√≥n, per√≠odo de tiempo y temas en Internet Archive.

![Filtros de b√∫squeda](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## 16 de julio de 2025

Descargu√© alrededor de 500 archivos txt del Internet Archive y despu√©s de limpiarlos (solo eliminando espacios en blanco, encabezados de Gutenberg, etc.) tengo alrededor de 500MB de datos. Es un conjunto de datos peque√±o pero la √∫ltima vez entren√© con 187MB as√≠ que deber√≠a haber al menos alguna diferencia notable en la salida despu√©s de entrenar el segundo modelo. Espero que este modelo pueda al menos producir oraciones m√°s coherentes que tengan algo de sentido. No es una garant√≠a por supuesto ya que sigue siendo un conjunto de datos muy peque√±o, pero es m√°s que lo que us√© la √∫ltima vez.

Esto deber√≠a ser posible en mi propio hardware, tambi√©n es bueno porque espero ver alg√∫n tipo de mejora antes de pasar a un conjunto de datos m√°s grande que requerir√≠a alquilar una GPU. Pero no te preocupes, todav√≠a planeo alquilar una GPU pronto, pero antes de hacerlo quiero asegurarme de que mi conjunto de datos est√© lo m√°s curado y limpio posible. Uno de los problemas que tengo es la limpieza, muchos de estos archivos txt tienen galimat√≠as mezcladas. Los scripts que he usado para limpiar funcionan pero no son 100% efectivos.

Entrenar√© este conjunto de datos hoy y deber√≠a tomar alrededor de 4-5 horas. Una vez que est√© hecho y lo pruebe, dar√© actualizaciones. Gracias de nuevo a todos los que est√°n revisando mi proyecto, incluso ha habido personas que me han dado enlaces a recursos de OCR as√≠ que ¬°Gracias! Espero que m√°s personas intenten esto y experimenten con sus propios conjuntos de datos.


## 12 de agosto de 2025

La versi√≥n 1 est√° completa, esta versi√≥n utiliza Phi 1.5 (700M par√°metros) y est√° entrenada en un conjunto de datos que contiene casi 7000 textos. Las principales mejoras de este modelo son su capacidad de a veces referenciar nombres reales y eventos hist√≥ricos. Las versiones 0 y 0.5 no pod√≠an hacer esto; cuando les indicaba algo como "En el a√±o 1821", la salida eran solo alucinaciones, cosas que nunca sucedieron. Un ejemplo de esto con la Versi√≥n 1:

Mi indicaci√≥n: Era el a√±o de nuestro Se√±or 1834,

La salida: "Era el a√±o de nuestro Se√±or 1834 y las calles de Londres estaban llenas de protestas y peticiones. La causa, como muchos recordaban, no estaba ligada al √°mbito privado, sino que hab√≠a sido tomada ese mismo d√≠a en el d√≠a de Lord Palmerston, el p√∫blico recibir√° una breve declaraci√≥n de las dificultades bajo las cuales el d√≠a de la ley nos ha alcanzado. Es motivo de profundo pesar que los acontecimientos actuales en la historia del mundo sean claros, y en consecuencia ser√°n conocidos. No es cierto que los mismos hombres que primero se establecieron en el Evangelio en Jerusal√©n tengan un registro tan extenso e interesante de la prosperidad y prosperidad"

Al principio supuse que podr√≠a ser una coincidencia, pero mira esto: ![1834protest](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)


### Actualizaci√≥n de Entrenamiento

Comenc√© a entrenar con un corpus de 435MB (108 M tokens), va bastante bien por ahora. La p√©rdida de entrenamiento baj√≥ de 10.9 a 4.9 en las primeras 2800 iteraciones. Espero que tarde unas 8 o 9 horas en completarse. Publicar√© otra actualizaci√≥n cuando termine.

## 17 de julio de 2025

El entrenamiento est√° terminado para el segundo modelo, mi 4060 tard√≥ unas 8 horas y 40 minutos (3,900 iteraciones/hr) para 33,000 iteraciones (5 √©pocas). La p√©rdida final de entrenamiento fue de 3.73. Las salidas fueron sorprendentemente buenas, ahora realmente genera frases coherentes al estilo del siglo XIX.

## 28 de julio de 2025

He subido la v0.5 a Hugging Face, [√âchale un vistazo](https://huggingface.co/haykgrigorian/TimeCapsuleLLM) si quieres. Ahora puedes descargar mi repositorio y ejecutarlo localmente. Desafortunadamente nanoGPT no funciona de manera nativa con HuggingFace, as√≠ que tendr√°s que descargar y ejecutar el modelo localmente.

Tambi√©n comenzar√© a curar datos para mi pr√≥xima sesi√≥n de entrenamiento, creo que necesitar√© quiz√° 5-10x m√°s datos para lograr capacidades de razonamiento.

## 2 de agosto de 2025

Pronto comenzar√© a trabajar en la Versi√≥n 1. Necesitar√© hacer la transici√≥n de la arquitectura de nanoGPT a algo m√°s moderno. Tengo en mente varias arquitecturas LLM de c√≥digo abierto, incluyendo: OpenLLaMA v3, Phi-2 y Qwen 1.5B. Y para soportar el salto a la V1, tendr√© que curar cuidadosamente un conjunto de datos mucho m√°s grande y diverso. Necesitar√© al menos 5GB de datos limpios de entrenamiento.

# Comportamiento y Limitaciones del Modelo V0

Las primeras indicaciones muestran al modelo respondiendo con lenguaje y comportamiento de los a√±os 1800. Por ejemplo, le pregunt√© "¬øQui√©n es Henry?" y respondi√≥ "Conozco a ese hombre, no tengo negro, la tormenta." y s√≠, esa frase no tiene sentido pero el LLM reconoce que estoy preguntando por una persona.

![TimeLockLLM Salida de Muestra](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

No hay menci√≥n de conceptos modernos, las salidas contienen principalmente palabras y frases de los a√±os 1800.

Todav√≠a necesita mucho trabajo, entrenar con 187MB no te dar√° un modelo que produzca texto con razonamiento complejo.

Ahora mismo produce oraciones que carecen de estructura completa y en general simplemente no tienen sentido, pero esto es normal para el tama√±o de entrenamiento.

# Comportamiento y Limitaciones del Modelo V0.5

Esto es una buena mejora en comparaci√≥n con el √∫ltimo modelo. El estilo de escritura y el vocabulario son victorianos y casi todas las oraciones son gramaticalmente correctas con la puntuaci√≥n adecuada. Y nuevamente, esto est√° entrenado desde cero as√≠ que se apega a temas de los a√±os 1800.

![Salida de muestra de TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

Hay muchas alucinaciones f√°cticas. Muchos (como el 100%) de los detalles (fechas, eventos, figuras hist√≥ricas) son inventados. Adem√°s, las oraciones realmente no tienen conexi√≥n entre s√≠, a veces tal vez 2 oraciones se relacionan, pero m√°s all√° de eso no lo hacen. Otro problema es que a veces aparece un pie de p√°gina perdido de ‚ÄúDigitized by Google‚Äù, as√≠ que la pr√≥xima vez que entrene realmente debo asegurarme de que los textos est√©n bien limpiados. En general estoy muy contento con los resultados, a√∫n est√° lejos de ser un LLM pero definitivamente es un generador de oraciones.

Estoy aprendiendo mucho y comenzar√© a averiguar qu√© debo mejorar en las pr√≥ximas semanas. ¬°Pronto subir√© archivos!

# Comportamiento y Limitaciones del Modelo V1

Pronto subir√© algunos ejemplos de salida y tambi√©n har√© comparaciones entre los 3 modelos con el mismo prompt. Tambi√©n subir√© la V1 a huggingface como hice con mi √∫ltima versi√≥n, puedes encontrar mi cuenta de huggingface aqu√≠: https://huggingface.co/haykgrigorian/TimeCapsuleLLM

# Planes Futuros

(Completado) Voy a comenzar a trabajar en la versi√≥n 0.5, en vez de entrenar usando 50 libros, entrenar√© usando idealmente 500-600. Ahora mismo estoy entrenando nanoGPT usando libros de 1800-1850 y espec√≠ficamente de Londres. Hay algunos desaf√≠os como asegurarme de que los libros que encuentre no est√©n actualizados ni tengan interpretaciones modernas, sino libros intactos publicados dentro del per√≠odo de tiempo elegido.

Quiero entrenar un nuevo modelo (v1) con un corpus mucho m√°s grande, tal vez 5-10 veces mayor que el que us√© para v0.5. Mi objetivo es ver si puedo lograr que emerjan habilidades de razonamiento solo a trav√©s de Entrenamiento Temporal Selectivo, esta ser√° una tarea m√°s dif√≠cil y ni siquiera estoy completamente seguro de que sea posible debido a las limitaciones de los datos hist√≥ricos. En las pr√≥ximas semanas intentar√© curar suficientes datos para un corpus de 5-10GB. Creo que si consigo datos limpios y de alta calidad y alquilo una GPU, habr√° progreso.

# C√≥mo Usar Este Proyecto

Este proyecto se centra principalmente en curar datos hist√≥ricos, prepararlos para el entrenamiento y construir un tokenizador. No voy a cubrir el proceso completo de entrenamiento de LLM, para eso consulta nanoGPT de Andrej Karpathy.

# Paso 1: Recopilar y Preparar Textos Hist√≥ricos

Recolecta archivos .txt de libros de dominio p√∫blico, documentos, etc. de tu per√≠odo de tiempo elegido (por ejemplo, Londres 1800-1850)

Puedes usar download_texts_improved.py para descargar libros si lo necesitas.

Limpia los archivos de texto usando un script o elimina manualmente encabezados/pies de p√°gina de Project Gutenberg, anotaciones modernas o errores de OCR.

prepare_dataset.py deber√≠a funcionar bien.

# Paso 2: Construir un Tokenizador Personalizado

Ejecuta train_tokenizer.py o train_tokenizer_hf.py en los datos limpios.
Esto te dar√° vocab.json y merges.txt
Estos archivos definen el vocabulario y las reglas de combinaci√≥n para tu modelo

# Paso 3: Entrena Tu Modelo (nanoGPT) 

Consulta [nanoGPT de Andrej Karpathy](https://github.com/karpathy/nanoGPT) para el proceso de entrenamiento.

Puedes entrenar un LLM diferente si lo deseas, pero yo utilic√© nanoGPT 

# FAQ

## ¬øQu√© es el Entrenamiento Temporal Selectivo?

El Entrenamiento Temporal Selectivo (STT) es una metodolog√≠a de aprendizaje autom√°tico donde todos los datos de entrenamiento se seleccionan espec√≠ficamente para que pertenezcan a un per√≠odo hist√≥rico determinado. Se realiza para modelar el lenguaje y el conocimiento de esa √©poca sin influencia de conceptos modernos. Por ejemplo, el modelo actual que tengo (v0.5) est√° entrenado exclusivamente con datos de 1800-1875, no est√° ajustado sino entrenado desde cero, lo que resulta en una salida que refleja el estilo ling√º√≠stico y contexto hist√≥rico de ese per√≠odo.

## ¬øPor qu√© no simplemente usar fine-tuning o LoRA?

Para este proyecto quiero crear un modelo de lenguaje que no est√© influenciado por sesgos modernos. Si hago fine-tuning a algo como GPT-2, ya est√° preentrenado y esa informaci√≥n no se elimina. Si entreno desde cero, el modelo de lenguaje no fingir√° ser antiguo, simplemente lo ser√°. El objetivo de este proyecto ahora es crear algo que razone exclusivamente usando conocimiento de libros londinenses publicados entre 1800 y 1850.

## ¬øQu√© tipo de datos usaste para el entrenamiento?

Estoy usando libros, documentos legales, peri√≥dicos y otros escritos de Londres entre 1800 y 1850. La lista que enlac√© tiene como 200, pero para el primer entrenamiento utilic√© solo 50 archivos de unos ~187 MB. Puedes ver una lista de los documentos en:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## ¬øQu√© tama√±o tienen los modelos?

V0: 16M Par√°metros

V0.5: 123M Par√°metros

V1: 700M Par√°metros

# ¬øEspecificaciones de entrenamiento?

#V0/V0.5
GPU: Geforce rtx 4060
CPU: i5-13400F 
Ram: 16GB DDR5.

#V1

GPU: A100 rented









---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-08-12

---