<div align="right">
  <details>
    <summary >üåê Idioma</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="#" title="Coming soon">ÁπÅÈ´î‰∏≠Êñá (pr√≥ximamente)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="#" title="Coming soon">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä (pr√≥ximamente)</a> |
        | <a href="#" title="Coming soon">‡πÑ‡∏ó‡∏¢ (pr√≥ximamente)</a> |
        | <a href="#" title="Coming soon">Fran√ßais (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">Deutsch (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">Espa√±ol (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">Italiano (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">–†—É—Å—Å–∫–∏–π (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">Portugu√™s (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">Nederlands (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">Polski (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">ŸÅÿßÿ±ÿ≥€å (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">T√ºrk√ße (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">Ti·∫øng Vi·ªát (pr√≥ximamente)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (pr√≥ximamente)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM
Un LLM entrenado solo con datos de ciertos periodos para reducir el sesgo moderno.

Imagina si un modelo de IA no solo pretendiera ser hist√≥rico, sino que realmente lo fuera.

Construido sobre [nanoGPT de Andrej Karpathy](https://github.com/karpathy/nanoGPT). Los scripts principales de entrenamiento y la arquitectura del modelo son de su autor√≠a.

# Objetivos del Proyecto

TimeCapsule LLM es un proyecto experimental que solo ser√° entrenado con textos escritos durante ciertos periodos hist√≥ricos. El objetivo es simular la cosmovisi√≥n y el lenguaje de √©pocas hist√≥ricas espec√≠ficas.

# Por qu√© el fine tuning no es suficiente

Si solo haces fine tuning a un modelo preentrenado, tu LLM seguir√° conociendo conceptos modernos. Por supuesto, lograr cero sesgo moderno es dif√≠cil, pero quiero acercarme lo m√°ximo posible a esto. Lograr que no haya sesgo moderno requiere entrenar un modelo desde cero.

# Resultados esperados

Espero que, cuando est√© terminado, este modelo no conozca conceptos modernos y no pueda razonar m√°s all√° de lo que ha sido entrenado. No deber√≠a reconocer vocabulario/conceptos modernos y espero que no alucine conocimientos actuales.

# Actualizaciones de Progreso

## 9 de julio de 2025

He definido mi periodo de entrenamiento en 1800-1850 y la regi√≥n: Londres

He reunido una lista de textos, libros, documentos

Hasta ahora he conseguido 50 archivos txt y pronto comenzar√© a entrenar NanoGPT

Actualizar√© esto mientras se haga progreso

## 13 de julio de 2025

Entren√© nanoGPT con 187MB de datos de texto hist√≥ricos.

## 15 de julio de 2025

He comenzado a descargar textos para la segunda ronda de entrenamiento. Estoy obteniendo todo del Internet Archive y he ampliado el periodo a 1800-1875. Para obtener una gama diversa de textos, puedes usar los filtros de tema y b√∫squeda por ubicaci√≥n de publicaci√≥n, periodo y temas en Internet Archive.

![Filtros de B√∫squeda](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## 16 de julio de 2025

Descargu√© unos 500 archivos txt del Internet Archive y, tras limpiarlos (solo eliminando espacios en blanco, encabezados de Gutenberg, etc.) tengo alrededor de 500MB de datos. Es un conjunto de datos peque√±o, pero la √∫ltima vez entren√© con 187MB, as√≠ que deber√≠a haber al menos alguna diferencia notable en la salida tras entrenar el segundo modelo. Espero que este modelo pueda al menos producir frases m√°s coherentes que tengan cierto sentido. Por supuesto, no es garant√≠a ya que sigue siendo un conjunto de datos muy peque√±o, pero es m√°s de lo que us√© la vez pasada.

Esto deber√≠a ser posible en mi propio hardware, lo cual es bueno porque espero ver alguna mejora antes de saltar a un conjunto de datos m√°s grande que requerir√≠a alquilar una GPU. Pero no te preocupes, a√∫n planeo alquilar una GPU pronto, pero antes quiero asegurarme de que mi conjunto de datos est√© lo m√°s curado y limpio posible. Uno de los problemas que tengo es la limpieza, muchos de estos archivos txt tienen basura mezclada. Los scripts que he usado para limpiar funcionan pero no son 100% efectivos.

Voy a entrenar este conjunto de datos hoy y deber√≠a tomar unas 4-5 horas. Una vez termine y lo pruebe, dar√© actualizaciones. ¬°Gracias de nuevo a todos los que revisan mi proyecto, incluso he recibido enlaces a recursos de OCR as√≠ que gracias! Espero que m√°s gente lo intente y experimente con sus propios conjuntos de datos.


### Actualizaci√≥n de Entrenamiento

Comenc√© a entrenar con un corpus de 435MB (108 M tokens), por ahora va bastante fluido. La p√©rdida de entrenamiento baj√≥ de 10.9 a 4.9 en las primeras 2800 iteraciones. Espero que tome unas 8 o 9 horas completar. Publicar√© otra actualizaci√≥n cuando termine.

## 17 de julio de 2025 2:13AM

El entrenamiento del segundo modelo ha terminado, mi 4060 tard√≥ unas 8 horas y 40 minutos (3,900 iteraciones/hora) para 33,000 iteraciones (5 √©pocas). La p√©rdida final de entrenamiento fue 3.73. Las salidas fueron sorprendentemente buenas: ahora realmente genera frases coherentes al estilo del siglo XIX.

## 28 de julio de 2025

He subido la v0.5 a Hugging Face, [√©chale un vistazo](https://huggingface.co/haykgrigorian/TimeCapsuleLLM) si quieres. Ahora puedes descargar mi repositorio y ejecutarlo localmente. Desafortunadamente, nanoGPT no funciona de manera nativa con HuggingFace, as√≠ que tendr√°s que descargar y ejecutar el modelo localmente.

Adem√°s, comenzar√© a curar datos para mi pr√≥xima ronda de entrenamiento. Creo que necesitar√© quiz√° 5-10 veces m√°s datos para lograr capacidades de razonamiento.


# Comportamiento y Limitaciones del Modelo V0

Las primeras pruebas muestran que el modelo responde con lenguaje y comportamiento de los a√±os 1800. Por ejemplo, lo prob√© con "Who art Henry?" y respondi√≥ "I know that man, I have did not a black, the storm." y s√≠, esa frase no tiene sentido pero el LLM est√° reconociendo que pregunto por una persona.


![Salida de muestra de TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

No hay menci√≥n de conceptos modernos, las salidas contienen principalmente palabras y frases propias de los a√±os 1800.

Todav√≠a necesita mucho trabajo, entrenar con 187MB no te dar√° un modelo que produzca texto con razonamiento complejo.

Actualmente produce oraciones que carecen de estructura completa y en general no tienen sentido, pero esto es normal para el tama√±o de entrenamiento.

# Comportamiento y Limitaciones del Modelo V0.5

Esto es una buena mejora en comparaci√≥n con el √∫ltimo modelo. El estilo de escritura y el vocabulario son victorianos y casi todas las oraciones son gramaticalmente correctas con la puntuaci√≥n adecuada. Y nuevamente, esto est√° entrenado desde cero, por lo que se apega a temas de los a√±os 1800.

![Salida de muestra de TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

Hay muchas alucinaciones f√°cticas. Muchos (como el 100%) de los detalles (fechas, eventos, figuras hist√≥ricas) son inventados. Adem√°s, las oraciones realmente no tienen conexi√≥n entre s√≠, a veces tal vez 2 oraciones se relacionan pero m√°s all√° de eso no. Otro problema es que a veces aparece un pie de p√°gina errante como ‚ÄúDigitized by Google‚Äù, as√≠ que la pr√≥xima vez que entrene realmente tengo que asegurarme de limpiar bien los textos. En general estoy muy contento con los resultados, a√∫n est√° lejos de ser un LLM pero definitivamente es un generador de oraciones.

Estoy aprendiendo mucho y en las pr√≥ximas semanas empezar√© a averiguar qu√© necesito mejorar. ¬°Subir√© archivos pronto!

# Planes Futuros

(Completado) Voy a empezar a trabajar en la versi√≥n 0.5, en lugar de entrenar usando 50 libros, entrenar√© usando idealmente 500-600. Actualmente estoy entrenando nanoGPT usando libros de 1800-1850 y espec√≠ficamente de Londres. Hay algunos desaf√≠os como asegurar que los libros que encuentro no est√©n actualizados o tengan interpretaciones modernas, sino libros intactos publicados dentro del periodo de tiempo elegido.

Quiero entrenar un nuevo modelo (v1) con un corpus mucho m√°s grande, tal vez 5-10 veces mayor que el que us√© para v0.5. Mi objetivo es ver si puedo lograr que surjan habilidades de razonamiento solo con el Entrenamiento Temporal Selectivo, esto ser√° una tarea m√°s dif√≠cil y ni siquiera estoy completamente seguro de si es posible debido a las limitaciones de datos hist√≥ricos. En las pr√≥ximas semanas intentar√© curar suficientes datos para un corpus de 5-10GB. Creo que si consigo datos limpios de alta calidad y alquilo una GPU, habr√° progreso.

# C√≥mo Usar Este Proyecto

Este proyecto se enfoca principalmente en curar datos hist√≥ricos, prepararlos para el entrenamiento y construir un tokenizador. No voy a cubrir el proceso completo de entrenamiento de LLM, para eso consulta nanoGPT de Andrej Karpathy.

# Paso 1: Recopila y Prepara Textos Hist√≥ricos

Recopila archivos .txt de libros de dominio p√∫blico, documentos, etc. de tu periodo de tiempo elegido (por ejemplo, Londres 1800-1850)

Puedes usar download_texts_improved.py para descargar libros si lo necesitas.

Limpia los archivos de texto usando un script o eliminando manualmente encabezados/pies de p√°gina de Project Gutenberg, anotaciones modernas o cosas como errores de OCR.

prepare_dataset.py deber√≠a funcionar bien.

# Paso 2: Construye un Tokenizador Personalizado

Ejecuta train_tokenizer.py o train_tokenizer_hf.py sobre los datos limpios.
Esto te dar√° vocab.json y merges.txt

Estos archivos definen el vocabulario y las reglas de combinaci√≥n para tu modelo

# Paso 3: Entrena Tu Modelo (nanoGPT)

Consulta [nanoGPT de Andrej Karpathy](https://github.com/karpathy/nanoGPT) para el proceso de entrenamiento.

Puedes entrenar un LLM diferente si lo deseas, pero yo us√© nanoGPT

# FAQ

## ¬øQu√© es el Entrenamiento Temporal Selectivo?

El Entrenamiento Temporal Selectivo (STT) es una metodolog√≠a de aprendizaje autom√°tico donde todos los datos de entrenamiento se curan espec√≠ficamente para que pertenezcan a un periodo hist√≥rico espec√≠fico. Se hace para modelar el lenguaje y el conocimiento de esa √©poca sin influencia de conceptos modernos. Por ejemplo, el modelo actual que tengo ahora (v0.5) est√° entrenado exclusivamente con datos de 1800-1875, no est√° afinado sino entrenado desde cero, resultando en una salida que refleja el estilo ling√º√≠stico y contexto hist√≥rico de esa √©poca.

## ¬øPor qu√© no simplemente usar fine-tuning o LoRA?

Para este proyecto estoy tratando de crear un modelo de lenguaje sin sesgo moderno. Si afino algo como GPT-2, ya est√° preentrenado y esa informaci√≥n no desaparecer√°. Si entreno desde cero el modelo de lenguaje no fingir√° ser antiguo, simplemente lo ser√°. El objetivo de este proyecto ahora es crear algo que pueda razonar usando exclusivamente conocimiento de libros de Londres publicados entre 1800 y 1850.

## ¬øQu√© tipo de datos usaste para entrenar?

Estoy usando libros, documentos legales, peri√≥dicos y otros escritos de Londres 1800-1850. La lista que enlac√© tiene como 200 pero para el primer entrenamiento solo us√© 50 archivos de unos ~187 MB. Puedes ver una lista de los documentos:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## ¬øQu√© tan grande es el Modelo Versi√≥n 0?

Este modelo es muy peque√±o ahora, solo lo hago por diversi√≥n y siguiendo una regla estricta de entrenamiento de no usar fuentes modernas. Tiene casi 16 millones de par√°metros pero voy a empezar a recopilar m√°s textos antiguos para comenzar otro entrenamiento de modelo. Ir√© dando actualizaciones conforme avance.

## ¬øEspecificaciones de entrenamiento?

GPU: Geforce rtx 4060
CPU: i5-13400F
Ram: 16GB DDR5.


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-08-02

---