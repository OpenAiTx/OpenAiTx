<div align="right">
  <details>
    <summary >ğŸŒ JÄ™zyk</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ç®€ä½“ä¸­æ–‡</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-TW">ç¹é«”ä¸­æ–‡</a> 
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">æ—¥æœ¬èª</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">í•œêµ­ì–´</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=as">à¤¹à¤¿à¤¨à¥à¤¦à¥€</a> 
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=th">à¹„à¸—à¸¢</a> 
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=fr">FranÃ§ais</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=es">EspaÃ±ol</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=it">Italiano</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=ru">Ğ ÑƒÑÑĞºĞ¸Ğ¹</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=pt">PortuguÃªs</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=ar">Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©</a>
        | <a href="#" title="Coming soon">ÙØ§Ø±Ø³ÛŒ (wkrÃ³tce)</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=tr">TÃ¼rkÃ§e</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=vi">Tiáº¿ng Viá»‡t</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (wkrÃ³tce)</a>

      </div>
    </div>
  </details>
</div>


# TimeCapsule LLM

*Model jÄ™zykowy wytrenowany **od podstaw** wyÅ‚Ä…cznie na danych z okreÅ›lonych miejsc i epok, aby ograniczyÄ‡ wspÃ³Å‚czesne uprzedzenia i odwzorowaÄ‡ jÄ™zyk, sÅ‚ownictwo oraz Å›wiatopoglÄ…d danej ery.*

WyobraÅº sobie, Å¼e model AI nie tylko udaje historyczny, ale faktycznie nim jest.

Wersje v0 oraz v0.5 zbudowane na [nanoGPT autorstwa Andreja Karpathy'ego](https://github.com/karpathy/nanoGPT) Podstawowe skrypty treningowe i architektura modelu to jego dzieÅ‚o. 

v1 zbudowany na [Phi 1.5 firmy Microsoft](https://huggingface.co/microsoft/phi-1_5)

[Link do Hugging Face](https://huggingface.co/haykgrigorian/TimeCapsuleLLM)


##  Zachowanie modelu i ograniczenia

### **v0**  

Wczesne zapytania pokazujÄ…, Å¼e model odpowiada jÄ™zykiem i zachowaniem z XIX wieku. 
PrzykÅ‚ad: Zapytanie: "Who art Henry?" i odpowiedÅº: "I know that man, I have did not a black, the storm." 

![PrzykÅ‚adowe wyjÅ›cie TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

- Brak wzmianki o wspÃ³Å‚czesnych koncepcjach  
- GÅ‚Ã³wnie sÅ‚ownictwo zgodne z epokÄ…  
- Zdania przewaÅ¼nie niespÃ³jne (oczekiwane przy ~187MB danych treningowych)

### **v0.5** 

ZnaczÄ…ca poprawa wzglÄ™dem v0.  
- WiktoriaÅ„ski styl pisania, poprawna interpunkcja, przewaÅ¼nie gramatyczne zdania  
- Nadal wysoki poziom halucynacji faktÃ³w  
- Szumy OCR (â€Digitized by Googleâ€) nadal obecne w odpowiedziach

![PrzykÅ‚adowe wyjÅ›cie TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

### **v1**

Pierwszy model, ktÃ³ry przywoÅ‚uje i Å‚Ä…czy prawdziwe wydarzenie historyczne z rzeczywistÄ… postaciÄ… z zestawu danych.

PrzykÅ‚ad: Zapytanie: "It was the year of our Lord 1834" 

OdpowiedÅº: "It was the year of our Lord 1834 and the streets of London were filled with protest and petition. The cause, as many re counted, was not bound in the way of private, but having taken up the same day in the day of Lord Palmerston, the public will receive a short statement of the difficulties under which the day of law has reached us. It is a matter of deep regret, that the present events in the history of the world are clear, and consequently will be'known. It is not true that the very men who first settled in the Gospel at Jerusalem should have so extensive and so interesting a record of the prosperity and prosperity" 

Na poczÄ…tku zaÅ‚oÅ¼yÅ‚em, Å¼e protest mÃ³gÅ‚ przypadkowo mieÄ‡ miejsce w tym samym roku, ale zobaczcie to: ![1834protest](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)

### **v2mini-eval1**

Wytrenowany przy uÅ¼yciu 15GB prÃ³bki z 90GB zestawu danych v2.
Model wytrenowany tylko do 10 000 krokÃ³w.

WystÄ…piÅ‚ problem z tokenizacjÄ…, ktÃ³ry powoduje, Å¼e wynik wyglÄ…da tak:

ZAPYTANIE:
Kim jest Charles Dickens?


W ho is Charles D ic ens ? D oes that work more of h ise x cell ent st ir ring , in his pl ays , int he G reat C omp any 's f arm ? What I have y et to qu ote from J ack P ick ett ? D oy oun ot know th att hey were as pe cies of g all oping , ors aw ing oft heir bre asts , or what w ast ob ed one about the t im ew hen J acks on was looking ont hew ind ow ? What ist hes uccess of an English man , and which h isson has not be end is cover ing t ome , where by to acc omp lish such a we ight ? D idy ou ever m ake ap ass age int othe old road way , ort o an anch or - b ree ze att he foot of our boat , which you m ust le ave us ? The fact is , that whe ther the wind would rise up from the pl ain on S atur day night orn o ont ide , ort ill then orth , or other wise , wes hall be com pelled t od esc ribe af orm idable bar rier , with t hes ame e ffect s ast he p resent . In th iss itu ation , at le ast , itis not to omuch t os ay that we have le ft that room . I bel ieve there are three c op ies int he " F ive H undred - fold ," to bere fer red to , ast he first n umber of our read ers wh ow is ht

Poprawione:

â€Kim jest Charles Dickens? Czy to dzieÅ‚o bardziej jego doskonaÅ‚ego poruszania, w jego sztukach, na farmie Wielkiej SpÃ³Å‚ki? Co mam jeszcze zacytowaÄ‡ od Jacka Picketta? Czy nie wiesz, Å¼e byli gatunkiem galopujÄ…cych, albo piÅ‚ujÄ…cych swoje piersi, albo co naleÅ¼aÅ‚o zrobiÄ‡ w czasie, gdy Jackson patrzyÅ‚ przez okno? Czym jest sukces Anglika i co jego syn nie odkryÅ‚ przede mnÄ…, aby osiÄ…gnÄ…Ä‡ taki ciÄ™Å¼ar? Czy kiedykolwiek przebyÅ‚eÅ› starÄ… drogÄ™, albo do bryzy kotwicznej u stÃ³p naszej Å‚odzi, ktÃ³rÄ… musisz nas opuÅ›ciÄ‡? Faktem jest, Å¼e czy wiatr powstanie z rÃ³wniny w sobotniÄ… noc, czy w poÅ‚udnie, czy aÅ¼ do pÃ³Å‚nocy, czy inaczej, bÄ™dziemy zmuszeni opisaÄ‡ potÄ™Å¼nÄ… barierÄ™, z tymi samymi skutkami co obecnie. W tej sytuacji, przynajmniej, nie jest przesadÄ… powiedzieÄ‡, Å¼e opuÅ›ciliÅ›my tamten pokÃ³j. WierzÄ™, Å¼e sÄ… trzy egzemplarze w â€PiÄ™Ä‡setkrotnymâ€, do ktÃ³rych moÅ¼na siÄ™ odwoÅ‚aÄ‡, jako pierwszy numer naszych czytelnikÃ³w, ktÃ³rzy chcÄ… t[o]...â€

### **v2mini-eval2**

Wytrenowany na tej samej prÃ³bce 15GB uÅ¼ytej do trenowania eval1.
Model ponownie wytrenowany tylko do 10K krokÃ³w.

PROMPT: Charles Darwin

Charles Darwin ECCEMACY. ProszÄ™ pana, â€” NastÄ™pujÄ…cy przypadek jest dla mnie interesujÄ…cy: â€” ByÅ‚em w Londynie przez dwa tygodnie i bardzo dotknÄ…Å‚ mnie atak reumatyzmu. Pierwszy atak reumatyzmu nastÄ…piÅ‚ tydzieÅ„ przed tym, jak siÄ™ z panem zobaczyÅ‚em, drugi gdy siÄ™ spotkaliÅ›my, trzeci gdy siÄ™ zobaczyliÅ›my, i trzeci w tym samym czasie. Drugi atak dny nie byÅ‚ jednak zwiÄ…zany z objawami gorÄ…czkowymi, lecz towarzyszyÅ‚ mu zwiÄ™kszony przepÅ‚yw moczu i obfitszy jego wypÅ‚yw. Trzeci atak nastÄ…piÅ‚ godzinÄ™ po naszym spotkaniu i zakoÅ„czyÅ‚ siÄ™ powrotem napadu dny oraz szybszym nawrotem dny. Czwarty atak rÃ³wnieÅ¼ byÅ‚ zwiÄ…zany z gorÄ…czkÄ…, ale nie zawsze z objawami gorÄ…czkowymi. Trzeci atak dny byÅ‚ dwa tygodnie po tym, jak byÅ‚ pan chory, a czwarty nastÄ…piÅ‚ po napadzie dny. Czwarty atak nastÄ…piÅ‚ dwa tygodnie po tym, jak zostaÅ‚ pan zaatakowany, i towarzyszyÅ‚o mu uczucie


##  Zbiory danych

### **v2**

- 90GB tekstÃ³w londyÅ„skich z lat 1800-1875
- 136 344 dokumentÃ³w
- CaÅ‚e 90GB nie jest jeszcze dostÄ™pne, poniewaÅ¼ nie zostaÅ‚o ztokenizowane, ale prÃ³bkÄ™ 15GB moÅ¼na znaleÅºÄ‡ tutaj: https://huggingface.co/datasets/haykgrigorian/TimeCapsuleLLM-London-1800-1875-v2-15GB

 ### Statystyki uprzedzeÅ„ 
  ![Uprzedzenie zaimkowe](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v2mini_eval1/pronoun_bias.png)

  ![Uprzedzenie geograficzne](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v2mini_eval1/geographic_bias.png)

  ![Uprzedzenie czasowe](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v2mini_eval1/temporal_bias.png)

WiÄ™cej informacji znajdziesz w [raporcie o uprzedzeniach v2](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v2mini_eval1/v2_bias_report.json).


## Jak uÅ¼ywaÄ‡

Ten projekt koncentruje siÄ™ gÅ‚Ã³wnie na gromadzeniu danych historycznych, przygotowaniu ich do treningu i budowie tokenizera. Nie bÄ™dÄ™ omawiaÄ‡ peÅ‚nego procesu treningu LLM, w tym celu odsyÅ‚am do nanoGPT autorstwa Andreja Karpathy.

### Krok 1: Zbierz i przygotuj historyczne teksty 

- Zbierz pliki .txt z ksiÄ…Å¼kami, dokumentami itp. bÄ™dÄ…cymi w domenie publicznej z wybranego okresu (np. Londyn 1800-1850)
- Trzymaj je w wybranym przez siebie oknie czasowym/miejscu  
- OczyÅ›Ä‡ pliki tekstowe za pomocÄ… skryptu lub rÄ™cznie usuÅ„ nagÅ‚Ã³wki/stopki z Project Gutenberg, nowoczesne adnotacje lub bÅ‚Ä™dy OCR.

### Krok 2: Zbuduj wÅ‚asny tokenizator

- Uruchom train_tokenizer.py lub train_tokenizer_hf.py na oczyszczonych danych.
- To da ci pliki vocab.json i merges.txt
- Te pliki definiujÄ… sÅ‚ownik oraz zasady Å‚Ä…czenia dla twojego modelu

### Krok 3: Wytrenuj swÃ³j model 

- OdnieÅ› siÄ™ do [nanoGPT Andreja Karpathyâ€™ego](https://github.com/karpathy/nanoGPT) w celu procesu treningu lub dokumentacji wybranej architektury.

# FAQ

## Czym jest Selektywne Szkolenie Czasowe?

Selektywne Szkolenie Czasowe (STT) to metodologia uczenia maszynowego, w ktÃ³rej wszystkie dane treningowe sÄ… starannie wyselekcjonowane tak, by pochodziÅ‚y z okreÅ›lonego okresu historycznego. Ma to na celu modelowanie jÄ™zyka i wiedzy tamtej epoki bez wpÅ‚ywu wspÃ³Å‚czesnych koncepcji. PrzykÅ‚adowo, obecny model (v0.5) zostaÅ‚ wytrenowany wyÅ‚Ä…cznie na danych z lat 1800-1875, nie zostaÅ‚ dostrojony, lecz wytrenowany od zera, dziÄ™ki czemu jego wyniki odzwierciedlajÄ… styl jÄ™zykowy i kontekst historyczny tamtego czasu.

## Dlaczego nie uÅ¼yÄ‡ po prostu fine-tuningu lub LoRA?

W tym projekcie prÃ³bujÄ™ stworzyÄ‡ model jÄ™zykowy wolny od wspÃ³Å‚czesnych uprzedzeÅ„. JeÅ›li zrobiÄ™ fine-tuning np. GPT-2, to juÅ¼ jest wstÄ™pnie wytrenowany i tej wiedzy nie da siÄ™ usunÄ…Ä‡. JeÅ›li wytrenujÄ™ od zera, model jÄ™zykowy nie bÄ™dzie udawaÅ‚ starego â€“ po prostu taki bÄ™dzie. Celem projektu na teraz jest stworzenie czegoÅ›, co potrafi rozumowaÄ‡ wyÅ‚Ä…cznie na podstawie wiedzy z londyÅ„skich ksiÄ…Å¼ek wydanych miÄ™dzy 1800 a 1875 rokiem.

## Na jakich danych trenowaÅ‚eÅ› model?

UÅ¼ywam ksiÄ…Å¼ek, dokumentÃ³w prawnych, gazet i innych tekstÃ³w z Londynu z lat 1800â€“1875. Lista, ktÃ³rÄ… podlinkowaÅ‚em (dla v0), zawiera okoÅ‚o 200 pozycji, ale do pierwszego treningu uÅ¼yÅ‚em tylko 50 plikÃ³w o Å‚Ä…cznej wielkoÅ›ci ~187 MB. ListÄ™ dokumentÃ³w moÅ¼esz zobaczyÄ‡ tutaj:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

Rozmiary zbiorÃ³w danych:
- v0: ~187MB
- v0.5: ~435MB 
- v1: ~6.25GB 
- v2mini-eval1: 15GB

## Jak duÅ¼e sÄ… modele?

v0: 16M parametrÃ³w

v0.5 123M parametrÃ³w

v1: 700M Parameters

v2mini-eval1: 300M Parameters

# Training Specs ? 

# v0/v0.5
GPU: Geforce rtx 4060
CPU: i5-13400F 
Ram: 16GB DDR5.

# v1
GPU: A100 SXM rented

# v2mini-eval1

GPU: A100 SXM rented





























---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2026-01-09

---