<div align="right">
  <details>
    <summary >ğŸŒ JÄ™zyk</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ç®€ä½“ä¸­æ–‡</a>
        | <a href="#" title="Coming soon">ç¹é«”ä¸­æ–‡ (wkrÃ³tce)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">æ—¥æœ¬èª</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">í•œêµ­ì–´</a>
        | <a href="#" title="Coming soon">à¤¹à¤¿à¤¨à¥à¤¦à¥€ (wkrÃ³tce)</a> |
        | <a href="#" title="Coming soon">à¹„à¸—à¸¢ (wkrÃ³tce)</a> |
        | <a href="#" title="Coming soon">FranÃ§ais (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Deutsch (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">EspaÃ±ol (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Italiano (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Ğ ÑƒÑÑĞºĞ¸Ğ¹ (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">PortuguÃªs (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Nederlands (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Polski (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">ÙØ§Ø±Ø³ÛŒ (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">TÃ¼rkÃ§e (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Tiáº¿ng Viá»‡t (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (wkrÃ³tce)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM
LLM trenowany wyÅ‚Ä…cznie na danych z okreÅ›lonych okresÃ³w historycznych, by zredukowaÄ‡ wspÃ³Å‚czesne uprzedzenia.

WyobraÅº sobie, Å¼e model AI nie tylko udaje historyczny, ale rzeczywiÅ›cie nim jest.

Zbudowany na [nanoGPT autorstwa Andreja Karpathyâ€™ego](https://github.com/karpathy/nanoGPT). Podstawowe skrypty treningowe i architektura modelu to jego praca.

# Cele projektu

TimeCapsule LLM to eksperymentalny projekt, ktÃ³ry bÄ™dzie trenowany wyÅ‚Ä…cznie na tekstach napisanych w okreÅ›lonych okresach historycznych. Celem jest symulacja Å›wiatopoglÄ…du i jÄ™zyka charakterystycznych dla wybranych epok.

# Dlaczego samo dostrajanie nie wystarcza

JeÅ›li tylko dostroisz wstÄ™pnie wytrenowany model, twÃ³j LLM i tak bÄ™dzie znaÅ‚ wspÃ³Å‚czesne pojÄ™cia. OczywiÅ›cie osiÄ…gniÄ™cie caÅ‚kowitego braku wspÃ³Å‚czesnych uprzedzeÅ„ jest trudne, ale chcÄ™ byÄ‡ jak najbliÅ¼ej tego ideaÅ‚u. Brak wspÃ³Å‚czesnych wpÅ‚ywÃ³w wymaga trenowania modelu od zera.

# Oczekiwane rezultaty

Mam nadziejÄ™, Å¼e po zakoÅ„czeniu ten model nie bÄ™dzie znaÅ‚ wspÃ³Å‚czesnych pojÄ™Ä‡ i nie bÄ™dzie w stanie rozumowaÄ‡ poza tym, na czym byÅ‚ trenowany. Nie powinien rozpoznawaÄ‡ wspÃ³Å‚czesnych pojÄ™Ä‡/sÅ‚ownictwa i mam nadziejÄ™, Å¼e nie bÄ™dzie halucynowaÅ‚ wspÃ³Å‚czesnej wiedzy.

# Aktualizacje postÄ™pÃ³w

## 9 lipca 2025

WybraÅ‚em okres: 1800-1850 oraz region: Londyn

ZebraÅ‚em listÄ™ tekstÃ³w, ksiÄ…Å¼ek, dokumentÃ³w

Jak dotÄ…d mam 50 plikÃ³w txt i wkrÃ³tce rozpocznÄ™ trening NanoGPT

BÄ™dÄ™ aktualizowaÄ‡ ten wpis wraz z postÄ™pami

## 13 lipca 2025

WytrenowaÅ‚em nanoGPT na 187MB historycznych danych tekstowych.

## 15 lipca 2025

RozpoczÄ…Å‚em pobieranie tekstÃ³w do drugiego treningu. Wszystko Å›ciÄ…gam z Internet Archive i poszerzyÅ‚em okres do 1800-1875. Aby uzyskaÄ‡ zrÃ³Å¼nicowany zestaw tekstÃ³w, moÅ¼na uÅ¼ywaÄ‡ filtrÃ³w tematycznych i wyszukiwania wedÅ‚ug miejsca publikacji, okresu i tematÃ³w na Internet Archive.

![Filtry wyszukiwania](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## 16 lipca 2025

PobraÅ‚em okoÅ‚o 500 plikÃ³w txt z Internet Archive i po ich oczyszczeniu (usuwanie biaÅ‚ych znakÃ³w, nagÅ‚Ã³wkÃ³w Gutenberg itp.) mam okoÅ‚o 500MB danych. To wciÄ…Å¼ maÅ‚y zbiÃ³r, ale poprzednio trenowaÅ‚em na 187MB, wiÄ™c powinno byÄ‡ przynajmniej trochÄ™ zauwaÅ¼alnej rÃ³Å¼nicy w wynikach po treningu drugiego modelu. Mam nadziejÄ™, Å¼e ten model bÄ™dzie przynajmniej generowaÅ‚ bardziej spÃ³jne zdania, ktÃ³re majÄ… sens. OczywiÅ›cie nie jest to gwarantowane, bo to nadal bardzo maÅ‚y zbiÃ³r, ale wiÄ™kszy niÅ¼ poprzednio.

Powinienem byÄ‡ w stanie zrobiÄ‡ to na wÅ‚asnym sprzÄ™cie, co jest dobre, bo mam szansÄ™ zobaczyÄ‡ jakieÅ› ulepszenia zanim przejdÄ™ do wiÄ™kszego zbioru, ktÃ³ry wymagaÅ‚by wynajÄ™cia GPU. Ale nie martw siÄ™, planujÄ™ niedÅ‚ugo wynajÄ…Ä‡ GPU, ale zanim to zrobiÄ™, chcÄ™ mieÄ‡ moÅ¼liwie najlepiej przygotowany i oczyszczony zbiÃ³r. Jednym z problemÃ³w jest czyszczenie, wiele tych plikÃ³w txt zawiera beÅ‚kot. Skrypty, ktÃ³rych uÅ¼yÅ‚em do czyszczenia, dziaÅ‚ajÄ…, ale nie sÄ… w 100% skuteczne.

Dzisiaj wytrenujÄ™ ten zbiÃ³r i powinno to zajÄ…Ä‡ okoÅ‚o 4-5 godzin. Gdy skoÅ„czÄ™ i przetestujÄ™, dam znaÄ‡ o efektach. DziÄ™kujÄ™ wszystkim, ktÃ³rzy Å›ledzÄ… mÃ³j projekt, dostaÅ‚em nawet kilka linkÃ³w do zasobÃ³w OCR, wiÄ™c dziÄ™kujÄ™! Mam nadziejÄ™, Å¼e wiÄ™cej osÃ³b sprÃ³buje tego podejÅ›cia i poeksperymentuje z wÅ‚asnymi zbiorami danych.


### Aktualizacja treningu

RozpoczÄ…Å‚em trening na korpusie 435MB (108 mln tokenÃ³w), na razie idzie caÅ‚kiem gÅ‚adko. Strata trenowania spadÅ‚a z 10.9 do 4.9 w pierwszych 2800 iteracjach. PrzewidujÄ™, Å¼e caÅ‚oÅ›Ä‡ zajmie 8-9 godzin. Po zakoÅ„czeniu wrzucÄ™ kolejnÄ… aktualizacjÄ™.

## 17 lipca 2025, 2:13

Trening drugiego modelu zakoÅ„czony, moja 4060 potrzebowaÅ‚a okoÅ‚o 8 godzin i 40 minut (3900 iteracji/godz.) na 33 000 iteracji (5 epok). KoÅ„cowa strata treningowa wyniosÅ‚a 3.73. Wyniki byÅ‚y zaskakujÄ…co dobre â€“ model autentycznie generuje spÃ³jne zdania w stylu XIX wieku.

## 28 lipca 2025

WrzuciÅ‚em v0.5 na Hugging Face, [sprawdÅº tutaj](https://huggingface.co/haykgrigorian/TimeCapsuleLLM) jeÅ›li chcesz. MoÅ¼esz teraz pobraÄ‡ moje repo i uruchomiÄ‡ lokalnie. Niestety nanoGPT nie dziaÅ‚a natywnie z HuggingFace, wiÄ™c trzeba pobraÄ‡ i uruchomiÄ‡ model lokalnie.

WkrÃ³tce zacznÄ™ kuratorowaÄ‡ dane do kolejnego treningu, myÅ›lÄ™ Å¼e bÄ™dÄ™ potrzebowaÅ‚ 5-10x wiÄ™cej danych, by uzyskaÄ‡ moÅ¼liwoÅ›ci rozumowania.


# Zachowanie modelu V0 i ograniczenia

WstÄ™pne podpowiedzi pokazujÄ…, Å¼e model odpowiada jÄ™zykiem i zachowaniem z XIX wieku. Na przykÅ‚ad, zapytaÅ‚em "Who art Henry?" i odpowiedziaÅ‚ "I know that man, I have did not a black, the storm." â€“ i tak, to zdanie nie ma sensu, ale LLM rozpoznaje, Å¼e pytam o osobÄ™.


![PrzykÅ‚adowe wyjÅ›cie TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

Nie ma tu wzmianki o nowoczesnych pojÄ™ciach, wyniki zawierajÄ… gÅ‚Ã³wnie sÅ‚owa i zwroty z XIX wieku.

Nadal wymaga to duÅ¼o pracy, trenowanie na 187 MB nie da modelu, ktÃ³ry generuje tekst o zÅ‚oÅ¼onym rozumowaniu.

Obecnie generuje zdania, ktÃ³re nie majÄ… peÅ‚nej struktury zdaniowej i ogÃ³lnie nie majÄ… sensu, ale to normalne przy takiej wielkoÅ›ci zbioru treningowego.

# Zachowanie modelu V0.5 i ograniczenia

To duÅ¼y postÄ™p w porÃ³wnaniu do poprzedniego modelu. Styl pisania i sÅ‚ownictwo sÄ… wiktoriaÅ„skie, a prawie kaÅ¼de zdanie jest gramatycznie poprawne i posiada wÅ‚aÅ›ciwÄ… interpunkcjÄ™. Model ten zostaÅ‚ wytrenowany od zera, wiÄ™c trzyma siÄ™ tematÃ³w z XIX wieku.

![PrzykÅ‚adowe wyjÅ›cie TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

Jest duÅ¼o halucynacji faktÃ³w. Bardzo duÅ¼o (praktycznie 100%) szczegÃ³Å‚Ã³w (daty, wydarzenia, postaci historyczne) jest zmyÅ›lonych. Zdania nie sÄ… ze sobÄ… powiÄ…zane, czasem moÅ¼e dwa zdania siÄ™ Å‚Ä…czÄ…, ale poza tym juÅ¼ nie. Kolejnym problemem jest czasem pojawiajÄ…cy siÄ™ losowy podpis â€Digitized by Googleâ€, wiÄ™c przy nastÄ™pnym treningu muszÄ™ lepiej oczyÅ›ciÄ‡ teksty. OgÃ³lnie jestem bardzo zadowolony z efektÃ³w, do LLM to jeszcze daleko, ale na pewno to juÅ¼ generator zdaÅ„.

DuÅ¼o siÄ™ uczÄ™ i w najbliÅ¼szych tygodniach zacznÄ™ analizowaÄ‡, co muszÄ™ zrobiÄ‡ lepiej. WkrÃ³tce wrzucÄ™ pliki!

# NadchodzÄ…ce plany

(ZakoÅ„czone) Zaczynam pracÄ™ nad wersjÄ… 0.5, zamiast trenowaÄ‡ na 50 ksiÄ…Å¼kach, bÄ™dÄ™ trenowaÅ‚ na 500-600. Obecnie trenujÄ™ nanoGPT na ksiÄ…Å¼kach z lat 1800-1850, szczegÃ³lnie z Londynu. SÄ… wyzwania, jak upewniÄ‡ siÄ™, Å¼e ksiÄ…Å¼ki nie sÄ… zaktualizowane czy majÄ… nowoczesne interpretacje, ale tylko oryginalne wydania z wybranego okresu.

ChcÄ™ wytrenowaÄ‡ nowy model (v1) na znacznie wiÄ™kszym korpusie, moÅ¼e 5-10x wiÄ™kszym niÅ¼ uÅ¼yty dla v0.5. Moim celem jest sprawdziÄ‡, czy umiejÄ™tnoÅ›ci rozumowania mogÄ… siÄ™ pojawiÄ‡ tylko dziÄ™ki Selektywnemu Treningowi Czasowemu, to bÄ™dzie trudniejsze zadanie i nie mam pewnoÅ›ci, czy to moÅ¼liwe z powodu ograniczeÅ„ historycznych danych. W nadchodzÄ…cych tygodniach postaram siÄ™ zebraÄ‡ dane na korpus 5-10 GB. WierzÄ™, Å¼e jeÅ›li zdobÄ™dÄ™ czyste, wysokiej jakoÅ›ci dane i wynajmÄ™ GPU, bÄ™dzie postÄ™p.

# Jak korzystaÄ‡ z tego projektu

Projekt skupia siÄ™ gÅ‚Ã³wnie na gromadzeniu historycznych danych, przygotowaniu ich do treningu i budowie tokenizera. Nie opisujÄ™ caÅ‚ego procesu trenowania LLM, do tego odsyÅ‚am do nanoGPT Andreja Karpathy.

# Krok 1: Zbierz i przygotuj historyczne teksty

Zbierz pliki .txt ksiÄ…Å¼ek, dokumentÃ³w itp. z wybranego okresu historycznego (np. Londyn 1800-1850)

MoÅ¼esz uÅ¼yÄ‡ download_texts_improved.py do pobrania ksiÄ…Å¼ek, jeÅ›li chcesz.

OczyÅ›Ä‡ pliki tekstowe uÅ¼ywajÄ…c skryptu lub rÄ™cznie usuÅ„ nagÅ‚Ã³wki/stopki z Project Gutenberg, nowoczesne adnotacje lub bÅ‚Ä™dy OCR.

prepare_dataset.py powinien dziaÅ‚aÄ‡ dobrze.

# Krok 2: Zbuduj wÅ‚asny tokenizer

Uruchom train_tokenizer.py lub train_tokenizer_hf.py na oczyszczonych danych.
Dostaniesz vocab.json i merges.txt

Te pliki definiujÄ… sÅ‚ownictwo i reguÅ‚y Å‚Ä…czenia dla twojego modelu

# Krok 3: Trenuj swÃ³j model (nanoGPT)

Zapoznaj siÄ™ z [nanoGPT by Andrej Karpathy](https://github.com/karpathy/nanoGPT) w celu przeprowadzenia treningu.

MoÅ¼esz trenowaÄ‡ inny LLM, jeÅ›li chcesz, ale ja uÅ¼yÅ‚em nanoGPT

# FAQ

## Czym jest Selektywny Trening Czasowy?

Selective Temporal Training (STT) to metodologia uczenia maszynowego, w ktÃ³rej wszystkie dane treningowe sÄ… specjalnie kuratorowane, by pochodziÅ‚y z okreÅ›lonego okresu historycznego. Robi siÄ™ to, aby modelowaÄ‡ jÄ™zyk i wiedzÄ™ tej epoki bez wpÅ‚ywu wspÃ³Å‚czesnych pojÄ™Ä‡. Na przykÅ‚ad obecny model (v0.5) jest trenowany wyÅ‚Ä…cznie na danych z lat 1800-1875, nie jest to fine-tuning, tylko trening od zera, dziÄ™ki czemu wynik odzwierciedla styl jÄ™zykowy i kontekst historyczny tego okresu.

## Dlaczego nie po prostu fine-tuning lub LoRA?

W tym projekcie prÃ³bujÄ™ stworzyÄ‡ model jÄ™zykowy pozbawiony wspÃ³Å‚czesnych uprzedzeÅ„. JeÅ›li zrobiÄ™ fine-tuning np. GPT-2, to juÅ¼ jest wytrenowany i tych informacji nie da siÄ™ usunÄ…Ä‡. JeÅ›li wytrenujÄ™ od zera, model jÄ™zykowy nie bÄ™dzie udawaÅ‚ starego, tylko taki bÄ™dzie. Celem projektu jest stworzenie czegoÅ›, co rozumuje wyÅ‚Ä…cznie na podstawie wiedzy z londyÅ„skich ksiÄ…Å¼ek z lat 1800-1850.

## Jakie dane wykorzystaÅ‚eÅ› do treningu?

UÅ¼ywam ksiÄ…Å¼ek, dokumentÃ³w prawnych, gazet i innych pism z Londynu z lat 1800â€“1850. Lista, ktÃ³rÄ… podlinkowaÅ‚em, zawiera ok. 200 pozycji, ale do pierwszego treningu uÅ¼yÅ‚em tylko 50 plikÃ³w o Å‚Ä…cznej wielkoÅ›ci ~187 MB. MoÅ¼esz zobaczyÄ‡ listÄ™ dokumentÃ³w:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## Jak duÅ¼y jest model w wersji 0?

Model jest teraz bardzo maÅ‚y, robiÄ™ to dla zabawy, trzymajÄ…c siÄ™ zasady braku wspÃ³Å‚czesnych ÅºrÃ³deÅ‚. Ma prawie 16 milionÃ³w parametrÃ³w, ale zaczynam zbieraÄ‡ wiÄ™cej starych tekstÃ³w, by rozpoczÄ…Ä‡ nowy trening modelu. BÄ™dÄ™ informowaÄ‡ na bieÅ¼Ä…co.

## Specyfikacja treningu?

GPU: Geforce rtx 4060
CPU: i5-13400F
Ram: 16GB DDR5.


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-08-02

---