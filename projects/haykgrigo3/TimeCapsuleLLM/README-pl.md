
<div align="right">
  <details>
    <summary >ğŸŒ JÄ™zyk</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ç®€ä½“ä¸­æ–‡</a>
        | <a href="#" title="Coming soon">ç¹é«”ä¸­æ–‡ (wkrÃ³tce)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">æ—¥æœ¬èª</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">í•œêµ­ì–´</a>
        | <a href="#" title="Coming soon">à¤¹à¤¿à¤¨à¥à¤¦à¥€ (wkrÃ³tce)</a> |
        | <a href="#" title="Coming soon">à¹„à¸—à¸¢ (wkrÃ³tce)</a> |
        | <a href="#" title="Coming soon">FranÃ§ais (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Deutsch (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">EspaÃ±ol (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Italiano (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Ğ ÑƒÑÑĞºĞ¸Ğ¹ (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">PortuguÃªs (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Nederlands (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Polski (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">ÙØ§Ø±Ø³ÛŒ (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">TÃ¼rkÃ§e (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Tiáº¿ng Viá»‡t (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (wkrÃ³tce)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM
LLM trenowany wyÅ‚Ä…cznie na danych z wybranych okresÃ³w czasu, aby ograniczyÄ‡ wspÃ³Å‚czesne uprzedzenia.

WyobraÅº sobie, Å¼e model AI nie tylko udaje historyczny, lecz rzeczywiÅ›cie nim jest.

Zbudowany na [nanoGPT Andreja Karpathy](https://github.com/karpathy/nanoGPT) GÅ‚Ã³wne skrypty treningowe oraz architektura modelu to jego dzieÅ‚o. 

# Cele projektu
TimeCapsule LLM to eksperymentalny projekt, ktÃ³ry bÄ™dzie trenowany wyÅ‚Ä…cznie na tekstach napisanych w okreÅ›lonych okresach historycznych. Celem jest zasymulowanie Å›wiatopoglÄ…du i jÄ™zyka charakterystycznego dla konkretnych epok.

# Dlaczego samo fine-tuning nie wystarczy

JeÅ›li tylko dostroisz wczeÅ›niej wytrenowany model, TwÃ³j LLM nadal bÄ™dzie znaÅ‚ wspÃ³Å‚czesne pojÄ™cia. OczywiÅ›cie osiÄ…gniÄ™cie caÅ‚kowitego braku wspÃ³Å‚czesnych naleciaÅ‚oÅ›ci jest trudne, ale chcÄ™ siÄ™ do tego maksymalnie zbliÅ¼yÄ‡. Brak wspÃ³Å‚czesnych naleciaÅ‚oÅ›ci wymaga trenowania modelu od zera.

# Oczekiwane rezultaty

Mam nadziejÄ™, Å¼e gdy model bÄ™dzie gotowy, nie bÄ™dzie znaÅ‚ wspÃ³Å‚czesnych pojÄ™Ä‡ i nie bÄ™dzie potrafiÅ‚ wykraczaÄ‡ poza wiedzÄ™, na ktÃ³rej zostaÅ‚ wytrenowany. Nie powinien rozpoznawaÄ‡ wspÃ³Å‚czesnego sÅ‚ownictwa/pojÄ™Ä‡ ani halucynowaÄ‡ wspÃ³Å‚czesnej wiedzy.

# Aktualizacje postÄ™pÃ³w

## 9 lipca 2025

WybraÅ‚em okres czasowy 1800-1850 oraz region: Londyn

ZebraÅ‚em listÄ™ tekstÃ³w, ksiÄ…Å¼ek, dokumentÃ³w

Jak dotÄ…d zebraÅ‚em 50 plikÃ³w txt i wkrÃ³tce rozpocznÄ™ trening NanoGPT

BÄ™dÄ™ aktualizowaÅ‚ ten wpis w miarÄ™ postÄ™pu prac

## 13 lipca 2025

WytrenowaÅ‚em nanoGPT na 187 MB danych historycznych.

## 15 lipca 2025

RozpoczÄ…Å‚em pobieranie tekstÃ³w do drugiego treningu. Wszystko pobieram z Internet Archive i poszerzyÅ‚em okres do 1800-1875. Aby uzyskaÄ‡ rÃ³Å¼norodne teksty, moÅ¼na uÅ¼ywaÄ‡ filtrÃ³w tematycznych i wyszukiwania wedÅ‚ug miejsca publikacji, okresu i tematÃ³w w Internet Archive.

![Filtry wyszukiwania](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## 16 lipca 2025

PobraÅ‚em okoÅ‚o 500 plikÃ³w txt z Internet Archive, a po oczyszczeniu ich (usuniÄ™ciu biaÅ‚ych znakÃ³w, nagÅ‚Ã³wkÃ³w Gutenberga itd.) mam okoÅ‚o 500 MB danych. To niewielki zbiÃ³r, ale ostatnio trenowaÅ‚em na 187 MB, wiÄ™c powinny byÄ‡ przynajmniej jakieÅ› zauwaÅ¼alne rÃ³Å¼nice w wynikach po wytrenowaniu drugiego modelu. Mam nadziejÄ™, Å¼e ten model bÄ™dzie w stanie generowaÄ‡ bardziej spÃ³jne zdania, ktÃ³re majÄ… sens. OczywiÅ›cie nie ma na to gwarancji, bo to wciÄ…Å¼ bardzo maÅ‚y zbiÃ³r, ale wiÄ™cej niÅ¼ miaÅ‚em ostatnio.

To powinno byÄ‡ wykonalne na moim sprzÄ™cie, co jest dobre, bo mam nadziejÄ™ zauwaÅ¼yÄ‡ jakieÅ› postÄ™py zanim przejdÄ™ do wiÄ™kszego zbioru danych, co wymagaÅ‚oby wynajÄ™cia GPU. Ale nie martwcie siÄ™, wciÄ…Å¼ planujÄ™ wkrÃ³tce wynajÄ…Ä‡ GPU, ale zanim to zrobiÄ™ chcÄ™ siÄ™ upewniÄ‡, Å¼e mÃ³j zbiÃ³r danych jest jak najlepiej wyselekcjonowany i oczyszczony. Jednym z problemÃ³w jest wÅ‚aÅ›nie czyszczenie â€” wiele z tych plikÃ³w txt ma pomieszane Å›mieci. Skrypty, ktÃ³rych uÅ¼ywaÅ‚em do czyszczenia, dziaÅ‚ajÄ…, ale nie sÄ… w 100% skuteczne.

Zamierzam wytrenowaÄ‡ ten zbiÃ³r dzisiaj i powinno to potrwaÄ‡ okoÅ‚o 4-5 godzin. Po zakoÅ„czeniu i przetestowaniu dam znaÄ‡ o postÄ™pach. Jeszcze raz dziÄ™kujÄ™ wszystkim, ktÃ³rzy Å›ledzÄ… mÃ³j projekt, nawet dostaÅ‚em linki do zasobÃ³w OCR, wiÄ™c dziÄ™kujÄ™! Mam nadziejÄ™, Å¼e wiÄ™cej osÃ³b sprÃ³buje tego podejÅ›cia i poeksperymentuje z wÅ‚asnymi zbiorami danych.


## 12 sierpnia 2025

Wersja 1 jest ukoÅ„czona, ta wersja wykorzystuje Phi 1.5 (700M parametrÃ³w) i zostaÅ‚a wytrenowana na zbiorze danych zawierajÄ…cym prawie 7000 tekstÃ³w. GÅ‚Ã³wne ulepszenia tego modelu to jego zdolnoÅ›Ä‡ do czasami odnoszenia siÄ™ do prawdziwych nazwisk i wydarzeÅ„ historycznych. Wersje 0 i 0.5 nie potrafiÅ‚y tego, gdy podawaÅ‚em tym modelom coÅ› jak "W roku 1821", wynik to byÅ‚y tylko halucynacje, rzeczy, ktÃ³re nigdy siÄ™ nie wydarzyÅ‚y. PrzykÅ‚ad tego z Wersji 1:

MÃ³j prompt: ByÅ‚ to rok PaÅ„ski 1834, 

Wynik: "ByÅ‚ to rok PaÅ„ski 1834, a ulice Londynu byÅ‚y peÅ‚ne protestÃ³w i petycji. PowÃ³d, jak wielu wspominaÅ‚o, nie byÅ‚ zwiÄ…zany z prywatnymi sprawami, lecz zostaÅ‚ podjÄ™ty tego samego dnia w dniu lorda Palmerstona, spoÅ‚eczeÅ„stwo otrzyma krÃ³tkie oÅ›wiadczenie o trudnoÅ›ciach, z jakimi dzieÅ„ prawa nas dotknÄ…Å‚. To sprawa gÅ‚Ä™bokiego Å¼alu, Å¼e obecne wydarzenia w historii Å›wiata sÄ… jasne i w zwiÄ…zku z tym bÄ™dÄ… znane. Nie jest prawdÄ…, Å¼e ludzie, ktÃ³rzy pierwsi osiedlili siÄ™ w Ewangelii w Jerozolimie, powinni mieÄ‡ tak rozlegÅ‚y i interesujÄ…cy zapis dobrobytu i dobrobytu" 

Na poczÄ…tku sÄ…dziÅ‚em, Å¼e to moÅ¼e byÄ‡ przypadek, ale spÃ³jrzcie na to: ![1834protest](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)


### Aktualizacja treningu 

RozpoczÄ…Å‚em trening na korpusie 435MB (108 mln tokenÃ³w), idzie caÅ‚kiem pÅ‚ynnie. Strata treningowa spadÅ‚a z 10,9 do 4,9 w pierwszych 2800 iteracjach. PrzewidujÄ™, Å¼e zajmie to okoÅ‚o 8 lub 9 godzin. OpublikujÄ™ kolejnÄ… aktualizacjÄ™, gdy bÄ™dzie gotowe.

## 17 lipca 2025

Trening drugiego modelu zakoÅ„czony, mÃ³j 4060 potrzebowaÅ‚ okoÅ‚o 8 godzin i 40 minut (3900 iteracji/godz.) na 33 000 iteracji (5 epok). KoÅ„cowa strata treningowa wyniosÅ‚a 3,73. Wyniki byÅ‚y zaskakujÄ…co dobre, model rzeczywiÅ›cie generuje spÃ³jne zdania w stylu XIX wieku.

## 28 lipca 2025 

WgraÅ‚em wersjÄ™ v0.5 na Hugging Face, [SprawdÅº tutaj](https://huggingface.co/haykgrigorian/TimeCapsuleLLM) jeÅ›li chcesz. MoÅ¼esz teraz pobraÄ‡ moje repozytorium i uruchomiÄ‡ je lokalnie. Niestety nanoGPT nie dziaÅ‚a natywnie z HuggingFace, wiÄ™c musisz pobraÄ‡ i uruchomiÄ‡ model lokalnie.

Zaczynam takÅ¼e zbieraÄ‡ dane do kolejnego treningu, sÄ…dzÄ™, Å¼e bÄ™dÄ™ potrzebowaÅ‚ 5-10x wiÄ™cej danych, aby uzyskaÄ‡ zdolnoÅ›Ä‡ rozumowania.

## 2 sierpnia 2025

WkrÃ³tce zacznÄ™ pracÄ™ nad WersjÄ… 1. BÄ™dÄ™ musiaÅ‚ przejÅ›Ä‡ z architektury nanoGPT na coÅ› bardziej nowoczesnego. Mam na uwadze kilka architektur LLM open-source, w tym: OpenLLaMA v3, Phi-2 i Qwen 1.5B. Aby wesprzeÄ‡ przejÅ›cie do V1, bÄ™dÄ™ musiaÅ‚ starannie wyselekcjonowaÄ‡ znacznie wiÄ™kszy i bardziej zrÃ³Å¼nicowany zbiÃ³r danych. PotrzebujÄ™ przynajmniej 5GB czystych danych treningowych.

# Zachowanie modelu V0 i ograniczenia 

Wczesne prompty pokazujÄ…, Å¼e model odpowiada jÄ™zykiem i zachowaniem z XIX wieku. Na przykÅ‚ad zapytaÅ‚em go "Who art Henry?" i odpowiedziaÅ‚ "Znam tego czÅ‚owieka, nie jestem czarny, burza." i tak, to zdanie nie ma sensu, ale LLM rozpoznaje, Å¼e pytam o osobÄ™.

![PrzykÅ‚adowy wynik TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

Nie ma wzmianki o nowoczesnych koncepcjach, wyniki zawierajÄ… gÅ‚Ã³wnie sÅ‚owa i zwroty z XIX wieku.

WciÄ…Å¼ wymaga duÅ¼o pracy, trenowanie na 187MB nie pozwoli uzyskaÄ‡ modelu generujÄ…cego tekst o zÅ‚oÅ¼onym rozumowaniu. 

Obecnie generuje zdania, ktÃ³re nie majÄ… peÅ‚nej struktury i ogÃ³lnie nie majÄ… sensu, ale to normalne przy takim rozmiarze treningowym.

# Zachowanie i ograniczenia modelu V0.5

To wyraÅºna poprawa w porÃ³wnaniu z poprzednim modelem. Styl pisania i sÅ‚ownictwo sÄ… wiktoriaÅ„skie, a prawie kaÅ¼de zdanie jest poprawne gramatycznie z wÅ‚aÅ›ciwÄ… interpunkcjÄ…. Ponownie, model trenowany jest od zera, wiÄ™c trzyma siÄ™ tematÃ³w z XIX wieku.

![PrzykÅ‚adowy wynik TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

Jest duÅ¼o halucynacji faktÃ³w. Wiele (jak 100%) szczegÃ³Å‚Ã³w (daty, wydarzenia, postacie historyczne) jest zmyÅ›lonych. Dodatkowo zdania nie sÄ… ze sobÄ… powiÄ…zane, czasem moÅ¼e dwa zdania siÄ™ odnosiÄ‡ do siebie, ale poza tym nie. Kolejnym problemem jest czasami pojawiajÄ…cy siÄ™ nagÅ‚Ã³wek â€Digitized by Googleâ€, wiÄ™c przy nastÄ™pnym trenowaniu muszÄ™ naprawdÄ™ upewniÄ‡ siÄ™, Å¼e teksty sÄ… dobrze oczyszczone. OgÃ³lnie jestem bardzo zadowolony z rezultatÃ³w, to jeszcze nie LLM, ale na pewno generator zdaÅ„.

DuÅ¼o siÄ™ uczÄ™ i w nadchodzÄ…cych tygodniach zacznÄ™ rozumieÄ‡, co muszÄ™ robiÄ‡ lepiej. WkrÃ³tce wrzucÄ™ pliki!

# Zachowanie i ograniczenia modelu V1

WkrÃ³tce wrzucÄ™ przykÅ‚adowy wynik i dokonam porÃ³wnaÅ„ miÄ™dzy trzema modelami z tym samym promptem. WrzucÄ™ teÅ¼ V1 na huggingface, tak jak poprzedniÄ… wersjÄ™, mÃ³j profil huggingface znajdziesz tutaj: https://huggingface.co/haykgrigorian/TimeCapsuleLLM

# NadchodzÄ…ce plany

(Zrealizowane) ZacznÄ™ pracÄ™ nad wersjÄ… 0.5, zamiast trenowaÄ‡ na 50 ksiÄ…Å¼kach, bÄ™dÄ™ trenowaÄ‡ na 500-600. Obecnie trenujÄ™ nanoGPT na ksiÄ…Å¼kach z lat 1800-1850, szczegÃ³lnie z Londynu. SÄ… pewne wyzwania, np. upewnienie siÄ™, Å¼e ksiÄ…Å¼ki nie sÄ… aktualizowane ani majÄ… nowoczesnych interpretacji, tylko oryginalne publikacje z wybranego okresu.

ChcÄ™ wytrenowaÄ‡ nowy model (v1) na znacznie wiÄ™kszym korpusie, moÅ¼e 5-10 razy wiÄ™kszym niÅ¼ ten uÅ¼yty w v0.5. Moim celem jest sprawdzenie, czy dziÄ™ki selektywnemu treningowi czasowemu pojawiÄ… siÄ™ zdolnoÅ›ci rozumowania; to trudniejsze zadanie i nie jestem pewien, czy to moÅ¼liwe ze wzglÄ™du na ograniczenia danych historycznych. W najbliÅ¼szych tygodniach sprÃ³bujÄ™ zebraÄ‡ wystarczajÄ…co duÅ¼o danych na korpus 5-10GB. WierzÄ™, Å¼e jeÅ›li zdobÄ™dÄ™ czyste, wysokiej jakoÅ›ci dane i wynajmÄ™ GPU, bÄ™dÄ… postÄ™py.

# Jak korzystaÄ‡ z tego projektu

Projekt skupia siÄ™ gÅ‚Ã³wnie na kuracji danych historycznych, ich przygotowaniu do treningu i budowie tokenizera. Nie bÄ™dÄ™ tu omawiaÄ‡ peÅ‚nego procesu treningu LLM, do tego odsyÅ‚am do nanoGPT Andreja Karpathy.

# Krok 1: Zbierz i przygotuj teksty historyczne

Zbierz pliki .txt ksiÄ…Å¼ek, dokumentÃ³w itd. z wybranego okresu (np. Londyn 1800-1850)

MoÅ¼esz uÅ¼yÄ‡ download_texts_improved.py do pobrania ksiÄ…Å¼ek, jeÅ›li potrzebujesz.

WyczyÅ›Ä‡ pliki tekstowe za pomocÄ… skryptu lub rÄ™cznie usuÅ„ nagÅ‚Ã³wki/stopki z Project Gutenberg, nowoczesne adnotacje czy bÅ‚Ä™dy OCR.

prepare_dataset.py powinien dziaÅ‚aÄ‡ poprawnie.

# Krok 2: Zbuduj wÅ‚asny tokenizer

Uruchom train_tokenizer.py lub train_tokenizer_hf.py na oczyszczonych danych.
To wygeneruje vocab.json i merges.txt
Te pliki definiujÄ… sÅ‚ownictwo i zasady scalania dla Twojego modelu

# Krok 3: Trenuj swÃ³j model (nanoGPT)

Zapoznaj siÄ™ z [nanoGPT autorstwa Andreja Karpathyâ€™ego](https://github.com/karpathy/nanoGPT) w celu poznania procesu treningowego.

MoÅ¼esz trenowaÄ‡ inny LLM, jeÅ›li chcesz, ale ja uÅ¼yÅ‚em nanoGPT

# FAQ

## Czym jest Selektywne Trening Czasowy?

Selektywny Trening Czasowy (STT) to metodologia uczenia maszynowego, w ktÃ³rej wszystkie dane treningowe sÄ… starannie wyselekcjonowane, aby mieÅ›ciÅ‚y siÄ™ w okreÅ›lonym okresie historycznym. Robi siÄ™ to w celu modelowania jÄ™zyka i wiedzy z tamtej epoki, bez wpÅ‚ywu wspÃ³Å‚czesnych koncepcji. Na przykÅ‚ad, obecny model, ktÃ³ry mam (v0.5), zostaÅ‚ wytrenowany wyÅ‚Ä…cznie na danych z lat 1800â€“1875, nie zostaÅ‚ dostrojony, lecz wytrenowany od zera, dziÄ™ki czemu wynik odzwierciedla styl jÄ™zykowy i kontekst historyczny tamtego okresu.

## Dlaczego nie uÅ¼yÄ‡ po prostu fine-tuningu lub LoRA?

W tym projekcie prÃ³bujÄ™ stworzyÄ‡ model jÄ™zykowy wolny od wspÃ³Å‚czesnych uprzedzeÅ„. JeÅ›li dostrojÄ™ coÅ› takiego jak GPT-2, to juÅ¼ jest ono wstÄ™pnie wytrenowane i te informacje nie zniknÄ…. JeÅ›li wytrenujÄ™ model od zera, nie bÄ™dzie on udawaÅ‚ â€staregoâ€ â€” po prostu taki bÄ™dzie. Celem projektu jest obecnie stworzenie czegoÅ›, co potrafi rozumowaÄ‡ wyÅ‚Ä…cznie na podstawie wiedzy z londyÅ„skich ksiÄ…Å¼ek wydanych miÄ™dzy 1800 a 1850 rokiem.

## Jakich danych uÅ¼yÅ‚eÅ› do trenowania?

UÅ¼ywam ksiÄ…Å¼ek, dokumentÃ³w prawnych, gazet i innych tekstÃ³w z Londynu z lat 1800â€“1850. Lista, ktÃ³rÄ… podlinkowaÅ‚em, zawiera okoÅ‚o 200 pozycji, ale do pierwszego treningu uÅ¼yÅ‚em tylko 50 plikÃ³w (~187 MB). MoÅ¼esz zobaczyÄ‡ listÄ™ dokumentÃ³w:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## Jak duÅ¼e sÄ… modele?

V0: 16M parametrÃ³w

V0.5: 123M parametrÃ³w

V1: 700M parametrÃ³w

# Specyfikacja treningu?

#V0/V0.5
GPU: Geforce rtx 4060
CPU: i5-13400F
Ram: 16GB DDR5.

#V1

GPU: A100 rented









---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-08-12

---