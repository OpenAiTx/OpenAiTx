
<div align="right">
  <details>
    <summary >ğŸŒ JÄ™zyk</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ç®€ä½“ä¸­æ–‡</a>
        | <a href="#" title="Coming soon">ç¹é«”ä¸­æ–‡ (wkrÃ³tce)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">æ—¥æœ¬èª</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">í•œêµ­ì–´</a>
        | <a href="#" title="Coming soon">à¤¹à¤¿à¤¨à¥à¤¦à¥€ (wkrÃ³tce)</a> |
        | <a href="#" title="Coming soon">à¹„à¸—à¸¢ (wkrÃ³tce)</a> |
        | <a href="#" title="Coming soon">FranÃ§ais (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Deutsch (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">EspaÃ±ol (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Italiano (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Ğ ÑƒÑÑĞºĞ¸Ğ¹ (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">PortuguÃªs (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Nederlands (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Polski (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">ÙØ§Ø±Ø³ÛŒ (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">TÃ¼rkÃ§e (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Tiáº¿ng Viá»‡t (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (wkrÃ³tce)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM

*Model jÄ™zykowy wytrenowany **od podstaw** wyÅ‚Ä…cznie na danych z wybranych miejsc i okresÃ³w, aby ograniczyÄ‡ wspÃ³Å‚czesne uprzedzenia i odtworzyÄ‡ styl, sÅ‚ownictwo oraz Å›wiatopoglÄ…d epoki.*

WyobraÅº sobie, Å¼e model AI nie tylko udaje, Å¼e jest historyczny, ale faktycznie taki jest.

v0 i v0.5 oparte na [nanoGPT Andreja Karpathy](https://github.com/karpathy/nanoGPT). GÅ‚Ã³wne skrypty treningowe oraz architektura modelu to jego dzieÅ‚o. 

v1 zbudowany na [Phi 1.5 by Microsoft](https://huggingface.co/microsoft/phi-1_5)

##  Zachowanie modelu i ograniczenia

### **v0**  

Wczesne podpowiedzi pokazujÄ…, Å¼e model odpowiada jÄ™zykiem i zachowaniem z XIX wieku.  
PrzykÅ‚ad: PodpowiedÅº: "Who art Henry?" i odpowiedÅº: "I know that man, I have did not a black, the storm." 

![PrzykÅ‚adowy wynik TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

- Brak wzmianki o nowoczesnych konceptach  
- W wiÄ™kszoÅ›ci sÅ‚ownictwo zgodne z epokÄ…  
- Zdania sÄ… w wiÄ™kszoÅ›ci niespÃ³jne (oczekiwane dla ~187MB danych treningowych)

### **v0.5** 

ZnaczÄ…ca poprawa w stosunku do v0.  
- WiktoriaÅ„ski styl pisania, poprawna interpunkcja, w wiÄ™kszoÅ›ci gramatyczne zdania  
- Nadal wysoki poziom halucynacji faktÃ³w  
- Szum OCR (â€Digitized by Googleâ€) nadal obecny w wynikach

![PrzykÅ‚adowy wynik TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

### **v1**

Pierwszy model, ktÃ³ry przypomina sobie i Å‚Ä…czy prawdziwe wydarzenie historyczne z rzeczywistÄ… postaciÄ… z zestawu danych.

PrzykÅ‚ad: PodpowiedÅº: "It was the year of our Lord 1834" 

OdpowiedÅº: "It was the year of our Lord 1834 and the streets of London were filled with protest and petition. The cause, as many re counted, was not bound in the way of private, but having taken up the same day in the day of Lord Palmerston, the public will receive a short statement of the difficulties under which the day of law has reached us. It is a matter of deep regret, that the present events in the history of the world are clear, and consequently will be'known. It is not true that the very men who first settled in the Gospel at Jerusalem should have so extensive and so interesting a record of the prosperity and prosperity" 

Na poczÄ…tku zaÅ‚oÅ¼yÅ‚em, Å¼e protest mÃ³gÅ‚ przypadkowo mieÄ‡ miejsce w tym samym roku, ale spÃ³jrz na to: ![1834protest](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)

### Dlaczego to jest waÅ¼ne:

To pierwszy przykÅ‚ad, gdy jeden z moich modeli poÅ‚Ä…czyÅ‚ rok z prawdziwym wydarzeniem historycznym i rzeczywistÄ… osobÄ… zwiÄ…zanÄ… z tym wydarzeniem (Lord Palmerston). WczeÅ›niejsze modele (v0 i v0.5) potrafiÅ‚y naÅ›ladowaÄ‡ style pisania z XIX wieku, ale zawsze wymyÅ›laÅ‚y wydarzenia, osoby i fakty. To pokazuje, Å¼e model zaczyna zapamiÄ™tywaÄ‡ informacje z zestawu danych 

## NadchodzÄ…ce plany 

- Na Internet Archive znajduje siÄ™ prawie 175 000 tekstÃ³w opublikowanych w Londynie w latach 1800-1875
- PlanujÄ™ rozszerzyÄ‡ korpus i bardziej go oczyÅ›ciÄ‡, aby uzyskaÄ‡ lepsze zdolnoÅ›ci rozumowania
- Rozszerzenie na rÃ³Å¼ne regiony i okresy czasu w celu stworzenia bardziej historycznych modeli


## Jak korzystaÄ‡

Ten projekt koncentruje siÄ™ gÅ‚Ã³wnie na kuracji danych historycznych, przygotowaniu ich do treningu i budowie tokenizera. Nie bÄ™dÄ™ omawiaÄ‡ peÅ‚nego procesu treningu LLM, w tym celu odsyÅ‚am do nanoGPT Andreja Karpathy.

### Krok 1: Zbierz i przygotuj teksty historyczne

- Zbierz pliki .txt ksiÄ…Å¼ek, dokumentÃ³w itd. z domeny publicznej z wybranego okresu (np. Londyn 1800-1850)
- Trzymaj siÄ™ wybranego okna czasowego/miejsca  
- OczyÅ›Ä‡ pliki tekstowe za pomocÄ… skryptu lub rÄ™cznie usuÅ„ nagÅ‚Ã³wki/stopki z Project Gutenberg, nowoczesne adnotacje oraz bÅ‚Ä™dy OCR.

### Krok 2: Zbuduj wÅ‚asny tokenizer

- Uruchom train_tokenizer.py lub train_tokenizer_hf.py na oczyszczonych danych.
- To wygeneruje vocab.json i merges.txt
- Te pliki definiujÄ… sÅ‚ownik i reguÅ‚y Å‚Ä…czenia dla Twojego modelu

### Krok 3: Wytrenuj swÃ³j model

- Zapoznaj siÄ™ z [nanoGPT Andreja Karpathy](https://github.com/karpathy/nanoGPT) w celu poznania procesu treningu lub dokumentacjÄ… wybranej architektury.

# FAQ

## Czym jest Selektywne Trening Temporalny?

Selektywne Trening Temporalny (STT) to metodologia uczenia maszynowego, w ktÃ³rej wszystkie dane treningowe sÄ… specjalnie dobrane, aby mieÅ›ciÅ‚y siÄ™ w okreÅ›lonym okresie historycznym. Robi siÄ™ to, aby modelowaÄ‡ jÄ™zyk i wiedzÄ™ z danej epoki bez wpÅ‚ywu nowoczesnych koncepcji. Na przykÅ‚ad, obecny model (v0.5) jest wytrenowany wyÅ‚Ä…cznie na danych z lat 1800-1875, nie jest dostrajany, lecz trenowany od zera, co skutkuje wynikami odzwierciedlajÄ…cymi styl jÄ™zykowy i kontekst historyczny tamtego okresu.

## Dlaczego nie uÅ¼yÄ‡ po prostu fine-tuningu lub LoRA?

W tym projekcie staram siÄ™ stworzyÄ‡ model jÄ™zykowy wolny od wspÃ³Å‚czesnych uprzedzeÅ„. JeÅ›li dostrojÄ™ coÅ› takiego jak GPT-2, to jest juÅ¼ wstÄ™pnie wytrenowane i tych informacji nie da siÄ™ usunÄ…Ä‡. JeÅ›li wytrenujÄ™ od zera, model jÄ™zykowy nie bÄ™dzie udawaÅ‚ starego, po prostu nim bÄ™dzie. Celem projektu jest obecnie stworzenie czegoÅ›, co potrafi rozumowaÄ‡ wyÅ‚Ä…cznie na podstawie wiedzy z londyÅ„skich ksiÄ…Å¼ek wydanych w latach 1800-1875.

## Jakie dane wykorzystaÅ‚eÅ› do treningu?

Korzystam z ksiÄ…Å¼ek, dokumentÃ³w prawnych, gazet i innych pism z Londynu z lat 1800â€“1875. Lista, ktÃ³rÄ… podlinkowaÅ‚em (dla v0), zawiera okoÅ‚o 200 pozycji, ale do pierwszego treningu uÅ¼yÅ‚em tylko 50 plikÃ³w o Å‚Ä…cznej wielkoÅ›ci ~187 MB. MoÅ¼esz zobaczyÄ‡ listÄ™ dokumentÃ³w:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt


Rozmiary zbiorÃ³w danych:
v0: ~187MB
v0.5: ~435MB 
v1: ~6,25GB 

## Jak duÅ¼e sÄ… modele?

V0: 16M parametrÃ³w

V0.5 123M parametrÃ³w

V1: 700M parametrÃ³w

# Specyfikacje treningu?

# V0/V0.5
GPU: Geforce rtx 4060
CPU: i5-13400F 
Ram: 16GB DDR5.

# V1
GPU: A100 wynajÄ™ty














---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-08-19

---