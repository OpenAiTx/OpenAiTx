
<div align="right">
  <details>
    <summary >ğŸŒ JÄ™zyk</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ç®€ä½“ä¸­æ–‡</a>
        | <a href="#" title="Coming soon">ç¹é«”ä¸­æ–‡ (wkrÃ³tce)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">æ—¥æœ¬èª</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">í•œêµ­ì–´</a>
        | <a href="#" title="Coming soon">à¤¹à¤¿à¤¨à¥à¤¦à¥€ (wkrÃ³tce)</a> |
        | <a href="#" title="Coming soon">à¹„à¸—à¸¢ (wkrÃ³tce)</a> |
        | <a href="#" title="Coming soon">FranÃ§ais (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Deutsch (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">EspaÃ±ol (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Italiano (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Ğ ÑƒÑÑĞºĞ¸Ğ¹ (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">PortuguÃªs (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Nederlands (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Polski (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">ÙØ§Ø±Ø³ÛŒ (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">TÃ¼rkÃ§e (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Tiáº¿ng Viá»‡t (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (wkrÃ³tce)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM
LLM wytrenowany wyÅ‚Ä…cznie na danych z okreÅ›lonych okresÃ³w czasu, aby ograniczyÄ‡ wspÃ³Å‚czesne uprzedzenia.

WyobraÅº sobie, Å¼e model AI nie tylko udaje historyczny â€” on faktycznie taki jest.

Zbudowano na [nanoGPT Andreja Karpathy'ego](https://github.com/karpathy/nanoGPT). Podstawowe skrypty treningowe i architektura modelu to jego dzieÅ‚o.

# Cele projektu

TimeCapsule LLM to eksperymentalny projekt, ktÃ³ry bÄ™dzie trenowany wyÅ‚Ä…cznie na tekstach napisanych w okreÅ›lonych epokach. Celem jest symulacja Å›wiatopoglÄ…du i jÄ™zyka wybranych okresÃ³w historycznych.

# Dlaczego samo dostrajanie nie wystarcza

JeÅ›li tylko dostroisz wstÄ™pnie wytrenowany model, TwÃ³j LLM nadal bÄ™dzie znaÅ‚ wspÃ³Å‚czesne pojÄ™cia. OczywiÅ›cie osiÄ…gniÄ™cie caÅ‚kowitego braku wspÃ³Å‚czesnych uprzedzeÅ„ jest trudne, ale chcÄ™ zbliÅ¼yÄ‡ siÄ™ do tego jak najbardziej. Brak wspÃ³Å‚czesnych odniesieÅ„ wymaga trenowania modelu od zera.

# Oczekiwane rezultaty

Mam nadziejÄ™, Å¼e po ukoÅ„czeniu model nie bÄ™dzie znaÅ‚ wspÃ³Å‚czesnych pojÄ™Ä‡ i nie bÄ™dzie potrafiÅ‚ rozumowaÄ‡ poza tym, na czym byÅ‚ trenowany. Nie powinien rozpoznawaÄ‡ wspÃ³Å‚czesnych koncepcji/sÅ‚ownictwa i liczÄ™, Å¼e nie bÄ™dzie halucynowaÅ‚ wspÃ³Å‚czesnej wiedzy.

# Aktualizacje postÄ™pÃ³w

## 9 lipca 2025

UstaliÅ‚em okres treningowy na lata 1800-1850 oraz region: Londyn

ZebraÅ‚em listÄ™ tekstÃ³w, ksiÄ…Å¼ek, dokumentÃ³w

Do tej pory mam 50 plikÃ³w txt i wkrÃ³tce rozpocznÄ™ trening NanoGPT

BÄ™dÄ™ aktualizowaÄ‡ ten wpis wraz z postÄ™pami

## 13 lipca 2025

WytrenowaÅ‚em nanoGPT na 187MB historycznych danych tekstowych.

## 15 lipca 2025

ZaczÄ…Å‚em pobieraÄ‡ teksty do drugiej sesji treningowej. Wszystko pobieram z Internet Archive i rozszerzyÅ‚em okres do 1800-1875. Aby uzyskaÄ‡ rÃ³Å¼norodne teksty, moÅ¼na uÅ¼yÄ‡ filtrÃ³w tematycznych i wyszukiwania wedÅ‚ug miejsca publikacji, okresu oraz tematÃ³w na Internet Archive.

![Filtry wyszukiwania](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## 16 lipca 2025

PobraÅ‚em okoÅ‚o 500 plikÃ³w txt z Internet Archive i po ich oczyszczeniu (usuniÄ™cie biaÅ‚ych znakÃ³w, nagÅ‚Ã³wkÃ³w Gutenberga itp.) mam okoÅ‚o 500MB danych. To bardzo maÅ‚y zbiÃ³r, ale ostatnim razem trenowaÅ‚em na 187MB, wiÄ™c powinno byÄ‡ zauwaÅ¼alnie lepiej po treningu drugiego modelu. Mam nadziejÄ™, Å¼e ten model bÄ™dzie przynajmniej generowaÄ‡ bardziej spÃ³jne zdania, ktÃ³re majÄ… sens. OczywiÅ›cie nie jest to gwarantowane, bo to wciÄ…Å¼ mikroskopijny zbiÃ³r danych, ale wiÄ™kszy niÅ¼ poprzedni.

To powinno byÄ‡ wykonalne na moim sprzÄ™cie, co jest dobre, bo bÄ™dÄ™ mÃ³gÅ‚ zobaczyÄ‡ jakieÅ› ulepszenia zanim przejdÄ™ do wiÄ™kszego zbioru, co wymagaÅ‚oby wynajÄ™cia GPU. Ale nie martwcie siÄ™, wkrÃ³tce planujÄ™ wynajÄ…Ä‡ GPU, ale zanim to zrobiÄ™, chcÄ™ mieÄ‡ jak najbardziej wyselekcjonowany i czysty zbiÃ³r danych. Jednym z problemÃ³w jest czyszczenie â€” wiele z tych plikÃ³w txt ma wtrÄ…cone bzdury. Skrypty do czyszczenia dziaÅ‚ajÄ…, ale nie sÄ… w 100% skuteczne.

Ten zbiÃ³r danych wytrenujÄ™ dzisiaj i powinno to potrwaÄ‡ okoÅ‚o 4-5 godzin. Po zakoÅ„czeniu i przetestowaniu dam znaÄ‡. DziÄ™kujÄ™ wszystkim, ktÃ³rzy Å›ledzÄ… mÃ³j projekt, niektÃ³rzy nawet przesÅ‚ali mi linki do zasobÃ³w OCR, wiÄ™c dziÄ™kujÄ™! Mam nadziejÄ™, Å¼e wiÄ™cej osÃ³b sprÃ³buje czegoÅ› podobnego i poeksperymentuje z wÅ‚asnymi zbiorami danych.

## 28 lipca 2025

WrzuciÅ‚em wersjÄ™ v0.5 na Hugging Face, [zobacz tutaj](https://huggingface.co/haykgrigorian/TimeCapsuleLLM) jeÅ›li chcesz. MoÅ¼esz teraz pobraÄ‡ moje repo i uruchomiÄ‡ je lokalnie. Niestety nanoGPT nie dziaÅ‚a natywnie z HuggingFace, wiÄ™c musisz pobraÄ‡ i uruchomiÄ‡ model lokalnie.

RozpocznÄ™ takÅ¼e selekcjonowanie danych do kolejnego treningu â€” sÄ…dzÄ™, Å¼e bÄ™dÄ™ potrzebowaÅ‚ 5-10x wiÄ™cej danych, aby osiÄ…gnÄ…Ä‡ zdolnoÅ›Ä‡ rozumowania.

### Aktualizacja treningu

RozpoczÄ…Å‚em trening na korpusie 435MB (108 mln tokenÃ³w), na razie idzie bardzo gÅ‚adko. Strata treningowa spadÅ‚a z 10,9 do 4,9 w pierwszych 2800 iteracjach. MyÅ›lÄ™, Å¼e caÅ‚oÅ›Ä‡ potrwa okoÅ‚o 8-9 godzin. ZamieszczÄ™ kolejnÄ… aktualizacjÄ™ po zakoÅ„czeniu.

## 17 lipca 2025, 2:13

Trening drugiego modelu zakoÅ„czony, mojej 4060 zajÄ™Å‚o to okoÅ‚o 8 godzin i 40 minut (3 900 iteracji/godz.) przez 33 000 iteracji (5 epok). Finalna strata treningowa wyniosÅ‚a 3,73. Wyniki byÅ‚y zaskakujÄ…co dobre â€” model naprawdÄ™ generuje spÃ³jne zdania w stylu XIX wieku.

# Zachowanie i ograniczenia modelu v0

Pierwsze promptowania pokazujÄ…, Å¼e model odpowiada jÄ™zykiem i stylem XIX wieku. Na przykÅ‚ad zapytaÅ‚em "Who art Henry?" i odpowiedziaÅ‚ "I know that man, I have did not a black, the storm." i jasne, to zdanie nie ma sensu, ale LLM rozpoznaje, Å¼e pytam o osobÄ™.

![PrzykÅ‚adowe wyjÅ›cie TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

Nie ma wzmianki o nowoczesnych koncepcjach, wyniki zawierajÄ… gÅ‚Ã³wnie sÅ‚owa i sformuÅ‚owania z XIX wieku.

Nadal wymaga to duÅ¼o pracy, trenowanie na 187 MB nie da modelu produkujÄ…cego tekst zÅ‚oÅ¼ony pod wzglÄ™dem rozumowania.

Obecnie generowane zdania nie majÄ… peÅ‚nej struktury i ogÃ³lnie po prostu nie majÄ… sensu, ale jest to normalne przy takiej wielkoÅ›ci zbioru treningowego.

# Zachowanie i ograniczenia modelu V0.5

To miÅ‚a poprawa w porÃ³wnaniu do poprzedniego modelu. Styl pisania i sÅ‚ownictwo jest wiktoriaÅ„skie i prawie kaÅ¼de zdanie jest gramatycznie poprawne z odpowiedniÄ… interpunkcjÄ…. Ponownie, model trenowany jest od zera, wiÄ™c trzyma siÄ™ tematÃ³w z XIX wieku.

![PrzykÅ‚adowy wynik TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

Jest duÅ¼o halucynacji faktÃ³w. Wiele (praktycznie 100%) szczegÃ³Å‚Ã³w (daty, wydarzenia, postaci historyczne) jest zmyÅ›lonych. RÃ³wnieÅ¼ zdania nie majÄ… ze sobÄ… powiÄ…zaÅ„, czasem moÅ¼e dwa siÄ™ Å‚Ä…czÄ…, ale poza tym juÅ¼ nie. Kolejnym problemem jest czasem pojawiajÄ…ca siÄ™ stopka â€Digitized by Googleâ€, wiÄ™c nastÄ™pnym razem muszÄ™ lepiej wyczyÅ›ciÄ‡ teksty. OgÃ³lnie jestem bardzo zadowolony z rezultatÃ³w, to jeszcze nie jest LLM, ale na pewno generator zdaÅ„.

DuÅ¼o siÄ™ uczÄ™ i w najbliÅ¼szych tygodniach bÄ™dÄ™ analizowaÄ‡, co mogÄ™ zrobiÄ‡ lepiej. WkrÃ³tce udostÄ™pniÄ™ pliki!

# NadchodzÄ…ce plany

(ZakoÅ„czone) ZacznÄ™ pracÄ™ nad wersjÄ… 0.5, zamiast trenowaÄ‡ na 50 ksiÄ…Å¼kach, uÅ¼yjÄ™ najlepiej 500â€“600. Obecnie trenujÄ™ nanoGPT na ksiÄ…Å¼kach z lat 1800â€“1850, konkretnie z Londynu. SÄ… pewne wyzwania, np. upewnienie siÄ™, Å¼e znalezione ksiÄ…Å¼ki nie majÄ… nowoczesnych interpretacji, ale sÄ… nienaruszone i wydane w wybranym okresie.

ChcÄ™ wytrenowaÄ‡ nowy model (v1) na znacznie wiÄ™kszym korpusie, moÅ¼e 5â€“10 razy wiÄ™kszym niÅ¼ uÅ¼yty do v0.5. Moim celem jest sprawdziÄ‡, czy zdolnoÅ›ci rozumowania pojawiÄ… siÄ™ tylko na podstawie Selektywnego Treningu Czasowego, to trudniejsze zadanie i nie mam pewnoÅ›ci, czy jest moÅ¼liwe ze wzglÄ™du na ograniczenia danych historycznych. W nadchodzÄ…cych tygodniach postaram siÄ™ zebraÄ‡ dane na 5â€“10 GB korpus. WierzÄ™, Å¼e jeÅ›li zdobÄ™dÄ™ czyste i wysokiej jakoÅ›ci dane oraz wynajmÄ™ GPU, bÄ™dÄ… postÄ™py.

# Jak korzystaÄ‡ z tego projektu

Projekt koncentruje siÄ™ gÅ‚Ã³wnie na gromadzeniu danych historycznych, ich przygotowaniu do treningu i budowie tokenizera. Nie opisujÄ™ tutaj peÅ‚nego procesu treningu LLM, do tego odsyÅ‚am do nanoGPT Andreja Karpathy'ego.

# Krok 1: Zbierz i przygotuj teksty historyczne

Zbierz pliki .txt ksiÄ…Å¼ek, dokumentÃ³w itp. z domeny publicznej z wybranego okresu (np. Londyn 1800â€“1850)

MoÅ¼esz uÅ¼yÄ‡ download_texts_improved.py, by pobraÄ‡ ksiÄ…Å¼ki automatycznie, jeÅ›li potrzebujesz.

WyczyÅ›Ä‡ pliki tekstowe skryptem lub rÄ™cznie usuÅ„ nagÅ‚Ã³wki/stopki z Project Gutenberg, nowoczesne adnotacje lub bÅ‚Ä™dy OCR.

prepare_dataset.py powinien dziaÅ‚aÄ‡ dobrze.

# Krok 2: Zbuduj wÅ‚asny tokenizer

Uruchom train_tokenizer.py lub train_tokenizer_hf.py na wyczyszczonych danych.
Dostaniesz vocab.json i merges.txt

Te pliki definiujÄ… sÅ‚ownik i reguÅ‚y scalania tokenÃ³w dla twojego modelu.

# Krok 3: Trenuj swÃ³j model (nanoGPT)

Zapoznaj siÄ™ z [nanoGPT Andreja Karpathy'ego](https://github.com/karpathy/nanoGPT) w celu poznania procesu treningu.

MoÅ¼esz trenowaÄ‡ inny LLM, jeÅ›li chcesz, ale ja uÅ¼yÅ‚em nanoGPT.

# FAQ

## Czym jest Selektywny Trening Czasowy?

Selective Temporal Training (STT) to metodologia uczenia maszynowego, gdzie wszystkie dane treningowe sÄ… starannie dobrane tak, by pochodziÅ‚y z konkretnego okresu historycznego. Ma to na celu modelowanie jÄ™zyka i wiedzy z danej epoki bez wpÅ‚ywu nowoczesnych koncepcji. Na przykÅ‚ad obecny model (v0.5) trenowany jest wyÅ‚Ä…cznie na danych z lat 1800â€“1875, nie jest dostrajany, ale trenowany od zera, przez co odzwierciedla styl jÄ™zykowy i kontekst historyczny tamtego okresu.

## Dlaczego nie po prostu fine-tuning lub LoRA?

W tym projekcie staram siÄ™ stworzyÄ‡ model jÄ™zykowy pozbawiony wspÃ³Å‚czesnych naleciaÅ‚oÅ›ci. JeÅ›li dostrojÄ™ coÅ› jak GPT-2, jest juÅ¼ wstÄ™pnie wytrenowane i tej wiedzy nie da siÄ™ usunÄ…Ä‡. JeÅ›li trenujÄ™ od zera, model jÄ™zykowy nie bÄ™dzie udawaÅ‚, Å¼e jest stary â€” po prostu taki bÄ™dzie. Celem jest stworzenie czegoÅ›, co rozumuje wyÅ‚Ä…cznie na podstawie wiedzy z ksiÄ…Å¼ek londyÅ„skich wydanych miÄ™dzy 1800 a 1850 rokiem.

## Jakiego rodzaju dane wykorzystaÅ‚eÅ› do treningu?

UÅ¼ywam ksiÄ…Å¼ek, dokumentÃ³w prawnych, gazet i innych tekstÃ³w z Londynu z lat 1800â€“1850. Lista, ktÃ³rÄ… podlinkowaÅ‚em, ma okoÅ‚o 200 pozycji, ale do pierwszego treningu uÅ¼yÅ‚em tylko 50 plikÃ³w, ok. 187 MB. ListÄ™ dokumentÃ³w moÅ¼esz zobaczyÄ‡ tu:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## Jak duÅ¼y jest model w wersji 0?

Model jest obecnie bardzo maÅ‚y, robiÄ™ to hobbystycznie i trzymam siÄ™ zasady braku nowoczesnych ÅºrÃ³deÅ‚. Ma prawie 16 milionÃ³w parametrÃ³w, ale zacznÄ™ zbieraÄ‡ wiÄ™cej starych tekstÃ³w do kolejnego treningu. BÄ™dÄ™ informowaÄ‡ na bieÅ¼Ä…co.

## Specyfikacja treningu?

GPU: Geforce rtx 4060
CPU: i5-13400F
Ram: 16GB DDR5.


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-29

---