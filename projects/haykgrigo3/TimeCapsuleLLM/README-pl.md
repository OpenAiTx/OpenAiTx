
<div align="right">
  <details>
    <summary >ğŸŒ JÄ™zyk</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ç®€ä½“ä¸­æ–‡</a>
        | <a href="#" title="Coming soon">ç¹é«”ä¸­æ–‡ (wkrÃ³tce)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">æ—¥æœ¬èª</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">í•œêµ­ì–´</a>
        | <a href="#" title="Coming soon">à¤¹à¤¿à¤¨à¥à¤¦à¥€ (wkrÃ³tce)</a> |
        | <a href="#" title="Coming soon">à¹„à¸—à¸¢ (wkrÃ³tce)</a> |
        | <a href="#" title="Coming soon">FranÃ§ais (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Deutsch (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">EspaÃ±ol (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Italiano (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Ğ ÑƒÑÑĞºĞ¸Ğ¹ (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">PortuguÃªs (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Nederlands (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Polski (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">ÙØ§Ø±Ø³ÛŒ (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">TÃ¼rkÃ§e (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Tiáº¿ng Viá»‡t (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (wkrÃ³tce)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM

*Model jÄ™zykowy wytrenowany **od podstaw** wyÅ‚Ä…cznie na danych z wybranych miejsc i okresÃ³w, aby ograniczyÄ‡ wspÃ³Å‚czesne uprzedzenia i odtworzyÄ‡ styl, sÅ‚ownictwo oraz Å›wiatopoglÄ…d epoki.*

WyobraÅº sobie, Å¼e model AI nie tylko udaje, Å¼e jest historyczny, ale faktycznie taki jest.

v0 i v0.5 oparte na [nanoGPT Andreja Karpathy](https://github.com/karpathy/nanoGPT). GÅ‚Ã³wne skrypty treningowe oraz architektura modelu to jego dzieÅ‚o. 

v1 zbudowany na [Phi 1.5 od Microsoft](https://huggingface.co/microsoft/phi-1_5)

[Link do Hugging Face](https://huggingface.co/haykgrigorian/TimeCapsuleLLM)

##  Zachowanie modelu i ograniczenia

### **v0**  

Wczesne zapytania pokazujÄ…, Å¼e model odpowiada jÄ™zykiem i zachowaniem z XIX wieku. 
PrzykÅ‚ad: Zapytanie: "Who art Henry?" a odpowiedÅº: "I know that man, I have did not a black, the storm." 

![PrzykÅ‚adowy wynik TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

- Brak wzmianki o nowoczesnych koncepcjach  
- PrzewaÅ¼nie sÅ‚ownictwo zgodne z epokÄ…  
- Zdania gÅ‚Ã³wnie niespÃ³jne (oczekiwane dla ~187MB danych treningowych)

### **v0.5** 

Znaczna poprawa wzglÄ™dem v0.  
- WiktoriaÅ„ski styl pisania, poprawna interpunkcja, przewaÅ¼nie poprawne gramatycznie zdania  
- Nadal wysoki poziom halucynacji faktÃ³w  
- Szumy OCR (â€Digitized by Googleâ€) nadal obecne w wynikach

![PrzykÅ‚adowy wynik TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

### **v1**

Pierwszy model, ktÃ³ry przypomina sobie i Å‚Ä…czy prawdziwe wydarzenie historyczne z rzeczywistÄ… postaciÄ… z zestawu danych.

PrzykÅ‚ad: Zapytanie: "It was the year of our Lord 1834" 

Wynik: "It was the year of our Lord 1834 and the streets of London were filled with protest and petition. The cause, as many re counted, was not bound in the way of private, but having taken up the same day in the day of Lord Palmerston, the public will receive a short statement of the difficulties under which the day of law has reached us. It is a matter of deep regret, that the present events in the history of the world are clear, and consequently will be'known. It is not true that the very men who first settled in the Gospel at Jerusalem should have so extensive and so interesting a record of the prosperity and prosperity" 

Na poczÄ…tku zaÅ‚oÅ¼yÅ‚em, Å¼e protest mÃ³gÅ‚ przypadkowo mieÄ‡ miejsce w tym samym roku, ale spÃ³jrz na to: ![1834protest](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)

### Dlaczego to ma znaczenie:

To pierwszy przykÅ‚ad, gdy jeden z moich modeli poÅ‚Ä…czyÅ‚ rok zarÃ³wno z prawdziwym wydarzeniem historycznym, jak i z rzeczywistÄ… osobÄ… powiÄ…zanÄ… z tym wydarzeniem (Lord Palmerston). WczeÅ›niejsze modele (v0 i v0.5) potrafiÅ‚y naÅ›ladowaÄ‡ styl pisania XIX wieku, ale zawsze wymyÅ›laÅ‚y wydarzenia, osoby i fakty. Pokazuje to, Å¼e model zaczyna zapamiÄ™tywaÄ‡ rzeczy z zestawu danych


## NadchodzÄ…ce plany

- Na Internet Archive opublikowano prawie 175 000 tekstÃ³w wydanych w Londynie w latach 1800-1875
- PlanujÄ™ rozszerzyÄ‡ korpus i dokÅ‚adniej go oczyÅ›ciÄ‡, aby poprawiÄ‡ zdolnoÅ›ci rozumowania
- Rozszerzenie na inne regiony i okresy historyczne w celu budowy bardziej zrÃ³Å¼nicowanych modeli historycznych

## Jak uÅ¼ywaÄ‡

Ten projekt koncentruje siÄ™ gÅ‚Ã³wnie na gromadzeniu danych historycznych, ich przygotowaniu do treningu oraz budowie tokenizera. Nie bÄ™dÄ™ omawiaÄ‡ peÅ‚nego procesu treningu LLM, w tym celu odsyÅ‚am do nanoGPT Andreja Karpathy'ego.

### Krok 1: Zbierz i przygotuj teksty historyczne

- Zbierz pliki .txt ksiÄ…Å¼ek, dokumentÃ³w itd. z domeny publicznej z wybranego okresu (np. Londyn 1800-1850)
- Trzymaj je w wybranym przez siebie oknie czasowo-miejscowym
- OczyÅ›Ä‡ pliki tekstowe za pomocÄ… skryptu lub rÄ™cznie usuÅ„ nagÅ‚Ã³wki/stopki z Project Gutenberg, nowoczesne adnotacje czy bÅ‚Ä™dy OCR

### Krok 2: Zbuduj wÅ‚asny tokenizer

- Uruchom train_tokenizer.py lub train_tokenizer_hf.py na oczyszczonych danych.
- Otrzymasz vocab.json oraz merges.txt
- Te pliki definiujÄ… sÅ‚ownictwo i zasady Å‚Ä…czenia dla Twojego modelu

### Krok 3: Wytrenuj swÃ³j model

- W celu przeprowadzenia treningu odsyÅ‚am do [nanoGPT Andreja Karpathy'ego](https://github.com/karpathy/nanoGPT) lub dokumentacji wybranej architektury.

# FAQ

## Czym jest Selective Temporal Training?

Selective Temporal Training (STT) to metodologia uczenia maszynowego, w ktÃ³rej wszystkie dane treningowe sÄ… starannie wyselekcjonowane, aby pochodziÅ‚y z okreÅ›lonego okresu historycznego. Ma to na celu modelowanie jÄ™zyka i wiedzy tamtej epoki bez wpÅ‚ywu nowoczesnych koncepcji. Na przykÅ‚ad, obecny model (v0.5) zostaÅ‚ wytrenowany wyÅ‚Ä…cznie na danych z lat 1800-1875, nie byÅ‚ dostrajany, lecz trenowany od zera, co skutkuje wyjÅ›ciem odzwierciedlajÄ…cym styl jÄ™zykowy i kontekst historyczny tamtego okresu.

## Dlaczego nie uÅ¼yÄ‡ po prostu fine-tuningu lub LoRA?

W tym projekcie prÃ³bujÄ™ stworzyÄ‡ model jÄ™zykowy wolny od wspÃ³Å‚czesnych uprzedzeÅ„. JeÅ›li dostrojÄ™ coÅ› takiego jak GPT-2, jest juÅ¼ wstÄ™pnie wytrenowany i ta informacja nie zniknie. JeÅ›li wytrenujÄ™ od zera, model jÄ™zykowy nie bÄ™dzie udawaÅ‚ "starego" â€“ po prostu nim bÄ™dzie. Celem projektu na ten moment jest stworzenie czegoÅ›, co potrafi rozumowaÄ‡ wyÅ‚Ä…cznie na podstawie wiedzy z londyÅ„skich ksiÄ…Å¼ek wydanych w latach 1800-1875.

## Jakich danych uÅ¼ywaÅ‚eÅ› do treningu?


UÅ¼ywam ksiÄ…Å¼ek, dokumentÃ³w prawnych, gazet i innych tekstÃ³w z Londynu z lat 1800â€“1875. Lista, do ktÃ³rej podaÅ‚em link (dla v0), zawiera okoÅ‚o 200 pozycji, ale do pierwszego treningu uÅ¼yÅ‚em tylko 50 plikÃ³w o Å‚Ä…cznej wielkoÅ›ci ok. 187 MB. MoÅ¼esz zobaczyÄ‡ listÄ™ dokumentÃ³w:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

Rozmiary zbiorÃ³w danych:
v0: ~187MB
v0.5: ~435MB 
v1: ~6,25GB 

## Jak duÅ¼e sÄ… modele?

V0: 16M parametrÃ³w

V0.5 123M parametrÃ³w

V1: 700M parametrÃ³w

# Specyfikacja treningu?

# V0/V0.5
GPU: Geforce rtx 4060
CPU: i5-13400F 
Ram: 16GB DDR5.

# V1
GPU: A100 wynajÄ™ta















---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-09-30

---