<div align="right">
  <details>
    <summary >ğŸŒ JÄ™zyk</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ç®€ä½“ä¸­æ–‡</a>
        | <a href="#" title="Coming soon">ç¹é«”ä¸­æ–‡ (coming soon)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">æ—¥æœ¬èª</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">í•œêµ­ì–´</a>
        | <a href="#" title="Coming soon">à¤¹à¤¿à¤¨à¥à¤¦à¥€ (coming soon)</a> |
        | <a href="#" title="Coming soon">à¹„à¸—à¸¢ (coming soon)</a> |
        | <a href="#" title="Coming soon">FranÃ§ais (coming soon)</a>
        | <a href="#" title="Coming soon">Deutsch (coming soon)</a>
        | <a href="#" title="Coming soon">EspaÃ±ol (coming soon)</a>
        | <a href="#" title="Coming soon">Italiano (coming soon)</a>
        | <a href="#" title="Coming soon">Ğ ÑƒÑÑĞºĞ¸Ğ¹ (coming soon)</a>
        | <a href="#" title="Coming soon">PortuguÃªs (coming soon)</a>
        | <a href="#" title="Coming soon">Nederlands (coming soon)</a>
        | <a href="#" title="Coming soon">Polski (coming soon)</a>
        | <a href="#" title="Coming soon">Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© (coming soon)</a>
        | <a href="#" title="Coming soon">ÙØ§Ø±Ø³ÛŒ (coming soon)</a>
        | <a href="#" title="Coming soon">TÃ¼rkÃ§e (coming soon)</a>
        | <a href="#" title="Coming soon">Tiáº¿ng Viá»‡t (coming soon)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (coming soon)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM
LLM trenowany wyÅ‚Ä…cznie na danych z okreÅ›lonych okresÃ³w historycznych w celu ograniczenia wspÃ³Å‚czesnych uprzedzeÅ„.

WyobraÅº sobie, Å¼e model AI nie tylko udaje historyczny, ale rzeczywiÅ›cie nim jest.

Zbudowany na [nanoGPT autorstwa Andreja Karpathy](https://github.com/karpathy/nanoGPT). Podstawowe skrypty treningowe i architektura modelu sÄ… jego dzieÅ‚em.

# Cele projektu

TimeCapsule LLM to eksperymentalny projekt trenowany wyÅ‚Ä…cznie na tekstach napisanych w okreÅ›lonych epokach. Celem jest symulacja Å›wiatopoglÄ…du i jÄ™zyka konkretnych okresÃ³w historycznych.

# Dlaczego samo dostrajanie nie wystarczy

JeÅ›li tylko dostroisz wytrenowany juÅ¼ model, TwÃ³j LLM nadal bÄ™dzie znaÅ‚ wspÃ³Å‚czesne pojÄ™cia. OczywiÅ›cie osiÄ…gniÄ™cie zupeÅ‚nie zerowego wspÃ³Å‚czesnego uprzedzenia jest trudne, ale chcÄ™ siÄ™ do tego jak najbardziej zbliÅ¼yÄ‡. Brak wspÃ³Å‚czesnych uprzedzeÅ„ wymaga trenowania modelu od zera.

# Oczekiwane rezultaty

Mam nadziejÄ™, Å¼e po zakoÅ„czeniu model nie bÄ™dzie znaÅ‚ wspÃ³Å‚czesnych pojÄ™Ä‡ i nie bÄ™dzie w stanie rozumowaÄ‡ poza tym, na czym zostaÅ‚ wytrenowany. Nie powinien rozpoznawaÄ‡ wspÃ³Å‚czesnych pojÄ™Ä‡/sÅ‚Ã³w i mam nadziejÄ™, Å¼e nie bÄ™dzie halucynowaÅ‚ wspÃ³Å‚czesnej wiedzy.

# Aktualizacje postÄ™pÃ³w

## 9 lipca 2025

WybraÅ‚em okres 1800-1850 oraz region: Londyn

ZebraÅ‚em listÄ™ tekstÃ³w, ksiÄ…Å¼ek, dokumentÃ³w

Jak dotÄ…d mam 50 plikÃ³w txt i wkrÃ³tce rozpocznÄ™ trening NanoGPT

BÄ™dÄ™ aktualizowaÄ‡ ten wpis, o ile bÄ™dÄ… postÄ™py

## 13 lipca 2025

WytrenowaÅ‚em nanoGPT na 187MB historycznych danych tekstowych.

## 15 lipca 2025

ZaczÄ…Å‚em pobieraÄ‡ teksty do drugiego treningu. Wszystko biorÄ™ z Internet Archive i rozszerzyÅ‚em okres do 1800-1875. Aby uzyskaÄ‡ zrÃ³Å¼nicowane teksty, moÅ¼na uÅ¼ywaÄ‡ filtrÃ³w tematycznych i wyszukiwania wedÅ‚ug miejsca publikacji, okresu i tematÃ³w na Internet Archive.

![Filtry wyszukiwania](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## 16 lipca 2025

PobraÅ‚em okoÅ‚o 500 plikÃ³w txt z Internet Archive i po ich oczyszczeniu (usuniÄ™cie biaÅ‚ych znakÃ³w, nagÅ‚Ã³wkÃ³w Gutenberga itp.) mam okoÅ‚o 500MB danych. To maÅ‚y zbiÃ³r danych, ale ostatnio trenowaÅ‚em na 187MB, wiÄ™c powinno byÄ‡ przynajmniej zauwaÅ¼alne ulepszenie w wynikach po wytrenowaniu drugiego modelu. Mam nadziejÄ™, Å¼e ten model przynajmniej wygeneruje bardziej spÃ³jne zdania, ktÃ³re majÄ… sens. OczywiÅ›cie nie jest to gwarantowane, bo to nadal bardzo maÅ‚y zbiÃ³r, ale wiÄ™kszy niÅ¼ ostatnio.

Powinno siÄ™ to daÄ‡ zrobiÄ‡ na moim sprzÄ™cie, co jest dobre, bo bÄ™dÄ™ mÃ³gÅ‚ zobaczyÄ‡ pewne ulepszenia zanim przejdÄ™ do wiÄ™kszego zbioru danych, co wymagaÅ‚oby wynajmu GPU. Ale nie martw siÄ™, planujÄ™ wkrÃ³tce wynajÄ…Ä‡ GPU, ale zanim to zrobiÄ™, chcÄ™ mieÄ‡ pewnoÅ›Ä‡, Å¼e mÃ³j zbiÃ³r danych jest jak najbardziej uporzÄ…dkowany i czysty. Jednym z problemÃ³w jest czyszczenie, wiele plikÃ³w txt ma w sobie beÅ‚kot. Skrypty, ktÃ³rych uÅ¼ywam, dziaÅ‚ajÄ…, ale nie sÄ… w 100% skuteczne.

Dzisiaj wytrenujÄ™ ten zbiÃ³r, powinno to zajÄ…Ä‡ okoÅ‚o 4-5 godzin. Po zakoÅ„czeniu i przetestowaniu dam znaÄ‡. DziÄ™kujÄ™ wszystkim, ktÃ³rzy Å›ledzÄ… mÃ³j projekt, niektÃ³rzy nawet podsyÅ‚ali mi linki do zasobÃ³w OCR, wiÄ™c dziÄ™kujÄ™! Mam nadziejÄ™, Å¼e wiÄ™cej osÃ³b sprÃ³buje tego i poeksperymentuje z wÅ‚asnymi zbiorami danych.

### Aktualizacja treningu

RozpoczÄ…Å‚em trening na korpusie 435MB (108 mln tokenÃ³w), na razie idzie caÅ‚kiem gÅ‚adko. Strata treningowa spadÅ‚a z 10,9 do 4,9 w pierwszych 2800 iteracjach. MyÅ›lÄ™, Å¼e caÅ‚oÅ›Ä‡ potrwa okoÅ‚o 8-9 godzin. Dam znaÄ‡, gdy bÄ™dzie gotowe.

## 17 lipca 2025 2:13AM

Trening drugiego modelu zakoÅ„czony, mÃ³j 4060 potrzebowaÅ‚ ok. 8 godzin i 40 minut (3 900 iteracji/godz.) na 33 000 iteracji (5 epok). Ostateczna strata treningowa wyniosÅ‚a 3,73. Wyniki byÅ‚y zaskakujÄ…co dobre, model rzeczywiÅ›cie generuje teraz spÃ³jne zdania w stylu XIX wieku.

# Zachowanie i ograniczenia modelu V0

Pierwsze zapytania pokazujÄ…, Å¼e model odpowiada jÄ™zykiem i zachowaniem z XIX wieku. Na przykÅ‚ad zapytaÅ‚em "Who art Henry?" i odpowiedziaÅ‚ "I know that man, I have did not a black, the storm." i tak, to zdanie nie ma sensu, ale LLM rozpoznaje, Å¼e pytam o osobÄ™.

![PrzykÅ‚adowy wynik TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

Nie ma Å¼adnych odniesieÅ„ do wspÃ³Å‚czesnych koncepcji, wyniki zawierajÄ… gÅ‚Ã³wnie sÅ‚owa i zwroty z XIX wieku.

Model wciÄ…Å¼ wymaga duÅ¼o pracy, trenowanie na 187MB nie da modelu generujÄ…cego tekst zÅ‚oÅ¼ony pod wzglÄ™dem rozumowania.

Obecnie generuje zdania bez peÅ‚nej struktury zdaniowej i ogÃ³lnie sÄ… bez sensu, ale to normalne przy takim rozmiarze zbioru.

# Zachowanie i ograniczenia modelu V0.5

To znaczna poprawa w porÃ³wnaniu do poprzedniego modelu. Styl pisania i sÅ‚ownictwo sÄ… wiktoriaÅ„skie, a niemal kaÅ¼de zdanie jest poprawne gramatycznie i ma prawidÅ‚owÄ… interpunkcjÄ™. I ponownie, model byÅ‚ trenowany od zera, wiÄ™c trzyma siÄ™ tematÃ³w z XIX wieku.

![PrzykÅ‚adowy wynik TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

WystÄ™puje wiele halucynacji faktÃ³w. Wiele (praktycznie 100%) szczegÃ³Å‚Ã³w (daty, wydarzenia, postacie historyczne) jest zmyÅ›lonych. Zdania czÄ™sto nie sÄ… ze sobÄ… powiÄ…zane, czasem moÅ¼e dwa siÄ™ odnoszÄ… do siebie, ale poza tym juÅ¼ nie. Kolejnym problemem jest pojawiajÄ…cy siÄ™ czasami przypadkowy â€Digitized by Googleâ€ w stopce, wiÄ™c nastÄ™pnym razem muszÄ™ lepiej oczyÅ›ciÄ‡ teksty przed trenowaniem. OgÃ³lnie jestem bardzo zadowolony z efektÃ³w â€“ to jeszcze nie jest LLM, ale zdecydowanie generator zdaÅ„.

DuÅ¼o siÄ™ uczÄ™ i w najbliÅ¼szych tygodniach zacznÄ™ analizowaÄ‡, co mogÄ™ zrobiÄ‡ lepiej. WkrÃ³tce wrzucÄ™ pliki!

# NadchodzÄ…ce plany

(Zrealizowane) Zamierzam rozpoczÄ…Ä‡ prace nad wersjÄ… 0.5 â€“ zamiast trenowaÄ‡ na 50 ksiÄ…Å¼kach, uÅ¼yjÄ™ idealnie 500-600. Aktualnie trenujÄ™ nanoGPT na ksiÄ…Å¼kach z lat 1800-1850, konkretnie z Londynu. SÄ… pewne wyzwania, jak upewnienie siÄ™, Å¼e znalezione ksiÄ…Å¼ki nie sÄ… zaktualizowane ani nie majÄ… wspÃ³Å‚czesnych interpretacji, tylko sÄ… oryginalnymi wydaniami z wybranego okresu.

ChcÄ™ wytrenowaÄ‡ nowy model (v1) na znacznie wiÄ™kszym korpusie, moÅ¼e 5-10 razy wiÄ™kszym niÅ¼ dla v0.5. Moim celem jest sprawdzenie, czy sama Selektywna Trening Temporalny pozwoli wyÅ‚oniÄ‡ umiejÄ™tnoÅ›ci rozumowania â€“ to trudniejsze zadanie i nie mam pewnoÅ›ci, czy to moÅ¼liwe, ze wzglÄ™du na ograniczenia historycznych danych. W najbliÅ¼szych tygodniach sprÃ³bujÄ™ zebraÄ‡ wystarczajÄ…co danych, by mieÄ‡ korpus 5-10GB. WierzÄ™, Å¼e jeÅ›li zdobÄ™dÄ™ czyste, wysokiej jakoÅ›ci dane i wynajmÄ™ GPU, nastÄ…pi postÄ™p.

# Jak korzystaÄ‡ z tego projektu

Projekt skupia siÄ™ gÅ‚Ã³wnie na zbieraniu danych historycznych, przygotowaniu ich do treningu oraz budowie tokenizera. Nie bÄ™dÄ™ opisywaÄ‡ peÅ‚nego procesu trenowania LLM â€“ w tym celu odsyÅ‚am do nanoGPT autorstwa Andreja Karpathy'ego.

# Krok 1: Zbierz i przygotuj teksty historyczne

Zbierz pliki .txt ksiÄ…Å¼ek, dokumentÃ³w itp. z domeny publicznej z wybranego okresu (np. Londyn 1800-1850).

MoÅ¼esz uÅ¼yÄ‡ download_texts_improved.py, aby pobraÄ‡ ksiÄ…Å¼ki, jeÅ›li potrzebujesz.

OczyÅ›Ä‡ pliki tekstowe przy pomocy skryptu lub rÄ™cznie usuÅ„ nagÅ‚Ã³wki/stopki z Project Gutenberg, nowoczesne adnotacje czy bÅ‚Ä™dy OCR.

prepare_dataset.py powinien dziaÅ‚aÄ‡ poprawnie.

# Krok 2: Zbuduj wÅ‚asny tokenizer

Uruchom train_tokenizer.py lub train_tokenizer_hf.py na oczyszczonych danych.
Otrzymasz pliki vocab.json i merges.txt

Te pliki definiujÄ… sÅ‚ownik i reguÅ‚y Å‚Ä…czenia dla Twojego modelu.

# Krok 3: Trenuj swÃ³j model (nanoGPT)

OdnieÅ› siÄ™ do [nanoGPT autorstwa Andreja Karpathy'ego](https://github.com/karpathy/nanoGPT) po opis procesu trenowania.

MoÅ¼esz trenowaÄ‡ inny LLM, jeÅ›li chcesz, ale ja uÅ¼yÅ‚em nanoGPT.

# FAQ

## Czym jest Selektywny Trening Temporalny?

Selektywny Trening Temporalny (STT) to metoda uczenia maszynowego, gdzie wszystkie dane treningowe sÄ… starannie dobrane tak, by pochodziÅ‚y z okreÅ›lonego okresu historycznego. Robi siÄ™ to, aby odtworzyÄ‡ jÄ™zyk i wiedzÄ™ tej epoki bez wpÅ‚ywu wspÃ³Å‚czesnych pojÄ™Ä‡. Na przykÅ‚ad aktualny model (v0.5) byÅ‚ trenowany wyÅ‚Ä…cznie na danych z lat 1800-1875; nie byÅ‚ dostrajany, tylko uczony od zera, co skutkuje wyjÅ›ciem odzwierciedlajÄ…cym styl jÄ™zykowy i kontekst historyczny tamtego czasu.

## Dlaczego nie uÅ¼yÄ‡ po prostu fine-tuningu albo LoRA?

W tym projekcie prÃ³bujÄ™ stworzyÄ‡ model jÄ™zykowy wolny od wspÃ³Å‚czesnych uprzedzeÅ„. JeÅ›li zrobiÄ™ fine-tuning np. na GPT-2, to model juÅ¼ jest wstÄ™pnie wytrenowany i tej wiedzy nie da siÄ™ usunÄ…Ä‡. JeÅ›li natomiast trenujÄ™ od zera, model jÄ™zykowy nie bÄ™dzie udawaÅ‚ starego â€“ on taki po prostu bÄ™dzie. Celem projektu jest stworzenie czegoÅ›, co potrafi rozumowaÄ‡ wyÅ‚Ä…cznie na bazie wiedzy z londyÅ„skich ksiÄ…Å¼ek wydanych w latach 1800â€“1850.

## Jakie dane wykorzystaÅ‚eÅ› do treningu?

UÅ¼ywam ksiÄ…Å¼ek, dokumentÃ³w prawnych, gazet i innych tekstÃ³w z Londynu z lat 1800â€“1850. Lista, ktÃ³rÄ… podlinkowaÅ‚em, zawiera okoÅ‚o 200 pozycji, ale do pierwszego treningu uÅ¼yÅ‚em tylko 50 plikÃ³w (~187 MB). MoÅ¼esz zobaczyÄ‡ listÄ™ dokumentÃ³w:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## Jak duÅ¼y jest model w wersji 0?

Ten model jest obecnie bardzo maÅ‚y â€“ robiÄ™ to dla zabawy i trzymam siÄ™ zasady braku wspÃ³Å‚czesnych ÅºrÃ³deÅ‚. Ma prawie 16 milionÃ³w parametrÃ³w, ale zamierzam zebraÄ‡ wiÄ™cej starych tekstÃ³w i rozpoczÄ…Ä‡ kolejny trening. BÄ™dÄ™ informowaÄ‡ o postÄ™pach.

## Parametry trenowania?

GPU: Geforce rtx 4060
CPU: i5-13400F
Ram: 16GB DDR5.



---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-21

---