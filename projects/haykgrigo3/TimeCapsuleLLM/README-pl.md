
<div align="right">
  <details>
    <summary >ğŸŒ JÄ™zyk</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ç®€ä½“ä¸­æ–‡</a>
        | <a href="#" title="Coming soon">ç¹é«”ä¸­æ–‡ (wkrÃ³tce)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">æ—¥æœ¬èª</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">í•œêµ­ì–´</a>
        | <a href="#" title="Coming soon">à¤¹à¤¿à¤¨à¥à¤¦à¥€ (wkrÃ³tce)</a> |
        | <a href="#" title="Coming soon">à¹„à¸—à¸¢ (wkrÃ³tce)</a> |
        | <a href="#" title="Coming soon">FranÃ§ais (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Deutsch (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">EspaÃ±ol (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Italiano (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Ğ ÑƒÑÑĞºĞ¸Ğ¹ (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">PortuguÃªs (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Nederlands (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Polski (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">ÙØ§Ø±Ø³ÛŒ (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">TÃ¼rkÃ§e (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Tiáº¿ng Viá»‡t (wkrÃ³tce)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (wkrÃ³tce)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM
LLM wytrenowany wyÅ‚Ä…cznie na danych z okreÅ›lonych okresÃ³w, aby ograniczyÄ‡ wspÃ³Å‚czesne uprzedzenia.

WyobraÅº sobie, Å¼e model AI nie tylko udaje historyczny, ale faktycznie nim jest.

Zbudowany na [nanoGPT autorstwa Andreja Karpathy'ego](https://github.com/karpathy/nanoGPT). Podstawowe skrypty treningowe i architektura modelu to jego praca.

# Cele projektu

TimeCapsule LLM to eksperymentalny projekt, ktÃ³ry bÄ™dzie trenowany wyÅ‚Ä…cznie na tekstach napisanych w okreÅ›lonych epokach. Celem jest symulacja Å›wiatopoglÄ…du i jÄ™zyka konkretnych historycznych okresÃ³w.

# Dlaczego samo dostrajanie nie wystarczy

JeÅ›li tylko dostroisz juÅ¼ wytrenowany model, TwÃ³j LLM nadal bÄ™dzie znaÅ‚ wspÃ³Å‚czesne pojÄ™cia. OczywiÅ›cie osiÄ…gniÄ™cie caÅ‚kowitego braku wspÃ³Å‚czesnych uprzedzeÅ„ jest trudne, ale chcÄ™ byÄ‡ jak najbliÅ¼ej tego celu. Brak wspÃ³Å‚czesnych uprzedzeÅ„ wymaga trenowania modelu od podstaw.

# Oczekiwane rezultaty

Mam nadziejÄ™, Å¼e po ukoÅ„czeniu model nie bÄ™dzie znaÅ‚ wspÃ³Å‚czesnych pojÄ™Ä‡ i nie bÄ™dzie w stanie wykraczaÄ‡ poza to, na czym byÅ‚ trenowany. Nie powinien rozpoznawaÄ‡ nowoczesnego sÅ‚ownictwa/pojÄ™Ä‡ i mam nadziejÄ™, Å¼e nie bÄ™dzie halucynowaÅ‚ wspÃ³Å‚czesnej wiedzy.

# Aktualizacje postÄ™pÃ³w

## 9 lipca 2025

WybraÅ‚em okres czasowy: 1800-1850 oraz region: Londyn

ZebraÅ‚em listÄ™ tekstÃ³w, ksiÄ…Å¼ek, dokumentÃ³w

Jak dotÄ…d mam 50 plikÃ³w txt i wkrÃ³tce rozpocznÄ™ trenowanie NanoGPT

BÄ™dÄ™ aktualizowaÄ‡ ten wpis tak dÅ‚ugo, jak bÄ™dÄ… postÄ™py

## 13 lipca 2025

WytrenowaÅ‚em nanoGPT na 187MB historycznych danych tekstowych.

## 15 lipca 2025

ZaczÄ…Å‚em pobieraÄ‡ teksty do drugiego cyklu treningowego. Wszystko biorÄ™ z Internet Archive i rozszerzyÅ‚em okres do 1800-1875. Aby uzyskaÄ‡ rÃ³Å¼norodnoÅ›Ä‡ tekstÃ³w, moÅ¼na uÅ¼ywaÄ‡ filtrÃ³w tematycznych i wyszukiwania wg miejsca publikacji, okresu i tematÃ³w na Internet Archive.

![Filtry wyszukiwania](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## 16 lipca 2025

PobraÅ‚em okoÅ‚o 500 plikÃ³w txt z Internet Archive i po ich oczyszczeniu (usuwanie biaÅ‚ych znakÃ³w, nagÅ‚Ã³wkÃ³w Gutenberg itd.) mam okoÅ‚o 500MB danych. To maÅ‚y zbiÃ³r, ale poprzednio trenowaÅ‚em na 187MB, wiÄ™c po treningu drugiego modelu powinien byÄ‡ zauwaÅ¼alny efekt. Mam nadziejÄ™, Å¼e ten model bÄ™dzie przynajmniej generowaÅ‚ bardziej spÃ³jne zdania majÄ…ce sens. OczywiÅ›cie nie ma gwarancji, bo to wciÄ…Å¼ bardzo maÅ‚y zbiÃ³r, ale wiÄ™kszy niÅ¼ poprzednio.

Powinno siÄ™ to udaÄ‡ na moim sprzÄ™cie, co jest dobre, bo bÄ™dÄ™ mÃ³gÅ‚ zobaczyÄ‡ postÄ™py zanim przejdÄ™ do wiÄ™kszego zbioru, ktÃ³ry wymagaÅ‚by wynajmu GPU. Nie martwcie siÄ™ â€“ planujÄ™ wynajÄ…Ä‡ GPU wkrÃ³tce, ale najpierw chcÄ™ mieÄ‡ jak najlepiej przygotowany i czysty zbiÃ³r. Jednym z problemÃ³w jest czyszczenie â€“ wiele plikÃ³w txt zawiera bzdury. Skrypty do czyszczenia dziaÅ‚ajÄ…, ale nie sÄ… w 100% skuteczne.

Dzisiaj wytrenujÄ™ ten zbiÃ³r, powinno to zajÄ…Ä‡ ok. 4-5 godzin. Po zakoÅ„czeniu i testach dam znaÄ‡. DziÄ™kujÄ™ wszystkim, ktÃ³rzy zaglÄ…dajÄ… na mÃ³j projekt â€“ niektÃ³rzy nawet podesÅ‚ali mi linki do zasobÃ³w OCR, wiÄ™c dziÄ™ki! Mam nadziejÄ™, Å¼e wiÄ™cej osÃ³b sprÃ³buje i poeksperymentuje ze swoimi zbiorami.

### Aktualizacja treningu

RozpoczÄ…Å‚em trening na korpusie 435MB (108 mln tokenÃ³w), idzie caÅ‚kiem sprawnie. Strata treningowa spadÅ‚a z 10,9 do 4,9 w pierwszych 2800 iteracjach. PrzewidujÄ™, Å¼e zakoÅ„czy siÄ™ za 8-9 godzin. Dam znaÄ‡ po zakoÅ„czeniu.

## 17 lipca 2025

Trening drugiego modelu zakoÅ„czony, moja 4060 potrzebowaÅ‚a ok. 8 godzin i 40 minut (3900 iteracji/godz.) na 33 000 iteracji (5 epok). Ostateczna strata treningowa wyniosÅ‚a 3,73. Wyniki sÄ… zaskakujÄ…co dobre â€“ model generuje juÅ¼ naprawdÄ™ spÃ³jne zdania w stylu XIX wieku.

## 28 lipca 2025

UdostÄ™pniÅ‚em wersjÄ™ v0.5 na Hugging Face, [sprawdÅº tutaj](https://huggingface.co/haykgrigorian/TimeCapsuleLLM) jeÅ›li chcesz. MoÅ¼na pobraÄ‡ repozytorium i uruchomiÄ‡ lokalnie. Niestety nanoGPT nie dziaÅ‚a natywnie z HuggingFace, wiÄ™c trzeba pobraÄ‡ i uruchomiÄ‡ model lokalnie.

ZacznÄ™ teÅ¼ zbieraÄ‡ dane do kolejnego treningu, sÄ…dzÄ™, Å¼e bÄ™dÄ™ potrzebowaÅ‚ 5-10 razy wiÄ™cej danych, aby osiÄ…gnÄ…Ä‡ zdolnoÅ›Ä‡ do wnioskowania.

## 2 sierpnia 2025

WkrÃ³tce zacznÄ™ pracÄ™ nad wersjÄ… 1. BÄ™dÄ™ musiaÅ‚ przejÅ›Ä‡ z architektury nanoGPT na coÅ› nowszego. Mam na oku kilka otwartych architektur LLM, m.in.: OpenLLaMA v3, Phi-2 i Qwen 1.5B. Do przejÅ›cia na V1 bÄ™dÄ™ musiaÅ‚ skrupulatnie przygotowaÄ‡ znacznie wiÄ™kszy i bardziej zrÃ³Å¼nicowany zbiÃ³r. PotrzebujÄ™ przynajmniej 5GB czystych danych treningowych.


# Zachowanie i ograniczenia modelu V0

Wczesne promptowania pokazujÄ…, Å¼e model odpowiada jÄ™zykiem i zachowaniem z lat 1800-tych. Na przykÅ‚ad, zapytaÅ‚em "Who art Henry?" i odpowiedziaÅ‚ "I know that man, I have did not a black, the storm." i tak, to zdanie nie ma sensu, ale LLM rozpoznaje, Å¼e pytam o osobÄ™.

![PrzykÅ‚adowy wynik TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

Nie ma Å¼adnej wzmianki o nowoczesnych koncepcjach, odpowiedzi zawierajÄ… gÅ‚Ã³wnie sÅ‚owa i zwroty z XIX wieku.

WciÄ…Å¼ wymaga duÅ¼o pracy, trenowanie na 187MB nie da modelu, ktÃ³ry produkuje teksty zÅ‚oÅ¼onego rozumowania.

Obecnie generuje zdania bez peÅ‚nej struktury i generalnie nie majÄ… one sensu, ale to normalne przy tak maÅ‚ym zbiorze treningowym.

# Zachowanie i ograniczenia modelu V0.5

To znaczna poprawa w porÃ³wnaniu do poprzedniego modelu. Styl pisania i sÅ‚ownictwo sÄ… wiktoriaÅ„skie, niemal kaÅ¼de zdanie jest gramatycznie poprawne i posiada wÅ‚aÅ›ciwÄ… interpunkcjÄ™. I znÃ³w, model trenowany jest od zera, wiÄ™c trzyma siÄ™ tematÃ³w z XIX wieku.

![PrzykÅ‚adowy wynik TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

Jest wiele halucynacji faktÃ³w. Wiele (praktycznie 100%) szczegÃ³Å‚Ã³w (daty, wydarzenia, postacie historyczne) jest zmyÅ›lonych. Zdania teÅ¼ nie sÄ… ze sobÄ… powiÄ…zane, czasem dwa siÄ™ Å‚Ä…czÄ…, ale rzadko wiÄ™cej. Dodatkowy problem to pojawiajÄ…cy siÄ™ czasem przypadkowy â€œDigitized by Googleâ€ w stopce, wiÄ™c przy kolejnym treningu muszÄ™ lepiej oczyÅ›ciÄ‡ teksty. OgÃ³lnie jestem bardzo zadowolony z efektÃ³w, to jeszcze nie LLM, ale juÅ¼ generator zdaÅ„.

DuÅ¼o siÄ™ uczÄ™ i w najbliÅ¼szych tygodniach bÄ™dÄ™ siÄ™ staraÅ‚ poprawiÄ‡ proces. WkrÃ³tce wrzucÄ™ pliki!

# NadchodzÄ…ce plany

(Zrobione) Zamierzam zaczÄ…Ä‡ pracÄ™ nad wersjÄ… 0.5, zamiast trenowaÄ‡ na 50 ksiÄ…Å¼kach, bÄ™dÄ™ trenowaÅ‚ na 500-600. Obecnie trenujÄ™ nanoGPT na ksiÄ…Å¼kach z lat 1800-1850, szczegÃ³lnie z Londynu. Wyzwania to upewnienie siÄ™, Å¼e ksiÄ…Å¼ki nie sÄ… zaktualizowane lub z nowoczesnymi interpretacjami, tylko oryginalne publikacje z wybranego okresu.

ChcÄ™ wytrenowaÄ‡ nowy model (v1) na znacznie wiÄ™kszym korpusie, moÅ¼e 5-10x wiÄ™kszym niÅ¼ dla v0.5. Moim celem jest sprawdzenie, czy same Selektywne Treningi Czasowe pozwolÄ… uzyskaÄ‡ zdolnoÅ›ci rozumowania â€” to bÄ™dzie trudniejsze i nie mam pewnoÅ›ci, czy siÄ™ uda ze wzglÄ™du na ograniczenia danych historycznych. W nadchodzÄ…cych tygodniach postaram siÄ™ zebraÄ‡ dane na korpus 5-10GB. WierzÄ™, Å¼e jeÅ›li znajdÄ™ czyste, wysokiej jakoÅ›ci dane i wynajmÄ™ GPU, bÄ™dzie postÄ™p.

# Jak korzystaÄ‡ z projektu

Projekt skupia siÄ™ gÅ‚Ã³wnie na zbieraniu danych historycznych, przygotowaniu ich do treningu i budowie tokenizera. Nie bÄ™dÄ™ tutaj opisywaÄ‡ peÅ‚nego procesu treningu LLM â€” do tego odsyÅ‚am do nanoGPT autorstwa Andreja Karpathyâ€™ego.

# Krok 1: Zbierz i przygotuj teksty historyczne

Zbierz pliki .txt z ksiÄ…Å¼kami, dokumentami itp. z wybranego okresu (np. Londyn 1800-1850).

MoÅ¼esz uÅ¼yÄ‡ download_texts_improved.py, aby pobraÄ‡ ksiÄ…Å¼ki jeÅ›li potrzebujesz.

OczyÅ›Ä‡ pliki tekstowe skryptem lub rÄ™cznie usuÅ„ nagÅ‚Ã³wki/stopki z Project Gutenberg, nowoczesne adnotacje lub bÅ‚Ä™dy OCR.

prepare_dataset.py powinien dziaÅ‚aÄ‡ poprawnie.

# Krok 2: Zbuduj wÅ‚asny tokenizer

Uruchom train_tokenizer.py lub train_tokenizer_hf.py na oczyszczonych danych.
Otrzymasz vocab.json oraz merges.txt

Te pliki definiujÄ… sÅ‚ownik i reguÅ‚y scalania dla twojego modelu

# Krok 3: Trenuj swÃ³j model (nanoGPT)

SzczegÃ³Å‚y procesu treningowego znajdziesz w [nanoGPT Andreja Karpathyâ€™ego](https://github.com/karpathy/nanoGPT).

MoÅ¼esz trenowaÄ‡ inny LLM, jeÅ›li chcesz, ale ja uÅ¼yÅ‚em nanoGPT

# FAQ

## Czym jest Selektywne Treningi Czasowe?

Selective Temporal Training (STT) to metoda uczenia maszynowego, w ktÃ³rej wszystkie dane treningowe sÄ… starannie dobrane, by pochodziÅ‚y z okreÅ›lonego okresu historycznego. Robi siÄ™ to, by odwzorowaÄ‡ jÄ™zyk i wiedzÄ™ tej epoki bez wpÅ‚ywÃ³w nowoczesnych koncepcji. Na przykÅ‚ad, obecny model (v0.5) zostaÅ‚ wytrenowany wyÅ‚Ä…cznie na danych z lat 1800-1875, nie jest dostrajany, tylko trenowany od podstaw, co daje wyjÅ›cie odzwierciedlajÄ…ce styl jÄ™zykowy i kontekst historyczny tamtego czasu.

## Dlaczego nie uÅ¼yÄ‡ fine-tuningu lub LoRA?

W tym projekcie prÃ³bujÄ™ stworzyÄ‡ model jÄ™zykowy wolny od wspÃ³Å‚czesnych uprzedzeÅ„. JeÅ›li dostrojÄ™ coÅ› jak GPT-2, to i tak bÄ™dzie juÅ¼ wytrenowany i tej wiedzy nie da siÄ™ usunÄ…Ä‡. JeÅ›li trenujÄ™ od zera, model nie bÄ™dzie udawaÅ‚ staroÅ¼ytnego â€” po prostu nim bÄ™dzie. Celem projektu jest stworzenie czegoÅ›, co potrafi rozumowaÄ‡ wyÅ‚Ä…cznie na podstawie wiedzy z ksiÄ…Å¼ek londyÅ„skich wydanych miÄ™dzy 1800 a 1850 rokiem.

## Jakie dane byÅ‚y uÅ¼yte do treningu?

UÅ¼ywam ksiÄ…Å¼ek, dokumentÃ³w prawnych, gazet i innych pism z Londynu z lat 1800â€“1850. Lista, ktÃ³rÄ… podlinkowaÅ‚em, zawiera okoÅ‚o 200 pozycji, ale do pierwszego treningu uÅ¼yÅ‚em 50 plikÃ³w o wadze ~187 MB. ListÄ™ dokumentÃ³w znajdziesz pod adresem:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## Jak duÅ¼y jest model w wersji 0?

Ten model jest teraz bardzo maÅ‚y, robiÄ™ to hobbystycznie i trzymam siÄ™ zasady braku wspÃ³Å‚czesnych ÅºrÃ³deÅ‚. Ma prawie 16 milionÃ³w parametrÃ³w, ale zamierzam zebraÄ‡ wiÄ™cej starych tekstÃ³w, by rozpoczÄ…Ä‡ kolejny trening. BÄ™dÄ™ na bieÅ¼Ä…co informowaÅ‚ o postÄ™pach.

## Specyfikacja treningu?

GPU: Geforce rtx 4060
CPU: i5-13400F
Ram: 16GB DDR5.



---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-08-07

---