[
  {
    "Id": 1,
    "Content": "\n<div align=\"right\">\n  <details>\n    <summary >🌐 Language</summary>\n    <div>\n      <div align=\"center\">\n        <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en\">English</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN\">简体中文</a>\n        | <a href=\"#\" title=\"Coming soon\">繁體中文 (coming soon)</a> |\n        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja\">日本語</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko\">한국어</a>\n        | <a href=\"#\" title=\"Coming soon\">हिन्दी (coming soon)</a> |\n        | <a href=\"#\" title=\"Coming soon\">ไทย (coming soon)</a> |\n        | <a href=\"#\" title=\"Coming soon\">Français (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Deutsch (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Español (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Italiano (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Русский (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Português (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Nederlands (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Polski (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">العربية (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">فارسی (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Türkçe (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Tiếng Việt (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Bahasa Indonesia (coming soon)</a>\n\n      </div>\n    </div>\n  </details>\n</div>\n\n# TimeCapsule LLM\nAn LLM trained only on data from certain time periods to reduce modern bias.\n\nImagine if an AI model didnt just pretend to be historical but actually was.\n\nBuilt on [nanoGPT by Andrej Karpathy](https://github.com/karpathy/nanoGPT) Core training scripts and model architecture are his work. \n\n# Project Goals ",
    "ContentSha": "SPxRGX6kwMeeqK2xE3WVtPozZ/VG8V2tFGFKxEXEAm8=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "\n<div align=\"right\">\n  <details>\n    <summary >🌐 言語</summary>\n    <div>\n      <div align=\"center\">\n        <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en\">English</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN\">简体中文</a>\n        | <a href=\"#\" title=\"Coming soon\">繁體中文 (近日公開)</a> |\n        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja\">日本語</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko\">한국어</a>\n        | <a href=\"#\" title=\"Coming soon\">हिन्दी (近日公開)</a> |\n        | <a href=\"#\" title=\"Coming soon\">ไทย (近日公開)</a> |\n        | <a href=\"#\" title=\"Coming soon\">Français (近日公開)</a>\n        | <a href=\"#\" title=\"Coming soon\">Deutsch (近日公開)</a>\n        | <a href=\"#\" title=\"Coming soon\">Español (近日公開)</a>\n        | <a href=\"#\" title=\"Coming soon\">Italiano (近日公開)</a>\n        | <a href=\"#\" title=\"Coming soon\">Русский (近日公開)</a>\n        | <a href=\"#\" title=\"Coming soon\">Português (近日公開)</a>\n        | <a href=\"#\" title=\"Coming soon\">Nederlands (近日公開)</a>\n        | <a href=\"#\" title=\"Coming soon\">Polski (近日公開)</a>\n        | <a href=\"#\" title=\"Coming soon\">العربية (近日公開)</a>\n        | <a href=\"#\" title=\"Coming soon\">فارسی (近日公開)</a>\n        | <a href=\"#\" title=\"Coming soon\">Türkçe (近日公開)</a>\n        | <a href=\"#\" title=\"Coming soon\">Tiếng Việt (近日公開)</a>\n        | <a href=\"#\" title=\"Coming soon\">Bahasa Indonesia (近日公開)</a>\n\n      </div>\n    </div>\n  </details>\n</div>\n\n# TimeCapsule LLM\n特定の時代のデータのみで訓練されたLLMで、現代的なバイアスを低減します。\n\nAIモデルが単に歴史的なふりをするのではなく、本当に歴史的な存在だったらどうでしょう。\n\n[Andrej Karpathy氏のnanoGPT](https://github.com/karpathy/nanoGPT) を基盤としています。コアのトレーニングスクリプトとモデルアーキテクチャは彼の作品です。\n\n# プロジェクト目標",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 2,
        "rowsha": "cWgam+tnnXudu7i74+ahMEGk/A9dQS+EwWLAIfi3dHk=",
        "originContent": "<div align=\"right\">",
        "translatedContent": "<div align=\"right\">"
      },
      {
        "row": 3,
        "rowsha": "orOcu5ARna/hb3RUkj6dBI8pHTM3WHeTvby17l5E0h0=",
        "originContent": "  <details>",
        "translatedContent": "  <details>"
      },
      {
        "row": 4,
        "rowsha": "TtgkLzblnvP0q9aAIVXt6s2LczXjy5k+QvHKcU0/5Ms=",
        "originContent": "    <summary >🌐 Language</summary>",
        "translatedContent": "    <summary >🌐 言語</summary>"
      },
      {
        "row": 5,
        "rowsha": "fZtk4rPTAJEEslnbhSVkHEcPlsctYSzAV7CDPL3rJmA=",
        "originContent": "    <div>",
        "translatedContent": "    <div>"
      },
      {
        "row": 6,
        "rowsha": "9KQxOeJSigvTmGWO+mtnl8kZY9zQfueoy8sk4lYm09Q=",
        "originContent": "      <div align=\"center\">",
        "translatedContent": "      <div align=\"center\">"
      },
      {
        "row": 7,
        "rowsha": "CeOhdpchZBoZSEUDtSE417JEcMBSZw18jeJuHJBKB2Y=",
        "originContent": "        <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en\">English</a>",
        "translatedContent": "        <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en\">English</a>"
      },
      {
        "row": 8,
        "rowsha": "ToO7MFa3QrNNljdQWIagsnOPxe8cXuuA2m5msIm+Kbs=",
        "originContent": "        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN\">简体中文</a>",
        "translatedContent": "        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN\">简体中文</a>"
      },
      {
        "row": 9,
        "rowsha": "MRATmWdRMRw0JU4u9h5pMb6GU17lQFgG9v/bpGLr9pM=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">繁體中文 (coming soon)</a> |",
        "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">繁體中文 (近日公開)</a> |"
      },
      {
        "row": 10,
        "rowsha": "GY7LXxG3rk5eFh9itcqM0cTtmHybyjLTf1icB3jN31I=",
        "originContent": "        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja\">日本語</a>",
        "translatedContent": "        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja\">日本語</a>"
      },
      {
        "row": 11,
        "rowsha": "b5TwunGJh+gsAe7aQU3dkfobXF/nknCEta1msDa7XBU=",
        "originContent": "        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko\">한국어</a>",
        "translatedContent": "        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko\">한국어</a>"
      },
      {
        "row": 12,
        "rowsha": "1/HCgPsVh2ChqMY+k/VVxEWHPRRmWWCjy5nDRibi3mM=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">हिन्दी (coming soon)</a> |",
        "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">हिन्दी (近日公開)</a> |"
      },
      {
        "row": 13,
        "rowsha": "3lfEHT+5HYFEvbE5cl+xujQPYjtVmzTifT37iqPTWII=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">ไทย (coming soon)</a> |",
        "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">ไทย (近日公開)</a> |"
      },
      {
        "row": 14,
        "rowsha": "KmG3P0px2E3bt1lU/w3eGop+zeA1j8xL0k280Zd9m2s=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">Français (coming soon)</a>",
        "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Français (近日公開)</a>"
      },
      {
        "row": 15,
        "rowsha": "CSdHSEXgIs3M2Q/6zIIJ8NbKkZWhydhBqNus94qrPvg=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">Deutsch (coming soon)</a>",
        "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Deutsch (近日公開)</a>"
      },
      {
        "row": 16,
        "rowsha": "8wz7pDuXc3dk+ZcqZ1jmmh8zh6xN3Wb6qWbCjxAj7dA=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">Español (coming soon)</a>",
        "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Español (近日公開)</a>"
      },
      {
        "row": 17,
        "rowsha": "op/NqIZs7OjCSpNgpXk8RnqDnTegVPyWUQhuQxvTR7U=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">Italiano (coming soon)</a>",
        "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Italiano (近日公開)</a>"
      },
      {
        "row": 18,
        "rowsha": "tAvlfwut/Ad9q1huxc8EREZGv7vYHbrEujzUS8xoaQo=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">Русский (coming soon)</a>",
        "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Русский (近日公開)</a>"
      },
      {
        "row": 19,
        "rowsha": "WhhSpeeCUUAqJiVTS4Fvyc6A2c+24Jnj3MW7XLQuIcI=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">Português (coming soon)</a>",
        "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Português (近日公開)</a>"
      },
      {
        "row": 20,
        "rowsha": "0yPXPrWh+Vzc6FBE9iiciw5HwpOSmo05HNe36wfTWCI=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">Nederlands (coming soon)</a>",
        "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Nederlands (近日公開)</a>"
      },
      {
        "row": 21,
        "rowsha": "mdW6YUUXf5KzI4CwZxrE08ofaLonUOMnJpN3vPR7Y2A=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">Polski (coming soon)</a>",
        "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Polski (近日公開)</a>"
      },
      {
        "row": 22,
        "rowsha": "sw1AXxAGQNvn4eSG9enTWNkwKH0yr6LlVtXBH1j9z8s=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">العربية (coming soon)</a>",
        "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">العربية (近日公開)</a>"
      },
      {
        "row": 23,
        "rowsha": "I8dh9zmXisU0+CpddA55QQgvujH03J/dEnXgj5aFtQM=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">فارسی (coming soon)</a>",
        "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">فارسی (近日公開)</a>"
      },
      {
        "row": 24,
        "rowsha": "7VFv8o6de72ciJrbh3mctfrEgCJhNvuKGWJNOmCaPdM=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">Türkçe (coming soon)</a>",
        "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Türkçe (近日公開)</a>"
      },
      {
        "row": 25,
        "rowsha": "C+XRvFz/D3o9/JyPqwitsxtskFZleJC/oFUr4SEeiHA=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">Tiếng Việt (coming soon)</a>",
        "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Tiếng Việt (近日公開)</a>"
      },
      {
        "row": 26,
        "rowsha": "ntGI5B+n9x96pV3ZG5GG83nmocQbxTJjKY7VVwa6Rq8=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">Bahasa Indonesia (coming soon)</a>",
        "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Bahasa Indonesia (近日公開)</a>"
      },
      {
        "row": 27,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 28,
        "rowsha": "0OM5wNEm0TO56MEBvQzL7AUZM7/3OpgIeqRf2zFre3Q=",
        "originContent": "      </div>",
        "translatedContent": "      </div>"
      },
      {
        "row": 29,
        "rowsha": "fcjTfY+fs8YnY5slBs1sZvWPAqEQR7tzaBDO54skkGQ=",
        "originContent": "    </div>",
        "translatedContent": "    </div>"
      },
      {
        "row": 30,
        "rowsha": "+fQNH2ldI7UM/rqRscP3hUSWAmw1HvQ2wEKDN8JagT0=",
        "originContent": "  </details>",
        "translatedContent": "  </details>"
      },
      {
        "row": 31,
        "rowsha": "qsMmUbEPVnxGG5tPJV1vsfpoWbU2jYvZpRr5IKshzyM=",
        "originContent": "</div>",
        "translatedContent": "</div>"
      },
      {
        "row": 32,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 33,
        "rowsha": "VRGjp0FtfvQ89lbX/wJLis2ypCRtNJwe8ViIi29+Rko=",
        "originContent": "# TimeCapsule LLM",
        "translatedContent": "# TimeCapsule LLM"
      },
      {
        "row": 34,
        "rowsha": "XGlykErifWX9oIzV4ZXDc4AUnsuesz8LvpruG76e6uY=",
        "originContent": "An LLM trained only on data from certain time periods to reduce modern bias.",
        "translatedContent": "特定の時代のデータのみで訓練されたLLMで、現代的なバイアスを低減します。"
      },
      {
        "row": 35,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 36,
        "rowsha": "06wDXO9Un3ot9kUKAGg7CaRsIVSkfS1d2m+EQ6HOFog=",
        "originContent": "Imagine if an AI model didnt just pretend to be historical but actually was.",
        "translatedContent": "AIモデルが単に歴史的なふりをするのではなく、本当に歴史的な存在だったらどうでしょう。"
      },
      {
        "row": 37,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 38,
        "rowsha": "2773v/qIXSAsW4pN2HtYcVltfzG1vzgVbHgfjStBQIY=",
        "originContent": "Built on [nanoGPT by Andrej Karpathy](https://github.com/karpathy/nanoGPT) Core training scripts and model architecture are his work. ",
        "translatedContent": "[Andrej Karpathy氏のnanoGPT](https://github.com/karpathy/nanoGPT) を基盤としています。コアのトレーニングスクリプトとモデルアーキテクチャは彼の作品です。"
      },
      {
        "row": 39,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 40,
        "rowsha": "wITJJBD/4abiy4E37iMdOcGmifkmz4dALLyk6AhA1kc=",
        "originContent": "# Project Goals ",
        "translatedContent": "# プロジェクト目標"
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 2,
    "Content": "\nTimeCapsule LLM is an expirimental project that will only be trained on texts written during certain time periods. The goal is to simulate the worldview and language of specific historical eras.\n\n# Why fine tuning isn't enough \n\nIf you just fine tune a pre-trained model, your LLM is still gonna know modern concepts. Of course achieving zero modern bias is difficult but I want to get as close as possible to this. Getting no modern bias requires training a model from scratch.\n\n# Expected outcomes \n\nHopefully when finished, this model will not know modern concepts and will not be able to reason beyond what it's been trained on. It shouldnt recognize modern concepts/vocab and should not hallucinate modern knowledge.\n\n# Progress Updates\n\n## July 9th, 2025\n\nI've set my time period for 1800-1850 and region: London \n\nI've gathered a list of texts, books, documents \n\nSo far I've gotten 50 as txt files and will begin training NanoGPT soon \n\nWill update this as long as progress is made\n\n## July 13th, 2025\n\nTrained nanoGPT with 187MB of historial text data. \n\n## July 15th, 2025\n\nI started downloading texts for the second training run. I'm getting everything from Internet Archive and I've expanded the time period to 1800-1875. To get a diverse range of texts, you can use subject and search filters for publication location, time period and subjects on Internet Archive. \n\n![Search Filters](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)\n\n## July 16th, 2025\n\nI downloaded around 500 txt files from Internet Archive and after cleaning them (just deleting whitespaces, Gutenberg headers, etc) I have around 500MB of data. It's a tiny dataset but last time I trained off of 187MB so there should be at least some kind of noticable difference in the output after I train the second model. I'm hoping this model can at least produce more coherent sentences that kind of make sense. It's not a guarantee of course since this is still a tiny tiny dataset, but it's more than what I used last time. \n\nThis should be doable on my own hardware, it's good too because I can hopefully see some kind of improvements before I jump to a bigger dataset which would require me to rent a GPU. But don't worry I still plan on renting a GPU soon, but before I do that I wanna make sure my dataset is as curated and clean as possible. One of the issues I have is cleaning, a lot of these txt files have gibberish mixed in. The scripts I've used for cleaning do work but they're not 100% effective. \n\nI will train this dataset today and it should take around 4-5 hours. Once it's done and I test it, I will give updates. Thank you again to everyone whos checking out my project, I've even had some people even giving me links to OCR resources so Thank you! I hope more people try this out and expirement with they're own datasets. ",
    "ContentSha": "vGXIcg75eWRGiCHXtZ9akEdROygoLiG36YylIl8Ow38=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "TimeCapsule LLMは、特定の時代に書かれたテキストのみで訓練される実験的プロジェクトです。目的は、特定の歴史的時代の世界観と言語をシミュレートすることです。\n\n# 微調整だけでは不十分な理由\n\n既存のモデルを微調整するだけでは、LLMは現代の概念を知ってしまいます。もちろん、現代バイアスを完全にゼロにするのは難しいですが、できる限りそれに近づきたいです。現代バイアスをなくすには、モデルをゼロから訓練する必要があります。\n\n# 期待される成果\n\n完成時には、このモデルは現代の概念を知らず、訓練された範囲を超えて推論できないはずです。現代の概念や語彙を認識せず、現代の知識を幻覚することもありません。\n\n# 進捗状況\n\n## 2025年7月9日\n\n1800〜1850年の期間、地域はロンドンに設定しました。\n\nテキスト、書籍、文書のリストを集めました。\n\n現在、50件のテキストファイルを取得し、NanoGPTの訓練を開始する予定です。\n\n進捗があれば随時更新します。\n\n## 2025年7月13日\n\nnanoGPTを187MBの歴史的テキストデータで訓練しました。\n\n## 2025年7月15日\n\n2回目の訓練用にテキストのダウンロードを始めました。Internet Archiveからすべて取得しており、期間を1800〜1875年に拡大しました。多様なテキストを得るために、Internet Archiveの検索フィルターで出版場所、期間、主題で絞り込むことができます。\n\n![Search Filters](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)\n\n## 2025年7月16日\n\nInternet Archiveから約500件のテキストファイルをダウンロードし、（空白やグーテンベルクのヘッダーなどを削除するだけですが）クリーニングした結果、約500MBのデータになりました。これはごく小規模なデータセットですが、前回は187MBで訓練したので、2回目のモデル訓練後には何らかの違いが出るはずです。このモデルが、少なくともより一貫性のある、意味が通る文章を生成できることを期待しています。もちろん、これは保証ではありません。まだ非常に小さなデータセットですが、前回よりは多いです。\n\nこれなら自分のハードウェアでも実行できそうです。これは良いことで、GPUをレンタルする前に何らかの改善が見られることを期待しています。もちろんGPUのレンタルもすぐに予定していますが、その前にデータセットをできる限り精査し、クリーンにしておきたいです。クリーニングの問題の一つは、多くのテキストファイルに意味不明な文字列が混在していることです。これまで使ったクリーニングスクリプトは機能しますが、100％の効果はありません。\n\nこのデータセットで本日訓練を行い、4〜5時間ほどかかる見込みです。訓練が終わりテストしたら、また進捗を報告します。プロジェクトを見てくれている皆さん、本当にありがとうございます。OCRリソースへのリンクを教えてくれる方もいて感謝しています！もっと多くの人が自分のデータセットで試してみてほしいです。\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "TimeCapsule LLMは、特定の時代に書かれたテキストのみで訓練される実験的プロジェクトです。目的は、特定の歴史的時代の世界観と言語をシミュレートすることです。"
      },
      {
        "row": 2,
        "rowsha": "LlW7r/H8NhftFgGAHce1f4KThGgsoT8aJ88/IsiLntc=",
        "originContent": "TimeCapsule LLM is an expirimental project that will only be trained on texts written during certain time periods. The goal is to simulate the worldview and language of specific historical eras.",
        "translatedContent": ""
      },
      {
        "row": 3,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "# 微調整だけでは不十分な理由"
      },
      {
        "row": 4,
        "rowsha": "obYFMCTDj8qHZGo0BQtA2AlwA8JgNcjDK9WlMRI4eq8=",
        "originContent": "# Why fine tuning isn't enough ",
        "translatedContent": ""
      },
      {
        "row": 5,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "既存のモデルを微調整するだけでは、LLMは現代の概念を知ってしまいます。もちろん、現代バイアスを完全にゼロにするのは難しいですが、できる限りそれに近づきたいです。現代バイアスをなくすには、モデルをゼロから訓練する必要があります。"
      },
      {
        "row": 6,
        "rowsha": "yNEBOKV/RnG7CvDjiWhkXKK6vqbwki9QKC+Zs+8PzbM=",
        "originContent": "If you just fine tune a pre-trained model, your LLM is still gonna know modern concepts. Of course achieving zero modern bias is difficult but I want to get as close as possible to this. Getting no modern bias requires training a model from scratch.",
        "translatedContent": ""
      },
      {
        "row": 7,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "# 期待される成果"
      },
      {
        "row": 8,
        "rowsha": "SdJkrN/DUD4+aOCh9lfDM3AAqMxlyukDfye/nzXzxN0=",
        "originContent": "# Expected outcomes ",
        "translatedContent": ""
      },
      {
        "row": 9,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "完成時には、このモデルは現代の概念を知らず、訓練された範囲を超えて推論できないはずです。現代の概念や語彙を認識せず、現代の知識を幻覚することもありません。"
      },
      {
        "row": 10,
        "rowsha": "gEYZXexiMGI/47WQffUoZrmXtyl2Y/NvOPHilNrWEvg=",
        "originContent": "Hopefully when finished, this model will not know modern concepts and will not be able to reason beyond what it's been trained on. It shouldnt recognize modern concepts/vocab and should not hallucinate modern knowledge.",
        "translatedContent": ""
      },
      {
        "row": 11,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "# 進捗状況"
      },
      {
        "row": 12,
        "rowsha": "8EWRxPaogE2BaXxVJE1VFNAXNdS6KUYPLDFN8xlQ9LE=",
        "originContent": "# Progress Updates",
        "translatedContent": ""
      },
      {
        "row": 13,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## 2025年7月9日"
      },
      {
        "row": 14,
        "rowsha": "oq91hnNV5WwmrEF0amya8kSN7gu21MN5nOcR2dPRBZ0=",
        "originContent": "## July 9th, 2025",
        "translatedContent": ""
      },
      {
        "row": 15,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "1800〜1850年の期間、地域はロンドンに設定しました。"
      },
      {
        "row": 16,
        "rowsha": "yU3u8taDdAaD23tJB9+n/2wmry0GfF+KXqADT4YLuJ8=",
        "originContent": "I've set my time period for 1800-1850 and region: London ",
        "translatedContent": ""
      },
      {
        "row": 17,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "テキスト、書籍、文書のリストを集めました。"
      },
      {
        "row": 18,
        "rowsha": "uQe95shOfOi8NA0M2/CQCXlOjNsiSmGrt5dZbeP4ANs=",
        "originContent": "I've gathered a list of texts, books, documents ",
        "translatedContent": ""
      },
      {
        "row": 19,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "現在、50件のテキストファイルを取得し、NanoGPTの訓練を開始する予定です。"
      },
      {
        "row": 20,
        "rowsha": "i9Kzka7MMa5yKjfdslauZFzKk+gAcnyILwscFoaepYs=",
        "originContent": "So far I've gotten 50 as txt files and will begin training NanoGPT soon ",
        "translatedContent": ""
      },
      {
        "row": 21,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "進捗があれば随時更新します。"
      },
      {
        "row": 22,
        "rowsha": "Wov5RgnyTA0P0gtJKLL0GTcSf7t8WrgFcAzDufE5Xh4=",
        "originContent": "Will update this as long as progress is made",
        "translatedContent": ""
      },
      {
        "row": 23,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## 2025年7月13日"
      },
      {
        "row": 24,
        "rowsha": "FSOOW2G6pyPozg+3u76To6E4Pthd9lRoZE396fwY2I4=",
        "originContent": "## July 13th, 2025",
        "translatedContent": ""
      },
      {
        "row": 25,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "nanoGPTを187MBの歴史的テキストデータで訓練しました。"
      },
      {
        "row": 26,
        "rowsha": "GszU76q+4dgDkDO4uNZpx/9WhQTTCZlq4VYIt0ZdgD0=",
        "originContent": "Trained nanoGPT with 187MB of historial text data. ",
        "translatedContent": ""
      },
      {
        "row": 27,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## 2025年7月15日"
      },
      {
        "row": 28,
        "rowsha": "pGPL3z/t2hDtXa67ubNgmRC/+b4O2Yp/0ff3R/9mraE=",
        "originContent": "## July 15th, 2025",
        "translatedContent": ""
      },
      {
        "row": 29,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "2回目の訓練用にテキストのダウンロードを始めました。Internet Archiveからすべて取得しており、期間を1800〜1875年に拡大しました。多様なテキストを得るために、Internet Archiveの検索フィルターで出版場所、期間、主題で絞り込むことができます。"
      },
      {
        "row": 30,
        "rowsha": "+jwjqBw9Cr+lRnmxUzCQ0SnVBfXYeJDycuYf/p0JJgg=",
        "originContent": "I started downloading texts for the second training run. I'm getting everything from Internet Archive and I've expanded the time period to 1800-1875. To get a diverse range of texts, you can use subject and search filters for publication location, time period and subjects on Internet Archive. ",
        "translatedContent": ""
      },
      {
        "row": 31,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "![Search Filters](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)"
      },
      {
        "row": 32,
        "rowsha": "XE9Xts6Q8wsZVZHg8uD/1ZXBQ/j2uFLsR9HwsiaqMds=",
        "originContent": "![Search Filters](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)",
        "translatedContent": ""
      },
      {
        "row": 33,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## 2025年7月16日"
      },
      {
        "row": 34,
        "rowsha": "iREIjirFxs+ic0QmjQG1FQYKHC5brQaZ5JjPaEto+lU=",
        "originContent": "## July 16th, 2025",
        "translatedContent": ""
      },
      {
        "row": 35,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "Internet Archiveから約500件のテキストファイルをダウンロードし、（空白やグーテンベルクのヘッダーなどを削除するだけですが）クリーニングした結果、約500MBのデータになりました。これはごく小規模なデータセットですが、前回は187MBで訓練したので、2回目のモデル訓練後には何らかの違いが出るはずです。このモデルが、少なくともより一貫性のある、意味が通る文章を生成できることを期待しています。もちろん、これは保証ではありません。まだ非常に小さなデータセットですが、前回よりは多いです。"
      },
      {
        "row": 36,
        "rowsha": "c1Ww8CUkqpg5TNm17QY7m130dQycuSFaAia2gfx/uLw=",
        "originContent": "I downloaded around 500 txt files from Internet Archive and after cleaning them (just deleting whitespaces, Gutenberg headers, etc) I have around 500MB of data. It's a tiny dataset but last time I trained off of 187MB so there should be at least some kind of noticable difference in the output after I train the second model. I'm hoping this model can at least produce more coherent sentences that kind of make sense. It's not a guarantee of course since this is still a tiny tiny dataset, but it's more than what I used last time. ",
        "translatedContent": ""
      },
      {
        "row": 37,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "これなら自分のハードウェアでも実行できそうです。これは良いことで、GPUをレンタルする前に何らかの改善が見られることを期待しています。もちろんGPUのレンタルもすぐに予定していますが、その前にデータセットをできる限り精査し、クリーンにしておきたいです。クリーニングの問題の一つは、多くのテキストファイルに意味不明な文字列が混在していることです。これまで使ったクリーニングスクリプトは機能しますが、100％の効果はありません。"
      },
      {
        "row": 38,
        "rowsha": "h/hyxvgOlOm5er9sn3CL2wmktMoq2q+qZi5Vi7upXGI=",
        "originContent": "This should be doable on my own hardware, it's good too because I can hopefully see some kind of improvements before I jump to a bigger dataset which would require me to rent a GPU. But don't worry I still plan on renting a GPU soon, but before I do that I wanna make sure my dataset is as curated and clean as possible. One of the issues I have is cleaning, a lot of these txt files have gibberish mixed in. The scripts I've used for cleaning do work but they're not 100% effective. ",
        "translatedContent": ""
      },
      {
        "row": 39,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "このデータセットで本日訓練を行い、4〜5時間ほどかかる見込みです。訓練が終わりテストしたら、また進捗を報告します。プロジェクトを見てくれている皆さん、本当にありがとうございます。OCRリソースへのリンクを教えてくれる方もいて感謝しています！もっと多くの人が自分のデータセットで試してみてほしいです。"
      },
      {
        "row": 40,
        "rowsha": "8LYxYjHwrXnU59N+13Fd9m653Tgyom3yQQAWGIrSPSc=",
        "originContent": "I will train this dataset today and it should take around 4-5 hours. Once it's done and I test it, I will give updates. Thank you again to everyone whos checking out my project, I've even had some people even giving me links to OCR resources so Thank you! I hope more people try this out and expirement with they're own datasets. ",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 3,
    "Content": "\n## August 12, 2025\n\nVersion 1 is complete, this version uses Phi 1.5 (700M parameters) and is trained on a dataset that has almost 7000 texts. The main improvements with this model is it's ability to sometimes reference real names and historical events. Version 0 and 0.5 could not do this, whenever I prompted those models with something like \"In the year 1821\" the output would just be hallucinations, stuff that never happened. An example of this with Version 1:\n\nMy prompt: It was the year of our Lord 1834, \n\nThe output: \"It was the year of our Lord 1834 and the streets of London were filled with protest and petition. The cause, as many re counted, was not bound in the way of private, but having taken up the same day in the day of Lord Palmerston, the public will receive a short statement of the difficulties under which the day of law has reached us. It is a matter of deep regret, that the present events in the history of the world are clear, and consequently will be'known. It is not true that the very men who first settled in the Gospel at Jerusalem should have so extensive and so interesting a record of the prosperity and prosperity\" \n\nAt first I assumed this might be a coincidence but take a look at this: ![1834protest](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)\n\n\n### Training Update \n\nI started training on a 435MB (108 M tokens) corpus, it's going pretty smooth right now. Train loss dropped from 10.9 to 4.9 in the first 2800 iterations. I expect it'll take around 8 or 9 hours to complete. I'll post another update once it's done.\n\n## July 17th, 2025\n\nThe training is done for the second model, it took my 4060 around 8 hours and 40 minutes (3,900 iters/hr) for 33,000 iters (5 epochs). Final train loss was 3.73. The outputs were suprisingly good it genuinely generates coherent 19th century style sentences now. \n\n## July 28th, 2025 \n\nI've gone ahead and uploaded v0.5 to Hugging Face, [Check it out](https://huggingface.co/haykgrigorian/TimeCapsuleLLM) if youd like. You can now download my repo and run it locally. Unfortunately nanoGPT doesn't work natively with HuggingFace, so you'll have to download and run the model locally. \n\nAlso I will begin curating data for my next training run, I believe I'll need maybe 5-10x more data to achieve reasoning capabilities. \n\n## August 2nd, 2025\n\nI'm going to start work on Version 1 soon. I will need to transition from nanoGPT's architecture to soemthing more modern. I have several open-source LLM archictectures in mind, including: OpenLLaMA v3, Phi-2 and Qwen 1.5B. And to support the jump to V1, I'll need to carefully curate a much bigger and diverse dataset. I'll need at least 5GB of clean training data.\n\n# V0 Model Behavior & Limitations \n\nEarly prompts show the model responding with 1800's language and behavior. For example, I prompted it with \"Who art Henry?\" and it replied \"I know that man, I have did not a black, the storm.\" and yeah that sentence makes no sense but the LLM is recognizing I'm asking about a person. \n\n![TimeLockLLM Sample Output](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)\n\nThere is no mention of modern concetps, outputs contain mostly words and phrasing from the 1800's.\n\nIt still needs alot of work, training off of 187MB will not give you a model that produces text with complex reasoning. \n",
    "ContentSha": "jUt6pT45GKA12z2YTDu7nLDirOvNXDBNisRZOVoLw3I=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "\n## 2025年8月12日\n\nバージョン1が完成しました。このバージョンはPhi 1.5（7億パラメータ）を使用し、約7000件のテキストからなるデータセットでトレーニングされています。このモデルの主な改善点は、実際の名前や歴史的出来事を参照できる場合があることです。バージョン0および0.5ではこれができず、「1821年に」といったプロンプトを与えても、出力は現実には存在しない空想の内容ばかりでした。バージョン1での例：\n\n私のプロンプト：「我らが主の1834年だった。」\n\n出力：「我らが主の1834年であり、ロンドンの街は抗議と嘆願で溢れていた。その原因は多くの人が語ったように私的なものではなく、同じ日にパーマストン卿の日に採られたことで、公衆は法の日が我々に到達した困難について簡単な声明を受け取ることになる。世界史の現状が明白であり、したがって知られることになるのは非常に遺憾である。エルサレムで福音を最初に広めたまさにその人々が、繁栄の興味深く広範な記録を持つべきではないというのは真実ではない。」\n\n最初は偶然かと思いましたが、これをご覧ください: ![1834protest](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)\n\n\n### トレーニングアップデート\n\n435MB（1億800万トークン）のコーパスでトレーニングを開始しましたが、今のところ順調です。トレインロスは最初の2800イテレーションで10.9から4.9に下がりました。完了までに8〜9時間ほどかかると予想しています。完了次第、またアップデートを投稿します。\n\n## 2025年7月17日\n\n2番目のモデルのトレーニングが完了しました。4060で約8時間40分（3,900イテレーション/時）、合計33,000イテレーション（5エポック）かかりました。最終トレインロスは3.73でした。出力は驚くほど良く、本当に一貫した19世紀風の文を生成できるようになっています。\n\n## 2025年7月28日\n\nv0.5をHugging Faceにアップロードしました。[こちらからご覧ください](https://huggingface.co/haykgrigorian/TimeCapsuleLLM)。リポジトリをダウンロードしてローカルで実行できます。ただしnanoGPTはHuggingFaceでネイティブには動作しないため、モデルをダウンロードしてローカルで実行する必要があります。\n\nまた、次のトレーニング用データのキュレーションも開始します。推論能力を得るには5〜10倍のデータが必要だと考えています。\n\n## 2025年8月2日\n\nまもなくバージョン1の開発を開始します。nanoGPTのアーキテクチャからよりモダンなものへ移行する必要があります。検討中のオープンソースLLMアーキテクチャには、OpenLLaMA v3、Phi-2、Qwen 1.5Bなどがあります。そしてV1への移行をサポートするために、より大規模かつ多様なデータセットを厳選する必要があります。少なくとも5GBのクリーンなトレーニングデータが必要です。\n\n# V0モデルの挙動と制限\n\n初期のプロンプトでは、モデルが1800年代の言葉遣いや行動で応答していました。例えば「Who art Henry?」とプロンプトを与えると、「I know that man, I have did not a black, the storm.」と返し、文としては意味不明ですが、LLMが人物について尋ねられていると認識していることがわかります。\n\n![TimeLockLLM サンプル出力](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)\n\n現代の概念にはまったく言及しておらず、出力はほとんど1800年代の単語や言い回しを含んでいます。\n\nまだ多くの改良が必要であり、187MBのデータで学習しても複雑な推論を行うテキストを生成できるモデルにはなりません。\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 2,
        "rowsha": "ih02CyPS6D3hq8065ysVT4Teh0oimcSXOr3a0ZlSDUc=",
        "originContent": "## August 12, 2025",
        "translatedContent": "## 2025年8月12日"
      },
      {
        "row": 3,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 4,
        "rowsha": "Vu3bDLQeTGUJ45FQ5g9LkghhutbMWGXdzzPrlGb0Jzc=",
        "originContent": "Version 1 is complete, this version uses Phi 1.5 (700M parameters) and is trained on a dataset that has almost 7000 texts. The main improvements with this model is it's ability to sometimes reference real names and historical events. Version 0 and 0.5 could not do this, whenever I prompted those models with something like \"In the year 1821\" the output would just be hallucinations, stuff that never happened. An example of this with Version 1:",
        "translatedContent": "バージョン1が完成しました。このバージョンはPhi 1.5（7億パラメータ）を使用し、約7000件のテキストからなるデータセットでトレーニングされています。このモデルの主な改善点は、実際の名前や歴史的出来事を参照できる場合があることです。バージョン0および0.5ではこれができず、「1821年に」といったプロンプトを与えても、出力は現実には存在しない空想の内容ばかりでした。バージョン1での例："
      },
      {
        "row": 5,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 6,
        "rowsha": "9IdQmw3qQSL7u/5HdEkUTrA3mB3XinzAhLuryah0cD0=",
        "originContent": "My prompt: It was the year of our Lord 1834, ",
        "translatedContent": "私のプロンプト：「我らが主の1834年だった。」"
      },
      {
        "row": 7,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 8,
        "rowsha": "zvm/1tcaMFjcQmuN7y0hczAbbgAwaKRtjBpah5/+kb8=",
        "originContent": "The output: \"It was the year of our Lord 1834 and the streets of London were filled with protest and petition. The cause, as many re counted, was not bound in the way of private, but having taken up the same day in the day of Lord Palmerston, the public will receive a short statement of the difficulties under which the day of law has reached us. It is a matter of deep regret, that the present events in the history of the world are clear, and consequently will be'known. It is not true that the very men who first settled in the Gospel at Jerusalem should have so extensive and so interesting a record of the prosperity and prosperity\" ",
        "translatedContent": "出力：「我らが主の1834年であり、ロンドンの街は抗議と嘆願で溢れていた。その原因は多くの人が語ったように私的なものではなく、同じ日にパーマストン卿の日に採られたことで、公衆は法の日が我々に到達した困難について簡単な声明を受け取ることになる。世界史の現状が明白であり、したがって知られることになるのは非常に遺憾である。エルサレムで福音を最初に広めたまさにその人々が、繁栄の興味深く広範な記録を持つべきではないというのは真実ではない。」"
      },
      {
        "row": 9,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 10,
        "rowsha": "D65A3DA1psBrWR2f/2JT9alA5NzUEjFgRJ2C66XhdLE=",
        "originContent": "At first I assumed this might be a coincidence but take a look at this: ![1834protest](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)",
        "translatedContent": "最初は偶然かと思いましたが、これをご覧ください: ![1834protest](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)"
      },
      {
        "row": 11,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 12,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 13,
        "rowsha": "mNjt3ebwxfwyPM2/AK7E/GDTFM6i95HG8GZ/8TZs9EA=",
        "originContent": "### Training Update ",
        "translatedContent": "### トレーニングアップデート"
      },
      {
        "row": 14,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 15,
        "rowsha": "RxsWSGTgM12Md9v3+GBLg3PyoUxQ9kl1AglpanRmZqE=",
        "originContent": "I started training on a 435MB (108 M tokens) corpus, it's going pretty smooth right now. Train loss dropped from 10.9 to 4.9 in the first 2800 iterations. I expect it'll take around 8 or 9 hours to complete. I'll post another update once it's done.",
        "translatedContent": "435MB（1億800万トークン）のコーパスでトレーニングを開始しましたが、今のところ順調です。トレインロスは最初の2800イテレーションで10.9から4.9に下がりました。完了までに8〜9時間ほどかかると予想しています。完了次第、またアップデートを投稿します。"
      },
      {
        "row": 16,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 17,
        "rowsha": "+KGe6w5lIW875VBdo8dnngqlaSzfoN0zFImo+SuVZ3k=",
        "originContent": "## July 17th, 2025",
        "translatedContent": "## 2025年7月17日"
      },
      {
        "row": 18,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 19,
        "rowsha": "Q0uM34dBNqytALNUZSPxoBQZT3LxqlwyioEi3nTshXQ=",
        "originContent": "The training is done for the second model, it took my 4060 around 8 hours and 40 minutes (3,900 iters/hr) for 33,000 iters (5 epochs). Final train loss was 3.73. The outputs were suprisingly good it genuinely generates coherent 19th century style sentences now. ",
        "translatedContent": "2番目のモデルのトレーニングが完了しました。4060で約8時間40分（3,900イテレーション/時）、合計33,000イテレーション（5エポック）かかりました。最終トレインロスは3.73でした。出力は驚くほど良く、本当に一貫した19世紀風の文を生成できるようになっています。"
      },
      {
        "row": 20,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 21,
        "rowsha": "13xzxThQO3iEGB5PU51V1kYgK85//yEFbCkwtu07a2U=",
        "originContent": "## July 28th, 2025 ",
        "translatedContent": "## 2025年7月28日"
      },
      {
        "row": 22,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 23,
        "rowsha": "R6HmXRpR+7izGCE6kI9wyELs4kYvAtLMWO75nOXAf1M=",
        "originContent": "I've gone ahead and uploaded v0.5 to Hugging Face, [Check it out](https://huggingface.co/haykgrigorian/TimeCapsuleLLM) if youd like. You can now download my repo and run it locally. Unfortunately nanoGPT doesn't work natively with HuggingFace, so you'll have to download and run the model locally. ",
        "translatedContent": "v0.5をHugging Faceにアップロードしました。[こちらからご覧ください](https://huggingface.co/haykgrigorian/TimeCapsuleLLM)。リポジトリをダウンロードしてローカルで実行できます。ただしnanoGPTはHuggingFaceでネイティブには動作しないため、モデルをダウンロードしてローカルで実行する必要があります。"
      },
      {
        "row": 24,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 25,
        "rowsha": "3/uUOOHuCoxdKkQ6c+1J96AfmOsQ59aZxo/FYGK9OlI=",
        "originContent": "Also I will begin curating data for my next training run, I believe I'll need maybe 5-10x more data to achieve reasoning capabilities. ",
        "translatedContent": "また、次のトレーニング用データのキュレーションも開始します。推論能力を得るには5〜10倍のデータが必要だと考えています。"
      },
      {
        "row": 26,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 27,
        "rowsha": "UXyL9heEocaX5oRp2rVKQgcRB28wVdVl+Kl7/DqbKLc=",
        "originContent": "## August 2nd, 2025",
        "translatedContent": "## 2025年8月2日"
      },
      {
        "row": 28,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 29,
        "rowsha": "PTcIrGtJ33LlUO6TFsGwvV/mCnbcxmmcYIasCuuLx40=",
        "originContent": "I'm going to start work on Version 1 soon. I will need to transition from nanoGPT's architecture to soemthing more modern. I have several open-source LLM archictectures in mind, including: OpenLLaMA v3, Phi-2 and Qwen 1.5B. And to support the jump to V1, I'll need to carefully curate a much bigger and diverse dataset. I'll need at least 5GB of clean training data.",
        "translatedContent": "まもなくバージョン1の開発を開始します。nanoGPTのアーキテクチャからよりモダンなものへ移行する必要があります。検討中のオープンソースLLMアーキテクチャには、OpenLLaMA v3、Phi-2、Qwen 1.5Bなどがあります。そしてV1への移行をサポートするために、より大規模かつ多様なデータセットを厳選する必要があります。少なくとも5GBのクリーンなトレーニングデータが必要です。"
      },
      {
        "row": 30,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 31,
        "rowsha": "YbYVsndAe75CgxPU/J34HsXOaqcQlIjTCm06o4eVQIg=",
        "originContent": "# V0 Model Behavior & Limitations ",
        "translatedContent": "# V0モデルの挙動と制限"
      },
      {
        "row": 32,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 33,
        "rowsha": "OCh97kyOITXqFpKyZsol6voS+nrzc9n9flpDpNyf35w=",
        "originContent": "Early prompts show the model responding with 1800's language and behavior. For example, I prompted it with \"Who art Henry?\" and it replied \"I know that man, I have did not a black, the storm.\" and yeah that sentence makes no sense but the LLM is recognizing I'm asking about a person. ",
        "translatedContent": "初期のプロンプトでは、モデルが1800年代の言葉遣いや行動で応答していました。例えば「Who art Henry?」とプロンプトを与えると、「I know that man, I have did not a black, the storm.」と返し、文としては意味不明ですが、LLMが人物について尋ねられていると認識していることがわかります。"
      },
      {
        "row": 34,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 35,
        "rowsha": "yKIR0teTc66wVDG+jdIyNmAzItXb2JH2ld3D7tm4qnM=",
        "originContent": "![TimeLockLLM Sample Output](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)",
        "translatedContent": "![TimeLockLLM サンプル出力](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)"
      },
      {
        "row": 36,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 37,
        "rowsha": "/GR84OQ/Xp3d+Yv3/vVIIQFyl8ExKQLgjT1Go5rPplE=",
        "originContent": "There is no mention of modern concetps, outputs contain mostly words and phrasing from the 1800's.",
        "translatedContent": "現代の概念にはまったく言及しておらず、出力はほとんど1800年代の単語や言い回しを含んでいます。"
      },
      {
        "row": 38,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 39,
        "rowsha": "aojQk3KjkX8shkLnJSZvUZBo0vk4tKDT9aysMfY9yK4=",
        "originContent": "It still needs alot of work, training off of 187MB will not give you a model that produces text with complex reasoning. ",
        "translatedContent": "まだ多くの改良が必要であり、187MBのデータで学習しても複雑な推論を行うテキストを生成できるモデルにはなりません。"
      },
      {
        "row": 40,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 4,
    "Content": "Right now it produces sentences that lack full sentence structure and overall just make no sense but this is normal for the training size. \n\n# V0.5 Model Behavior & Limitations\n\nThis is a nice improvement compared to the last model. The writing style and vocab is Victorian and almost every sentence is grammatically correct with proper punctuation. And again this is trained from scratch so it sticks to 1800's subjects. \n\n![TimeLockLLM Sample Output](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)\n\nThere are a lot of factual hallucinations. A lot (like 100%) of the details (dates, events, historical figures)  are made up. Also the sentences don't really have connections to each other, sometimes maybe 2 sentences will relate to each other but beyond that they dont. Another issue is sometimes a stray “Digitized by Google” footer shows up, so the next time I train I really have to make sure the texts are cleaned well. Overall I'm very happy with the results, it's nowhere near an LLM yet but definitely a sentence generator. \n\nI'm learning a lot and will start figuring out what I need to do better in the coming weeks. I will upload files soon! \n\n# V1 Model Behavior & Limitations \n\nI will upload some example output soon and also do comparisons between the 3 models with the same prompt. I will also upload V1 to huggingface like I did with my last verison, you can find my huggingface account here: https://huggingface.co/haykgrigorian/TimeCapsuleLLM\n\n# Upcoming Plans \n\n(Completed) I'm going to start work on version 0.5 , instead of training using 50 books, I'll train using ideally 500-600. Right now I'm training nanoGPT using books from 1800-1850 and specifically from London. There is some challeneges like making sure the books I find are not updated or have modern interpretations but untouched books published withtin my chosen time period.\n\nI want to train a new model (v1) with a much larger corpus, maybe 5-10x larger than the one I used for v0.5. My goal is to see if I can get reasoning abilities to emerge from Selective Temporal Training alone, this will be a more difficult task and I'm not even entirely sure if it's possible due to the fact that there is historical data limitations. In the upcoming weeks I will try to curate enough data for a 5-10GB corpus. I believe If I can get clean clean high quality data and rent a GPU, there will be progress.\n\n# How to Use This Project \n\nThis project focuses mostly on curating historical data, preparing it for training and building a tokenizer. I am not going to cover the full LLM training process, for that refer to nanoGPT by Andrej Karpathy.\n\n# Step 1: Gather and Prepare Historical Texts \n\nCollect .txt files of public domain books, documents, etc from your chosen time period (e.g., London 1800-1850)\n\nYou can use download_texts_improved.py to download books for you if you need to.\n\nClean the text files using a script or manually remove headers/footer from Project Gutenberg, Modern annotations or things like OCR errors.\n\nprepare_dataset.py should work fine.\n\n# Step 2: Build a Custom Tokenizer\n\nRun train_tokenizer.py or train_tokenizer_hf.py on the cleaned data.\nThis will give you vocab.json and merges.txt",
    "ContentSha": "2insR4PaYWSK/WVBoUmyJTaKVTE5tDObKk5oCcRpMh8=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "現在は、完全な文構造が欠如しており、全体的に意味をなさない文が生成されますが、これはトレーニングサイズとしては正常です。\n\n# V0.5 モデルの挙動と制限事項\n\nこれは前回のモデルと比べて大きな改善です。文体と語彙はヴィクトリア朝風で、ほぼすべての文が正しい句読点を伴い文法的にも正確です。そして繰り返しますが、これはゼロからトレーニングしたので、1800年代の主題に固執しています。\n\n![TimeLockLLM サンプル出力](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)\n\n事実に基づかない幻覚が多く見られます。詳細（日時、出来事、歴史上の人物）はほぼ100％作り話です。また、文同士のつながりもあまりなく、せいぜい2文ほどが関連する程度で、それ以上の繋がりはありません。もう一つの問題として、ときどき「Digitized by Google」というフッターが紛れ込むことがあるので、次回のトレーニングではテキストのクリーンアップを徹底する必要があります。全体としては結果に非常に満足しており、まだLLMにはほど遠いですが、確実に文生成器にはなっています。\n\n多くのことを学んでおり、今後数週間でより良くするために何をすべきかを模索し始めます。ファイルはまもなくアップロードします！\n\n# V1 モデルの挙動と制限事項\n\nまもなくいくつかのサンプル出力をアップロードし、同じプロンプトで3つのモデルを比較します。また、前回と同様にV1もhuggingfaceにアップロードする予定です。私のhuggingfaceアカウントはこちらです：https://huggingface.co/haykgrigorian/TimeCapsuleLLM\n\n# 今後の計画\n\n（完了済み）バージョン0.5の作業を開始します。今回は50冊ではなく、理想的には500～600冊を使用してトレーニングします。現在は1800～1850年のロンドンの本を使ってnanoGPTをトレーニング中です。見つけた本が更新されていないか、現代的な解釈が含まれていないか、選択した期間内に出版された無改訂の本であることを確認することが課題となっています。\n\nより大きなコーパス（v0.5の5～10倍）で新しいモデル（v1）をトレーニングしたいと考えています。私の目標は、選択的時系列トレーニングだけで推論能力が現れるかどうかを確認することです。これはより難しい課題であり、歴史的データの制限があるため、実現可能かどうかも完全には分かりません。今後数週間で5～10GBのコーパスのために十分なデータをキュレートするつもりです。クリーンで高品質なデータを揃えてGPUをレンタルできれば、進展があると信じています。\n\n# このプロジェクトの使い方\n\nこのプロジェクトは主に歴史的データの収集、トレーニング用の準備、およびトークナイザーの構築に焦点を当てています。完全なLLMトレーニングプロセスについては触れませんので、Andrej Karpathy氏のnanoGPTを参照してください。\n\n# ステップ1：歴史的テキストを収集・準備\n\n選択した期間（例：ロンドン1800～1850年）のパブリックドメインの本やドキュメント等の.txtファイルを集めます。\n\n必要であれば、download_texts_improved.py を使って本をダウンロードできます。\n\nテキストファイルをスクリプトまたは手動でクリーンアップし、プロジェクト・グーテンベルクのヘッダー/フッター、現代の注釈、OCRエラー等を削除します。\n\nprepare_dataset.pyは問題なく動作するはずです。\n\n# ステップ2：カスタムトークナイザーの構築\n\nクリーンなデータで train_tokenizer.py または train_tokenizer_hf.py を実行します。\nこれで vocab.json と merges.txt が得られます。",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "SzQ3fs08W+JGBPGw6+aEzLsm/vGfQNDJ6YZnfFE0F4I=",
        "originContent": "Right now it produces sentences that lack full sentence structure and overall just make no sense but this is normal for the training size. ",
        "translatedContent": "現在は、完全な文構造が欠如しており、全体的に意味をなさない文が生成されますが、これはトレーニングサイズとしては正常です。"
      },
      {
        "row": 2,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 3,
        "rowsha": "eeaXlcgOJhwk4liDhaA6/6S4URmAdV8vj9hlqGr3LFw=",
        "originContent": "# V0.5 Model Behavior & Limitations",
        "translatedContent": "# V0.5 モデルの挙動と制限事項"
      },
      {
        "row": 4,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 5,
        "rowsha": "tII9OX/M7KpPoQKPIphmQBWtsSVucUevvTnzVsTXcaI=",
        "originContent": "This is a nice improvement compared to the last model. The writing style and vocab is Victorian and almost every sentence is grammatically correct with proper punctuation. And again this is trained from scratch so it sticks to 1800's subjects. ",
        "translatedContent": "これは前回のモデルと比べて大きな改善です。文体と語彙はヴィクトリア朝風で、ほぼすべての文が正しい句読点を伴い文法的にも正確です。そして繰り返しますが、これはゼロからトレーニングしたので、1800年代の主題に固執しています。"
      },
      {
        "row": 6,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 7,
        "rowsha": "8DhXpgpVtg05XdyplRHf49EFOQNCJVzXA9RpmJQ+y9U=",
        "originContent": "![TimeLockLLM Sample Output](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)",
        "translatedContent": "![TimeLockLLM サンプル出力](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)"
      },
      {
        "row": 8,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 9,
        "rowsha": "YgdLunUOAWWHsq+eAfyWcDz1fCc0rCcobnRpEXHoK94=",
        "originContent": "There are a lot of factual hallucinations. A lot (like 100%) of the details (dates, events, historical figures)  are made up. Also the sentences don't really have connections to each other, sometimes maybe 2 sentences will relate to each other but beyond that they dont. Another issue is sometimes a stray “Digitized by Google” footer shows up, so the next time I train I really have to make sure the texts are cleaned well. Overall I'm very happy with the results, it's nowhere near an LLM yet but definitely a sentence generator. ",
        "translatedContent": "事実に基づかない幻覚が多く見られます。詳細（日時、出来事、歴史上の人物）はほぼ100％作り話です。また、文同士のつながりもあまりなく、せいぜい2文ほどが関連する程度で、それ以上の繋がりはありません。もう一つの問題として、ときどき「Digitized by Google」というフッターが紛れ込むことがあるので、次回のトレーニングではテキストのクリーンアップを徹底する必要があります。全体としては結果に非常に満足しており、まだLLMにはほど遠いですが、確実に文生成器にはなっています。"
      },
      {
        "row": 10,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 11,
        "rowsha": "V44Ne2sN8v7ZVpRZ5vo7B2aUZFjGppYthg0fjsQAqd0=",
        "originContent": "I'm learning a lot and will start figuring out what I need to do better in the coming weeks. I will upload files soon! ",
        "translatedContent": "多くのことを学んでおり、今後数週間でより良くするために何をすべきかを模索し始めます。ファイルはまもなくアップロードします！"
      },
      {
        "row": 12,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 13,
        "rowsha": "KYAZ5TGCcW3BtPPWPqj6LyktFuBuQHXKdjySkeqqliY=",
        "originContent": "# V1 Model Behavior & Limitations ",
        "translatedContent": "# V1 モデルの挙動と制限事項"
      },
      {
        "row": 14,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 15,
        "rowsha": "5ERqUEB22vCoQvNkDV/y8ULDJA4ly6K7xDVXDOYkAW4=",
        "originContent": "I will upload some example output soon and also do comparisons between the 3 models with the same prompt. I will also upload V1 to huggingface like I did with my last verison, you can find my huggingface account here: https://huggingface.co/haykgrigorian/TimeCapsuleLLM",
        "translatedContent": "まもなくいくつかのサンプル出力をアップロードし、同じプロンプトで3つのモデルを比較します。また、前回と同様にV1もhuggingfaceにアップロードする予定です。私のhuggingfaceアカウントはこちらです：https://huggingface.co/haykgrigorian/TimeCapsuleLLM"
      },
      {
        "row": 16,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 17,
        "rowsha": "iMvQyl++GAiLpFMT+58v3e9/hb7zXnIJauchG5p986Y=",
        "originContent": "# Upcoming Plans ",
        "translatedContent": "# 今後の計画"
      },
      {
        "row": 18,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 19,
        "rowsha": "OIt/QAbIp2G7kt87dbAIf6Ec0U3HnwjOW6LYRmdf68I=",
        "originContent": "(Completed) I'm going to start work on version 0.5 , instead of training using 50 books, I'll train using ideally 500-600. Right now I'm training nanoGPT using books from 1800-1850 and specifically from London. There is some challeneges like making sure the books I find are not updated or have modern interpretations but untouched books published withtin my chosen time period.",
        "translatedContent": "（完了済み）バージョン0.5の作業を開始します。今回は50冊ではなく、理想的には500～600冊を使用してトレーニングします。現在は1800～1850年のロンドンの本を使ってnanoGPTをトレーニング中です。見つけた本が更新されていないか、現代的な解釈が含まれていないか、選択した期間内に出版された無改訂の本であることを確認することが課題となっています。"
      },
      {
        "row": 20,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 21,
        "rowsha": "13sW7eekt8pSDdOxr/oKukwKEOVmw/M8xiAJC1VFXZc=",
        "originContent": "I want to train a new model (v1) with a much larger corpus, maybe 5-10x larger than the one I used for v0.5. My goal is to see if I can get reasoning abilities to emerge from Selective Temporal Training alone, this will be a more difficult task and I'm not even entirely sure if it's possible due to the fact that there is historical data limitations. In the upcoming weeks I will try to curate enough data for a 5-10GB corpus. I believe If I can get clean clean high quality data and rent a GPU, there will be progress.",
        "translatedContent": "より大きなコーパス（v0.5の5～10倍）で新しいモデル（v1）をトレーニングしたいと考えています。私の目標は、選択的時系列トレーニングだけで推論能力が現れるかどうかを確認することです。これはより難しい課題であり、歴史的データの制限があるため、実現可能かどうかも完全には分かりません。今後数週間で5～10GBのコーパスのために十分なデータをキュレートするつもりです。クリーンで高品質なデータを揃えてGPUをレンタルできれば、進展があると信じています。"
      },
      {
        "row": 22,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 23,
        "rowsha": "sO+voevLpUtEbqum6Gntntle+nPVa66c5GATAMLrgf0=",
        "originContent": "# How to Use This Project ",
        "translatedContent": "# このプロジェクトの使い方"
      },
      {
        "row": 24,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 25,
        "rowsha": "XVoXr9uzZwN09vboETojEQJe057RBzcMUjXmQRCB/jo=",
        "originContent": "This project focuses mostly on curating historical data, preparing it for training and building a tokenizer. I am not going to cover the full LLM training process, for that refer to nanoGPT by Andrej Karpathy.",
        "translatedContent": "このプロジェクトは主に歴史的データの収集、トレーニング用の準備、およびトークナイザーの構築に焦点を当てています。完全なLLMトレーニングプロセスについては触れませんので、Andrej Karpathy氏のnanoGPTを参照してください。"
      },
      {
        "row": 26,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 27,
        "rowsha": "kDK7XtqFkTiZvD804yYG4VEojvLrRbdEhmHEBzDAQz4=",
        "originContent": "# Step 1: Gather and Prepare Historical Texts ",
        "translatedContent": "# ステップ1：歴史的テキストを収集・準備"
      },
      {
        "row": 28,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 29,
        "rowsha": "oRoOYaG+mx3SqTnhGcOKeH7W3W4wSRQmhZS4jPGMH8s=",
        "originContent": "Collect .txt files of public domain books, documents, etc from your chosen time period (e.g., London 1800-1850)",
        "translatedContent": "選択した期間（例：ロンドン1800～1850年）のパブリックドメインの本やドキュメント等の.txtファイルを集めます。"
      },
      {
        "row": 30,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 31,
        "rowsha": "EBDhZvTogrL4hqRuH095O7/tXXoy2OEskLQlang/pOA=",
        "originContent": "You can use download_texts_improved.py to download books for you if you need to.",
        "translatedContent": "必要であれば、download_texts_improved.py を使って本をダウンロードできます。"
      },
      {
        "row": 32,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 33,
        "rowsha": "q5OO8x+9kIbzaQRWimI/Vo9ZowzVBtCX+TodObLsQoY=",
        "originContent": "Clean the text files using a script or manually remove headers/footer from Project Gutenberg, Modern annotations or things like OCR errors.",
        "translatedContent": "テキストファイルをスクリプトまたは手動でクリーンアップし、プロジェクト・グーテンベルクのヘッダー/フッター、現代の注釈、OCRエラー等を削除します。"
      },
      {
        "row": 34,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 35,
        "rowsha": "N4wsIjC0LRlClodmfCtYX3qswttJnk0psU28/mlCRTw=",
        "originContent": "prepare_dataset.py should work fine.",
        "translatedContent": "prepare_dataset.pyは問題なく動作するはずです。"
      },
      {
        "row": 36,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 37,
        "rowsha": "jDM2lr7pP+MT0pt+L0cd5nBXI83IPoT27NzIgplt7R8=",
        "originContent": "# Step 2: Build a Custom Tokenizer",
        "translatedContent": "# ステップ2：カスタムトークナイザーの構築"
      },
      {
        "row": 38,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 39,
        "rowsha": "+90EorsgO/X2bFK2pYJw+vZIpjIbuMm4QR6W/xfj8C8=",
        "originContent": "Run train_tokenizer.py or train_tokenizer_hf.py on the cleaned data.",
        "translatedContent": "クリーンなデータで train_tokenizer.py または train_tokenizer_hf.py を実行します。"
      },
      {
        "row": 40,
        "rowsha": "tkP3Eg1rWphTQMNhN2yYg/1+AA1IdcXbGT96aRMpnwc=",
        "originContent": "This will give you vocab.json and merges.txt",
        "translatedContent": "これで vocab.json と merges.txt が得られます。"
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 5,
    "Content": "\nThes files define vocab and merge rules for your model\n\n# Step 3: Train Your Model (nanoGPT) \n\nRefer to [nanoGPT by Andrej Karpathy](https://github.com/karpathy/nanoGPT) for the training process.\n\nYou can train a different LLM if you want, but I used nanoGPT \n\n# FAQ\n\n## What is Selective Temporal Training ?\n\nSelective Temporal Training (STT) is a machine learning methodology where all training data is specifically curated to fall within a specific historical time period. It's done in order to model the language and knowledge of that era without influence from modern concepts. For example, the current model I have now (v0.5) is trained on data exclusively from 1800-1875, it's not fine tuned but trained from scratch resulting in output that reflects the linguistic style and historical context of that time period.\n\n## Why not just use fine-tuning or LoRA?\n\nFor this project I'm trying to create a language model that is unclouded from modern bias. If I fine-tune something like GPT-2, it's already pre-trained and that information won't go away. If I train from scratch the language model won't pretend to be old, it just will be. The Goal for this project right now is to create something can reason exclusively using knowledge from London books published between 1800 and 1850.\n\n## What kind of data did you use for training?\n\nI'm using books, legal documents, newspapers, and other writings from 1800–1850 London. The list I linked has like 200 but for the first training I just used 50 files about ~187 MB. You can view a list of the documents:\nhttps://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt\n\n## How large are the models ?\n\nV0: 16M Parameters\n\nV0.5 123M Parameters\n\nV1: 700M Parameters\n\n# Training Specs ? \n\n#V0/V0.5\nGPU: Geforce rtx 4060\nCPU: i5-13400F \nRam: 16GB DDR5.\n\n#V1",
    "ContentSha": "1EDC2Z8y6MaxBbgALr3Zh6XVrqKnPwurprSgXDM8Qrc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "これらのファイルは、モデルの語彙とマージルールを定義します。\n\n# ステップ3: モデルのトレーニング (nanoGPT)\n\nトレーニングプロセスについては [Andrej KarpathyのnanoGPT](https://github.com/karpathy/nanoGPT) を参照してください。\n\n別のLLMをトレーニングすることもできますが、私はnanoGPTを使用しました。\n\n# FAQ\n\n## Selective Temporal Trainingとは何ですか？\n\nSelective Temporal Training（STT）は、すべてのトレーニングデータが特定の歴史的時代内に厳選される機械学習手法です。その時代の言語と知識を現代の概念から影響を受けずにモデル化するために行われます。例えば、現在私が持っているモデル（v0.5）は1800～1875年のデータのみでトレーニングされており、ファインチューニングはされていません。最初からトレーニングすることで、その時代の言語スタイルや歴史的文脈が反映された出力となります。\n\n## なぜファインチューニングやLoRAを使わないのですか？\n\nこのプロジェクトでは、現代のバイアスに影響されない言語モデルを作りたいと考えています。GPT-2のようなものをファインチューニングしても、すでに事前学習されている情報は消えません。最初からトレーニングすれば、言語モデルは古いふりをするのではなく、本当に古いものになります。現時点での目標は、1800～1850年のロンドンで出版された本の知識のみで推論できるものを作ることです。\n\n## トレーニングにはどんなデータを使いましたか？\n\n私は1800～1850年ロンドンの本、法的文書、新聞、その他の文書を使用しています。リンクしたリストには約200ありますが、最初のトレーニングには約50ファイル（約187MB）だけ使用しました。文書のリストはこちらで見られます：\nhttps://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt\n\n## モデルのサイズはどれくらいですか？\n\nV0: 16M パラメータ\n\nV0.5: 123M パラメータ\n\nV1: 700M パラメータ\n\n# トレーニングスペックは？\n\n#V0/V0.5\nGPU: Geforce RTX 4060\nCPU: i5-13400F\nRAM: 16GB DDR5.\n\n#V1\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "これらのファイルは、モデルの語彙とマージルールを定義します。"
      },
      {
        "row": 2,
        "rowsha": "/wqxgtOu72+x3a2xi7q23jDkx+WQv2SHrJddzpvm1Ys=",
        "originContent": "Thes files define vocab and merge rules for your model",
        "translatedContent": ""
      },
      {
        "row": 3,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "# ステップ3: モデルのトレーニング (nanoGPT)"
      },
      {
        "row": 4,
        "rowsha": "vKPAEsPxc9uYtzjTX9/rNADSDnxkKwYcYpX/aiAp8Hc=",
        "originContent": "# Step 3: Train Your Model (nanoGPT) ",
        "translatedContent": ""
      },
      {
        "row": 5,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "トレーニングプロセスについては [Andrej KarpathyのnanoGPT](https://github.com/karpathy/nanoGPT) を参照してください。"
      },
      {
        "row": 6,
        "rowsha": "tCDY5iXt+Z7YYeTPouMSYDX5uuFnGROxZMvHyTOIblY=",
        "originContent": "Refer to [nanoGPT by Andrej Karpathy](https://github.com/karpathy/nanoGPT) for the training process.",
        "translatedContent": ""
      },
      {
        "row": 7,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "別のLLMをトレーニングすることもできますが、私はnanoGPTを使用しました。"
      },
      {
        "row": 8,
        "rowsha": "bLpr9snECDSJ5ejrqDWYNT8VXSupCQKQxyLxvAmQ2Kc=",
        "originContent": "You can train a different LLM if you want, but I used nanoGPT ",
        "translatedContent": ""
      },
      {
        "row": 9,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "# FAQ"
      },
      {
        "row": 10,
        "rowsha": "OoCxyGfPN5TmdzAkaPphtPx303MJJ7vpfWbKrufGH5g=",
        "originContent": "# FAQ",
        "translatedContent": ""
      },
      {
        "row": 11,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## Selective Temporal Trainingとは何ですか？"
      },
      {
        "row": 12,
        "rowsha": "+5dDgPw4ILEotxso4tjjjz1cxwUei16yNQPDUKbgxoo=",
        "originContent": "## What is Selective Temporal Training ?",
        "translatedContent": ""
      },
      {
        "row": 13,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "Selective Temporal Training（STT）は、すべてのトレーニングデータが特定の歴史的時代内に厳選される機械学習手法です。その時代の言語と知識を現代の概念から影響を受けずにモデル化するために行われます。例えば、現在私が持っているモデル（v0.5）は1800～1875年のデータのみでトレーニングされており、ファインチューニングはされていません。最初からトレーニングすることで、その時代の言語スタイルや歴史的文脈が反映された出力となります。"
      },
      {
        "row": 14,
        "rowsha": "hooEARKH4r/sDPh7JUtZAZ6TYMvBkTLZIcfw3g83xos=",
        "originContent": "Selective Temporal Training (STT) is a machine learning methodology where all training data is specifically curated to fall within a specific historical time period. It's done in order to model the language and knowledge of that era without influence from modern concepts. For example, the current model I have now (v0.5) is trained on data exclusively from 1800-1875, it's not fine tuned but trained from scratch resulting in output that reflects the linguistic style and historical context of that time period.",
        "translatedContent": ""
      },
      {
        "row": 15,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## なぜファインチューニングやLoRAを使わないのですか？"
      },
      {
        "row": 16,
        "rowsha": "dVMKQ2mPI1Spc6x6r/jNG0PIR5YKpalU4MXx9JmKp/I=",
        "originContent": "## Why not just use fine-tuning or LoRA?",
        "translatedContent": ""
      },
      {
        "row": 17,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "このプロジェクトでは、現代のバイアスに影響されない言語モデルを作りたいと考えています。GPT-2のようなものをファインチューニングしても、すでに事前学習されている情報は消えません。最初からトレーニングすれば、言語モデルは古いふりをするのではなく、本当に古いものになります。現時点での目標は、1800～1850年のロンドンで出版された本の知識のみで推論できるものを作ることです。"
      },
      {
        "row": 18,
        "rowsha": "oNvWlJHtQSyq1TwlqJyGtMzk4Z4mBIn8AW2SudzvUYs=",
        "originContent": "For this project I'm trying to create a language model that is unclouded from modern bias. If I fine-tune something like GPT-2, it's already pre-trained and that information won't go away. If I train from scratch the language model won't pretend to be old, it just will be. The Goal for this project right now is to create something can reason exclusively using knowledge from London books published between 1800 and 1850.",
        "translatedContent": ""
      },
      {
        "row": 19,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## トレーニングにはどんなデータを使いましたか？"
      },
      {
        "row": 20,
        "rowsha": "ByP4WlNmMoG6WIiLJNd6b080/DSciCgWmj9aYSJjAF0=",
        "originContent": "## What kind of data did you use for training?",
        "translatedContent": ""
      },
      {
        "row": 21,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "私は1800～1850年ロンドンの本、法的文書、新聞、その他の文書を使用しています。リンクしたリストには約200ありますが、最初のトレーニングには約50ファイル（約187MB）だけ使用しました。文書のリストはこちらで見られます："
      },
      {
        "row": 22,
        "rowsha": "Kj6EF7wZdUrAFg4ErGmJuh9Q5Xujmb+tunpfssPKXkA=",
        "originContent": "I'm using books, legal documents, newspapers, and other writings from 1800–1850 London. The list I linked has like 200 but for the first training I just used 50 files about ~187 MB. You can view a list of the documents:",
        "translatedContent": "https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt"
      },
      {
        "row": 23,
        "rowsha": "0mxyGiLJxzp9JPCg1oA+nbIwAKJbEC4ei9kSV3Gp84Y=",
        "originContent": "https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt",
        "translatedContent": ""
      },
      {
        "row": 24,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## モデルのサイズはどれくらいですか？"
      },
      {
        "row": 25,
        "rowsha": "rrsRMYLHKsjbkzYRKilseYJKKApHSgo9buiBkZFSvlY=",
        "originContent": "## How large are the models ?",
        "translatedContent": ""
      },
      {
        "row": 26,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "V0: 16M パラメータ"
      },
      {
        "row": 27,
        "rowsha": "MVeUG+6lIlH89E699W2WfE+eQmKEul25KNMIY8AtB6M=",
        "originContent": "V0: 16M Parameters",
        "translatedContent": ""
      },
      {
        "row": 28,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "V0.5: 123M パラメータ"
      },
      {
        "row": 29,
        "rowsha": "hQIpZXf/UvJi3QFJEM6jrjG7vnb9cvTFg/cvKv0buqM=",
        "originContent": "V0.5 123M Parameters",
        "translatedContent": ""
      },
      {
        "row": 30,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "V1: 700M パラメータ"
      },
      {
        "row": 31,
        "rowsha": "26WDqv9Q/x8glaWZTkjUWL5VNwuuXgUuXs51BhKEskk=",
        "originContent": "V1: 700M Parameters",
        "translatedContent": ""
      },
      {
        "row": 32,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "# トレーニングスペックは？"
      },
      {
        "row": 33,
        "rowsha": "zaP9HMP5oBcc9MGTkbFGzMO2XxeWWl+QlS9Yz9nfNXc=",
        "originContent": "# Training Specs ? ",
        "translatedContent": ""
      },
      {
        "row": 34,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "#V0/V0.5"
      },
      {
        "row": 35,
        "rowsha": "F/31V18ZJZVpqdqexHnDrgTPaWCFGcintqPNuIkiXuE=",
        "originContent": "#V0/V0.5",
        "translatedContent": "GPU: Geforce RTX 4060"
      },
      {
        "row": 36,
        "rowsha": "EH8H1HW/C4Tb7LfJgVUnVGsk4pF9l40Rlev8tAkKhjI=",
        "originContent": "GPU: Geforce rtx 4060",
        "translatedContent": "CPU: i5-13400F"
      },
      {
        "row": 37,
        "rowsha": "vo3FdN37kY6VUB7PruRKfBPJDgsVJyBHIUCn/g8mt68=",
        "originContent": "CPU: i5-13400F ",
        "translatedContent": "RAM: 16GB DDR5."
      },
      {
        "row": 38,
        "rowsha": "W8fXPiQKUkoNso0PPfTvjYMy0IYo85j+gNXmB0aERO4=",
        "originContent": "Ram: 16GB DDR5.",
        "translatedContent": ""
      },
      {
        "row": 39,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "#V1"
      },
      {
        "row": 40,
        "rowsha": "F5uxw2t7Mttv7FTw1w1bsQQrYW79NPWp2wnmKItLVk0=",
        "originContent": "#V1",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 6,
    "Content": "GPU: A100 rented\n\n\n\n\n\n\n\n",
    "ContentSha": "AJFDgV5fpiGwEqghNVAg6H7u3nm4rR+cj1PQCYIFveo=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "GPU: A100 rented\n\n\n\n\n\n\n\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "cBfLeLi5ORliO9eNHlkjuWbV5U2y03sn/wFBoChMpoc=",
        "originContent": "GPU: A100 rented",
        "translatedContent": "GPU: A100 rented"
      },
      {
        "row": 2,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 3,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 4,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 5,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 6,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 7,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 8,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 9,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  }
]