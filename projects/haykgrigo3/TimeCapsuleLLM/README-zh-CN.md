# TimeCapsule LLM
一个仅基于特定时间段数据训练的LLM，以减少现代偏见。

想象一下，如果一个AI模型不仅仅是假装是历史性的，而是真的如此。

基于[Andrej Karpathy的nanoGPT](https://github.com/karpathy/nanoGPT)构建，核心训练脚本和模型架构是他的工作。

# 项目目标

TimeCapsule LLM是一个实验性项目，只训练于特定时间段内的文本。目标是模拟特定历史时代的世界观和语言。

# 为什么微调不够

如果你只是微调一个预训练模型，你的LLM仍然会知道现代概念。当然，实现零现代偏见很困难，但我想尽可能接近这个目标。要完全避免现代偏见，需要从零开始训练模型。

# 预期成果

希望完成后，该模型不会知道现代概念，也无法超出所训练内容进行推理。它不应识别现代概念/词汇，我也希望它不会产生现代知识的幻觉。

# 进展更新

## 2025年7月9日

我设定的时间段为1800-1850，区域：伦敦

我收集了一批文本、书籍、文档

目前我已获得50个txt文件，即将开始训练NanoGPT

只要有进展，我会持续更新

## 2025年7月13日

用187MB的历史文本数据训练了nanoGPT。

## 2025年7月15日

我开始下载第二轮训练用的文本。我从Internet Archive获取所有资料，并将时间段扩展到1800-1875。为了获得多样化的文本，可以使用Internet Archive上的出版地点、时间段和主题的主题和搜索过滤器。

![搜索过滤器](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## 2025年7月16日

我从Internet Archive下载了约500个txt文件，清理后（仅删除空白、Gutenberg头部等）大约有500MB数据。数据集很小，但上次训练用的是187MB，训练第二个模型后输出应该至少会有一些明显差异。我希望这个模型至少能生成更连贯、稍微有意义的句子。当然这不保证，因为数据集依然非常小，但比上次多。

这应该可以在我自己的硬件上完成，这也好，因为我希望在转向更大数据集（需要租用GPU）之前看到一些改进。但别担心，我仍计划很快租用GPU，但在那之前我想确保数据集尽可能精心整理和清理。清理是个问题，很多txt文件中混有乱码。我用的清理脚本有效，但不是100%有效。

我今天将训练这个数据集，预计需要4-5小时。完成测试后会更新。感谢所有关注我项目的人，甚至有人给我提供了OCR资源的链接，感谢！希望更多人尝试并用自己的数据集做实验。

### 训练更新

我开始训练一个435MB（108百万token）的语料库，目前进展顺利。训练损失从10.9降至4.9，前2800次迭代。我预计完成需要8到9小时。完成后会再更新。

## 2025年7月17日凌晨2:13

第二个模型训练完成，我的4060显卡耗时约8小时40分钟（3,900迭代/小时），共33,000次迭代（5个周期）。最终训练损失为3.73。输出令人惊喜，确实能生成连贯的19世纪风格句子。

# V0模型行为与限制

早期提示显示模型用1800年代的语言和行为回应。例如，我提示“Who art Henry?”它回答“我认识那人，我并非黑色，风暴。”这个句子毫无意义，但LLM识别出我在问一个人。

![TimeLockLLM示例输出](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

没有提及现代概念，输出主要包含1800年代的词汇和表达。

仍需大量工作，使用187MB训练数据无法生成具复杂推理的文本。

目前生成的句子缺乏完整句子结构，整体难以理解，但这对训练规模而言是正常的。

# V0.5模型行为与限制

相比上一个模型有明显改进。写作风格和词汇为维多利亚时代风格，几乎所有句子语法正确，标点恰当。同样这是从零训练的，因此严格遵循1800年代主题。

![TimeLockLLM示例输出](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

存在大量事实幻觉。大部分（约100%）细节（日期、事件、历史人物）是虚构的。句子之间连接较少，有时只有两句相关，之后基本无关。还有个问题是偶尔会出现“Digitized by Google”的页脚，所以下次训练时我必须确保文本清理彻底。总体上我对结果很满意，离真正的LLM还远，但绝对是句子生成器。

我学到了很多，接下来几周会开始研究需要改进的地方。文件会尽快上传！

# 未来计划

我将开始开发1.0版本，不再用50本书训练，而是理想情况下用500-600本。目前我用1800-1850年、伦敦地区的书籍训练nanoGPT。面临的挑战是确保找到的书籍未被更新或带有现代解读，而是真正未被改动、发布于选定时间段内的书籍。

# 如何使用本项目

本项目主要关注历史数据的整理、训练准备和构建分词器。不涵盖完整LLM训练流程，详见Andrej Karpathy的nanoGPT。

# 第一步：收集并准备历史文本

收集所选时间段（如伦敦1800-1850年）内的公共领域书籍、文档等.txt文件

如果需要，可以使用download_texts_improved.py下载书籍。

用脚本或手动清理文本文件，去除古登堡项目的头尾、现代注释或OCR错误等。

prepare_dataset.py 应该可以正常工作。

# 第二步：构建自定义分词器

在清理后的数据上运行 train_tokenizer.py 或 train_tokenizer_hf.py。
这将给你 vocab.json 和 merges.txt

这些文件定义了你的模型的词汇表和合并规则

# 第3步：训练你的模型（nanoGPT）

训练过程请参考 [Andrej Karpathy 的 nanoGPT](https://github.com/karpathy/nanoGPT)。

如果你愿意，可以训练不同的LLM，但我使用的是 nanoGPT

# 常见问题解答

## 为什么不直接使用微调或 LoRA？

在这个项目中，我试图创建一个不受现代偏见影响的语言模型。如果我微调像 GPT-2 这样的模型，它已经经过预训练，这些信息不会消失。如果我从头开始训练，语言模型不会假装是旧的，它就是真的旧。这个项目的目标是创建一个只能使用1800年至1850年间伦敦出版的书籍知识进行推理的模型。

## 你用了什么样的数据进行训练？

我使用了1800年至1850年伦敦的书籍、法律文件、报纸和其他著作。我链接的列表大约有200个文件，但第一次训练我只用了50个文件，约187 MB。你可以查看文档列表：
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## 版本0模型有多大？

这个模型目前非常小，我只是为了好玩，并且严格遵守不使用现代资源的训练规则。它有近1600万个参数，但我将开始收集更多旧文本，开始另一个模型训练。会在进展中更新。

## 训练规格？

GPU：Geforce rtx 4060
CPU：i5-13400F
内存：16GB DDR5。


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-18

---