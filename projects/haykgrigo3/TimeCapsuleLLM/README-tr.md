<div align="right">
  <details>
    <summary >ğŸŒ Dil</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ç®€ä½“ä¸­æ–‡</a>
        | <a href="#" title="Coming soon">ç¹é«”ä¸­æ–‡ (yakÄ±nda)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">æ—¥æœ¬èª</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">í•œêµ­ì–´</a>
        | <a href="#" title="Coming soon">à¤¹à¤¿à¤¨à¥à¤¦à¥€ (yakÄ±nda)</a> |
        | <a href="#" title="Coming soon">à¹„à¸—à¸¢ (yakÄ±nda)</a> |
        | <a href="#" title="Coming soon">FranÃ§ais (yakÄ±nda)</a>
        | <a href="#" title="Coming soon">Deutsch (yakÄ±nda)</a>
        | <a href="#" title="Coming soon">EspaÃ±ol (yakÄ±nda)</a>
        | <a href="#" title="Coming soon">Italiano (yakÄ±nda)</a>
        | <a href="#" title="Coming soon">Ğ ÑƒÑÑĞºĞ¸Ğ¹ (yakÄ±nda)</a>
        | <a href="#" title="Coming soon">PortuguÃªs (yakÄ±nda)</a>
        | <a href="#" title="Coming soon">Nederlands (yakÄ±nda)</a>
        | <a href="#" title="Coming soon">Polski (yakÄ±nda)</a>
        | <a href="#" title="Coming soon">Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© (yakÄ±nda)</a>
        | <a href="#" title="Coming soon">ÙØ§Ø±Ø³ÛŒ (yakÄ±nda)</a>
        | <a href="#" title="Coming soon">TÃ¼rkÃ§e (yakÄ±nda)</a>
        | <a href="#" title="Coming soon">Tiáº¿ng Viá»‡t (yakÄ±nda)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (yakÄ±nda)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM
Belirli zaman dilimlerinden alÄ±nan verilerle eÄŸitilmiÅŸ, modern Ã¶nyargÄ±larÄ± azaltmayÄ± amaÃ§layan bir LLM.

Hayal edin ki bir yapay zeka modeli sadece tarihiymiÅŸ gibi davranmÄ±yor, gerÃ§ekten Ã¶yle.

[Andrej Karpathy'nin nanoGPT'si](https://github.com/karpathy/nanoGPT) Ã¼zerine kurulmuÅŸtur. Ã‡ekirdek eÄŸitim betikleri ve model mimarisi ona aittir.

# Proje Hedefleri

TimeCapsule LLM, yalnÄ±zca belirli zaman dilimlerinde yazÄ±lmÄ±ÅŸ metinlerle eÄŸitilecek deneysel bir projedir. Hedef, belirli tarihsel dÃ¶nemlerin dÃ¼nya gÃ¶rÃ¼ÅŸÃ¼nÃ¼ ve dilini simÃ¼le etmektir.

# Neden sadece ince ayar yeterli deÄŸil

Sadece Ã¶nceden eÄŸitilmiÅŸ bir modeli ince ayar yaparsanÄ±z, LLMâ€™iniz hÃ¢lÃ¢ modern kavramlarÄ± bilecek. Elbette sÄ±fÄ±r modern Ã¶nyargÄ± elde etmek zor, ama buna olabildiÄŸince yaklaÅŸmak istiyorum. HiÃ§ modern Ã¶nyargÄ± olmamasÄ±, modeli sÄ±fÄ±rdan eÄŸitmeyi gerektirir.

# Beklenen SonuÃ§lar

UmarÄ±m tamamlandÄ±ÄŸÄ±nda, bu model modern kavramlarÄ± bilmeyecek ve eÄŸitildiÄŸi bilginin Ã¶tesinde akÄ±l yÃ¼rÃ¼temeyecek. Modern kavramlarÄ±/kelime daÄŸarcÄ±ÄŸÄ±nÄ± tanÄ±mamalÄ± ve umarÄ±m modern bilgi uydurmaz.

# Ä°lerleme GÃ¼ncellemeleri

## 9 Temmuz 2025

Zaman dilimimi 1800-1850 ve bÃ¶lgeyi Londra olarak belirledim

Bir dizi metin, kitap, belge topladÄ±m

Åu ana kadar 50 tanesini txt dosyasÄ± olarak aldÄ±m ve yakÄ±nda NanoGPT eÄŸitmeye baÅŸlayacaÄŸÄ±m

Ä°lerleme kaydettikÃ§e burayÄ± gÃ¼ncelleyeceÄŸim

## 13 Temmuz 2025

nanoGPT'yi 187MB tarihsel metin verisiyle eÄŸittim.

## 15 Temmuz 2025

Ä°kinci eÄŸitim iÃ§in metin indirmeye baÅŸladÄ±m. Her ÅŸeyi Internet Archiveâ€™den alÄ±yorum ve zaman dilimini 1800-1875â€™e geniÅŸlettim. Ã‡eÅŸitli metinler almak iÃ§in, Internet Archiveâ€™de yayÄ±n yeri, zaman dilimi ve konu filtrelerini kullanabilirsiniz.

![Arama Filtreleri](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## 16 Temmuz 2025

Internet Archiveâ€™den yaklaÅŸÄ±k 500 txt dosyasÄ± indirdim ve bunlarÄ± temizledikten sonra (sadece boÅŸluklarÄ±, Gutenberg baÅŸlÄ±klarÄ±nÄ± vs. silerek) elimde yaklaÅŸÄ±k 500MB veri var. Bu Ã§ok kÃ¼Ã§Ã¼k bir veri kÃ¼mesi ama son seferinde 187MB ile eÄŸitmiÅŸtim, bu yÃ¼zden ikinci modeli eÄŸittikten sonra Ã§Ä±ktÄ±daki farkÄ±n en azÄ±ndan bir miktar hissedilmesi gerekir. En azÄ±ndan daha anlamlÄ± cÃ¼mleler Ã¼retebilen bir model elde etmeyi umuyorum. Elbette bu garanti deÄŸil Ã§Ã¼nkÃ¼ veri kÃ¼mesi hÃ¢lÃ¢ Ã§ok kÃ¼Ã§Ã¼k, ama geÃ§en sefer kullandÄ±ÄŸÄ±mdan daha fazla.

Bunu kendi donanÄ±mÄ±mda yapmak mÃ¼mkÃ¼n, bu da iyi Ã§Ã¼nkÃ¼ daha bÃ¼yÃ¼k bir veri kÃ¼mesine geÃ§meden Ã¶nce bir miktar geliÅŸme gÃ¶rmeyi umuyorum; bu, GPU kiralamamÄ± gerektirecek. Ama merak etmeyin, yakÄ±nda bir GPU kiralamayÄ± hÃ¢lÃ¢ planlÄ±yorum, fakat Ã¶nce veri kÃ¼memin olabildiÄŸince derlenmiÅŸ ve temiz olduÄŸundan emin olmak istiyorum. KarÅŸÄ±laÅŸtÄ±ÄŸÄ±m sorunlardan biri temizlik; bu txt dosyalarÄ±nÄ±n Ã§oÄŸunda anlamsÄ±z ÅŸeyler karÄ±ÅŸmÄ±ÅŸ durumda. Temizlik iÃ§in kullandÄ±ÄŸÄ±m betikler iÅŸe yarÄ±yor ama %100 etkili deÄŸiller.

BugÃ¼n bu veri kÃ¼mesiyle eÄŸitim yapacaÄŸÄ±m ve yaklaÅŸÄ±k 4-5 saat sÃ¼recek. BittiÄŸinde ve test ettiÄŸimde gÃ¼ncelleme yapacaÄŸÄ±m. Projeme bakan herkese tekrar teÅŸekkÃ¼rler, hatta bana OCR kaynaklarÄ±nÄ±n baÄŸlantÄ±larÄ±nÄ± bile verenler oldu, saÄŸ olun! UmarÄ±m daha fazla insan bunu dener ve kendi veri kÃ¼meleriyle deneyler yapar.


### EÄŸitim GÃ¼ncellemesi

435MB (108 M token) bir metinle eÄŸitime baÅŸladÄ±m, ÅŸu an oldukÃ§a sorunsuz ilerliyor. EÄŸitim kaybÄ± ilk 2800 iterasyonda 10.9â€™dan 4.9â€™a dÃ¼ÅŸtÃ¼. TamamlanmasÄ±nÄ±n 8 veya 9 saat sÃ¼receÄŸini tahmin ediyorum. BittiÄŸinde baÅŸka bir gÃ¼ncelleme daha paylaÅŸacaÄŸÄ±m.

## 17 Temmuz 2025 02:13

Ä°kinci modelin eÄŸitimi tamamlandÄ±, 4060â€™Ä±mda yaklaÅŸÄ±k 8 saat 40 dakika sÃ¼rdÃ¼ (3.900 iter/saat) ve 33.000 iterasyon (5 epoch) yaptÄ±. Son eÄŸitim kaybÄ± 3.73 oldu. Ã‡Ä±ktÄ±lar ÅŸaÅŸÄ±rtÄ±cÄ± ÅŸekilde iyiydi; gerÃ§ekten 19. yÃ¼zyÄ±l tarzÄ± anlamlÄ± cÃ¼mleler Ã¼retiyor artÄ±k.

## 28 Temmuz 2025

v0.5â€™i Hugging Faceâ€™e yÃ¼kledim, [Buradan bakabilirsiniz](https://huggingface.co/haykgrigorian/TimeCapsuleLLM) isterseniz. ArtÄ±k depomu indirip yerel olarak Ã§alÄ±ÅŸtÄ±rabilirsiniz. Ne yazÄ±k ki nanoGPT HuggingFace ile yerel olarak Ã§alÄ±ÅŸmÄ±yor, bu yÃ¼zden modeli indirip yerel olarak Ã§alÄ±ÅŸtÄ±rmanÄ±z gerekecek.

AyrÄ±ca bir sonraki eÄŸitim iÃ§in veri derlemeye baÅŸlayacaÄŸÄ±m, akÄ±l yÃ¼rÃ¼tme yeteneÄŸi iÃ§in belki 5-10 kat daha fazla veriye ihtiyacÄ±m olacaÄŸÄ±nÄ± dÃ¼ÅŸÃ¼nÃ¼yorum.


# V0 Model DavranÄ±ÅŸÄ± ve SÄ±nÄ±rlamalarÄ±

Erken istemlerde model, 1800â€™lerin dili ve davranÄ±ÅŸÄ±yla yanÄ±t veriyor. Ã–rneÄŸin, "Who art Henry?" diye sordum ve "I know that man, I have did not a black, the storm." ÅŸeklinde cevapladÄ± ve evet bu cÃ¼mle pek anlamlÄ± deÄŸil ama LLM bir kiÅŸiden bahsettiÄŸimi anlÄ±yor.


![TimeLockLLM Ã–rnek Ã‡Ä±ktÄ±](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

Modern kavramlardan hiÃ§ bahsedilmiyor, Ã§Ä±ktÄ±lar Ã§oÄŸunlukla 1800â€™lerden kelimeler ve ifadeler iÃ§eriyor.

HÃ¢lÃ¢ Ã§ok fazla Ã§alÄ±ÅŸmaya ihtiyacÄ± var, 187MB ile eÄŸitim yapmak size karmaÅŸÄ±k akÄ±l yÃ¼rÃ¼tme Ã¼reten bir model vermez.

Åu anda tam cÃ¼mle yapÄ±sÄ±ndan yoksun ve genel olarak hiÃ§ mantÄ±klÄ± olmayan cÃ¼mleler Ã¼retiyor ama bu eÄŸitim boyutu iÃ§in normal.

# V0.5 Model DavranÄ±ÅŸÄ± & SÄ±nÄ±rlamalarÄ±

Bu, Ã¶nceki modele gÃ¶re gÃ¼zel bir geliÅŸme. YazÄ±m tarzÄ± ve kelime hazinesi Viktorya dÃ¶nemi ve neredeyse her cÃ¼mle gramer aÃ§Ä±sÄ±ndan doÄŸru ve uygun noktalama iÅŸaretlerine sahip. Ve yine bu model sÄ±fÄ±rdan eÄŸitildiÄŸi iÃ§in 1800â€™ler konularÄ±na baÄŸlÄ± kalÄ±yor.

![TimeLockLLM Ã–rnek Ã‡Ä±ktÄ±](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

Ã‡ok fazla gerÃ§ek dÄ±ÅŸÄ± uydurma var. DetaylarÄ±n (tarihler, olaylar, tarihi kiÅŸiler gibi) neredeyse tamamÄ± (yani %100â€™Ã¼) uydurma. AyrÄ±ca cÃ¼mleler birbirleriyle pek baÄŸlantÄ±lÄ± deÄŸil, bazen belki 2 cÃ¼mle birbiriyle iliÅŸkili oluyor ama bunun Ã¶tesine geÃ§miyor. Bir diÄŸer sorun ise bazen â€œDigitized by Googleâ€ dipnotunun araya karÄ±ÅŸmasÄ±; bir dahaki eÄŸitimde metinlerin iyice temizlendiÄŸinden emin olmam gerek. Genel olarak sonuÃ§lardan Ã§ok memnunum, henÃ¼z bir LLM deÄŸil ama kesinlikle bir cÃ¼mle Ã¼reteci.

Ã‡ok ÅŸey Ã¶ÄŸreniyorum ve Ã¶nÃ¼mÃ¼zdeki haftalarda neleri daha iyi yapmam gerektiÄŸini anlamaya baÅŸlayacaÄŸÄ±m. DosyalarÄ± yakÄ±nda yÃ¼kleyeceÄŸim!

# YaklaÅŸan Planlar

(TamamlandÄ±) 0.5 sÃ¼rÃ¼mÃ¼nde Ã§alÄ±ÅŸmaya baÅŸlayacaÄŸÄ±m, 50 kitap yerine ideal olarak 500-600 kitapla eÄŸitim yapacaÄŸÄ±m. Åu anda 1800-1850 yÄ±llarÄ± arasÄ±ndaki, Ã¶zellikle Londraâ€™dan kitaplarla nanoGPT eÄŸitiyorum. KarÅŸÄ±laÅŸtÄ±ÄŸÄ±m zorluklardan biri bulduÄŸum kitaplarÄ±n gÃ¼ncellenmemiÅŸ veya modern yorumlara sahip olmamasÄ±; seÃ§tiÄŸim zaman diliminde yayÄ±mlanmÄ±ÅŸ dokunulmamÄ±ÅŸ kitaplar olmalÄ±.

Daha bÃ¼yÃ¼k bir veri setiyle (v1) yeni bir model eÄŸitmek istiyorum, belki v0.5â€™te kullandÄ±ÄŸÄ±mÄ±n 5-10 katÄ± bÃ¼yÃ¼klÃ¼ÄŸÃ¼nde. AmacÄ±m, sadece SeÃ§ici Zamansal EÄŸitim ile akÄ±l yÃ¼rÃ¼tme yeteneÄŸinin ortaya Ã§Ä±kÄ±p Ã§Ä±kamayacaÄŸÄ±nÄ± gÃ¶rmek, bu daha zor bir gÃ¶rev olacak ve tarihi veri kÄ±sÄ±tlamalarÄ± nedeniyle bunun mÃ¼mkÃ¼n olup olmadÄ±ÄŸÄ±ndan bile emin deÄŸilim. Ã–nÃ¼mÃ¼zdeki haftalarda 5-10GBâ€™lÄ±k bir veri seti oluÅŸturmak iÃ§in Ã§alÄ±ÅŸacaÄŸÄ±m. Temiz, yÃ¼ksek kaliteli veriler bulabilir ve bir GPU kiralayabilirsem ilerleme olacaÄŸÄ±na inanÄ±yorum.

# Bu Proje NasÄ±l KullanÄ±lÄ±r

Bu proje, esas olarak tarihi verileri derleme, eÄŸitim iÃ§in hazÄ±rlama ve bir belirteÃ§leyici oluÅŸturma Ã¼zerine odaklanÄ±yor. Tam LLM eÄŸitim sÃ¼recini kapsamayacaÄŸÄ±m, bunun iÃ§in Andrej Karpathyâ€™nin nanoGPTâ€™sine baÅŸvurun.

# AdÄ±m 1: Tarihi Metinleri ToplayÄ±n ve HazÄ±rlayÄ±n

SeÃ§tiÄŸiniz zaman dilimine ait kamu malÄ± kitaplarÄ±n, belgelerin vb. .txt dosyalarÄ±nÄ± toplayÄ±n (Ã¶r. Londra 1800-1850)

Ä°htiyacÄ±nÄ±z varsa download_texts_improved.py dosyasÄ±nÄ± kullanarak kitaplarÄ± indirebilirsiniz.

Metin dosyalarÄ±nÄ± bir betik ile veya elle Project Gutenberg baÅŸlÄ±k/dipnotlarÄ±nÄ±, modern aÃ§Ä±klamalarÄ± veya OCR hatalarÄ± gibi ÅŸeyleri Ã§Ä±kararak temizleyin.

prepare_dataset.py iyi Ã§alÄ±ÅŸacaktÄ±r.

# AdÄ±m 2: Ã–zel Bir BelirteÃ§leyici OluÅŸturun

train_tokenizer.py veya train_tokenizer_hf.pyâ€™yi temizlenmiÅŸ veriler Ã¼zerinde Ã§alÄ±ÅŸtÄ±rÄ±n.
Bu size vocab.json ve merges.txt dosyalarÄ±nÄ± verecek.

Bu dosyalar modeliniz iÃ§in kelime daÄŸarcÄ±ÄŸÄ±nÄ± ve birleÅŸtirme kurallarÄ±nÄ± tanÄ±mlar.

# AdÄ±m 3: Modelinizi EÄŸitin (nanoGPT)

EÄŸitim sÃ¼reci iÃ§in [Andrej Karpathyâ€™nin nanoGPTâ€™sine](https://github.com/karpathy/nanoGPT) bakÄ±n.

Ä°sterseniz baÅŸka bir LLM de eÄŸitebilirsiniz, ancak ben nanoGPT kullandÄ±m.

# SSS

## SeÃ§ici Zamansal EÄŸitim (Selective Temporal Training) nedir?

SeÃ§ici Zamansal EÄŸitim (STT), tÃ¼m eÄŸitim verilerinin belirli bir tarihi zaman dilimine ait olacak ÅŸekilde Ã¶zel olarak derlendiÄŸi bir makine Ã¶ÄŸrenimi yÃ¶ntemidir. AmacÄ±, Ã§aÄŸdaÅŸ kavramlarÄ±n etkisinden uzak, o dÃ¶nemin dilini ve bilgisini modellemektir. Ã–rneÄŸin, ÅŸu anki modelim (v0.5) sadece 1800-1875 verisiyle eÄŸitildi, ince ayar deÄŸil tamamen sÄ±fÄ±rdan eÄŸitildi ve Ã§Ä±ktÄ±larÄ± o dÃ¶nemin dil stilini ve tarihsel baÄŸlamÄ±nÄ± yansÄ±tÄ±yor.

## Neden sadece ince ayar ya da LoRA kullanmÄ±yorsun?

Bu projede modern Ã¶nyargÄ±dan uzak bir dil modeli oluÅŸturmayÄ± deniyorum. GPT-2 gibi bir ÅŸeyi ince ayar yaparsam, model zaten Ã¶nceden eÄŸitilmiÅŸ olur ve bu bilgi gitmez. SÄ±fÄ±rdan eÄŸitirsem dil modeli eskiyi taklit etmeye Ã§alÄ±ÅŸmaz, zaten eski olur. Åu an iÃ§in amacÄ±m 1800-1850 arasÄ±nda Londraâ€™da yayÄ±mlanmÄ±ÅŸ kitaplardan elde edilen bilgiyle sadece bu bilgilerle akÄ±l yÃ¼rÃ¼tebilen bir ÅŸey yapmak.

## EÄŸitim iÃ§in ne tÃ¼r veri kullandÄ±nÄ±z?

1800â€“1850 Londraâ€™sÄ±ndan kitaplar, yasal belgeler, gazeteler ve diÄŸer yazÄ±lÄ± materyalleri kullanÄ±yorum. BaÄŸlantÄ±sÄ±nÄ± verdiÄŸim listede yaklaÅŸÄ±k 200 belge var ama ilk eÄŸitim iÃ§in sadece 50 dosya (~187 MB) kullandÄ±m. Belge listesini gÃ¶rÃ¼ntÃ¼leyebilirsiniz:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## SÃ¼rÃ¼m 0 modeli ne kadar bÃ¼yÃ¼k?

Bu model ÅŸu anda Ã§ok kÃ¼Ã§Ã¼k, sadece eÄŸlence amaÃ§lÄ± yapÄ±yorum ve modern kaynak yok kuralÄ±na sÄ±kÄ± sÄ±kÄ±ya baÄŸlÄ±yÄ±m. Neredeyse 16 milyon parametresi var ama daha fazla eski metin toplayÄ±p yeni bir model eÄŸitmeye baÅŸlayacaÄŸÄ±m. GeliÅŸmeleri paylaÅŸacaÄŸÄ±m.

## EÄŸitim Teknik Ã–zellikleri?

GPU: Geforce rtx 4060
CPU: i5-13400F
Ram: 16GB DDR5.


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-08-02

---