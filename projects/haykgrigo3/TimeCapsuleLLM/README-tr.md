
<div align="right">
  <details>
    <summary >ğŸŒ Dil</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ç®€ä½“ä¸­æ–‡</a>
        | <a href="#" title="Coming soon">ç¹é«”ä¸­æ–‡ (yakÄ±nda)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">æ—¥æœ¬èª</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">í•œêµ­ì–´</a>
        | <a href="#" title="Coming soon">à¤¹à¤¿à¤¨à¥à¤¦à¥€ (yakÄ±nda)</a> |
        | <a href="#" title="Coming soon">à¹„à¸—à¸¢ (yakÄ±nda)</a> |
        | <a href="#" title="Coming soon">FranÃ§ais (yakÄ±nda)</a>
        | <a href="#" title="Coming soon">Deutsch (yakÄ±nda)</a>
        | <a href="#" title="Coming soon">EspaÃ±ol (yakÄ±nda)</a>
        | <a href="#" title="Coming soon">Italiano (yakÄ±nda)</a>
        | <a href="#" title="Coming soon">Ğ ÑƒÑÑĞºĞ¸Ğ¹ (yakÄ±nda)</a>
        | <a href="#" title="Coming soon">PortuguÃªs (yakÄ±nda)</a>
        | <a href="#" title="Coming soon">Nederlands (yakÄ±nda)</a>
        | <a href="#" title="Coming soon">Polski (yakÄ±nda)</a>
        | <a href="#" title="Coming soon">Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© (yakÄ±nda)</a>
        | <a href="#" title="Coming soon">ÙØ§Ø±Ø³ÛŒ (yakÄ±nda)</a>
        | <a href="#" title="Coming soon">TÃ¼rkÃ§e (yakÄ±nda)</a>
        | <a href="#" title="Coming soon">Tiáº¿ng Viá»‡t (yakÄ±nda)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (yakÄ±nda)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM
Belirli zaman dilimlerinden alÄ±nan verilerle eÄŸitilmiÅŸ, modern Ã¶nyargÄ±yÄ± azaltmaya yÃ¶nelik bir LLM.

Bir AI modelinin tarihselmiÅŸ gibi davranmakla kalmayÄ±p gerÃ§ekten tarihsel olduÄŸunu hayal edin.

[Andrej Karpathy'nin nanoGPT](https://github.com/karpathy/nanoGPT) tabanlÄ±dÄ±r. Ana eÄŸitim scriptleri ve model mimarisi ona aittir.

# Proje Hedefleri

TimeCapsule LLM, yalnÄ±zca belirli zaman dilimlerinde yazÄ±lmÄ±ÅŸ metinlerle eÄŸitilecek deneysel bir projedir. AmaÃ§, belirli tarihsel dÃ¶nemlerin dÃ¼nya gÃ¶rÃ¼ÅŸÃ¼nÃ¼ ve dilini simÃ¼le etmektir.

# Neden ince ayar yeterli deÄŸil

EÄŸer Ã¶nceden eÄŸitilmiÅŸ bir modeli sadece ince ayar ile dÃ¼zenlerseniz, LLM'niz yine de modern kavramlarÄ± bilecektir. Elbette sÄ±fÄ±r modern Ã¶nyargÄ± elde etmek zor, fakat buna mÃ¼mkÃ¼n olduÄŸunca yaklaÅŸmak istiyorum. HiÃ§bir modern Ã¶nyargÄ± olmamasÄ±, modelin sÄ±fÄ±rdan eÄŸitilmesini gerektirir.

# Beklenen sonuÃ§lar

UmarÄ±m tamamlandÄ±ÄŸÄ±nda, bu model modern kavramlarÄ± bilmeyecek ve eÄŸitildiÄŸi bilgilerin Ã¶tesinde akÄ±l yÃ¼rÃ¼temeyecek. Modern kavramlarÄ±/kelime daÄŸarcÄ±ÄŸÄ±nÄ± tanÄ±mamalÄ± ve umarÄ±m modern bilgi uydurmaz.

# Ä°lerleme GÃ¼ncellemeleri

## 9 Temmuz 2025

Zaman aralÄ±ÄŸÄ±mÄ± 1800-1850 ve bÃ¶lge olarak Londra olarak belirledim

Bir metin, kitap, belge listesi topladÄ±m

Åu ana kadar 50 tanesini txt dosyasÄ± olarak aldÄ±m ve yakÄ±nda NanoGPT eÄŸitimine baÅŸlayacaÄŸÄ±m

Ä°lerleme kaydedildikÃ§e burayÄ± gÃ¼ncelleyeceÄŸim

## 13 Temmuz 2025

nanoGPT'yi 187MB'lÄ±k tarihsel metin verisiyle eÄŸittim.

## 15 Temmuz 2025

Ä°kinci eÄŸitim Ã§alÄ±ÅŸmasÄ± iÃ§in metinleri indirmeye baÅŸladÄ±m. Her ÅŸeyi Internet Archive Ã¼zerinden alÄ±yorum ve zaman aralÄ±ÄŸÄ±nÄ± 1800-1875 olarak geniÅŸlettim. Ã‡eÅŸitli metinler elde etmek iÃ§in Internet Archive'da konu ve arama filtrelerini, yayÄ±n yeri, zaman dilimi ve konularÄ± kullanabilirsiniz.

![Arama Filtreleri](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## 16 Temmuz 2025

Internet Archive'dan yaklaÅŸÄ±k 500 txt dosyasÄ± indirdim ve bunlarÄ± temizledikten sonra (sadece boÅŸluklarÄ±, Gutenberg baÅŸlÄ±klarÄ±nÄ± silmek gibi) yaklaÅŸÄ±k 500MB veri elde ettim. KÃ¼Ã§Ã¼k bir veri kÃ¼mesi ama geÃ§en sefer 187MB ile eÄŸitmiÅŸtim, bu yÃ¼zden ikinci modeli eÄŸittikten sonra Ã§Ä±ktÄ±da en azÄ±ndan fark edilir bir deÄŸiÅŸiklik olmalÄ±. UmarÄ±m bu model en azÄ±ndan anlamlÄ± cÃ¼mleler Ã¼retebilir. Tabii ki bu bir garanti deÄŸil Ã§Ã¼nkÃ¼ bu hala Ã§ok kÃ¼Ã§Ã¼k bir veri kÃ¼mesi, ama geÃ§en seferkinden daha fazla. 

Bu kendi donanÄ±mÄ±mda yapÄ±labilir, bu da iyi Ã§Ã¼nkÃ¼ daha bÃ¼yÃ¼k bir veri kÃ¼mesine geÃ§meden Ã¶nce bazÄ± iyileÅŸmeleri gÃ¶rebilirim. Ama endiÅŸelenmeyin, yakÄ±nda GPU kiralamayÄ± da planlÄ±yorum, fakat Ã¶nce veri kÃ¼memin olabildiÄŸince seÃ§ilmiÅŸ ve temiz olduÄŸundan emin olmak istiyorum. KarÅŸÄ±laÅŸtÄ±ÄŸÄ±m sorunlardan biri temizlik, bu txt dosyalarÄ±nÄ±n Ã§oÄŸunda anlamsÄ±z karÄ±ÅŸÄ±k metinler var. Temizlik iÃ§in kullandÄ±ÄŸÄ±m scriptler iÅŸe yarÄ±yor ama %100 etkili deÄŸiller.

Bu veri kÃ¼mesini bugÃ¼n eÄŸiteceÄŸim ve yaklaÅŸÄ±k 4-5 saat sÃ¼recek. BittiÄŸinde ve test ettiÄŸimde gÃ¼ncellemeler vereceÄŸim. Projeme bakan herkese tekrar teÅŸekkÃ¼r ederim, bana OCR kaynaklarÄ± iÃ§in baÄŸlantÄ± gÃ¶nderenler bile oldu, Ã§ok teÅŸekkÃ¼rler! UmarÄ±m daha fazla kiÅŸi kendi veri kÃ¼meleriyle bunu dener ve deneme yapar.


### EÄŸitim GÃ¼ncellemesi

435MB (108 M token) bir korpus ile eÄŸitime baÅŸladÄ±m, ÅŸu an gayet sorunsuz gidiyor. EÄŸitim kaybÄ± ilk 2800 iterasyonda 10.9'dan 4.9'a dÃ¼ÅŸtÃ¼. TamamlanmasÄ±nÄ±n 8-9 saat sÃ¼receÄŸini tahmin ediyorum. BittiÄŸinde bir gÃ¼ncelleme daha paylaÅŸacaÄŸÄ±m.

## 17 Temmuz 2025

Ä°kinci modelin eÄŸitimi tamamlandÄ±, 4060'Ä±mda 33.000 iterasyon (5 epoch) iÃ§in yaklaÅŸÄ±k 8 saat 40 dakika (saatte 3.900 iterasyon) sÃ¼rdÃ¼. Son eÄŸitim kaybÄ± 3.73 oldu. Ã‡Ä±ktÄ±lar ÅŸaÅŸÄ±rtÄ±cÄ± derecede iyiydi, gerÃ§ekten 19. yÃ¼zyÄ±l tarzÄ±nda anlamlÄ± cÃ¼mleler Ã¼retebiliyor artÄ±k.

## 28 Temmuz 2025

v0.5'i Hugging Face'e yÃ¼kledim, [Buradan bakabilirsiniz](https://huggingface.co/haykgrigorian/TimeCapsuleLLM) isterseniz. ArtÄ±k depomu indirip yerelde Ã§alÄ±ÅŸtÄ±rabilirsiniz. Maalesef nanoGPT HuggingFace ile yerel olarak Ã§alÄ±ÅŸmÄ±yor, bu yÃ¼zden modeli indirip yerelde Ã§alÄ±ÅŸtÄ±rmanÄ±z gerekecek.

AyrÄ±ca bir sonraki eÄŸitim iÃ§in veri seÃ§meye baÅŸlayacaÄŸÄ±m, akÄ±l yÃ¼rÃ¼tme yetenekleri iÃ§in muhtemelen 5-10 kat daha fazla veriye ihtiyacÄ±m olacaÄŸÄ±nÄ± dÃ¼ÅŸÃ¼nÃ¼yorum.

## 2 AÄŸustos 2025

YakÄ±nda SÃ¼rÃ¼m 1 Ã¼zerinde Ã§alÄ±ÅŸmaya baÅŸlayacaÄŸÄ±m. nanoGPT'nin mimarisinden daha modern bir ÅŸeye geÃ§mem gerekecek. AklÄ±mda birkaÃ§ aÃ§Ä±k kaynaklÄ± LLM mimarisi var: OpenLLaMA v3, Phi-2 ve Qwen 1.5B dahil. V1'e geÃ§iÅŸi desteklemek iÃ§in Ã§ok daha bÃ¼yÃ¼k ve Ã§eÅŸitli bir veri kÃ¼mesini Ã¶zenle seÃ§mem gerekecek. En az 5GB temiz eÄŸitim verisine ihtiyacÄ±m olacak.

# V0 Model DavranÄ±ÅŸÄ± & SÄ±nÄ±rlamalarÄ±

Ä°lk istemlerde modelin 1800'lÃ¼ yÄ±llarÄ±n dili ve davranÄ±ÅŸÄ±yla yanÄ±t verdiÄŸi gÃ¶rÃ¼lÃ¼yor. Ã–rneÄŸin, "Who art Henry?" diye sordum ve model "I know that man, I have did not a black, the storm." ÅŸeklinde yanÄ±tladÄ±, evet bu cÃ¼mlenin bir anlamÄ± yok ama LLM bir kiÅŸiden bahsettiÄŸimi anlÄ±yor.

![TimeLockLLM Ã–rnek Ã‡Ä±ktÄ±](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

Modern kavramlardan hiÃ§ bahsedilmiyor, Ã§Ä±ktÄ±lar Ã§oÄŸunlukla 1800'lÃ¼ yÄ±llara ait kelime ve ifadelerden oluÅŸuyor.

Hala Ã§ok fazla Ã§alÄ±ÅŸma gerekiyor, 187MB ile eÄŸitmek size karmaÅŸÄ±k muhakeme yeteneÄŸine sahip bir model sunmaz.

Åu anda Ã¼rettiÄŸi cÃ¼mleler tam cÃ¼mle yapÄ±sÄ±ndan yoksun ve genel olarak anlam ifade etmiyor ama bu eÄŸitim boyutu iÃ§in normal.

# V0.5 Model DavranÄ±ÅŸÄ± & SÄ±nÄ±rlamalarÄ±

Bu, Ã¶nceki modele gÃ¶re gÃ¼zel bir geliÅŸme. YazÄ± stili ve kelime daÄŸarcÄ±ÄŸÄ± Viktorya dÃ¶nemi Ä°ngilizcesi, neredeyse her cÃ¼mle dilbilgisi aÃ§Ä±sÄ±ndan doÄŸru ve uygun noktalama iÃ§eriyor. Ve yine, bu model sÄ±fÄ±rdan eÄŸitildiÄŸi iÃ§in 1800'lÃ¼ yÄ±llarÄ±n konularÄ±na sadÄ±k kalÄ±yor.

![TimeLockLLM Ã–rnek Ã‡Ä±ktÄ±](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

Ã‡ok fazla olgusal halÃ¼sinasyon var. DetaylarÄ±n (tarih, olaylar, tarihi ÅŸahÄ±slar) neredeyse tamamÄ± (yaklaÅŸÄ±k %100) uydurma. AyrÄ±ca cÃ¼mleler birbirleriyle pek baÄŸlantÄ±lÄ± deÄŸil, bazen belki 2 cÃ¼mle birbiriyle iliÅŸkili oluyor ama bundan fazlasÄ± yok. DiÄŸer bir sorun ise bazen "Digitized by Google" altbilgisi araya giriyor, bu yÃ¼zden bir dahaki eÄŸitime baÅŸlamadan Ã¶nce metinlerin iyice temizlendiÄŸinden emin olmalÄ±yÄ±m. Genel olarak sonuÃ§lardan Ã§ok memnunum, henÃ¼z tam bir LLM deÄŸil ama kesinlikle bir cÃ¼mle Ã¼reticisi.

Ã‡ok ÅŸey Ã¶ÄŸreniyorum ve Ã¶nÃ¼mÃ¼zdeki haftalarda daha iyi neler yapmam gerektiÄŸini Ã§Ã¶zmeye baÅŸlayacaÄŸÄ±m. DosyalarÄ± yakÄ±nda yÃ¼kleyeceÄŸim!

# YaklaÅŸan Planlar

(TamamlandÄ±) 0.5 sÃ¼rÃ¼mÃ¼ne baÅŸlÄ±yorum, 50 kitap yerine ideal olarak 500-600 kitapla eÄŸitim yapacaÄŸÄ±m. Åu anda nanoGPT'yi 1800-1850 yÄ±llarÄ± arasÄ±ndaki Londra kitaplarÄ±yla eÄŸitiyorum. BazÄ± zorluklar var; bulduÄŸum kitaplarÄ±n gÃ¼ncellenmemiÅŸ veya modern yorumlar iÃ§ermediÄŸinden emin olmalÄ±yÄ±m, seÃ§tiÄŸim dÃ¶nemde yayÄ±mlanmÄ±ÅŸ, dokunulmamÄ±ÅŸ kitaplar olmalÄ±.

Daha bÃ¼yÃ¼k bir veri kÃ¼mesiyle (v1) yeni bir model eÄŸitmek istiyorum, belki v0.5 iÃ§in kullandÄ±ÄŸÄ±mÄ±n 5-10 katÄ± bÃ¼yÃ¼klÃ¼ÄŸÃ¼nde. AmacÄ±m, yalnÄ±zca SeÃ§ici Zamansal EÄŸitim ile muhakeme yeteneklerinin ortaya Ã§Ä±kÄ±p Ã§Ä±kmayacaÄŸÄ±nÄ± gÃ¶rmek; bu daha zor bir gÃ¶rev olacak ve tarihi veri kÄ±sÄ±tlamalarÄ± nedeniyle mÃ¼mkÃ¼n olup olmadÄ±ÄŸÄ±ndan tamamen emin deÄŸilim. Ã–nÃ¼mÃ¼zdeki haftalarda 5-10 GB'lÄ±k bir veri kÃ¼mesi oluÅŸturacak kadar veri toplamaya Ã§alÄ±ÅŸacaÄŸÄ±m. Temiz, yÃ¼ksek kaliteli veri toplayabilir ve bir GPU kiralayabilirsem ilerleme olacaÄŸÄ±na inanÄ±yorum.

# Bu Proje NasÄ±l KullanÄ±lÄ±r

Bu proje daha Ã§ok tarihsel verilerin toplanmasÄ±, eÄŸitim iÃ§in hazÄ±rlanmasÄ± ve bir tokenizer oluÅŸturulmasÄ± Ã¼zerine odaklanÄ±yor. TÃ¼m LLM eÄŸitim sÃ¼recini anlatmayacaÄŸÄ±m, bunun iÃ§in Andrej Karpathy'nin nanoGPT'sine bakabilirsiniz.

# AdÄ±m 1: Tarihsel Metinleri Topla ve HazÄ±rla

SeÃ§tiÄŸiniz dÃ¶neme ait (Ã¶rn. Londra 1800-1850) kamu malÄ± kitap, belge vb. .txt dosyalarÄ±nÄ± toplayÄ±n.

Ä°htiyacÄ±nÄ±z olursa kitaplarÄ± sizin iÃ§in indirmek iÃ§in download_texts_improved.py dosyasÄ±nÄ± kullanabilirsiniz.

Metin dosyalarÄ±nÄ± bir betik ile ya da manuel olarak, Project Gutenberg baÅŸlÄ±k/dipnotlarÄ±, modern aÃ§Ä±klamalar ya da OCR hatalarÄ± gibi ÅŸeyleri temizleyin.

prepare_dataset.py sorunsuz Ã§alÄ±ÅŸmalÄ±.

# AdÄ±m 2: Ã–zel Tokenizer OluÅŸtur

train_tokenizer.py veya train_tokenizer_hf.pyâ€™yi temizlenmiÅŸ veri Ã¼zerinde Ã§alÄ±ÅŸtÄ±rÄ±n.
Bu size vocab.json ve merges.txt dosyalarÄ±nÄ± verecek

Bu dosyalar modeliniz iÃ§in kelime daÄŸarcÄ±ÄŸÄ± ve birleÅŸtirme kurallarÄ±nÄ± tanÄ±mlar

# AdÄ±m 3: Modelinizi EÄŸitin (nanoGPT)

EÄŸitim sÃ¼reci iÃ§in [Andrej Karpathy'nin nanoGPT'sine](https://github.com/karpathy/nanoGPT) bakÄ±n.

Ä°sterseniz farklÄ± bir LLM de eÄŸitebilirsiniz ama ben nanoGPT kullandÄ±m

# SSS

## SeÃ§ici Zamansal EÄŸitim (STT) nedir?

SeÃ§ici Zamansal EÄŸitim (STT), tÃ¼m eÄŸitim verilerinin Ã¶zel olarak belirli bir tarihsel dÃ¶neme ait olacak ÅŸekilde seÃ§ildiÄŸi bir makine Ã¶ÄŸrenimi metodolojisidir. Bu, dÃ¶nemin dilini ve bilgisini modern kavramlardan etkilenmeden modellemek iÃ§in yapÄ±lÄ±r. Ã–rneÄŸin, ÅŸu anki modelim (v0.5) sadece 1800-1875 arasÄ±ndaki verilere dayalÄ± olarak eÄŸitildi; ince ayar yapÄ±lmadÄ±, sÄ±fÄ±rdan eÄŸitildi ve Ã§Ä±ktÄ±lar bu dÃ¶nemin dilsel tarzÄ±nÄ± ve tarihsel baÄŸlamÄ±nÄ± yansÄ±tÄ±yor.

## Neden sadece ince ayar veya LoRA kullanmÄ±yorsun?

Bu projede modern Ã¶nyargÄ±dan arÄ±nmÄ±ÅŸ bir dil modeli oluÅŸturmayÄ± hedefliyorum. GPT-2 gibi bir modeli ince ayar yapsam bile, model zaten Ã¶nceden eÄŸitilmiÅŸ olur ve bu bilgi yok olmaz. SÄ±fÄ±rdan eÄŸitirsem, dil modeli eski gibi davranmaz, gerÃ§ekten eski olur. Åu anki hedefim, 1800-1850 arasÄ±nda Londra'da yayÄ±mlanmÄ±ÅŸ kitaplardan alÄ±nan bilgilerle sadece bu bilgilerle akÄ±l yÃ¼rÃ¼tebilen bir ÅŸey oluÅŸturmak.

## EÄŸitim iÃ§in ne tÃ¼r veri kullandÄ±nÄ±z?

Kitaplar, hukuki belgeler, gazeteler ve diÄŸer 1800-1850 Londra yazÄ±lÄ± metinlerini kullanÄ±yorum. BaÄŸlantÄ±sÄ±nÄ± verdiÄŸim listede yaklaÅŸÄ±k 200 belge var ama ilk eÄŸitim iÃ§in sadece 50 dosya (~187 MB) kullandÄ±m. Belge listesini gÃ¶rebilirsiniz:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## SÃ¼rÃ¼m 0 modeli ne kadar bÃ¼yÃ¼k?

Bu model ÅŸu an Ã§ok kÃ¼Ã§Ã¼k, bunu eÄŸlence amaÃ§lÄ± yapÄ±yorum ve kesinlikle modern kaynak kullanmama kuralÄ±na baÄŸlÄ±yÄ±m. YaklaÅŸÄ±k 16 milyon parametresi var ama daha fazla eski metin toplayÄ±p yeni bir model eÄŸitmeye baÅŸlayacaÄŸÄ±m. GeliÅŸmeleri paylaÅŸacaÄŸÄ±m.

## EÄŸitim Ã–zellikleri?

GPU: Geforce rtx 4060
CPU: i5-13400F
Ram: 16GB DDR5.



---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-08-07

---