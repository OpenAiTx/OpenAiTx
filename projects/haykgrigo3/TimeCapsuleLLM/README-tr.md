<div align="right">
  <details>
    <summary >ğŸŒ Dil</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ç®€ä½“ä¸­æ–‡</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-TW">ç¹é«”ä¸­æ–‡</a> 
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">æ—¥æœ¬èª</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">í•œêµ­ì–´</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=as">à¤¹à¤¿à¤¨à¥à¤¦à¥€</a> 
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=th">à¹„à¸—à¸¢</a> 
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=fr">FranÃ§ais</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=es">EspaÃ±ol</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=it">Italiano</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=ru">Ğ ÑƒÑÑĞºĞ¸Ğ¹</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=pt">PortuguÃªs</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=ar">Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©</a>
        | <a href="#" title="YakÄ±nda">ÙØ§Ø±Ø³ÛŒ (yakÄ±nda)</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=tr">TÃ¼rkÃ§e</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=vi">Tiáº¿ng Viá»‡t</a>
        | <a href="#" title="YakÄ±nda">Bahasa Indonesia (yakÄ±nda)</a>

      </div>
    </div>
  </details>
</div>


# TimeCapsule LLM

*Sadece belirli yerlerin ve zaman dÃ¶nemlerinin verileriyle **sÄ±fÄ±rdan** eÄŸitilmiÅŸ bir dil modeli; modern Ã¶nyargÄ±yÄ± azaltmak ve dÃ¶nemin sesi, kelime daÄŸarcÄ±ÄŸÄ± ve dÃ¼nya gÃ¶rÃ¼ÅŸÃ¼nÃ¼ taklit etmek iÃ§in tasarlanmÄ±ÅŸtÄ±r.*

Hayal edin ki bir yapay zekÃ¢ modeli tarihi taklit etmiyor, gerÃ§ekten tarihi olmuÅŸ gibi davranÄ±yor.

v0 ve v0.5 [Andrej Karpathy'nin nanoGPT'si](https://github.com/karpathy/nanoGPT) Ã¼zerinde inÅŸa edilmiÅŸtir. Temel eÄŸitim komut dosyalarÄ± ve model mimarisi ona aittir.

v1 [Microsoft tarafÄ±ndan geliÅŸtirilen Phi 1.5](https://huggingface.co/microsoft/phi-1_5) Ã¼zerine inÅŸa edilmiÅŸtir

[Hugging Face Linki](https://huggingface.co/haykgrigorian/TimeCapsuleLLM)


##  Model DavranÄ±ÅŸÄ± & SÄ±nÄ±rlamalarÄ±

### **v0**  

Ä°lk istemlerde model, 1800'lerin dili ve davranÄ±ÅŸÄ±yla yanÄ±t vermektedir. 
Ã–rnek: Ä°stem: "Who art Henry?" ve yanÄ±tÄ± "I know that man, I have did not a black, the storm." 

![TimeLockLLM Ã–rnek Ã‡Ä±ktÄ±sÄ±](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

- Modern kavramlara hiÃ§ deÄŸinilmemiÅŸ  
- Ã‡oÄŸunlukla dÃ¶neme uygun kelime daÄŸarcÄ±ÄŸÄ±  
- CÃ¼mleler Ã§oÄŸunlukla tutarsÄ±z (yaklaÅŸÄ±k 187MB eÄŸitim verisi iÃ§in beklenen bir durum)

### **v0.5** 

v0'a gÃ¶re Ã¶nemli bir geliÅŸme.  
- Viktoryen yazÄ± stili, doÄŸru noktalama, Ã§oÄŸunlukla dilbilgisel olarak doÄŸru cÃ¼mleler  
- HÃ¢lÃ¢ yÃ¼ksek oranda gerÃ§ek dÄ±ÅŸÄ± Ã¼retim  
- Ã‡Ä±ktÄ±larda hÃ¢lÃ¢ OCR gÃ¼rÃ¼ltÃ¼sÃ¼ (â€œDigitized by Googleâ€) mevcut

![TimeLockLLM Ã–rnek Ã‡Ä±ktÄ±sÄ±](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

### **v1**

Veri kÃ¼mesinden gerÃ§ek bir tarihi olayÄ± ve bir figÃ¼rÃ¼ ilk kez hatÄ±rlayan ve baÄŸdaÅŸtÄ±ran model.

Ã–rnek: Ä°stem: "It was the year of our Lord 1834" 

Ã‡Ä±ktÄ±: "It was the year of our Lord 1834 and the streets of London were filled with protest and petition. The cause, as many re counted, was not bound in the way of private, but having taken up the same day in the day of Lord Palmerston, the public will receive a short statement of the difficulties under which the day of law has reached us. It is a matter of deep regret, that the present events in the history of the world are clear, and consequently will be'known. It is not true that the very men who first settled in the Gospel at Jerusalem should have so extensive and so interesting a record of the prosperity and prosperity" 

BaÅŸta protestonun tesadÃ¼fen aynÄ± yÄ±lda gerÃ§ekleÅŸmiÅŸ olabileceÄŸini varsaydÄ±m ancak buna bir bakÄ±n: ![1834protest](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)

### **v2mini-eval1**

v2'nin 90GB'lÄ±k veri kÃ¼mesinden 15GB'lÄ±k bir Ã¶rnekle eÄŸitildi.
Model yalnÄ±zca 10K adÄ±mda eÄŸitildi.

Tokenizasyonla ilgili bir sorun Ã§Ä±ktÄ± ve bu da Ã§Ä±ktÄ±nÄ±n ÅŸÃ¶yle gÃ¶rÃ¼nmesine neden oldu:

PROMPT:
Charles Dickens kimdir?


W ho is Charles D ic ens ? D oes that work more of h ise x cell ent st ir ring , in his pl ays , int he G reat C omp any 's f arm ? What I have y et to qu ote from J ack P ick ett ? D oy oun ot know th att hey were as pe cies of g all oping , ors aw ing oft heir bre asts , or what w ast ob ed one about the t im ew hen J acks on was looking ont hew ind ow ? What ist hes uccess of an English man , and which h isson has not be end is cover ing t ome , where by to acc omp lish such a we ight ? D idy ou ever m ake ap ass age int othe old road way , ort o an anch or - b ree ze att he foot of our boat , which you m ust le ave us ? The fact is , that whe ther the wind would rise up from the pl ain on S atur day night orn o ont ide , ort ill then orth , or other wise , wes hall be com pelled t od esc ribe af orm idable bar rier , with t hes ame e ffect s ast he p resent . In th iss itu ation , at le ast , itis not to omuch t os ay that we have le ft that room . I bel ieve there are three c op ies int he " F ive H undred - fold ," to bere fer red to , ast he first n umber of our read ers wh ow is ht
DÃ¼zeltilmiÅŸ:

"Charles Dickens kimdir? Onun mÃ¼kemmel heyecan verici eserlerinden, oyunlarÄ±nda, BÃ¼yÃ¼k Åirketâ€™in Ã§iftliÄŸinde daha fazlasÄ± var mÄ±? Jack Pickettâ€™ten alÄ±ntÄ± yapmam gereken daha bir ÅŸey var mÄ±? OnlarÄ±n bir tÃ¼r dÃ¶rtnala gitme, ya da gÃ¶ÄŸÃ¼slerini kesme, ya da Jackson pencereye bakarken ne yapÄ±lmasÄ± gerektiÄŸini bilmiyor musun? Bir Ä°ngilizâ€™in baÅŸarÄ±sÄ± nedir ve oÄŸlu bana henÃ¼z keÅŸfetmediÄŸi, bÃ¶yle bir yÃ¼kÃ¼ baÅŸarmak iÃ§in ne yaptÄ±ÄŸÄ± nedir? HiÃ§ eski yola geÃ§iÅŸ yaptÄ±n mÄ±, ya da teknemizin ayaÄŸÄ±nda bir demir meltemine, ki bizi bÄ±rakmak zorundasÄ±n? GerÃ§ek ÅŸu ki, rÃ¼zgar Cumartesi gecesi ya da Ã¶ÄŸle vakti, ya da kuzeye kadar, ya da baÅŸka tÃ¼rlÃ¼ dÃ¼zlÃ¼kten kalksa da, mevcutla aynÄ± etkilere sahip, korkunÃ§ bir engeli tanÄ±mlamak zorunda kalacaÄŸÄ±z. Bu durumda, en azÄ±ndan, o odadan ayrÄ±ldÄ±ÄŸÄ±mÄ±zÄ± sÃ¶ylemek abartÄ± olmaz. Ä°nanÄ±yorum ki, â€˜BeÅŸ YÃ¼z Katâ€™ta Ã¼Ã§ kopya var, baÅŸvurulacak, okuyucularÄ±mÄ±zÄ±n ilk sayÄ±sÄ± olarak...â€

### **v2mini-eval2**

Eval1 iÃ§in kullanÄ±lan aynÄ± 15GB Ã¶rnekle eÄŸitildi.
Model yine sadece 10K adÄ±mda eÄŸitildi.

PROMPT:Charles Darwin

Charles DarwinECCEMACY. Efendim, â€” AÅŸaÄŸÄ±daki vaka benim iÃ§in ilginÃ§tir: â€” Londraâ€™da iki hafta kaldÄ±m ve romatizma ataÄŸÄ±yla Ã§ok etkilendim. Ä°lk romatizma ataÄŸÄ± sizi gÃ¶rmeden bir hafta Ã¶nceydi, ikincisi sizi gÃ¶rdÃ¼ÄŸÃ¼mde, Ã¼Ã§Ã¼ncÃ¼sÃ¼ de sizi gÃ¶rdÃ¼ÄŸÃ¼mde ve yine aynÄ± sÃ¼rede. Ancak ikinci gut ataÄŸÄ± herhangi bir ateÅŸli belirtiyle birlikte olmadÄ±, ama idrarda artÄ±ÅŸ ve daha bol idrar atÄ±mÄ± ile birlikteydi. ÃœÃ§Ã¼ncÃ¼ atak sizi gÃ¶rdÃ¼kten bir saat sonra oldu ve bir gut nÃ¶betinin geri dÃ¶nÃ¼ÅŸÃ¼ ve daha hÄ±zlÄ± bir gut tekrarÄ± ile sonuÃ§landÄ±. DÃ¶rdÃ¼ncÃ¼ atak da bir ateÅŸle birlikteydi, ama her zaman ateÅŸli belirtilerle birlikte deÄŸildi. ÃœÃ§Ã¼ncÃ¼ gut ataÄŸÄ± hastalandÄ±ktan iki hafta sonra oldu ve dÃ¶rdÃ¼ncÃ¼sÃ¼ de bir gut nÃ¶betiyle takip edildi. DÃ¶rdÃ¼ncÃ¼ atak, saldÄ±rÄ±ya uÄŸradÄ±ktan iki hafta sonra oldu ve bir hisle birlikteydi.


##  Veri KÃ¼meleri

### **v2**

- 90GBâ€™lÄ±k 1800-1875 Londra metinleri
- 136.344 belge
- Tam 90GB henÃ¼z kullanÄ±labilir deÄŸil Ã§Ã¼nkÃ¼ henÃ¼z tokenize edilmedi, ama 15GBâ€™lÄ±k bir Ã¶rneÄŸe buradan ulaÅŸabilirsiniz: https://huggingface.co/datasets/haykgrigorian/TimeCapsuleLLM-London-1800-1875-v2-15GB

 ### Ã–nyargÄ± Ä°statistikleri 
  ![Zamir Ã¶nyargÄ±sÄ±](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v2mini_eval1/pronoun_bias.png)

  ![CoÄŸrafi Ã¶nyargÄ±](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v2mini_eval1/geographic_bias.png)

  ![Zamansal Ã¶nyargÄ±](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v2mini_eval1/temporal_bias.png)

Daha fazla bilgi iÃ§in [v2 Ã¶nyargÄ± raporuna](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v2mini_eval1/v2_bias_report.json) bakÄ±nÄ±z.


## NasÄ±l KullanÄ±lÄ±r

Bu proje Ã§oÄŸunlukla tarihsel verilerin derlenmesi, eÄŸitime hazÄ±rlanmasÄ± ve bir tokenizer oluÅŸturma Ã¼zerine odaklanmaktadÄ±r. Tam LLM eÄŸitim sÃ¼reci burada ele alÄ±nmamÄ±ÅŸtÄ±r; bunun iÃ§in Andrej Karpathyâ€™nin nanoGPTâ€™sine bakÄ±nÄ±z.

### AdÄ±m 1: Tarihsel Metinleri Topla ve HazÄ±rla 

- SeÃ§tiÄŸiniz dÃ¶nemden (Ã¶r. Londra 1800-1850) kamu malÄ± kitaplarÄ±n, belgelerin vs. .txt dosyalarÄ±nÄ± toplayÄ±n

- OnlarÄ± seÃ§tiÄŸiniz zaman/mekan penceresi iÃ§inde tutun  
- Metin dosyalarÄ±nÄ± bir betik kullanarak veya elle Project Gutenberg baÅŸlÄ±klarÄ±/dipnotlarÄ±, modern aÃ§Ä±klamalar veya OCR hatalarÄ± gibi ÅŸeyleri kaldÄ±rarak temizleyin.

### AdÄ±m 2: Ã–zel Bir Tokenizer OluÅŸturun

- train_tokenizer.py veya train_tokenizer_hf.py dosyasÄ±nÄ± temizlenmiÅŸ veri Ã¼zerinde Ã§alÄ±ÅŸtÄ±rÄ±n.
- Bu iÅŸlem size vocab.json ve merges.txt dosyalarÄ±nÄ± verecek.
- Bu dosyalar modeliniz iÃ§in kelime hazinesi ve birleÅŸtirme kurallarÄ±nÄ± tanÄ±mlar.

### AdÄ±m 3: Modelinizi EÄŸitin

- EÄŸitim sÃ¼reci iÃ§in [Andrej Karpathy'nin nanoGPT'sine](https://github.com/karpathy/nanoGPT) veya seÃ§tiÄŸiniz mimarinin belgelerine bakÄ±n.

# SSS

## SeÃ§ici Zamansal EÄŸitim (STT) nedir?

SeÃ§ici Zamansal EÄŸitim (STT), tÃ¼m eÄŸitim verisinin belirli bir tarihsel zaman dilimi iÃ§inde Ã¶zel olarak seÃ§ildiÄŸi bir makine Ã¶ÄŸrenimi metodolojisidir. Bu, dÃ¶nemin dilini ve bilgisini modern kavramlardan etkilenmeden modellemek iÃ§in yapÄ±lÄ±r. Ã–rneÄŸin, ÅŸu anda sahip olduÄŸum model (v0.5) yalnÄ±zca 1800-1875 yÄ±llarÄ± arasÄ±ndaki verilerle eÄŸitildi, ince ayar yapÄ±lmadÄ±, sÄ±fÄ±rdan eÄŸitildi ve Ã§Ä±ktÄ±larÄ± o dÃ¶nemin dilsel tarzÄ± ve tarihsel baÄŸlamÄ±nÄ± yansÄ±tmaktadÄ±r.

## Neden doÄŸrudan ince ayar veya LoRA kullanmÄ±yorsunuz?

Bu projede amacÄ±m modern Ã¶nyargÄ±dan arÄ±nmÄ±ÅŸ bir dil modeli oluÅŸturmak. GPT-2 gibi bir modeli ince ayar yaparsam, zaten Ã¶nceden eÄŸitilmiÅŸ ve bu bilgi kaybolmaz. SÄ±fÄ±rdan eÄŸitirsem dil modeli eskiyi taklit etmek yerine gerÃ§ekten Ã¶yle olur. Åu anda bu projenin amacÄ±, yalnÄ±zca 1800-1875 yÄ±llarÄ± arasÄ±nda Londra'da yayÄ±mlanmÄ±ÅŸ kitaplardan alÄ±nan bilgilerle mantÄ±k yÃ¼rÃ¼tebilen bir model oluÅŸturmaktÄ±r.

## EÄŸitim iÃ§in ne tÃ¼r veriler kullandÄ±nÄ±z?

Kitaplar, yasal belgeler, gazeteler ve 1800â€“1875 Londra'sÄ±ndan diÄŸer yazÄ±lÄ± eserleri kullanÄ±yorum. BaÄŸlantÄ±sÄ±nÄ± verdiÄŸim listede (v0 iÃ§in) yaklaÅŸÄ±k 200 belge var, ancak ilk eÄŸitim iÃ§in sadece 50 dosya (~187 MB) kullandÄ±m. Belgelerin bir listesini gÃ¶rÃ¼ntÃ¼leyebilirsiniz:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

Veri seti boyutlarÄ±:
- v0: ~187MB
- v0.5: ~435MB 
- v1: ~6.25GB 
- v2mini-eval1: 15GB

## Modeller ne kadar bÃ¼yÃ¼k?

v0: 16M Parametre

v0.5 123M Parametre

v1: 700M Parameters

v2mini-eval1: 300M Parameters

# Training Specs ? 

# v0/v0.5
GPU: Geforce rtx 4060
CPU: i5-13400F 
Ram: 16GB DDR5.

# v1
GPU: A100 SXM rented

# v2mini-eval1

GPU: A100 SXM rented





























---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2026-01-09

---