
<div align="right">
  <details>
    <summary >ğŸŒ Dil</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ç®€ä½“ä¸­æ–‡</a>
        | <a href="#" title="YakÄ±nda">ç¹é«”ä¸­æ–‡ (yakÄ±nda)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">æ—¥æœ¬èª</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">í•œêµ­ì–´</a>
        | <a href="#" title="YakÄ±nda">à¤¹à¤¿à¤¨à¥à¤¦à¥€ (yakÄ±nda)</a> |
        | <a href="#" title="YakÄ±nda">à¹„à¸—à¸¢ (yakÄ±nda)</a> |
        | <a href="#" title="YakÄ±nda">FranÃ§ais (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">Deutsch (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">EspaÃ±ol (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">Italiano (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">Ğ ÑƒÑÑĞºĞ¸Ğ¹ (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">PortuguÃªs (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">Nederlands (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">Polski (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">ÙØ§Ø±Ø³ÛŒ (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">TÃ¼rkÃ§e (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">Tiáº¿ng Viá»‡t (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">Bahasa Indonesia (yakÄ±nda)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM
Belirli zaman dilimlerinden alÄ±nan verilerle eÄŸitilen ve modern Ã¶nyargÄ±yÄ± azaltan bir LLM.

Bir yapay zeka modelinin tarihi sadece taklit etmekle kalmayÄ±p gerÃ§ekten o dÃ¶neme ait olduÄŸunu hayal edin.

[Andrej Karpathy'nin nanoGPT'si](https://github.com/karpathy/nanoGPT) Ã¼zerine inÅŸa edilmiÅŸtir. Temel eÄŸitim betikleri ve model mimarisi ona aittir.

# Proje Hedefleri

TimeCapsule LLM, yalnÄ±zca belirli zaman dilimlerinde yazÄ±lmÄ±ÅŸ metinler Ã¼zerinde eÄŸitilecek deneysel bir projedir. AmaÃ§, belirli tarihsel dÃ¶nemlerin dÃ¼nya gÃ¶rÃ¼ÅŸÃ¼nÃ¼ ve dilini simÃ¼le etmektir.

# Neden ince ayar yeterli deÄŸil 

EÄŸer sadece Ã¶nceden eÄŸitilmiÅŸ bir modeli ince ayar ile geliÅŸtirirseniz, LLM'iniz yine de modern kavramlarÄ± bilecektir. Elbette sÄ±fÄ±r modern Ã¶nyargÄ± elde etmek zordur fakat buna olabildiÄŸince yaklaÅŸmak istiyorum. HiÃ§ modern Ã¶nyargÄ± olmamasÄ±, bir modelin sÄ±fÄ±rdan eÄŸitilmesini gerektirir.

# Beklenen sonuÃ§lar 

UmarÄ±m tamamlandÄ±ÄŸÄ±nda, bu model modern kavramlarÄ± bilmeyecek ve eÄŸitim aldÄ±ÄŸÄ± bilgilerin Ã¶tesinde akÄ±l yÃ¼rÃ¼temeyecek. Modern kavramlarÄ±/kelime daÄŸarcÄ±ÄŸÄ±nÄ± tanÄ±mamalÄ± ve modern bilgi uydurmamalÄ±.

# Ä°lerleme GÃ¼ncellemeleri

## 9 Temmuz 2025

Zaman aralÄ±ÄŸÄ±mÄ± 1800-1850 olarak ve bÃ¶lgeyi: Londra olarak belirledim

Bir metin, kitap, belge listesi topladÄ±m

Åu ana kadar 50 adet txt dosyasÄ± edindim ve NanoGPT eÄŸitimine yakÄ±nda baÅŸlayacaÄŸÄ±m

Ä°lerleme olduÄŸu sÃ¼rece bunu gÃ¼ncelleyeceÄŸim

## 13 Temmuz 2025

NanoGPT'yi 187MB'lÄ±k tarihsel metin verisiyle eÄŸittim.

## 15 Temmuz 2025

Ä°kinci eÄŸitim turu iÃ§in metinleri indirmeye baÅŸladÄ±m. Her ÅŸeyi Internet Archive'dan alÄ±yorum ve zaman aralÄ±ÄŸÄ±nÄ± 1800-1875'e geniÅŸlettim. FarklÄ± metinler elde etmek iÃ§in Internet Archive'da konu ve arama filtrelerini yayÄ±n yeri, dÃ¶nem ve konuya gÃ¶re kullanabilirsiniz.

![Arama Filtreleri](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## 16 Temmuz 2025

Internet Archive'dan yaklaÅŸÄ±k 500 txt dosyasÄ± indirdim ve bunlarÄ± temizledikten sonra (sadece boÅŸluklarÄ±, Gutenberg baÅŸlÄ±klarÄ±nÄ± vs. sildim) yaklaÅŸÄ±k 500MB veri elde ettim. KÃ¼Ã§Ã¼k bir veri seti ama geÃ§en sefer 187MB ile eÄŸittim, bu nedenle ikinci modeli eÄŸittikten sonra Ã§Ä±ktÄ±da en azÄ±ndan bir tÃ¼r fark gÃ¶rÃ¼lmeli. UmarÄ±m bu model en azÄ±ndan daha anlamlÄ± cÃ¼mleler Ã¼retebilir. Elbette bu bir garanti deÄŸil Ã§Ã¼nkÃ¼ bu hala Ã§ok Ã§ok kÃ¼Ã§Ã¼k bir veri seti, ama geÃ§en sefer kullandÄ±ÄŸÄ±mdan daha fazla veri var.

Bunu kendi donanÄ±mÄ±mda yapmak mÃ¼mkÃ¼n, bu da iyi Ã§Ã¼nkÃ¼ daha bÃ¼yÃ¼k bir veri setine geÃ§meden Ã¶nce bazÄ± iyileÅŸmeler gÃ¶rebilirim, bu durumda bir GPU kiralamam gerekecek. Ama merak etmeyin, yakÄ±nda bir GPU kiralamayÄ± planlÄ±yorum, fakat bunu yapmadan Ã¶nce veri setimin olabildiÄŸince Ã¶zenli ve temiz olmasÄ±nÄ± istiyorum. KarÅŸÄ±laÅŸtÄ±ÄŸÄ±m sorunlardan biri temizlik, bu txt dosyalarÄ±nÄ±n birÃ§oÄŸunda anlamsÄ±z ÅŸeyler karÄ±ÅŸmÄ±ÅŸ durumda. Temizleme iÃ§in kullandÄ±ÄŸÄ±m scriptler iÅŸe yarÄ±yor ama %100 etkili deÄŸiller.

Bu veri setini bugÃ¼n eÄŸiteceÄŸim ve yaklaÅŸÄ±k 4-5 saat sÃ¼recek. BittiÄŸinde ve test ettiÄŸimde gÃ¼ncellemeler vereceÄŸim. Projeme gÃ¶z atan herkese tekrar teÅŸekkÃ¼r ederim, hatta bana OCR kaynaklarÄ± iÃ§in baÄŸlantÄ± gÃ¶nderenler bile oldu, Ã§ok teÅŸekkÃ¼rler! UmarÄ±m daha fazla kiÅŸi kendi veri setleriyle bunu dener ve deneyler yapar.

## 12 AÄŸustos 2025

SÃ¼rÃ¼m 1 tamamlandÄ±, bu sÃ¼rÃ¼m Phi 1.5'i (700M parametre) kullanÄ±yor ve neredeyse 7000 metinden oluÅŸan bir veri kÃ¼mesi Ã¼zerinde eÄŸitildi. Bu modeldeki baÅŸlÄ±ca geliÅŸme, bazen gerÃ§ek isimler ve tarihsel olaylara referans verebilme yeteneÄŸidir. SÃ¼rÃ¼m 0 ve 0.5 bunu yapamÄ±yordu, bu modellere "1821 yÄ±lÄ±nda" gibi bir ÅŸey sorduÄŸumda Ã§Ä±ktÄ±lar sadece halÃ¼sinasyonlar, hiÃ§ olmamÄ±ÅŸ ÅŸeyler oluyordu. SÃ¼rÃ¼m 1 ile ilgili bir Ã¶rnek:

Ä°steÄŸim: Efendimizin yÄ±lÄ± 1834 idi,

Ã‡Ä±ktÄ±: "Efendimizin yÄ±lÄ± 1834 idi ve Londra sokaklarÄ± protesto ve dilekÃ§elerle doluydu. Sebep, birÃ§oklarÄ±nÄ±n anlattÄ±ÄŸÄ± gibi, Ã¶zelde baÄŸlÄ± deÄŸildi, fakat Lord Palmerston'Ä±n gÃ¼nÃ¼nde aynÄ± gÃ¼n alÄ±nmÄ±ÅŸtÄ±, halk hukukun gÃ¼nÃ¼nÃ¼n bize ulaÅŸtÄ±ÄŸÄ± zorluklarla ilgili kÄ±sa bir aÃ§Ä±klama alacak. DÃ¼nyanÄ±n tarihindeki mevcut olaylarÄ±n aÃ§Ä±k olmasÄ± ve dolayÄ±sÄ±yla bilinecek olmasÄ± derin bir Ã¼zÃ¼ntÃ¼ konusudur. KudÃ¼s'te ilk olarak Ä°ncil'e yerleÅŸen adamlarÄ±n, refahÄ±n ve refahÄ±n bu kadar kapsamlÄ± ve ilgi Ã§ekici bir kaydÄ±na sahip olmasÄ± doÄŸru deÄŸildir." 

BaÅŸta bunun bir tesadÃ¼f olabileceÄŸini dÃ¼ÅŸÃ¼ndÃ¼m ama ÅŸuna bir bakÄ±n: ![1834protest](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)


### EÄŸitim GÃ¼ncellemesi

435MB'lÄ±k (108 M token) bir corpus Ã¼zerinde eÄŸitime baÅŸladÄ±m, ÅŸu anda oldukÃ§a sorunsuz gidiyor. EÄŸitim kaybÄ± ilk 2800 iterasyonda 10.9'dan 4.9'a dÃ¼ÅŸtÃ¼. TamamlanmasÄ± yaklaÅŸÄ±k 8 veya 9 saat sÃ¼receÄŸini tahmin ediyorum. BittiÄŸinde baÅŸka bir gÃ¼ncelleme paylaÅŸacaÄŸÄ±m.

## 17 Temmuz 2025

Ä°kinci model iÃ§in eÄŸitim tamamlandÄ±, 4060'Ä±mda yaklaÅŸÄ±k 8 saat 40 dakika sÃ¼rdÃ¼ (3.900 iter/saat) ve 33.000 iterasyon (5 epoch) yapÄ±ldÄ±. Son eÄŸitim kaybÄ± 3.73 oldu. Ã‡Ä±ktÄ±lar ÅŸaÅŸÄ±rtÄ±cÄ± derecede iyiydi, artÄ±k gerÃ§ekten tutarlÄ± 19. yÃ¼zyÄ±l tarzÄ± cÃ¼mleler Ã¼retiyor.

## 28 Temmuz 2025

v0.5'i Hugging Face'e yÃ¼kledim, [Buradan bakabilirsiniz](https://huggingface.co/haykgrigorian/TimeCapsuleLLM) isterseniz. Depomu indirip yerelde Ã§alÄ±ÅŸtÄ±rabilirsiniz. Maalesef nanoGPT HuggingFace ile yerel olarak Ã§alÄ±ÅŸmÄ±yor, bu yÃ¼zden modeli indirip yerelde Ã§alÄ±ÅŸtÄ±rmanÄ±z gerekecek.

AyrÄ±ca bir sonraki eÄŸitim Ã§alÄ±ÅŸmam iÃ§in veri toplamaya baÅŸlayacaÄŸÄ±m, akÄ±l yÃ¼rÃ¼tme yetenekleri elde etmek iÃ§in muhtemelen 5-10 kat daha fazla veriye ihtiyacÄ±m olacaÄŸÄ±nÄ± dÃ¼ÅŸÃ¼nÃ¼yorum.

## 2 AÄŸustos 2025

YakÄ±nda SÃ¼rÃ¼m 1 Ã¼zerinde Ã§alÄ±ÅŸmaya baÅŸlayacaÄŸÄ±m. nanoGPT'nin mimarisinden daha modern bir ÅŸeye geÃ§mem gerekecek. AklÄ±mda birkaÃ§ aÃ§Ä±k kaynaklÄ± LLM mimarisi var, bunlar arasÄ±nda: OpenLLaMA v3, Phi-2 ve Qwen 1.5B. Ve V1'e geÃ§iÅŸi desteklemek iÃ§in Ã§ok daha bÃ¼yÃ¼k ve Ã§eÅŸitli bir veri kÃ¼mesi titizlikle derlemem gerekecek. En az 5GB temiz eÄŸitim verisine ihtiyacÄ±m olacak.

# V0 Model DavranÄ±ÅŸÄ± ve SÄ±nÄ±rlamalarÄ±

Erken istemlerde modelin 1800'lerin dili ve davranÄ±ÅŸÄ±yla cevap verdiÄŸi gÃ¶rÃ¼lÃ¼yor. Ã–rneÄŸin, "Who art Henry?" diye sordum ve "O adamÄ± biliyorum, siyah deÄŸildim, fÄ±rtÄ±na." diye cevapladÄ± ve evet bu cÃ¼mle mantÄ±klÄ± deÄŸil ama LLM bir kiÅŸiden bahsettiÄŸimi anlÄ±yor.

![TimeLockLLM Ã–rnek Ã‡Ä±ktÄ±](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

Modern kavramlara dair hiÃ§bir ÅŸey yok, Ã§Ä±ktÄ±larda Ã§oÄŸunlukla 1800'lerin kelimeleri ve ifadeleri var.

Hala Ã§ok fazla Ã§alÄ±ÅŸmaya ihtiyacÄ± var, 187MB ile yapÄ±lan eÄŸitim karmaÅŸÄ±k akÄ±l yÃ¼rÃ¼tme Ã¼retecek bir model vermez.

Åu anda, tam cÃ¼mle yapÄ±sÄ±ndan yoksun ve genel olarak hiÃ§bir anlam ifade etmeyen cÃ¼mleler Ã¼retiyor ama bu eÄŸitim boyutu iÃ§in normal.

# V0.5 Model DavranÄ±ÅŸÄ± & SÄ±nÄ±rlamalarÄ±

Bu, son modele kÄ±yasla gÃ¼zel bir geliÅŸme. YazÄ± stili ve kelime daÄŸarcÄ±ÄŸÄ± Viktorya dÃ¶nemine ait ve neredeyse her cÃ¼mle doÄŸru noktalama ile dilbilgisel olarak doÄŸru. Ve yine bu sÄ±fÄ±rdan eÄŸitildiÄŸi iÃ§in 1800'ler konularÄ±na baÄŸlÄ± kalÄ±yor.

![TimeLockLLM Ã–rnek Ã‡Ä±ktÄ±](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

Ã‡ok fazla gerÃ§ek dÄ±ÅŸÄ± halÃ¼sinasyon var. AyrÄ±ntÄ±larÄ±n (tarihler, olaylar, tarihi figÃ¼rler) neredeyse tamamÄ± (%100 gibi) uydurma. AyrÄ±ca, cÃ¼mleler birbirleriyle gerÃ§ekten baÄŸlantÄ±lÄ± deÄŸil, bazen belki 2 cÃ¼mle birbiriyle iliÅŸkili olabiliyor ama bunun Ã¶tesine geÃ§miyorlar. Bir diÄŸer sorun ise bazen rastgele bir â€œGoogle tarafÄ±ndan dijitalleÅŸtirildiâ€ dipnotunun Ã§Ä±kmasÄ±, bu yÃ¼zden bir dahaki eÄŸitimde metinlerin iyi temizlendiÄŸinden emin olmam gerekecek. Genel olarak sonuÃ§lardan Ã§ok memnunum, henÃ¼z bir LLM seviyesinde deÄŸil ama kesinlikle bir cÃ¼mle Ã¼reticisi.

Ã‡ok ÅŸey Ã¶ÄŸreniyorum ve Ã¶nÃ¼mÃ¼zdeki haftalarda neleri daha iyi yapmam gerektiÄŸini anlamaya baÅŸlayacaÄŸÄ±m. YakÄ±nda dosyalarÄ± yÃ¼kleyeceÄŸim!

# V1 Model DavranÄ±ÅŸÄ± & SÄ±nÄ±rlamalarÄ±

YakÄ±nda bazÄ± Ã¶rnek Ã§Ä±ktÄ±lar yÃ¼kleyeceÄŸim ve aynÄ± istem ile 3 model arasÄ±nda karÅŸÄ±laÅŸtÄ±rmalar yapacaÄŸÄ±m. V1â€™i de Ã¶nceki versiyonda olduÄŸu gibi huggingfaceâ€™e yÃ¼kleyeceÄŸim, huggingface hesabÄ±mÄ± burada bulabilirsiniz: https://huggingface.co/haykgrigorian/TimeCapsuleLLM

# YaklaÅŸan Planlar

(TamamlandÄ±) 0.5 sÃ¼rÃ¼mÃ¼ Ã¼zerinde Ã§alÄ±ÅŸmaya baÅŸlayacaÄŸÄ±m, 50 kitap yerine ideal olarak 500-600 kitap kullanarak eÄŸitim yapacaÄŸÄ±m. Åu anda nanoGPTâ€™yi 1800-1850 yÄ±llarÄ± arasÄ±ndaki ve Ã¶zellikle Londraâ€™dan kitaplarla eÄŸitiyorum. BulduÄŸum kitaplarÄ±n gÃ¼ncellenmemiÅŸ veya modern yorumlara sahip olmamasÄ±na, seÃ§tiÄŸim zaman aralÄ±ÄŸÄ±nda yayÄ±mlanmÄ±ÅŸ dokunulmamÄ±ÅŸ kitaplar olmasÄ±na dikkat etmek gibi bazÄ± zorluklar var.

Daha bÃ¼yÃ¼k bir veri kÃ¼mesiyle (v1) yeni bir model eÄŸitmek istiyorum, belki v0.5 iÃ§in kullandÄ±ÄŸÄ±mdan 5-10 kat daha bÃ¼yÃ¼k. AmacÄ±m, yalnÄ±zca SeÃ§ici ZamanlÄ± EÄŸitim ile akÄ±l yÃ¼rÃ¼tme yeteneklerinin ortaya Ã§Ä±kÄ±p Ã§Ä±kmayacaÄŸÄ±nÄ± gÃ¶rmek, bu daha zor bir gÃ¶rev olacak ve tarihsel veri kÄ±sÄ±tlamalarÄ± nedeniyle bunun mÃ¼mkÃ¼n olup olmadÄ±ÄŸÄ±ndan bile tam emin deÄŸilim. Ã–nÃ¼mÃ¼zdeki haftalarda 5-10GBâ€™lÄ±k bir veri kÃ¼mesi iÃ§in yeterli veri toplamaya Ã§alÄ±ÅŸacaÄŸÄ±m. Temiz, yÃ¼ksek kaliteli veri bulabilir ve bir GPU kiralayabilirsem ilerleme olacaÄŸÄ±na inanÄ±yorum.

# Bu Proje NasÄ±l KullanÄ±lÄ±r

Bu proje aÄŸÄ±rlÄ±klÄ± olarak tarihsel veri derlemeye, eÄŸitime hazÄ±rlamaya ve bir ayrÄ±ÅŸtÄ±rÄ±cÄ± (tokenizer) oluÅŸturmaya odaklanÄ±yor. Tam LLM eÄŸitim sÃ¼recini kapsamayacaÄŸÄ±m, bunun iÃ§in Andrej Karpathyâ€™nin nanoGPTâ€™sine baÅŸvurun.

# AdÄ±m 1: Tarihsel Metinleri Topla ve HazÄ±rla

SeÃ§tiÄŸiniz zaman aralÄ±ÄŸÄ±ndan (Ã¶r. Londra 1800-1850) kamuya aÃ§Ä±k kitaplarÄ±n, belgelerin vb. .txt dosyalarÄ±nÄ± toplayÄ±n.

Ä°sterseniz kitaplarÄ± sizin iÃ§in indirmesi iÃ§in download_texts_improved.py dosyasÄ±nÄ± kullanabilirsiniz.

Metin dosyalarÄ±nÄ± bir script ile veya manuel olarak Project Gutenberg baÅŸlÄ±k/dipnotlarÄ±nÄ±, modern aÃ§Ä±klamalarÄ± ya da OCR hatalarÄ± gibi ÅŸeyleri temizleyin.

prepare_dataset.py gayet iyi Ã§alÄ±ÅŸacaktÄ±r.

# AdÄ±m 2: Ã–zel Bir AyrÄ±ÅŸtÄ±rÄ±cÄ± (Tokenizer) OluÅŸturun

TemizlenmiÅŸ veriler Ã¼zerinde train_tokenizer.py veya train_tokenizer_hf.py dosyasÄ±nÄ± Ã§alÄ±ÅŸtÄ±rÄ±n.
Bu size vocab.json ve merges.txt dosyalarÄ±nÄ± verecektir.

Bu dosyalar modeliniz iÃ§in kelime hazinesi ve birleÅŸtirme kurallarÄ±nÄ± tanÄ±mlar

# AdÄ±m 3: Modelinizi EÄŸitin (nanoGPT)

EÄŸitim sÃ¼reci iÃ§in [Andrej Karpathy'nin nanoGPT'sine](https://github.com/karpathy/nanoGPT) bakabilirsiniz.

Ä°sterseniz farklÄ± bir LLM de eÄŸitebilirsiniz ama ben nanoGPT kullandÄ±m

# SSS

## SeÃ§ici Zamansal EÄŸitim (Selective Temporal Training) nedir?

SeÃ§ici Zamansal EÄŸitim (STT), tÃ¼m eÄŸitim verilerinin belirli bir tarihsel dÃ¶neme Ã¶zel olarak seÃ§ildiÄŸi bir makine Ã¶ÄŸrenimi metodolojisidir. Bu, dÃ¶nemin dilini ve bilgisini modern kavramlarÄ±n etkisi olmadan modellemek iÃ§in yapÄ±lÄ±r. Ã–rneÄŸin, ÅŸu an elimdeki mevcut model (v0.5) yalnÄ±zca 1800-1875 arasÄ±ndaki verilerle sÄ±fÄ±rdan eÄŸitildi, ince ayar yapÄ±lmadÄ± ve Ã§Ä±ktÄ±lar dÃ¶nemin dilini ve tarihsel baÄŸlamÄ±nÄ± yansÄ±tÄ±yor.

## Neden sadece ince ayar ya da LoRA kullanmÄ±yorsunuz?

Bu projede modern Ã¶nyargÄ±dan arÄ±ndÄ±rÄ±lmÄ±ÅŸ bir dil modeli oluÅŸturmak istiyorum. GPT-2 gibi bir ÅŸeyi ince ayar yaparsam, zaten Ã¶nceden eÄŸitilmiÅŸ olur ve bu bilgi kaybolmaz. SÄ±fÄ±rdan eÄŸitirsem, dil modeli eski gibi davranmaz, gerÃ§ekten eski olur. Åu anki hedefim, yalnÄ±zca 1800-1850 yÄ±llarÄ± arasÄ±nda Londraâ€™da yayÄ±mlanan kitaplardan gelen bilgilerle mantÄ±k yÃ¼rÃ¼tebilen bir ÅŸey Ã¼retmek.

## EÄŸitmek iÃ§in ne tÃ¼r veriler kullandÄ±nÄ±z?

1800â€“1850 Londraâ€™sÄ±ndan kitaplar, yasal belgeler, gazeteler ve diÄŸer yazÄ±larÄ± kullanÄ±yorum. BaÄŸlantÄ±sÄ±nÄ± verdiÄŸim listede yaklaÅŸÄ±k 200 dosya var ama ilk eÄŸitim iÃ§in sadece 50 dosya (~187 MB) kullandÄ±m. Belgelerin listesine buradan bakabilirsiniz:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## Modeller ne kadar bÃ¼yÃ¼k?

V0: 16M Parametre

V0.5 123M Parametre

V1: 700M Parametre

# EÄŸitim Ã–zellikleri ?

#V0/V0.5
GPU: Geforce rtx 4060
CPU: i5-13400F
Ram: 16GB DDR5.

#V1
GPU: A100 rented









---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-08-12

---