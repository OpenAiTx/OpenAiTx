
<div align="right">
  <details>
    <summary >ğŸŒ Dil</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ç®€ä½“ä¸­æ–‡</a>
        | <a href="#" title="YakÄ±nda">ç¹é«”ä¸­æ–‡ (yakÄ±nda)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">æ—¥æœ¬èª</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">í•œêµ­ì–´</a>
        | <a href="#" title="YakÄ±nda">à¤¹à¤¿à¤¨à¥à¤¦à¥€ (yakÄ±nda)</a> |
        | <a href="#" title="YakÄ±nda">à¹„à¸—à¸¢ (yakÄ±nda)</a> |
        | <a href="#" title="YakÄ±nda">FranÃ§ais (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">Deutsch (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">EspaÃ±ol (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">Italiano (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">Ğ ÑƒÑÑĞºĞ¸Ğ¹ (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">PortuguÃªs (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">Nederlands (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">Polski (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">ÙØ§Ø±Ø³ÛŒ (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">TÃ¼rkÃ§e (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">Tiáº¿ng Viá»‡t (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">Bahasa Indonesia (yakÄ±nda)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM
YalnÄ±zca belirli zaman dilimlerinden veriyle eÄŸitilmiÅŸ, modern Ã¶nyargÄ±yÄ± azaltmayÄ± amaÃ§layan bir LLM.

Bir yapay zekÃ¢ modelinin tarihi taklit etmekle kalmayÄ±p, aslÄ±nda gerÃ§ekten Ã¶yle olduÄŸunu hayal edin.

[Andrej Karpathy'nin nanoGPT'si](https://github.com/karpathy/nanoGPT) Ã¼zerine inÅŸa edilmiÅŸtir. Temel eÄŸitim betikleri ve model mimarisi ona aittir.

# Proje Hedefleri

TimeCapsule LLM, yalnÄ±zca belirli zaman dilimlerinde yazÄ±lmÄ±ÅŸ metinlerle eÄŸitilecek deneysel bir projedir. AmaÃ§, belirli tarihÃ® dÃ¶nemlerin dÃ¼nya gÃ¶rÃ¼ÅŸÃ¼nÃ¼ ve dilini simÃ¼le etmektir.

# Neden ince ayar yeterli deÄŸil

Sadece Ã¶nceden eÄŸitilmiÅŸ bir modeli ince ayar yaparsanÄ±z, LLM'iniz hÃ¢lÃ¢ modern kavramlarÄ± bilecektir. Elbette sÄ±fÄ±r modern Ã¶nyargÄ± elde etmek zordur ama buna olabildiÄŸince yaklaÅŸmak istiyorum. HiÃ§ modern Ã¶nyargÄ± olmamasÄ± iÃ§in modeli sÄ±fÄ±rdan eÄŸitmek gerekiyor.

# Beklenen sonuÃ§lar

UmarÄ±m tamamlandÄ±ÄŸÄ±nda, bu model modern kavramlarÄ± bilmeyecek ve yalnÄ±zca eÄŸitildiÄŸi bilgilerle akÄ±l yÃ¼rÃ¼tebilecek. Modern kavramlarÄ±/kelime daÄŸarcÄ±ÄŸÄ±nÄ± tanÄ±mamalÄ± ve umarÄ±m modern bilgi uydurmaz.

# Ä°lerleme GÃ¼ncellemeleri

## 9 Temmuz 2025

Zaman dilimimi 1800-1850 ve bÃ¶lgeyi: Londra olarak belirledim.

Bir metin, kitap, belge listesi topladÄ±m.

Åu ana kadar 50 tanesini txt dosyasÄ± olarak aldÄ±m ve yakÄ±nda NanoGPT eÄŸitimine baÅŸlayacaÄŸÄ±m.

Ä°lerleme kaydettikÃ§e bunu gÃ¼ncelleyeceÄŸim.

## 13 Temmuz 2025

nanoGPT'yi 187MB tarihÃ® metin verisiyle eÄŸittim.

## 15 Temmuz 2025

Ä°kinci eÄŸitim turu iÃ§in metinleri indirmeye baÅŸladÄ±m. Her ÅŸeyi Internet Archive'dan alÄ±yorum ve zaman dilimini 1800-1875 olarak geniÅŸlettim. Ã‡eÅŸitli metinler elde etmek iÃ§in Internet Archive'da yayÄ±n yeri, zaman aralÄ±ÄŸÄ± ve konu filtrelerini kullanabilirsiniz.

![Arama Filtreleri](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## 16 Temmuz 2025

Internet Archive'dan yaklaÅŸÄ±k 500 adet txt dosyasÄ± indirdim ve bunlarÄ± temizledikten sonra (sadece boÅŸluklarÄ±, Gutenberg baÅŸlÄ±klarÄ±nÄ± vs. sildim) yaklaÅŸÄ±k 500MB veri elde ettim. Bu Ã§ok kÃ¼Ã§Ã¼k bir veri seti ama geÃ§en sefer 187MB ile eÄŸitmiÅŸtim, bu yÃ¼zden ikinci modeli eÄŸittikten sonra Ã§Ä±ktÄ±da en azÄ±ndan belirgin bir fark olmalÄ±. Bu modelin en azÄ±ndan daha anlamlÄ± ve tutarlÄ± cÃ¼mleler kurabilmesini umuyorum. Elbette bu bir garanti deÄŸil Ã§Ã¼nkÃ¼ hÃ¢lÃ¢ Ã§ok kÃ¼Ã§Ã¼k bir veri seti, fakat geÃ§en sefere gÃ¶re daha fazla veri var.

Bunu kendi donanÄ±mÄ±mda yapmak mÃ¼mkÃ¼n, bu da iyi Ã§Ã¼nkÃ¼ daha bÃ¼yÃ¼k bir veri setine geÃ§meden Ã¶nce en azÄ±ndan bazÄ± iyileÅŸmeleri gÃ¶rebilirim. BÃ¼yÃ¼k bir veri seti iÃ§in GPU kiralamam gerekecek ama endiÅŸelenmeyin, yakÄ±nda GPU kiralamayÄ± planlÄ±yorum; ancak bunu yapmadan Ã¶nce veri setimin olabildiÄŸince Ã¶zenli ve temiz olmasÄ±nÄ± istiyorum. Sorunlardan biri de temizlik, birÃ§ok txt dosyasÄ±nda saÃ§ma sapan yazÄ±lar karÄ±ÅŸÄ±yor. Temizlik iÃ§in kullandÄ±ÄŸÄ±m betikler iÅŸe yarÄ±yor ama %100 etkili deÄŸiller.

Bu veri setini bugÃ¼n eÄŸiteceÄŸim ve yaklaÅŸÄ±k 4-5 saat sÃ¼recek. Ä°ÅŸlem bittiÄŸinde ve test ettiÄŸimde gÃ¼ncellemeleri paylaÅŸacaÄŸÄ±m. Projeme bakan herkese tekrar teÅŸekkÃ¼r ederim, hatta bana OCR kaynaklarÄ± gÃ¶nderenler bile oldu, teÅŸekkÃ¼rler! UmarÄ±m daha fazla insan bunu dener ve kendi veri setleriyle deney yapar.

## 28 Temmuz 2025

v0.5 sÃ¼rÃ¼mÃ¼nÃ¼ Hugging Face'e yÃ¼kledim, [Buradan bakabilirsiniz](https://huggingface.co/haykgrigorian/TimeCapsuleLLM) isterseniz. ArtÄ±k depomu indirip yerel olarak Ã§alÄ±ÅŸtÄ±rabilirsiniz. Maalesef nanoGPT HuggingFace ile doÄŸrudan Ã§alÄ±ÅŸmÄ±yor, bu yÃ¼zden modeli indirip yerel olarak Ã§alÄ±ÅŸtÄ±rmanÄ±z gerekecek.

AyrÄ±ca bir sonraki eÄŸitim turum iÃ§in veri toplamaya baÅŸlayacaÄŸÄ±m, akÄ±l yÃ¼rÃ¼tme yeteneÄŸi elde etmek iÃ§in muhtemelen 5-10 kat daha fazla veriye ihtiyacÄ±m olacak.

### EÄŸitim GÃ¼ncellemesi

435MB'lÄ±k bir derlemede (108 M token) eÄŸitime baÅŸladÄ±m, ÅŸu an oldukÃ§a sorunsuz ilerliyor. EÄŸitim kaybÄ± ilk 2800 iterasyonda 10.9'dan 4.9'a dÃ¼ÅŸtÃ¼. TamamlanmasÄ±nÄ±n 8-9 saat sÃ¼receÄŸini tahmin ediyorum. BittiÄŸinde baÅŸka bir gÃ¼ncelleme paylaÅŸacaÄŸÄ±m.

## 17 Temmuz 2025 2:13AM

Ä°kinci modelin eÄŸitimi tamamlandÄ±, 4060 ekran kartÄ±mda yaklaÅŸÄ±k 8 saat 40 dakika sÃ¼rdÃ¼ (saatte 3.900 iterasyon) ve 33.000 iterasyon (5 epoch) tamamlandÄ±. Son eÄŸitim kaybÄ± 3.73 oldu. Ã‡Ä±ktÄ±lar ÅŸaÅŸÄ±rtÄ±cÄ± derecede iyi, artÄ±k gerÃ§ekten tutarlÄ± 19. yÃ¼zyÄ±l tarzÄ± cÃ¼mleler Ã¼retebiliyor.

# V0 Model DavranÄ±ÅŸÄ± ve SÄ±nÄ±rlamalarÄ±

Ä°lk istemlerde modelin 1800'lerin dili ve davranÄ±ÅŸÄ±yla yanÄ±t verdiÄŸi gÃ¶rÃ¼lÃ¼yor. Ã–rneÄŸin, "Who art Henry?" diye sordum ve ÅŸu cevabÄ± verdi: "I know that man, I have did not a black, the storm." evet, bu cÃ¼mlenin bir anlamÄ± yok ama LLM bir kiÅŸiden bahsettiÄŸimi anlÄ±yor.

![TimeLockLLM Ã–rnek Ã‡Ä±ktÄ±](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)
Modern kavramlardan bahsedilmiyor, Ã§Ä±ktÄ±lar Ã§oÄŸunlukla 1800'lerden kelimeler ve ifadeler iÃ§eriyor.

Hala Ã§ok fazla Ã§alÄ±ÅŸmaya ihtiyaÃ§ var, 187MB Ã¼zerinden eÄŸitmek karmaÅŸÄ±k muhakeme Ã¼reten bir model vermez.

Åu anda tam cÃ¼mle yapÄ±sÄ±ndan yoksun ve genel olarak anlamsÄ±z cÃ¼mleler Ã¼retiyor ama bu eÄŸitim boyutu iÃ§in normal.

# V0.5 Model DavranÄ±ÅŸÄ± & SÄ±nÄ±rlamalarÄ±

Bu, bir Ã¶nceki modele kÄ±yasla gÃ¼zel bir geliÅŸme. YazÄ± stili ve kelime daÄŸarcÄ±ÄŸÄ± Viktorya dÃ¶nemi ve neredeyse her cÃ¼mle dilbilgisi aÃ§Ä±sÄ±ndan doÄŸru, noktalama iÅŸaretleri yerinde. Ve yine bu sÄ±fÄ±rdan eÄŸitildiÄŸi iÃ§in 1800'ler konularÄ±na sadÄ±k kalÄ±yor.

![TimeLockLLM Ã–rnek Ã‡Ä±ktÄ±](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

Ã‡ok fazla gerÃ§ek dÄ±ÅŸÄ± uydurma var. DetaylarÄ±n (tarihler, olaylar, tarihi figÃ¼rler) Ã§oÄŸu (yaklaÅŸÄ±k %100'Ã¼) uydurma. AyrÄ±ca cÃ¼mleler birbirleriyle baÄŸlantÄ±lÄ± deÄŸil, bazen belki 2 cÃ¼mle birbirine baÄŸlÄ± olur ama onun dÄ±ÅŸÄ±nda olmaz. Bir diÄŸer sorun ise bazen â€œDigitized by Googleâ€ gibi bir dipnotun araya girmesi, bir dahaki eÄŸitimde metinlerin iyice temizlenmiÅŸ olduÄŸundan emin olmam gerekiyor. Genel olarak sonuÃ§lardan Ã§ok memnunum, henÃ¼z bir LLM deÄŸil ama kesinlikle bir cÃ¼mle Ã¼reteci.

Ã‡ok ÅŸey Ã¶ÄŸreniyorum ve Ã¶nÃ¼mÃ¼zdeki haftalarda neyi daha iyi yapmam gerektiÄŸini anlamaya baÅŸlayacaÄŸÄ±m. DosyalarÄ± yakÄ±nda yÃ¼kleyeceÄŸim!

# YaklaÅŸan Planlar

(TamamlandÄ±) 0.5 sÃ¼rÃ¼mÃ¼ Ã¼zerinde Ã§alÄ±ÅŸmaya baÅŸlayacaÄŸÄ±m, 50 kitapla eÄŸitmek yerine ideal olarak 500-600 kitapla eÄŸiteceÄŸim. Åu anda nanoGPT'yi 1800-1850 yÄ±llarÄ± arasÄ±ndan ve Ã¶zellikle Londra'dan kitaplarla eÄŸitiyorum. BulduÄŸum kitaplarÄ±n gÃ¼ncellenmemiÅŸ ya da modern yorumlar iÃ§ermediÄŸinden, seÃ§tiÄŸim zaman diliminde yayÄ±mlanmÄ±ÅŸ dokunulmamÄ±ÅŸ kitaplar olduÄŸundan emin olmak gibi bazÄ± zorluklar var.

Daha bÃ¼yÃ¼k bir derlemeyle (v1) yeni bir model eÄŸitmek istiyorum, belki v0.5 iÃ§in kullandÄ±ÄŸÄ±mÄ±n 5-10 katÄ± bÃ¼yÃ¼klÃ¼kte. AmacÄ±m, sadece SeÃ§ici Zamansal EÄŸitim ile muhakeme yeteneklerinin ortaya Ã§Ä±kÄ±p Ã§Ä±kamayacaÄŸÄ±nÄ± gÃ¶rmek, bu daha zor bir gÃ¶rev olacak ve tarihi veri sÄ±nÄ±rlamalarÄ± nedeniyle mÃ¼mkÃ¼n olup olmadÄ±ÄŸÄ±ndan emin deÄŸilim. Ã–nÃ¼mÃ¼zdeki haftalarda 5-10GB'lÄ±k bir derleme iÃ§in yeterli veri toplamaya Ã§alÄ±ÅŸacaÄŸÄ±m. Temiz, yÃ¼ksek kaliteli veri ve bir GPU kiralayabilirsem ilerleme olacaÄŸÄ±na inanÄ±yorum.

# Bu Proje NasÄ±l KullanÄ±lÄ±r

Bu proje Ã§oÄŸunlukla tarihi verilerin toplanmasÄ±, eÄŸitime hazÄ±rlanmasÄ± ve bir ayrÄ±ÅŸtÄ±rÄ±cÄ± oluÅŸturulmasÄ±na odaklanÄ±yor. Tam LLM eÄŸitim sÃ¼recini anlatmayacaÄŸÄ±m, bunun iÃ§in Andrej Karpathy'nin nanoGPT'sine bakÄ±nÄ±z.

# AdÄ±m 1: Tarihi Metinleri Topla ve HazÄ±rla

SeÃ§tiÄŸiniz zaman diliminden kamuya aÃ§Ä±k kitaplarÄ±n, belgelerin vb. .txt dosyalarÄ±nÄ± toplayÄ±n (Ã¶r. Londra 1800-1850)

Ä°sterseniz download_texts_improved.py dosyasÄ±nÄ± kitaplarÄ± indirmek iÃ§in kullanabilirsiniz.

Metin dosyalarÄ±nÄ± bir betik veya elle temizleyin, Project Gutenberg baÅŸlÄ±k/dipnotlarÄ±nÄ±, modern aÃ§Ä±klamalarÄ± ya da OCR hatalarÄ±nÄ± Ã§Ä±karÄ±n.

prepare_dataset.py sorunsuz Ã§alÄ±ÅŸacaktÄ±r.

# AdÄ±m 2: Ã–zel Bir Tokenizer OluÅŸtur

train_tokenizer.py veya train_tokenizer_hf.py dosyasÄ±nÄ± temizlenmiÅŸ veri Ã¼zerinde Ã§alÄ±ÅŸtÄ±rÄ±n.
Bu size vocab.json ve merges.txt dosyalarÄ±nÄ± verecek

Bu dosyalar modeliniz iÃ§in kelime daÄŸarcÄ±ÄŸÄ±nÄ± ve birleÅŸtirme kurallarÄ±nÄ± tanÄ±mlar

# AdÄ±m 3: Modelinizi EÄŸitin (nanoGPT)

EÄŸitim sÃ¼reci iÃ§in [Andrej Karpathy'nin nanoGPT'sine](https://github.com/karpathy/nanoGPT) bakÄ±n.

Ä°sterseniz farklÄ± bir LLM de eÄŸitebilirsiniz, ama ben nanoGPT kullandÄ±m

# SSS

## SeÃ§ici Zamansal EÄŸitim (Selective Temporal Training) nedir?

SeÃ§ici Zamansal EÄŸitim (STT), tÃ¼m eÄŸitim verilerinin belirli bir tarihsel zaman dilimine ait olacak ÅŸekilde Ã¶zel olarak seÃ§ildiÄŸi bir makine Ã¶ÄŸrenimi metodolojisidir. Bu, o dÃ¶nemin dilini ve bilgisini modern kavramlardan etkilenmeden modellemek iÃ§in yapÄ±lÄ±r. Ã–rneÄŸin, ÅŸu anki modelim (v0.5) yalnÄ±zca 1800-1875 verileriyle eÄŸitildi, ince ayar yapÄ±lmadÄ±, sÄ±fÄ±rdan eÄŸitildi ve bu da Ã§Ä±ktÄ±nÄ±n o zaman diliminin dil stilini ve tarihsel baÄŸlamÄ±nÄ± yansÄ±tmasÄ±nÄ± saÄŸladÄ±.

## Neden sadece ince ayar (fine-tuning) veya LoRA kullanmÄ±yorsun?

Bu projede modern Ã¶nyargÄ±dan arÄ±ndÄ±rÄ±lmÄ±ÅŸ bir dil modeli Ã¼retmeye Ã§alÄ±ÅŸÄ±yorum. GPT-2 gibi bir ÅŸeyi ince ayar yaparsam, zaten Ã¶nceden eÄŸitilmiÅŸ ve bu bilgi ortadan kalkmaz. SÄ±fÄ±rdan eÄŸitirsem, dil modeli eski gibi davranmayacak, gerÃ§ekten eski olacak. Åu anda bu projenin amacÄ±, yalnÄ±zca 1800-1850 yÄ±llarÄ± arasÄ±nda Londra'da yayÄ±mlanan kitaplardan bilgiyle mantÄ±k yÃ¼rÃ¼tebilen bir ÅŸey yaratmak.

## EÄŸitim iÃ§in ne tÃ¼r veri kullandÄ±nÄ±z?

Kitaplar, yasal belgeler, gazeteler ve 1800â€“1850 Londra'sÄ±ndan diÄŸer yazÄ±lar kullanÄ±yorum. BaÄŸlantÄ±sÄ±nÄ± verdiÄŸim listede 200 kadar var ama ilk eÄŸitimde sadece 50 dosya (~187 MB) kullandÄ±m. Belgelerin listesini ÅŸurada gÃ¶rebilirsiniz:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## SÃ¼rÃ¼m 0 modeli ne kadar bÃ¼yÃ¼k?

Bu model ÅŸu anda Ã§ok kÃ¼Ã§Ã¼k, sadece eÄŸlence amaÃ§lÄ± yapÄ±yorum ve kesin bir eÄŸitim kuralÄ± olarak modern kaynak kullanmÄ±yorum. YaklaÅŸÄ±k 16 milyon parametresi var ama daha fazla eski metin toplayÄ±p baÅŸka bir model eÄŸitimine baÅŸlayacaÄŸÄ±m. GeliÅŸmeleri aktaracaÄŸÄ±m.

## EÄŸitim Teknik Ã–zellikleri?

GPU: Geforce rtx 4060
CPU: i5-13400F
Ram: 16GB DDR5.



---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-29

---