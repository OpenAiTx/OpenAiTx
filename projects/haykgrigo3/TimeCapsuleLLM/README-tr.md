<div align="right">
  <details>
    <summary >ğŸŒ Dil</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">Ä°ngilizce</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ç®€ä½“ä¸­æ–‡</a>
        | <a href="#" title="YakÄ±nda">ç¹é«”ä¸­æ–‡ (yakÄ±nda)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">Japonca</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">Korece</a>
        | <a href="#" title="YakÄ±nda">à¤¹à¤¿à¤¨à¥à¤¦à¥€ (yakÄ±nda)</a> |
        | <a href="#" title="YakÄ±nda">à¹„à¸—à¸¢ (yakÄ±nda)</a> |
        | <a href="#" title="YakÄ±nda">FransÄ±zca (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">Almanca (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">Ä°spanyolca (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">Ä°talyanca (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">RusÃ§a (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">Portekizce (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">FelemenkÃ§e (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">LehÃ§e (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">ArapÃ§a (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">FarsÃ§a (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">TÃ¼rkÃ§e (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">Vietnamca (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">Endonezce (yakÄ±nda)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM
Modern Ã¶nyargÄ±yÄ± azaltmak iÃ§in yalnÄ±zca belirli zaman dilimlerinden alÄ±nan verilerle eÄŸitilmiÅŸ bir LLM.

Bir AI modelinin tarihiymiÅŸ gibi davranmasÄ±nÄ±n Ã¶tesinde, gerÃ§ekten tarihi olduÄŸunu hayal edin.

[Andrej Karpathy'nin nanoGPT'si](https://github.com/karpathy/nanoGPT) Ã¼zerine inÅŸa edilmiÅŸtir. Temel eÄŸitim betikleri ve model mimarisi ona aittir.

# Proje Hedefleri

TimeCapsule LLM, yalnÄ±zca belirli zaman dilimlerinde yazÄ±lmÄ±ÅŸ metinlerle eÄŸitilecek deneysel bir projedir. AmaÃ§, belirli tarihi dÃ¶nemlerin dÃ¼nya gÃ¶rÃ¼ÅŸÃ¼nÃ¼ ve dilini simÃ¼le etmektir.

# Neden sadece ince ayar yeterli deÄŸil

Sadece Ã¶nceden eÄŸitilmiÅŸ bir modeli ince ayarlarsanÄ±z, LLM'iniz yine de modern kavramlarÄ± bilecek. Elbette sÄ±fÄ±r modern Ã¶nyargÄ±ya ulaÅŸmak zor ama buna mÃ¼mkÃ¼n olduÄŸunca yaklaÅŸmak istiyorum. HiÃ§ modern Ã¶nyargÄ± olmamasÄ±, modelin sÄ±fÄ±rdan eÄŸitilmesini gerektirir.

# Beklenen SonuÃ§lar

UmarÄ±m tamamlandÄ±ÄŸÄ±nda, bu model modern kavramlarÄ± bilmeyecek ve eÄŸitildiÄŸi bilginin Ã¶tesinde akÄ±l yÃ¼rÃ¼temeyecek. Modern kavramlarÄ±/kelime daÄŸarcÄ±ÄŸÄ±nÄ± tanÄ±mamalÄ± ve umarÄ±m modern bilgi uydurmaz.

# Ä°lerleme GÃ¼ncellemeleri

## 9 Temmuz 2025

Zaman dilimimi 1800-1850 ve bÃ¶lgeyi Londra olarak belirledim.

Bir dizi metin, kitap, belge topladÄ±m.

Åimdiye kadar 50 tanesini txt dosyasÄ± olarak aldÄ±m ve yakÄ±nda NanoGPT'yi eÄŸitmeye baÅŸlayacaÄŸÄ±m.

Ä°lerleme kaydedildikÃ§e burayÄ± gÃ¼ncelleyeceÄŸim.

## 13 Temmuz 2025

nanoGPT'yi 187MB'lÄ±k tarihi metin verisiyle eÄŸittim.

## 15 Temmuz 2025

Ä°kinci eÄŸitim turu iÃ§in metinleri indirmeye baÅŸladÄ±m. TÃ¼m verileri Internet Archive'dan alÄ±yorum ve zaman dilimini 1800-1875 olarak geniÅŸlettim. FarklÄ± metinler almak iÃ§in Internet Archive'da yayÄ±n yeri, zaman dilimi ve konu filtrelerini kullanabilirsiniz.

![Arama Filtreleri](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## 16 Temmuz 2025

Internet Archive'dan yaklaÅŸÄ±k 500 txt dosyasÄ± indirdim ve bunlarÄ± temizledikten sonra (sadece boÅŸluklarÄ±, Gutenberg baÅŸlÄ±klarÄ±nÄ± vs. silerek) yaklaÅŸÄ±k 500MB veri elde ettim. KÃ¼Ã§Ã¼k bir veri seti ama geÃ§en sefer 187MB ile eÄŸittim, bu yÃ¼zden ikinci modeli eÄŸittikten sonra Ã§Ä±ktÄ±da en azÄ±ndan gÃ¶zle gÃ¶rÃ¼lÃ¼r bir fark olmalÄ±. UmarÄ±m bu model en azÄ±ndan anlamlÄ± cÃ¼mleler Ã¼retebilir. Tabii ki bu garanti deÄŸil Ã§Ã¼nkÃ¼ bu hÃ¢lÃ¢ Ã§ok kÃ¼Ã§Ã¼k bir veri seti, ama geÃ§en sefer kullandÄ±ÄŸÄ±mdan fazla.

Bu kendi donanÄ±mÄ±mda yapÄ±labilir, bu da iyi Ã§Ã¼nkÃ¼ daha bÃ¼yÃ¼k bir veri setine geÃ§meden Ã¶nce bazÄ± iyileÅŸtirmeleri gÃ¶rebilirim. Daha bÃ¼yÃ¼k bir veri seti iÃ§in GPU kiralamam gerekecek. Ama endiÅŸelenmeyin, yakÄ±nda GPU kiralamayÄ± hÃ¢lÃ¢ planlÄ±yorum, fakat bunu yapmadan Ã¶nce veri setimin olabildiÄŸince seÃ§ilmiÅŸ ve temiz olduÄŸundan emin olmak istiyorum. Sorunlardan biri temizlik; bu txt dosyalarÄ±nÄ±n Ã§oÄŸunda anlamsÄ±z karakterler karÄ±ÅŸmÄ±ÅŸ. Temizleme iÃ§in kullandÄ±ÄŸÄ±m betikler Ã§alÄ±ÅŸÄ±yor ama %100 etkili deÄŸiller.

Bu veri setini bugÃ¼n eÄŸiteceÄŸim ve yaklaÅŸÄ±k 4-5 saat sÃ¼rmeli. BittiÄŸinde ve test ettiÄŸimde gÃ¼ncelleme yapacaÄŸÄ±m. Projeme bakan herkese tekrar teÅŸekkÃ¼rler, bana OCR kaynaklarÄ± iÃ§in baÄŸlantÄ± gÃ¶nderenler bile oldu, Ã§ok teÅŸekkÃ¼r ederim! UmarÄ±m daha fazla kiÅŸi bunu dener ve kendi veri setleriyle deneyler yapar.

### EÄŸitim GÃ¼ncellemesi

435MB'lÄ±k (108 M token) bir veriyle eÄŸitime baÅŸladÄ±m, ÅŸu anda gayet iyi gidiyor. EÄŸitim kaybÄ± ilk 2800 iterasyonda 10,9'dan 4,9'a dÃ¼ÅŸtÃ¼. TamamlanmasÄ± yaklaÅŸÄ±k 8-9 saat sÃ¼recek gibi gÃ¶rÃ¼nÃ¼yor. BittiÄŸinde baÅŸka bir gÃ¼ncelleme yayÄ±nlayacaÄŸÄ±m.

## 17 Temmuz 2025 02:13

Ä°kinci modelin eÄŸitimi tamamlandÄ±, 4060 kartÄ±mda 8 saat 40 dakika (saatte 3.900 iterasyon) sÃ¼rdÃ¼, toplam 33.000 iterasyon (5 epoch). Son eÄŸitim kaybÄ± 3,73 oldu. Ã‡Ä±ktÄ±lar ÅŸaÅŸÄ±rtÄ±cÄ± derecede iyiydi, gerÃ§ekten 19. yÃ¼zyÄ±l tarzÄ±nda anlamlÄ± cÃ¼mleler Ã¼retiyor artÄ±k.

# V0 Model DavranÄ±ÅŸÄ± ve SÄ±nÄ±rlamalarÄ±

Ä°lk istemler, modelin 1800'lerin dili ve davranÄ±ÅŸÄ±yla yanÄ±t verdiÄŸini gÃ¶steriyor. Mesela, "Who art Henry?" diye sordum ve "I know that man, I have did not a black, the storm." diye yanÄ±tladÄ±. Evet, bu cÃ¼mlenin anlamÄ± yok ama LLM bir kiÅŸiden bahsettiÄŸimi anlÄ±yor.

![TimeLockLLM Ã–rnek Ã‡Ä±ktÄ±](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

Modern kavramlardan hiÃ§ bahsedilmiyor, Ã§Ä±ktÄ±lar Ã§oÄŸunlukla 1800'lerin kelimelerini ve ifadelerini iÃ§eriyor.

HÃ¢lÃ¢ Ã§ok fazla Ã§alÄ±ÅŸmaya ihtiyacÄ± var; 187MB ile eÄŸitim, karmaÅŸÄ±k akÄ±l yÃ¼rÃ¼tmeye sahip metin Ã¼reten bir model vermez.

Åu anda tam cÃ¼mle yapÄ±sÄ± olmayan ve genel olarak anlamlÄ± olmayan cÃ¼mleler Ã¼retiyor ama bu, eÄŸitim veri boyutu iÃ§in normal.

# V0.5 Model DavranÄ±ÅŸÄ± & SÄ±nÄ±rlamalarÄ±

Bu, son modele kÄ±yasla gÃ¼zel bir geliÅŸme. YazÄ± stili ve kelime daÄŸarcÄ±ÄŸÄ± Viktoryen ve neredeyse her cÃ¼mle gramer olarak doÄŸru, noktalama iÅŸaretleri de uygun. Ve yine bu model sÄ±fÄ±rdan eÄŸitildiÄŸi iÃ§in 1800â€™lerin konularÄ±na baÄŸlÄ± kalÄ±yor.

![TimeLockLLM Ã–rnek Ã‡Ä±ktÄ±](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

Ã‡ok fazla gerÃ§ek dÄ±ÅŸÄ± halÃ¼sinasyon var. AyrÄ±ntÄ±larÄ±n (tarih, olay, tarihi ÅŸahÄ±slar) Ã§oÄŸu (neredeyse %100â€™Ã¼) uydurma. AyrÄ±ca cÃ¼mleler arasÄ±nda gerÃ§ekten baÄŸlantÄ± yok, bazen belki 2 cÃ¼mle birbiriyle iliÅŸkili olabiliyor ama bunun Ã¶tesine geÃ§miyorlar. Bir diÄŸer sorun ise bazen rastgele bir â€œDigitized by Googleâ€ dipnotunun gÃ¶rÃ¼nmesi, bu yÃ¼zden bir dahaki eÄŸitime baÅŸlamadan Ã¶nce metinlerin iyice temizlendiÄŸinden emin olmam gerekecek. Genel olarak sonuÃ§lardan Ã§ok memnunum, henÃ¼z bir LLM olmaktan Ã§ok uzak ama kesinlikle bir cÃ¼mle Ã¼retici.

Ã‡ok ÅŸey Ã¶ÄŸreniyorum ve Ã¶nÃ¼mÃ¼zdeki haftalarda neleri daha iyi yapmam gerektiÄŸini belirlemeye baÅŸlayacaÄŸÄ±m. DosyalarÄ± yakÄ±nda yÃ¼kleyeceÄŸim!

# YaklaÅŸan Planlar

(TamamlandÄ±) 0.5 sÃ¼rÃ¼mÃ¼nde Ã§alÄ±ÅŸmaya baÅŸlayacaÄŸÄ±m, 50 kitapla eÄŸitim yapmak yerine mÃ¼mkÃ¼nse 500-600 kitapla eÄŸitim yapacaÄŸÄ±m. Åu anda 1800-1850 yÄ±llarÄ± arasÄ±ndaki kitaplarla, Ã¶zellikle Londraâ€™dan kitaplarla nanoGPT eÄŸitiyorum. KarÅŸÄ±laÅŸÄ±lan bazÄ± zorluklar, bulduÄŸum kitaplarÄ±n gÃ¼ncellenmemiÅŸ veya modern yorumlar iÃ§ermemesi, seÃ§tiÄŸim zaman aralÄ±ÄŸÄ±nda yayÄ±mlanmÄ±ÅŸ dokunulmamÄ±ÅŸ kitaplar olmasÄ±.

Ã‡ok daha bÃ¼yÃ¼k bir veri kÃ¼mesiyle (v1) yeni bir model eÄŸitmek istiyorum, belki v0.5â€™te kullandÄ±ÄŸÄ±mÄ±n 5-10 katÄ± bÃ¼yÃ¼klÃ¼ÄŸÃ¼nde. AmacÄ±m, yalnÄ±zca SeÃ§ici Zamansal EÄŸitim ile akÄ±l yÃ¼rÃ¼tme yeteneklerinin ortaya Ã§Ä±kÄ±p Ã§Ä±kamayacaÄŸÄ±nÄ± gÃ¶rmek, bu daha zor bir iÅŸ olacak ve tarihsel veri kÄ±sÄ±tlamalarÄ± nedeniyle mÃ¼mkÃ¼n olup olmadÄ±ÄŸÄ±ndan bile emin deÄŸilim. Ã–nÃ¼mÃ¼zdeki haftalarda 5-10GBâ€™lÄ±k bir veri kÃ¼mesi iÃ§in yeterli veri toplamaya Ã§alÄ±ÅŸacaÄŸÄ±m. EÄŸer temiz, yÃ¼ksek kaliteli veriler bulabilir ve bir GPU kiralayabilirsem ilerleme kaydedileceÄŸine inanÄ±yorum.

# Bu Proje NasÄ±l KullanÄ±lÄ±r

Bu proje esas olarak tarihsel verileri toplamaya, eÄŸitime hazÄ±rlamaya ve bir tokenizer oluÅŸturmaya odaklanÄ±yor. Tam LLM eÄŸitim sÃ¼recini burada ele almayacaÄŸÄ±m, bunun iÃ§in Andrej Karpathyâ€™nin nanoGPTâ€™sine baÅŸvurun.

# AdÄ±m 1: Tarihsel Metinleri Topla ve HazÄ±rla

SeÃ§tiÄŸiniz zaman aralÄ±ÄŸÄ±ndan (Ã¶rn. Londra 1800-1850) kamuya aÃ§Ä±k kitaplarÄ±n, belgelerin .txt dosyalarÄ±nÄ± toplayÄ±n.

Ä°sterseniz download_texts_improved.py dosyasÄ±nÄ± kullanarak sizin iÃ§in kitaplarÄ± indirebilirsiniz.

Metin dosyalarÄ±nÄ± bir betik veya elle Project Gutenberg baÅŸlÄ±k/dipnotlarÄ±, modern aÃ§Ä±klamalar veya OCR hatalarÄ± gibi ÅŸeyleri kaldÄ±rarak temizleyin.

prepare_dataset.py iyi Ã§alÄ±ÅŸacaktÄ±r.

# AdÄ±m 2: Ã–zel Bir Tokenizer OluÅŸtur

train_tokenizer.py veya train_tokenizer_hf.pyâ€™yi temizlenmiÅŸ veriler Ã¼zerinde Ã§alÄ±ÅŸtÄ±rÄ±n.
Bu size vocab.json ve merges.txt dosyalarÄ±nÄ± verecek.

Bu dosyalar modeliniz iÃ§in kelime daÄŸarcÄ±ÄŸÄ±nÄ± ve birleÅŸtirme kurallarÄ±nÄ± tanÄ±mlar.

# AdÄ±m 3: Modelinizi EÄŸitin (nanoGPT)

EÄŸitim sÃ¼reci iÃ§in [Andrej Karpathyâ€™nin nanoGPT](https://github.com/karpathy/nanoGPT) rehberine bakÄ±n.

Ä°sterseniz farklÄ± bir LLM eÄŸitebilirsiniz, ama ben nanoGPT kullandÄ±m.

# SSS

## SeÃ§ici Zamansal EÄŸitim nedir?

SeÃ§ici Zamansal EÄŸitim (STT), tÃ¼m eÄŸitim verisinin belirli bir tarihsel zaman aralÄ±ÄŸÄ±na ait olacak ÅŸekilde Ã¶zel olarak seÃ§ildiÄŸi bir makine Ã¶ÄŸrenimi metodolojisidir. AmaÃ§, o dÃ¶nemin dili ve bilgisini modern kavramlardan etkilenmeden modellemektir. Ã–rneÄŸin, elimdeki mevcut model (v0.5) yalnÄ±zca 1800-1875 dÃ¶nemine ait verilerle sÄ±fÄ±rdan eÄŸitildi, ince ayar yapÄ±lmadÄ±, sonuÃ§ olarak Ã§Ä±ktÄ±lar o zamanÄ±n dilsel stilini ve tarihsel baÄŸlamÄ±nÄ± yansÄ±tÄ±yor.

## Neden sadece ince ayar veya LoRA kullanÄ±lmÄ±yor?

Bu projede, modern Ã¶nyargÄ±lardan arÄ±ndÄ±rÄ±lmÄ±ÅŸ bir dil modeli oluÅŸturmaya Ã§alÄ±ÅŸÄ±yorum. GPT-2 gibi bir ÅŸeyi ince ayar yaparsam, zaten Ã¶nceden eÄŸitilmiÅŸ olur ve bu bilgi yok olmaz. SÄ±fÄ±rdan eÄŸitirsem, dil modeli eski gibi davranmayacak, gerÃ§ekten Ã¶yle olacak. Åu anki proje amacÄ±, yalnÄ±zca 1800-1850 arasÄ±nda Londraâ€™da yayÄ±mlanmÄ±ÅŸ kitaplardan alÄ±nan bilgilerle akÄ±l yÃ¼rÃ¼tebilen bir ÅŸey oluÅŸturmak.

## EÄŸitimde ne tÃ¼r veriler kullandÄ±nÄ±z?

1800â€“1850 Londraâ€™sÄ±ndan kitaplar, yasal belgeler, gazeteler ve diÄŸer yazÄ±larÄ± kullanÄ±yorum. BaÄŸlantÄ±sÄ±nÄ± verdiÄŸim listede yaklaÅŸÄ±k 200 belge var ama ilk eÄŸitim iÃ§in sadece 50 dosya (~187 MB) kullandÄ±m. Belgelerin listesine buradan bakabilirsiniz:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## SÃ¼rÃ¼m 0 modeli ne kadar bÃ¼yÃ¼k?

Bu model ÅŸu an Ã§ok kÃ¼Ã§Ã¼k, sadece eÄŸlence amaÃ§lÄ± ve kesinlikle modern kaynak kullanÄ±lmama kuralÄ±na uyuyorum. YaklaÅŸÄ±k 16 milyon parametre var ama daha fazla eski metin toplayÄ±p yeni bir model eÄŸitmeye baÅŸlayacaÄŸÄ±m. GeliÅŸmeleri paylaÅŸacaÄŸÄ±m.

## EÄŸitim Ã–zellikleri?

GPU: Geforce rtx 4060
CPU: i5-13400F
Ram: 16GB DDR5.



---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-21

---