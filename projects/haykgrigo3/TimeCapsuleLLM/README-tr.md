
<div align="right">
  <details>
    <summary >ğŸŒ Dil</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ç®€ä½“ä¸­æ–‡</a>
        | <a href="#" title="YakÄ±nda">ç¹é«”ä¸­æ–‡ (yakÄ±nda)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">æ—¥æœ¬èª</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">í•œêµ­ì–´</a>
        | <a href="#" title="YakÄ±nda">à¤¹à¤¿à¤¨à¥à¤¦à¥€ (yakÄ±nda)</a> |
        | <a href="#" title="YakÄ±nda">à¹„à¸—à¸¢ (yakÄ±nda)</a> |
        | <a href="#" title="YakÄ±nda">FranÃ§ais (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">Deutsch (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">EspaÃ±ol (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">Italiano (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">Ğ ÑƒÑÑĞºĞ¸Ğ¹ (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">PortuguÃªs (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">Nederlands (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">Polski (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">ÙØ§Ø±Ø³ÛŒ (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">TÃ¼rkÃ§e (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">Tiáº¿ng Viá»‡t (yakÄ±nda)</a>
        | <a href="#" title="YakÄ±nda">Bahasa Indonesia (yakÄ±nda)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM

*BazÄ± yerlerden ve zaman dÃ¶nemlerinden alÄ±nan verilerle **sÄ±fÄ±rdan** eÄŸitilmiÅŸ bir dil modeli; modern yanlÄ±lÄ±ÄŸÄ± azaltmak ve dÃ¶nemin sesi, kelime daÄŸarcÄ±ÄŸÄ± ve dÃ¼nya gÃ¶rÃ¼ÅŸÃ¼nÃ¼ taklit etmek iÃ§in tasarlanmÄ±ÅŸtÄ±r.*

Bir yapay zekÃ¢ modelinin tarihiymiÅŸ gibi davranmakla kalmayÄ±p gerÃ§ekten tarihi olduÄŸunu hayal edin.

v0 ve v0.5 [Andrej Karpathy'nin nanoGPT'si](https://github.com/karpathy/nanoGPT) Ã¼zerine inÅŸa edilmiÅŸtir. Temel eÄŸitim betikleri ve model mimarisi ona aittir.

v1 [Microsoft tarafÄ±ndan geliÅŸtirilen Phi 1.5 Ã¼zerinde oluÅŸturuldu](https://huggingface.co/microsoft/phi-1_5)


##  Model DavranÄ±ÅŸÄ± & SÄ±nÄ±rlamalar

### **v0**  

Ä°lk girdilerde modelin 1800'lerin dili ve davranÄ±ÅŸÄ±yla cevap verdiÄŸi gÃ¶rÃ¼lÃ¼yor. 
Ã–rnek: Girdi: "Who art Henry?" ve cevap olarak "I know that man, I have did not a black, the storm." dedi.

![TimeLockLLM Ã–rnek Ã‡Ä±ktÄ±](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

- Modern kavramlara hiÃ§ deÄŸinilmiyor  
- Ã‡oÄŸunlukla dÃ¶neme uygun kelime daÄŸarcÄ±ÄŸÄ±  
- CÃ¼mleler Ã§oÄŸunlukla tutarsÄ±z (yaklaÅŸÄ±k ~187MB eÄŸitim verisi iÃ§in beklenen)

### **v0.5** 

v0'a gÃ¶re Ã¶nemli bir geliÅŸme.  
- Viktorya dÃ¶nemi yazÄ± tarzÄ±, dÃ¼zgÃ¼n noktalama, Ã§oÄŸunlukla gramer kurallarÄ±na uygun cÃ¼mleler  
- HÃ¢lÃ¢ yÃ¼ksek oranda gerÃ§eklikten sapma (halÃ¼sinasyon)  
- OCR gÃ¼rÃ¼ltÃ¼sÃ¼ (â€œDigitized by Googleâ€) Ã§Ä±ktÄ±larda hÃ¢lÃ¢ mevcut

![TimeLockLLM Ã–rnek Ã‡Ä±ktÄ±](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

### **v1**

Modelin ilk kez gerÃ§ek bir tarihi olayÄ±, veri setindeki gerÃ§ek bir figÃ¼rle iliÅŸkilendirdiÄŸi sÃ¼rÃ¼m.

Ã–rnek: Girdi: "It was the year of our Lord 1834" 

Ã‡Ä±ktÄ±: "It was the year of our Lord 1834 and the streets of London were filled with protest and petition. The cause, as many re counted, was not bound in the way of private, but having taken up the same day in the day of Lord Palmerston, the public will receive a short statement of the difficulties under which the day of law has reached us. It is a matter of deep regret, that the present events in the history of the world are clear, and consequently will be'known. It is not true that the very men who first settled in the Gospel at Jerusalem should have so extensive and so interesting a record of the prosperity and prosperity" 

BaÅŸta bir protestonun aynÄ± yÄ±l tesadÃ¼fen gerÃ§ekleÅŸmiÅŸ olabileceÄŸini dÃ¼ÅŸÃ¼ndÃ¼m fakat ÅŸuna gÃ¶z atÄ±n: ![1834protesto](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)

### Neden Ã¶nemli:

Bu, modellerimden birinin bir yÄ±lÄ± hem gerÃ§ek bir tarihi olaya hem de o olayla baÄŸlantÄ±lÄ± gerÃ§ek bir kiÅŸiye (Lord Palmerston) baÄŸlamasÄ±nÄ±n ilk Ã¶rneÄŸi. Daha Ã¶nceki modeller (v0 ve v0.5) 19. yÃ¼zyÄ±lÄ±n yazÄ± tarzÄ±nÄ± taklit edebiliyordu ancak her zaman olaylar, kiÅŸiler ve gerÃ§ekler hakkÄ±nda halÃ¼sinasyon gÃ¶rÃ¼yordu. Bu, modelin veri setinden bir ÅŸeyleri hatÄ±rlamaya baÅŸladÄ±ÄŸÄ±nÄ± gÃ¶steriyor.

## YaklaÅŸan Planlar 

- 1800-1875 yÄ±llarÄ± arasÄ±nda Londra'da yayÄ±mlanmÄ±ÅŸ yaklaÅŸÄ±k 175.000 metin Internet Archive Ã¼zerinde bulunmaktadÄ±r
- Korpusu geniÅŸletmeyi ve daha iyi akÄ±l yÃ¼rÃ¼tme yetenekleri iÃ§in daha fazla temizlemeyi planlÄ±yorum
- Daha fazla tarihsel model iÃ§in farklÄ± bÃ¶lgelere ve zaman dilimlerine geniÅŸletme


## NasÄ±l KullanÄ±lÄ±r

Bu proje Ã§oÄŸunlukla tarihsel verileri derlemeye, eÄŸitime hazÄ±rlamaya ve bir belirteÃ§leyici (tokenizer) oluÅŸturmaya odaklanÄ±yor. Tam LLM eÄŸitim sÃ¼recini kapsamayacaÄŸÄ±m, bunun iÃ§in Andrej Karpathy'nin nanoGPT'sine bakabilirsiniz.

### AdÄ±m 1: Tarihsel Metinleri ToplayÄ±n ve HazÄ±rlayÄ±n 

- SeÃ§tiÄŸiniz zaman diliminden kamu malÄ± kitaplarÄ±n, belgelerin vs. .txt dosyalarÄ±nÄ± toplayÄ±n (Ã¶rneÄŸin, Londra 1800-1850)
- DosyalarÄ± seÃ§tiÄŸiniz zaman/mekan aralÄ±ÄŸÄ±nda tutun  
- Metin dosyalarÄ±nÄ± bir betik ile veya elle Project Gutenberg baÅŸlÄ±klarÄ±nÄ±/altbilgilerini, modern aÃ§Ä±klamalarÄ± veya OCR hatalarÄ± gibi ÅŸeyleri Ã§Ä±kararak temizleyin.

### AdÄ±m 2: Ã–zel Bir Tokenizer OluÅŸturun

- train_tokenizer.py veya train_tokenizer_hf.py'yi temizlenmiÅŸ veri Ã¼zerinde Ã§alÄ±ÅŸtÄ±rÄ±n.
- Bu iÅŸlem size vocab.json ve merges.txt dosyalarÄ±nÄ± verecektir
- Bu dosyalar modeliniz iÃ§in kelime daÄŸarcÄ±ÄŸÄ±nÄ± ve birleÅŸtirme kurallarÄ±nÄ± tanÄ±mlar

### AdÄ±m 3: Modelinizi EÄŸitin 

- EÄŸitim sÃ¼reci iÃ§in [Andrej Karpathy'nin nanoGPT](https://github.com/karpathy/nanoGPT) projesine veya seÃ§tiÄŸiniz mimarinin belgelerine bakÄ±n.

# SSS

## SeÃ§ici Zamansal EÄŸitim (Selective Temporal Training) Nedir?

SeÃ§ici Zamansal EÄŸitim (STT), tÃ¼m eÄŸitim verisinin Ã¶zel olarak belirli bir tarihsel zaman aralÄ±ÄŸÄ±na uygun ÅŸekilde seÃ§ildiÄŸi bir makine Ã¶ÄŸrenimi metodolojisidir. Bu, Ã§aÄŸdaÅŸ kavramlarÄ±n etkisi olmadan o dÃ¶nemin dilini ve bilgisini modellemek iÃ§in yapÄ±lÄ±r. Ã–rneÄŸin, ÅŸu anda elimde olan mevcut model (v0.5) tamamen 1800-1875 dÃ¶neminden alÄ±nan verilerle eÄŸitildi, ince ayar yapÄ±lmadÄ±, sÄ±fÄ±rdan eÄŸitildi ve Ã§Ä±ktÄ±larÄ± o zaman diliminin dilsel tarzÄ±nÄ± ve tarihsel baÄŸlamÄ±nÄ± yansÄ±tÄ±yor.

## Neden sadece ince ayar (fine-tuning) veya LoRA kullanmÄ±yorsunuz?

Bu proje iÃ§in amacÄ±m modern yanlÄ±lÄ±ktan uzak bir dil modeli oluÅŸturmak. Bir ÅŸeyin (Ã¶r. GPT-2) ince ayarÄ±nÄ± yaparsam, zaten Ã¶nceden eÄŸitilmiÅŸ olur ve o bilgi ortadan kalkmaz. SÄ±fÄ±rdan eÄŸitirsem dil modeli eskiyi taklit etmez, doÄŸrudan Ã¶yle olur. Åu anki hedefim, yalnÄ±zca 1800 ile 1875 arasÄ±nda Londra'da yayÄ±mlanmÄ±ÅŸ kitaplardan alÄ±nan bilgilerle akÄ±l yÃ¼rÃ¼tebilen bir ÅŸey yaratmak.

## EÄŸitim iÃ§in ne tÃ¼r veriler kullandÄ±nÄ±z?

Kitaplar, yasal belgeler, gazeteler ve 1800â€“1875 Londra'sÄ±ndan diÄŸer yazÄ±lÄ± eserleri kullanÄ±yorum. BaÄŸlantÄ±sÄ±nÄ± verdiÄŸim listede (v0) yaklaÅŸÄ±k 200 dosya var ama ilk eÄŸitimde sadece 50 dosya ve yaklaÅŸÄ±k ~187 MB kullandÄ±m. Belgelerin listesini buradan gÃ¶rebilirsiniz:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

Veri seti boyutlarÄ±:
v0: ~187MB
v0.5: ~435MB 
v1: ~6.25GB 

## Modeller ne kadar bÃ¼yÃ¼k?

V0: 16M Parametre

V0.5: 123M Parametre

V1: 700M Parametre

# EÄŸitim Ã–zellikleri?

# V0/V0.5
GPU: Geforce rtx 4060
CPU: i5-13400F 
Ram: 16GB DDR5.

# V1
GPU: KiralÄ±k A100

















---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-08-19

---