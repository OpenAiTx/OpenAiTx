[
  {
    "Id": 1,
    "Content": "\n<div align=\"right\">\n  <details>\n    <summary >🌐 Language</summary>\n    <div>\n      <div align=\"center\">\n        <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en\">English</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN\">简体中文</a>\n        | <a href=\"#\" title=\"Coming soon\">繁體中文 (coming soon)</a> |\n        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja\">日本語</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko\">한국어</a>\n        | <a href=\"#\" title=\"Coming soon\">हिन्दी (coming soon)</a> |\n        | <a href=\"#\" title=\"Coming soon\">ไทย (coming soon)</a> |\n        | <a href=\"#\" title=\"Coming soon\">Français (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Deutsch (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Español (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Italiano (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Русский (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Português (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Nederlands (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Polski (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">العربية (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">فارسی (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Türkçe (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Tiếng Việt (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Bahasa Indonesia (coming soon)</a>\n\n      </div>\n    </div>\n  </details>\n</div>\n\n# TimeCapsule LLM\nAn LLM trained only on data from certain time periods to reduce modern bias.\n\nImagine if an AI model didnt just pretend to be historical but actually was.\n\nBuilt on [nanoGPT by Andrej Karpathy](https://github.com/karpathy/nanoGPT) Core training scripts and model architecture are his work. \n\n# Project Goals \n\nTimeCapsule LLM is an expirimental project that will only be trained on texts written during certain time periods. The goal is to simulate the worldview and language of specific historical eras.\n\n# Why fine tuning isn't enough \n\nIf you just fine tune a pre-trained model, your LLM is still gonna know modern concepts. Of course achieving zero modern bias is difficult but I want to get as close as possible to this. Getting no modern bias requires training a model from scratch.\n\n# Expected outcomes \n\nHopefully when finished, this model will not know modern concepts and will not be able to reason beyond what it's been trained on. It shouldnt recognize modern concepts/vocab and I hope it doesn't hallucinate modern knowledge.\n\n# Progress Updates\n\n## July 9th, 2025\n\nI've set my time period for 1800-1850 and region: London \n\nI've gathered a list of texts, books, documents \n\nSo far I've gotten 50 as txt files and will begin training NanoGPT soon \n\nWill update this as long as progress is made\n\n## July 13th, 2025\n\nTrained nanoGPT with 187MB of historial text data. \n\n## July 15th, 2025\n\nI started downloading texts for the second training run. I'm getting everything from Internet Archive and I've expanded the time period to 1800-1875. To get a diverse range of texts, you can use subject and search filters for publication location, time period and subjects on Internet Archive. \n\n![Search Filters](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)\n\n## July 16th, 2025\n\nI downloaded around 500 txt files from Internet Archive and after cleaning them (just deleting whitespaces, Gutenberg headers, etc) I have around 500MB of data. It's a tiny dataset but last time I trained off of 187MB so there should be at least some kind of noticable difference in the output after I train the second model. I'm hoping this model can at least produce more coherent sentences that kind of make sense. It's not a guarantee of course since this is still a tiny tiny dataset, but it's more than what I used last time. \n\nThis should be doable on my own hardware, it's good too because I can hopefully see some kind of improvements before I jump to a bigger dataset which would require me to rent a GPU. But don't worry I still plan on renting a GPU soon, but before I do that I wanna make sure my dataset is as curated and clean as possible. One of the issues I have is cleaning, a lot of these txt files have gibberish mixed in. The scripts I've used for cleaning do work but they're not 100% effective. \n\nI will train this dataset today and it should take around 4-5 hours. Once it's done and I test it, I will give updates. Thank you again to everyone whos checking out my project, I've even had some people even giving me links to OCR resources so Thank you! I hope more people try this out and expirement with they're own datasets. \n\n\n### Training Update \n\nI started training on a 435MB (108 M tokens) corpus, it's going pretty smooth right now. Train loss dropped from 10.9 to 4.9 in the first 2800 iterations. I expect it'll take around 8 or 9 hours to complete. I'll post another update once it's done.\n\n## July 17th, 2025 2:13AM\n\nThe training is done for the second model, it took my 4060 around 8 hours and 40 minutes (3,900 iters/hr) for 33,000 iters (5 epochs). Final train loss was 3.73. The outputs were suprisingly good it genuinely generates coherent 19th century style sentences now. \n\n## July 28th, 2025 \n\nI've gone ahead and uploaded v0.5 to Hugging Face, [Check it out](https://huggingface.co/haykgrigorian/TimeCapsuleLLM) if youd like. You can now download my repo and run it locally. Unfortunately nanoGPT doesn't work natively with HuggingFace, so you'll have to download and run the model locally. \n\nAlso I will begin curating data for my next training run, I believe I'll need maybe 5-10x more data to achieve reasoning capabilities. \n\n\n# V0 Model Behavior & Limitations \n\nEarly prompts show the model responding with 1800's language and behavior. For example, I prompted it with \"Who art Henry?\" and it replied \"I know that man, I have did not a black, the storm.\" and yeah that sentence makes no sense but the LLM is recognizing I'm asking about a person. ",
    "ContentSha": "yAiPsWv+OgjAGuQjWuzA/MnKkx9kDkKPzTOcVSkDPLo=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "<div align=\"right\">\n  <details>\n    <summary >🌐 언어</summary>\n    <div>\n      <div align=\"center\">\n        <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en\">English</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN\">简体中文</a>\n        | <a href=\"#\" title=\"Coming soon\">繁體中文 (곧 제공 예정)</a> |\n        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja\">日本語</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko\">한국어</a>\n        | <a href=\"#\" title=\"Coming soon\">हिन्दी (곧 제공 예정)</a> |\n        | <a href=\"#\" title=\"Coming soon\">ไทย (곧 제공 예정)</a> |\n        | <a href=\"#\" title=\"Coming soon\">Français (곧 제공 예정)</a>\n        | <a href=\"#\" title=\"Coming soon\">Deutsch (곧 제공 예정)</a>\n        | <a href=\"#\" title=\"Coming soon\">Español (곧 제공 예정)</a>\n        | <a href=\"#\" title=\"Coming soon\">Italiano (곧 제공 예정)</a>\n        | <a href=\"#\" title=\"Coming soon\">Русский (곧 제공 예정)</a>\n        | <a href=\"#\" title=\"Coming soon\">Português (곧 제공 예정)</a>\n        | <a href=\"#\" title=\"Coming soon\">Nederlands (곧 제공 예정)</a>\n        | <a href=\"#\" title=\"Coming soon\">Polski (곧 제공 예정)</a>\n        | <a href=\"#\" title=\"Coming soon\">العربية (곧 제공 예정)</a>\n        | <a href=\"#\" title=\"Coming soon\">فارسی (곧 제공 예정)</a>\n        | <a href=\"#\" title=\"Coming soon\">Türkçe (곧 제공 예정)</a>\n        | <a href=\"#\" title=\"Coming soon\">Tiếng Việt (곧 제공 예정)</a>\n        | <a href=\"#\" title=\"Coming soon\">Bahasa Indonesia (곧 제공 예정)</a>\n\n      </div>\n    </div>\n  </details>\n</div>\n\n# TimeCapsule LLM\n특정 시대의 데이터만으로 훈련되어 현대 편향을 줄인 LLM.\n\nAI 모델이 단순히 과거를 흉내내는 것이 아니라 실제로 역사적 관점에 기반한다면 어떨까요?\n\n[Andrej Karpathy의 nanoGPT](https://github.com/karpathy/nanoGPT)를 기반으로 하였으며, 핵심 훈련 스크립트와 모델 구조는 그의 작업입니다.\n\n# 프로젝트 목표\n\nTimeCapsule LLM은 특정 시대에 작성된 텍스트만으로 훈련하는 실험적 프로젝트입니다. 목표는 특정 역사적 시기의 세계관과 언어를 시뮬레이션하는 것입니다.\n\n# 파인튜닝만으로는 부족한 이유\n\n사전 훈련된 모델을 파인튜닝만 하면, 여전히 현대 개념을 알고 있을 것입니다. 물론 완전한 무현대 편향을 이루기는 어렵지만, 최대한 근접하고자 합니다. 현대 편향을 없애려면 처음부터 모델을 훈련해야 합니다.\n\n# 기대 결과\n\n완성되면 이 모델은 현대 개념을 모르고, 훈련받은 범위를 넘어선 추론도 하지 않게 될 것입니다. 현대 개념/어휘를 인식하지 않아야 하며, 현대적 지식을 환각하지 않기를 기대합니다.\n\n# 진행 현황\n\n## 2025년 7월 9일\n\n1800~1850년, 지역: 런던으로 시기를 정했습니다.\n\n텍스트, 책, 문서 목록을 수집했습니다.\n\n현재까지 50개의 txt 파일을 확보했고 곧 NanoGPT 훈련을 시작할 예정입니다.\n\n진행 상황이 있으면 계속 업데이트하겠습니다.\n\n## 2025년 7월 13일\n\nnanoGPT를 187MB의 역사적 텍스트 데이터로 훈련했습니다.\n\n## 2025년 7월 15일\n\n두 번째 훈련을 위해 텍스트 다운로드를 시작했습니다. Internet Archive에서 모든 자료를 받으며, 시기를 1800~1875년으로 확장했습니다. 다양한 텍스트를 얻으려면 Internet Archive의 주제 및 검색 필터로 출판 위치, 시기, 주제를 지정할 수 있습니다.\n\n![검색 필터](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)\n\n## 2025년 7월 16일\n\nInternet Archive에서 약 500개의 txt 파일을 다운로드했고, 공백, 구텐베르크 헤더 등 불필요한 부분을 삭제해 정제한 뒤 약 500MB의 데이터가 남았습니다. 아주 작은 데이터셋이지만 지난번 187MB로 훈련했으니, 이번에 두 번째 모델을 훈련한 뒤에는 출력 결과에 적어도 뚜렷한 차이가 있을 것으로 기대합니다. 이번 모델은 더 일관성 있는 문장 생성이 가능하길 바랍니다. 물론 보장된 것은 아니지만, 지난번보다 데이터가 많으니 희망적입니다.\n\n제 하드웨어로도 충분히 할 수 있습니다. 이 과정에서 더 큰 데이터셋을 시도하기 전에 개선점을 직접 확인할 수 있다는 점도 좋습니다. GPU 대여 전에 데이터셋을 최대한 엄선하고 정제하고 싶습니다. 문제 중 하나는 정제 과정인데, 많은 txt 파일에 의미 없는 글자들이 섞여 있습니다. 사용한 스크립트로 어느 정도 정제가 되지만 100%는 아닙니다.\n\n오늘 이 데이터셋으로 훈련할 예정이며 4~5시간 정도 걸릴 것으로 보입니다. 완료 후 테스트해보고 결과를 공유하겠습니다. 프로젝트에 관심 가져주신 모든 분들께 감사드리며, OCR 자료 링크를 보내주신 분들도 계셔서 정말 감사합니다! 더 많은 분들이 직접 데이터셋으로 실험해보시길 바랍니다.\n\n### 훈련 업데이트\n\n435MB(1억 8백만 토큰) 코퍼스로 훈련을 시작했으며, 현재까지 순조롭습니다. 첫 2800회 반복에서 트레인 로스가 10.9에서 4.9로 떨어졌습니다. 약 8~9시간 후 완료될 것으로 예상합니다. 완료되면 다시 업데이트하겠습니다.\n\n## 2025년 7월 17일 오전 2:13\n\n두 번째 모델 훈련이 끝났습니다. 제 4060 GPU로 약 8시간 40분(시간당 3,900회 반복), 총 33,000회 반복(5에폭)이 소요되었습니다. 최종 트레인 로스는 3.73이었습니다. 출력 결과가 놀라울 정도로 좋아져, 이제 진짜 19세기 스타일의 일관성 있는 문장을 생성합니다.\n\n## 2025년 7월 28일\n\nv0.5 버전을 Hugging Face에 업로드했습니다. [여기서 확인](https://huggingface.co/haykgrigorian/TimeCapsuleLLM)하실 수 있습니다. 저장소를 다운로드해 로컬에서 실행할 수 있습니다. 안타깝게도 nanoGPT는 HuggingFace와 기본적으로 호환되지 않아, 모델을 직접 다운로드해 로컬에서 실행해야 합니다.\n\n또한 다음 훈련을 위해 데이터를 추가로 선별할 계획이며, 추론 능력 향상을 위해 데이터가 5~10배 더 필요할 것으로 생각합니다.\n\n# V0 모델의 동작 및 한계\n\n초기 프롬프트에서는 1800년대의 언어와 행동으로 응답하는 모델이 보입니다. 예를 들어, \"Who art Henry?\"라는 질문에 모델이 \"I know that man, I have did not a black, the storm.\"라고 대답했는데, 문장은 말이 안 되지만 LLM이 인물에 대한 질문임을 인식하고 있음을 알 수 있습니다.\n\n\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "<div align=\"right\">"
      },
      {
        "row": 2,
        "rowsha": "cWgam+tnnXudu7i74+ahMEGk/A9dQS+EwWLAIfi3dHk=",
        "originContent": "<div align=\"right\">",
        "translatedContent": "  <details>"
      },
      {
        "row": 3,
        "rowsha": "orOcu5ARna/hb3RUkj6dBI8pHTM3WHeTvby17l5E0h0=",
        "originContent": "  <details>",
        "translatedContent": "    <summary >🌐 언어</summary>"
      },
      {
        "row": 4,
        "rowsha": "TtgkLzblnvP0q9aAIVXt6s2LczXjy5k+QvHKcU0/5Ms=",
        "originContent": "    <summary >🌐 Language</summary>",
        "translatedContent": "    <div>"
      },
      {
        "row": 5,
        "rowsha": "fZtk4rPTAJEEslnbhSVkHEcPlsctYSzAV7CDPL3rJmA=",
        "originContent": "    <div>",
        "translatedContent": "      <div align=\"center\">"
      },
      {
        "row": 6,
        "rowsha": "9KQxOeJSigvTmGWO+mtnl8kZY9zQfueoy8sk4lYm09Q=",
        "originContent": "      <div align=\"center\">",
        "translatedContent": "        <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en\">English</a>"
      },
      {
        "row": 7,
        "rowsha": "CeOhdpchZBoZSEUDtSE417JEcMBSZw18jeJuHJBKB2Y=",
        "originContent": "        <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en\">English</a>",
        "translatedContent": "        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN\">简体中文</a>"
      },
      {
        "row": 8,
        "rowsha": "ToO7MFa3QrNNljdQWIagsnOPxe8cXuuA2m5msIm+Kbs=",
        "originContent": "        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN\">简体中文</a>",
        "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">繁體中文 (곧 제공 예정)</a> |"
      },
      {
        "row": 9,
        "rowsha": "MRATmWdRMRw0JU4u9h5pMb6GU17lQFgG9v/bpGLr9pM=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">繁體中文 (coming soon)</a> |",
        "translatedContent": "        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja\">日本語</a>"
      },
      {
        "row": 10,
        "rowsha": "GY7LXxG3rk5eFh9itcqM0cTtmHybyjLTf1icB3jN31I=",
        "originContent": "        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja\">日本語</a>",
        "translatedContent": "        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko\">한국어</a>"
      },
      {
        "row": 11,
        "rowsha": "b5TwunGJh+gsAe7aQU3dkfobXF/nknCEta1msDa7XBU=",
        "originContent": "        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko\">한국어</a>",
        "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">हिन्दी (곧 제공 예정)</a> |"
      },
      {
        "row": 12,
        "rowsha": "1/HCgPsVh2ChqMY+k/VVxEWHPRRmWWCjy5nDRibi3mM=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">हिन्दी (coming soon)</a> |",
        "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">ไทย (곧 제공 예정)</a> |"
      },
      {
        "row": 13,
        "rowsha": "3lfEHT+5HYFEvbE5cl+xujQPYjtVmzTifT37iqPTWII=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">ไทย (coming soon)</a> |",
        "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Français (곧 제공 예정)</a>"
      },
      {
        "row": 14,
        "rowsha": "KmG3P0px2E3bt1lU/w3eGop+zeA1j8xL0k280Zd9m2s=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">Français (coming soon)</a>",
        "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Deutsch (곧 제공 예정)</a>"
      },
      {
        "row": 15,
        "rowsha": "CSdHSEXgIs3M2Q/6zIIJ8NbKkZWhydhBqNus94qrPvg=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">Deutsch (coming soon)</a>",
        "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Español (곧 제공 예정)</a>"
      },
      {
        "row": 16,
        "rowsha": "8wz7pDuXc3dk+ZcqZ1jmmh8zh6xN3Wb6qWbCjxAj7dA=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">Español (coming soon)</a>",
        "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Italiano (곧 제공 예정)</a>"
      },
      {
        "row": 17,
        "rowsha": "op/NqIZs7OjCSpNgpXk8RnqDnTegVPyWUQhuQxvTR7U=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">Italiano (coming soon)</a>",
        "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Русский (곧 제공 예정)</a>"
      },
      {
        "row": 18,
        "rowsha": "tAvlfwut/Ad9q1huxc8EREZGv7vYHbrEujzUS8xoaQo=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">Русский (coming soon)</a>",
        "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Português (곧 제공 예정)</a>"
      },
      {
        "row": 19,
        "rowsha": "WhhSpeeCUUAqJiVTS4Fvyc6A2c+24Jnj3MW7XLQuIcI=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">Português (coming soon)</a>",
        "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Nederlands (곧 제공 예정)</a>"
      },
      {
        "row": 20,
        "rowsha": "0yPXPrWh+Vzc6FBE9iiciw5HwpOSmo05HNe36wfTWCI=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">Nederlands (coming soon)</a>",
        "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Polski (곧 제공 예정)</a>"
      },
      {
        "row": 21,
        "rowsha": "mdW6YUUXf5KzI4CwZxrE08ofaLonUOMnJpN3vPR7Y2A=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">Polski (coming soon)</a>",
        "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">العربية (곧 제공 예정)</a>"
      },
      {
        "row": 22,
        "rowsha": "sw1AXxAGQNvn4eSG9enTWNkwKH0yr6LlVtXBH1j9z8s=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">العربية (coming soon)</a>",
        "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">فارسی (곧 제공 예정)</a>"
      },
      {
        "row": 23,
        "rowsha": "I8dh9zmXisU0+CpddA55QQgvujH03J/dEnXgj5aFtQM=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">فارسی (coming soon)</a>",
        "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Türkçe (곧 제공 예정)</a>"
      },
      {
        "row": 24,
        "rowsha": "7VFv8o6de72ciJrbh3mctfrEgCJhNvuKGWJNOmCaPdM=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">Türkçe (coming soon)</a>",
        "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Tiếng Việt (곧 제공 예정)</a>"
      },
      {
        "row": 25,
        "rowsha": "C+XRvFz/D3o9/JyPqwitsxtskFZleJC/oFUr4SEeiHA=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">Tiếng Việt (coming soon)</a>",
        "translatedContent": "        | <a href=\"#\" title=\"Coming soon\">Bahasa Indonesia (곧 제공 예정)</a>"
      },
      {
        "row": 26,
        "rowsha": "ntGI5B+n9x96pV3ZG5GG83nmocQbxTJjKY7VVwa6Rq8=",
        "originContent": "        | <a href=\"#\" title=\"Coming soon\">Bahasa Indonesia (coming soon)</a>",
        "translatedContent": ""
      },
      {
        "row": 27,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "      </div>"
      },
      {
        "row": 28,
        "rowsha": "0OM5wNEm0TO56MEBvQzL7AUZM7/3OpgIeqRf2zFre3Q=",
        "originContent": "      </div>",
        "translatedContent": "    </div>"
      },
      {
        "row": 29,
        "rowsha": "fcjTfY+fs8YnY5slBs1sZvWPAqEQR7tzaBDO54skkGQ=",
        "originContent": "    </div>",
        "translatedContent": "  </details>"
      },
      {
        "row": 30,
        "rowsha": "+fQNH2ldI7UM/rqRscP3hUSWAmw1HvQ2wEKDN8JagT0=",
        "originContent": "  </details>",
        "translatedContent": "</div>"
      },
      {
        "row": 31,
        "rowsha": "qsMmUbEPVnxGG5tPJV1vsfpoWbU2jYvZpRr5IKshzyM=",
        "originContent": "</div>",
        "translatedContent": ""
      },
      {
        "row": 32,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "# TimeCapsule LLM"
      },
      {
        "row": 33,
        "rowsha": "VRGjp0FtfvQ89lbX/wJLis2ypCRtNJwe8ViIi29+Rko=",
        "originContent": "# TimeCapsule LLM",
        "translatedContent": "특정 시대의 데이터만으로 훈련되어 현대 편향을 줄인 LLM."
      },
      {
        "row": 34,
        "rowsha": "XGlykErifWX9oIzV4ZXDc4AUnsuesz8LvpruG76e6uY=",
        "originContent": "An LLM trained only on data from certain time periods to reduce modern bias.",
        "translatedContent": ""
      },
      {
        "row": 35,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "AI 모델이 단순히 과거를 흉내내는 것이 아니라 실제로 역사적 관점에 기반한다면 어떨까요?"
      },
      {
        "row": 36,
        "rowsha": "06wDXO9Un3ot9kUKAGg7CaRsIVSkfS1d2m+EQ6HOFog=",
        "originContent": "Imagine if an AI model didnt just pretend to be historical but actually was.",
        "translatedContent": ""
      },
      {
        "row": 37,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "[Andrej Karpathy의 nanoGPT](https://github.com/karpathy/nanoGPT)를 기반으로 하였으며, 핵심 훈련 스크립트와 모델 구조는 그의 작업입니다."
      },
      {
        "row": 38,
        "rowsha": "2773v/qIXSAsW4pN2HtYcVltfzG1vzgVbHgfjStBQIY=",
        "originContent": "Built on [nanoGPT by Andrej Karpathy](https://github.com/karpathy/nanoGPT) Core training scripts and model architecture are his work. ",
        "translatedContent": ""
      },
      {
        "row": 39,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "# 프로젝트 목표"
      },
      {
        "row": 40,
        "rowsha": "wITJJBD/4abiy4E37iMdOcGmifkmz4dALLyk6AhA1kc=",
        "originContent": "# Project Goals ",
        "translatedContent": ""
      },
      {
        "row": 41,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "TimeCapsule LLM은 특정 시대에 작성된 텍스트만으로 훈련하는 실험적 프로젝트입니다. 목표는 특정 역사적 시기의 세계관과 언어를 시뮬레이션하는 것입니다."
      },
      {
        "row": 42,
        "rowsha": "LlW7r/H8NhftFgGAHce1f4KThGgsoT8aJ88/IsiLntc=",
        "originContent": "TimeCapsule LLM is an expirimental project that will only be trained on texts written during certain time periods. The goal is to simulate the worldview and language of specific historical eras.",
        "translatedContent": ""
      },
      {
        "row": 43,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "# 파인튜닝만으로는 부족한 이유"
      },
      {
        "row": 44,
        "rowsha": "obYFMCTDj8qHZGo0BQtA2AlwA8JgNcjDK9WlMRI4eq8=",
        "originContent": "# Why fine tuning isn't enough ",
        "translatedContent": ""
      },
      {
        "row": 45,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "사전 훈련된 모델을 파인튜닝만 하면, 여전히 현대 개념을 알고 있을 것입니다. 물론 완전한 무현대 편향을 이루기는 어렵지만, 최대한 근접하고자 합니다. 현대 편향을 없애려면 처음부터 모델을 훈련해야 합니다."
      },
      {
        "row": 46,
        "rowsha": "yNEBOKV/RnG7CvDjiWhkXKK6vqbwki9QKC+Zs+8PzbM=",
        "originContent": "If you just fine tune a pre-trained model, your LLM is still gonna know modern concepts. Of course achieving zero modern bias is difficult but I want to get as close as possible to this. Getting no modern bias requires training a model from scratch.",
        "translatedContent": ""
      },
      {
        "row": 47,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "# 기대 결과"
      },
      {
        "row": 48,
        "rowsha": "SdJkrN/DUD4+aOCh9lfDM3AAqMxlyukDfye/nzXzxN0=",
        "originContent": "# Expected outcomes ",
        "translatedContent": ""
      },
      {
        "row": 49,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "완성되면 이 모델은 현대 개념을 모르고, 훈련받은 범위를 넘어선 추론도 하지 않게 될 것입니다. 현대 개념/어휘를 인식하지 않아야 하며, 현대적 지식을 환각하지 않기를 기대합니다."
      },
      {
        "row": 50,
        "rowsha": "bsSMnG6qSBf/pVtCQNGFlaKye8GxBKV660amPA/pINE=",
        "originContent": "Hopefully when finished, this model will not know modern concepts and will not be able to reason beyond what it's been trained on. It shouldnt recognize modern concepts/vocab and I hope it doesn't hallucinate modern knowledge.",
        "translatedContent": ""
      },
      {
        "row": 51,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "# 진행 현황"
      },
      {
        "row": 52,
        "rowsha": "8EWRxPaogE2BaXxVJE1VFNAXNdS6KUYPLDFN8xlQ9LE=",
        "originContent": "# Progress Updates",
        "translatedContent": ""
      },
      {
        "row": 53,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## 2025년 7월 9일"
      },
      {
        "row": 54,
        "rowsha": "oq91hnNV5WwmrEF0amya8kSN7gu21MN5nOcR2dPRBZ0=",
        "originContent": "## July 9th, 2025",
        "translatedContent": ""
      },
      {
        "row": 55,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "1800~1850년, 지역: 런던으로 시기를 정했습니다."
      },
      {
        "row": 56,
        "rowsha": "yU3u8taDdAaD23tJB9+n/2wmry0GfF+KXqADT4YLuJ8=",
        "originContent": "I've set my time period for 1800-1850 and region: London ",
        "translatedContent": ""
      },
      {
        "row": 57,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "텍스트, 책, 문서 목록을 수집했습니다."
      },
      {
        "row": 58,
        "rowsha": "uQe95shOfOi8NA0M2/CQCXlOjNsiSmGrt5dZbeP4ANs=",
        "originContent": "I've gathered a list of texts, books, documents ",
        "translatedContent": ""
      },
      {
        "row": 59,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "현재까지 50개의 txt 파일을 확보했고 곧 NanoGPT 훈련을 시작할 예정입니다."
      },
      {
        "row": 60,
        "rowsha": "i9Kzka7MMa5yKjfdslauZFzKk+gAcnyILwscFoaepYs=",
        "originContent": "So far I've gotten 50 as txt files and will begin training NanoGPT soon ",
        "translatedContent": ""
      },
      {
        "row": 61,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "진행 상황이 있으면 계속 업데이트하겠습니다."
      },
      {
        "row": 62,
        "rowsha": "Wov5RgnyTA0P0gtJKLL0GTcSf7t8WrgFcAzDufE5Xh4=",
        "originContent": "Will update this as long as progress is made",
        "translatedContent": ""
      },
      {
        "row": 63,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## 2025년 7월 13일"
      },
      {
        "row": 64,
        "rowsha": "FSOOW2G6pyPozg+3u76To6E4Pthd9lRoZE396fwY2I4=",
        "originContent": "## July 13th, 2025",
        "translatedContent": ""
      },
      {
        "row": 65,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "nanoGPT를 187MB의 역사적 텍스트 데이터로 훈련했습니다."
      },
      {
        "row": 66,
        "rowsha": "GszU76q+4dgDkDO4uNZpx/9WhQTTCZlq4VYIt0ZdgD0=",
        "originContent": "Trained nanoGPT with 187MB of historial text data. ",
        "translatedContent": ""
      },
      {
        "row": 67,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## 2025년 7월 15일"
      },
      {
        "row": 68,
        "rowsha": "pGPL3z/t2hDtXa67ubNgmRC/+b4O2Yp/0ff3R/9mraE=",
        "originContent": "## July 15th, 2025",
        "translatedContent": ""
      },
      {
        "row": 69,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "두 번째 훈련을 위해 텍스트 다운로드를 시작했습니다. Internet Archive에서 모든 자료를 받으며, 시기를 1800~1875년으로 확장했습니다. 다양한 텍스트를 얻으려면 Internet Archive의 주제 및 검색 필터로 출판 위치, 시기, 주제를 지정할 수 있습니다."
      },
      {
        "row": 70,
        "rowsha": "+jwjqBw9Cr+lRnmxUzCQ0SnVBfXYeJDycuYf/p0JJgg=",
        "originContent": "I started downloading texts for the second training run. I'm getting everything from Internet Archive and I've expanded the time period to 1800-1875. To get a diverse range of texts, you can use subject and search filters for publication location, time period and subjects on Internet Archive. ",
        "translatedContent": ""
      },
      {
        "row": 71,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "![검색 필터](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)"
      },
      {
        "row": 72,
        "rowsha": "XE9Xts6Q8wsZVZHg8uD/1ZXBQ/j2uFLsR9HwsiaqMds=",
        "originContent": "![Search Filters](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)",
        "translatedContent": ""
      },
      {
        "row": 73,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## 2025년 7월 16일"
      },
      {
        "row": 74,
        "rowsha": "iREIjirFxs+ic0QmjQG1FQYKHC5brQaZ5JjPaEto+lU=",
        "originContent": "## July 16th, 2025",
        "translatedContent": ""
      },
      {
        "row": 75,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "Internet Archive에서 약 500개의 txt 파일을 다운로드했고, 공백, 구텐베르크 헤더 등 불필요한 부분을 삭제해 정제한 뒤 약 500MB의 데이터가 남았습니다. 아주 작은 데이터셋이지만 지난번 187MB로 훈련했으니, 이번에 두 번째 모델을 훈련한 뒤에는 출력 결과에 적어도 뚜렷한 차이가 있을 것으로 기대합니다. 이번 모델은 더 일관성 있는 문장 생성이 가능하길 바랍니다. 물론 보장된 것은 아니지만, 지난번보다 데이터가 많으니 희망적입니다."
      },
      {
        "row": 76,
        "rowsha": "c1Ww8CUkqpg5TNm17QY7m130dQycuSFaAia2gfx/uLw=",
        "originContent": "I downloaded around 500 txt files from Internet Archive and after cleaning them (just deleting whitespaces, Gutenberg headers, etc) I have around 500MB of data. It's a tiny dataset but last time I trained off of 187MB so there should be at least some kind of noticable difference in the output after I train the second model. I'm hoping this model can at least produce more coherent sentences that kind of make sense. It's not a guarantee of course since this is still a tiny tiny dataset, but it's more than what I used last time. ",
        "translatedContent": ""
      },
      {
        "row": 77,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "제 하드웨어로도 충분히 할 수 있습니다. 이 과정에서 더 큰 데이터셋을 시도하기 전에 개선점을 직접 확인할 수 있다는 점도 좋습니다. GPU 대여 전에 데이터셋을 최대한 엄선하고 정제하고 싶습니다. 문제 중 하나는 정제 과정인데, 많은 txt 파일에 의미 없는 글자들이 섞여 있습니다. 사용한 스크립트로 어느 정도 정제가 되지만 100%는 아닙니다."
      },
      {
        "row": 78,
        "rowsha": "h/hyxvgOlOm5er9sn3CL2wmktMoq2q+qZi5Vi7upXGI=",
        "originContent": "This should be doable on my own hardware, it's good too because I can hopefully see some kind of improvements before I jump to a bigger dataset which would require me to rent a GPU. But don't worry I still plan on renting a GPU soon, but before I do that I wanna make sure my dataset is as curated and clean as possible. One of the issues I have is cleaning, a lot of these txt files have gibberish mixed in. The scripts I've used for cleaning do work but they're not 100% effective. ",
        "translatedContent": ""
      },
      {
        "row": 79,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "오늘 이 데이터셋으로 훈련할 예정이며 4~5시간 정도 걸릴 것으로 보입니다. 완료 후 테스트해보고 결과를 공유하겠습니다. 프로젝트에 관심 가져주신 모든 분들께 감사드리며, OCR 자료 링크를 보내주신 분들도 계셔서 정말 감사합니다! 더 많은 분들이 직접 데이터셋으로 실험해보시길 바랍니다."
      },
      {
        "row": 80,
        "rowsha": "8LYxYjHwrXnU59N+13Fd9m653Tgyom3yQQAWGIrSPSc=",
        "originContent": "I will train this dataset today and it should take around 4-5 hours. Once it's done and I test it, I will give updates. Thank you again to everyone whos checking out my project, I've even had some people even giving me links to OCR resources so Thank you! I hope more people try this out and expirement with they're own datasets. ",
        "translatedContent": ""
      },
      {
        "row": 81,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "### 훈련 업데이트"
      },
      {
        "row": 82,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 83,
        "rowsha": "mNjt3ebwxfwyPM2/AK7E/GDTFM6i95HG8GZ/8TZs9EA=",
        "originContent": "### Training Update ",
        "translatedContent": "435MB(1억 8백만 토큰) 코퍼스로 훈련을 시작했으며, 현재까지 순조롭습니다. 첫 2800회 반복에서 트레인 로스가 10.9에서 4.9로 떨어졌습니다. 약 8~9시간 후 완료될 것으로 예상합니다. 완료되면 다시 업데이트하겠습니다."
      },
      {
        "row": 84,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 85,
        "rowsha": "RxsWSGTgM12Md9v3+GBLg3PyoUxQ9kl1AglpanRmZqE=",
        "originContent": "I started training on a 435MB (108 M tokens) corpus, it's going pretty smooth right now. Train loss dropped from 10.9 to 4.9 in the first 2800 iterations. I expect it'll take around 8 or 9 hours to complete. I'll post another update once it's done.",
        "translatedContent": "## 2025년 7월 17일 오전 2:13"
      },
      {
        "row": 86,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 87,
        "rowsha": "CLW3lPsO6v+tT6iir338rG5+IYXM+JqRoag5w7K8exE=",
        "originContent": "## July 17th, 2025 2:13AM",
        "translatedContent": "두 번째 모델 훈련이 끝났습니다. 제 4060 GPU로 약 8시간 40분(시간당 3,900회 반복), 총 33,000회 반복(5에폭)이 소요되었습니다. 최종 트레인 로스는 3.73이었습니다. 출력 결과가 놀라울 정도로 좋아져, 이제 진짜 19세기 스타일의 일관성 있는 문장을 생성합니다."
      },
      {
        "row": 88,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 89,
        "rowsha": "Q0uM34dBNqytALNUZSPxoBQZT3LxqlwyioEi3nTshXQ=",
        "originContent": "The training is done for the second model, it took my 4060 around 8 hours and 40 minutes (3,900 iters/hr) for 33,000 iters (5 epochs). Final train loss was 3.73. The outputs were suprisingly good it genuinely generates coherent 19th century style sentences now. ",
        "translatedContent": "## 2025년 7월 28일"
      },
      {
        "row": 90,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 91,
        "rowsha": "13xzxThQO3iEGB5PU51V1kYgK85//yEFbCkwtu07a2U=",
        "originContent": "## July 28th, 2025 ",
        "translatedContent": "v0.5 버전을 Hugging Face에 업로드했습니다. [여기서 확인](https://huggingface.co/haykgrigorian/TimeCapsuleLLM)하실 수 있습니다. 저장소를 다운로드해 로컬에서 실행할 수 있습니다. 안타깝게도 nanoGPT는 HuggingFace와 기본적으로 호환되지 않아, 모델을 직접 다운로드해 로컬에서 실행해야 합니다."
      },
      {
        "row": 92,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 93,
        "rowsha": "R6HmXRpR+7izGCE6kI9wyELs4kYvAtLMWO75nOXAf1M=",
        "originContent": "I've gone ahead and uploaded v0.5 to Hugging Face, [Check it out](https://huggingface.co/haykgrigorian/TimeCapsuleLLM) if youd like. You can now download my repo and run it locally. Unfortunately nanoGPT doesn't work natively with HuggingFace, so you'll have to download and run the model locally. ",
        "translatedContent": "또한 다음 훈련을 위해 데이터를 추가로 선별할 계획이며, 추론 능력 향상을 위해 데이터가 5~10배 더 필요할 것으로 생각합니다."
      },
      {
        "row": 94,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 95,
        "rowsha": "3/uUOOHuCoxdKkQ6c+1J96AfmOsQ59aZxo/FYGK9OlI=",
        "originContent": "Also I will begin curating data for my next training run, I believe I'll need maybe 5-10x more data to achieve reasoning capabilities. ",
        "translatedContent": "# V0 모델의 동작 및 한계"
      },
      {
        "row": 96,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 97,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "초기 프롬프트에서는 1800년대의 언어와 행동으로 응답하는 모델이 보입니다. 예를 들어, \"Who art Henry?\"라는 질문에 모델이 \"I know that man, I have did not a black, the storm.\"라고 대답했는데, 문장은 말이 안 되지만 LLM이 인물에 대한 질문임을 인식하고 있음을 알 수 있습니다."
      },
      {
        "row": 98,
        "rowsha": "YbYVsndAe75CgxPU/J34HsXOaqcQlIjTCm06o4eVQIg=",
        "originContent": "# V0 Model Behavior & Limitations ",
        "translatedContent": ""
      },
      {
        "row": 99,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 100,
        "rowsha": "OCh97kyOITXqFpKyZsol6voS+nrzc9n9flpDpNyf35w=",
        "originContent": "Early prompts show the model responding with 1800's language and behavior. For example, I prompted it with \"Who art Henry?\" and it replied \"I know that man, I have did not a black, the storm.\" and yeah that sentence makes no sense but the LLM is recognizing I'm asking about a person. ",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 2,
    "Content": "\n![TimeLockLLM Sample Output](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)\n\nThere is no mention of modern concetps, outputs contain mostly words and phrasing from the 1800's.\n\nIt still needs alot of work, training off of 187MB will not give you a model that produces text with complex reasoning. \n\nRight now it produces sentences that lack full sentence structure and overall just make no sense but this is normal for the training size. \n\n# V0.5 Model Behavior & Limitations\n\nThis is a nice improvement compared to the last model. The writing style and vocab is Victorian and almost every sentence is grammatically correct with proper punctuation. And again this is trained from scratch so it sticks to 1800's subjects. \n\n![TimeLockLLM Sample Output](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)\n\nThere are a lot of factual hallucinations. A lot (like 100%) of the details (dates, events, historical figures)  are made up. Also the sentences don't really have connections to each other, sometimes maybe 2 sentences will relate to each other but beyond that they dont. Another issue is sometimes a stray “Digitized by Google” footer shows up, so the next time I train I really have to make sure the texts are cleaned well. Overall I'm very happy with the results, it's nowhere near an LLM yet but definitely a sentence generator. \n\nI'm learning a lot and will start figuring out what I need to do better in the coming weeks. I will upload files soon! \n\n# Upcoming Plans \n\n(Completed) I'm going to start work on version 0.5 , instead of training using 50 books, I'll train using ideally 500-600. Right now I'm training nanoGPT using books from 1800-1850 and specifically from London. There is some challeneges like making sure the books I find are not updated or have modern interpretations but untouched books published withtin my chosen time period.\n\nI want to train a new model (v1) with a much larger corpus, maybe 5-10x larger than the one I used for v0.5. My goal is to see if I can get reasoning abilities to emerge from Selective Temporal Training alone, this will be a more difficult task and I'm not even entirely sure if it's possible due to the fact that there is historical data limitations. In the upcoming weeks I will try to curate enough data for a 5-10GB corpus. I believe If I can get clean clean high quality data and rent a GPU, there will be progress.\n\n# How to Use This Project \n\nThis project focuses mostly on curating historical data, preparing it for training and building a tokenizer. I am not going to cover the full LLM training process, for that refer to nanoGPT by Andrej Karpathy.\n\n# Step 1: Gather and Prepare Historical Texts \n\nCollect .txt files of public domain books, documents, etc from your chosen time period (e.g., London 1800-1850)\n\nYou can use download_texts_improved.py to download books for you if you need to.\n\nClean the text files using a script or manually remove headers/footer from Project Gutenberg, Modern annotations or things like OCR errors.\n\nprepare_dataset.py should work fine.\n\n# Step 2: Build a Custom Tokenizer\n\nRun train_tokenizer.py or train_tokenizer_hf.py on the cleaned data.\nThis will give you vocab.json and merges.txt\n\nThes files define vocab and merge rules for your model\n\n# Step 3: Train Your Model (nanoGPT) \n\nRefer to [nanoGPT by Andrej Karpathy](https://github.com/karpathy/nanoGPT) for the training process.\n\nYou can train a different LLM if you want, but I used nanoGPT \n\n# FAQ\n\n## What is Selective Temporal Training ?\n\nSelective Temporal Training (STT) is a machine learning methodology where all training data is specifically curated to fall within a specific historical time period. It's done in order to model the language and knowledge of that era without influence from modern concepts. For example, the current model I have now (v0.5) is trained on data exclusively from 1800-1875, it's not fine tuned but trained from scratch resulting in output that reflects the linguistic style and historical context of that time period.\n\n## Why not just use fine-tuning or LoRA?\n\nFor this project I'm trying to create a language model that is unclouded from modern bias. If I fine-tune something like GPT-2, it's already pre-trained and that information won't go away. If I train from scratch the language model won't pretend to be old, it just will be. The Goal for this project right now is to create something can reason exclusively using knowledge from London books published between 1800 and 1850.\n\n## What kind of data did you use for training?\n\nI'm using books, legal documents, newspapers, and other writings from 1800–1850 London. The list I linked has like 200 but for the first training I just used 50 files about ~187 MB. You can view a list of the documents:\nhttps://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt\n\n## How large is the Version 0 model ?\n\nThis model is very small right now, I'm just doing this for fun and following a strict training rule of no modern sources. It has almost 16 million parameters but I'm gonna start gathering more old texts to begin another model training. Will give updates as I go.\n\n## Training Specs ? \n\nGPU: Geforce rtx 4060\nCPU: i5-13400F \nRam: 16GB DDR5.\n",
    "ContentSha": "i0Mn+YIkqM2K73Eiy+Ca+RXnfQ1CRfs/qgP3DyFRazk=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "\n![TimeLockLLM 샘플 출력](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)\n\n현대 개념에 대한 언급이 없으며, 출력물은 대부분 1800년대의 단어와 어투를 포함하고 있습니다.\n\n아직도 많은 개선이 필요하며, 187MB로 학습하면 복잡한 추론을 할 수 있는 모델이 나오지 않습니다.\n\n현재는 완전한 문장 구조가 부족하고 전반적으로 말이 되지 않는 문장들을 생성하지만, 이런 현상은 학습 데이터 크기 상 정상입니다.\n\n# V0.5 모델 동작 및 한계\n\n이전 모델에 비해 상당히 개선되었습니다. 문체와 어휘가 빅토리아 시대 스타일이고, 거의 모든 문장이 문법적으로 맞고 구두점도 적절합니다. 그리고 이 모델은 처음부터 학습했기 때문에 1800년대 주제에만 집중합니다.\n\n![TimeLockLLM 샘플 출력](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)\n\n사실과 다른 환각이 많습니다. 세부사항(날짜, 사건, 역사적 인물 등) 대부분(100%)이 만들어진 내용입니다. 또한 문장들끼리 연결이 잘 안 되고, 가끔 2개 문장이 서로 연관되어 있지만 그 이상은 없습니다. 또 다른 문제로는 \"Digitized by Google\"과 같은 하단 문구가 종종 출력되는데, 다음 학습 때는 텍스트를 더 철저히 정리해야 할 것 같습니다. 전반적으로 결과에 매우 만족하며, 아직 LLM 수준은 아니지만 문장 생성기로서의 역할은 충분합니다.\n\n많은 것을 배우고 있고, 앞으로 몇 주 동안 더 나은 방법을 고민해볼 예정입니다. 곧 파일도 업로드하겠습니다!\n\n# 앞으로의 계획\n\n(완료) 0.5 버전 작업을 시작할 예정이며, 50권 대신 500~600권의 책을 활용해 학습할 계획입니다. 현재는 1800~1850년 런던에서 출판된 책들로 nanoGPT를 학습 중입니다. 도전 과제는 찾은 책들이 현대적으로 수정되었거나 해석된 버전이 아닌, 제가 정한 기간에 출판된 원본임을 확실히 해야 한다는 점입니다.\n\n더 큰 말뭉치(코퍼스)로 새로운 모델(v1)을 학습하고 싶습니다. v0.5에 사용한 데이터보다 5~10배 많은 분량이 목표입니다. Selective Temporal Training만으로 추론 능력이 생기는지 실험해보고 싶고, 이는 역사적 데이터의 한계 때문에 어려울 수 있지만 도전해볼 생각입니다. 앞으로 몇 주간 5~10GB의 데이터를 큐레이션하려고 합니다. 깨끗하고 고품질의 데이터를 확보하고 GPU만 대여하면 진전이 있을 것이라 믿습니다.\n\n# 프로젝트 사용법\n\n이 프로젝트는 주로 역사적 데이터를 큐레이션하고, 학습용으로 준비하며, 토크나이저를 만드는 과정에 초점을 둡니다. 전체 LLM 학습 과정은 다루지 않으니, 해당 부분은 Andrej Karpathy의 nanoGPT를 참고하세요.\n\n# 1단계: 역사적 텍스트 수집 및 준비\n\n선택한 기간(예: 1800~1850년 런던)의 퍼블릭 도메인 책, 문서 등의 .txt 파일을 수집합니다.\n\n필요하다면 download_texts_improved.py를 사용하여 책을 다운로드할 수 있습니다.\n\n스크립트로 또는 수작업으로 Project Gutenberg의 머리말/꼬리말, 현대 주석, OCR 오류 등을 제거하여 텍스트 파일을 정리합니다.\n\nprepare_dataset.py를 사용하면 충분합니다.\n\n# 2단계: 커스텀 토크나이저 빌드\n\n정리한 데이터에 대해 train_tokenizer.py 또는 train_tokenizer_hf.py를 실행합니다.\n이 과정을 통해 vocab.json과 merges.txt가 생성됩니다.\n\n이 파일들은 모델의 어휘 및 병합 규칙을 정의합니다.\n\n# 3단계: 모델 학습(nanoGPT)\n\n학습 과정은 [Andrej Karpathy의 nanoGPT](https://github.com/karpathy/nanoGPT)를 참고하세요.\n\n원한다면 다른 LLM을 써도 되지만 저는 nanoGPT를 사용했습니다.\n\n# FAQ\n\n## Selective Temporal Training이란?\n\nSelective Temporal Training(STT)은 모든 학습 데이터를 특정한 역사적 시기에만 한정해 엄선하는 기계학습 방법론입니다. 이는 현대 개념의 영향을 받지 않고 해당 시대의 언어와 지식을 모델링하기 위해서입니다. 예를 들어, 현재의 v0.5 모델은 1800~1875년 데이터만으로 처음부터 학습되었으며, 그 결과 해당 시대의 언어 스타일과 역사적 맥락을 반영합니다.\n\n## 그냥 파인튜닝이나 LoRA를 쓰지 않는 이유는?\n\n이 프로젝트의 목표는 현대적 편견에서 벗어난 언어모델을 만드는 것입니다. 예를 들어 GPT-2를 파인튜닝하면 이미 사전 학습된 정보는 지워지지 않습니다. 처음부터 학습하면 언어 모델이 옛것인 척하는 것이 아니라 실제로 옛날 모델이 되는 것입니다. 지금의 목표는 1800~1850년 런던에서 출판된 책의 지식만으로 추론할 수 있는 모델을 만드는 것입니다.\n\n## 학습에 사용한 데이터는 어떤 종류입니까?\n\n1800~1850년 런던의 책, 법률 문서, 신문, 기타 문서를 사용하고 있습니다. 링크된 목록에는 약 200개가 있지만 첫 학습에는 50개 파일, 약 187MB만 사용했습니다. 문서 목록은 아래에서 볼 수 있습니다:\nhttps://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt\n\n## 0버전 모델의 크기는?\n\n이 모델은 아직 매우 작으며, 재미 삼아 현대 소스 없이 엄격한 학습 규칙을 따르고 있습니다. 현재 파라미터 수는 약 1,600만 개이며, 더 많은 옛날 텍스트를 수집해 다음 모델 학습을 시작할 예정입니다. 진행 상황은 계속 업데이트하겠습니다.\n\n## 학습 사양?\n\nGPU: Geforce rtx 4060\nCPU: i5-13400F\nRam: 16GB DDR5.\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 2,
        "rowsha": "yKIR0teTc66wVDG+jdIyNmAzItXb2JH2ld3D7tm4qnM=",
        "originContent": "![TimeLockLLM Sample Output](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)",
        "translatedContent": "![TimeLockLLM 샘플 출력](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)"
      },
      {
        "row": 3,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 4,
        "rowsha": "/GR84OQ/Xp3d+Yv3/vVIIQFyl8ExKQLgjT1Go5rPplE=",
        "originContent": "There is no mention of modern concetps, outputs contain mostly words and phrasing from the 1800's.",
        "translatedContent": "현대 개념에 대한 언급이 없으며, 출력물은 대부분 1800년대의 단어와 어투를 포함하고 있습니다."
      },
      {
        "row": 5,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 6,
        "rowsha": "aojQk3KjkX8shkLnJSZvUZBo0vk4tKDT9aysMfY9yK4=",
        "originContent": "It still needs alot of work, training off of 187MB will not give you a model that produces text with complex reasoning. ",
        "translatedContent": "아직도 많은 개선이 필요하며, 187MB로 학습하면 복잡한 추론을 할 수 있는 모델이 나오지 않습니다."
      },
      {
        "row": 7,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 8,
        "rowsha": "SzQ3fs08W+JGBPGw6+aEzLsm/vGfQNDJ6YZnfFE0F4I=",
        "originContent": "Right now it produces sentences that lack full sentence structure and overall just make no sense but this is normal for the training size. ",
        "translatedContent": "현재는 완전한 문장 구조가 부족하고 전반적으로 말이 되지 않는 문장들을 생성하지만, 이런 현상은 학습 데이터 크기 상 정상입니다."
      },
      {
        "row": 9,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 10,
        "rowsha": "eeaXlcgOJhwk4liDhaA6/6S4URmAdV8vj9hlqGr3LFw=",
        "originContent": "# V0.5 Model Behavior & Limitations",
        "translatedContent": "# V0.5 모델 동작 및 한계"
      },
      {
        "row": 11,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 12,
        "rowsha": "tII9OX/M7KpPoQKPIphmQBWtsSVucUevvTnzVsTXcaI=",
        "originContent": "This is a nice improvement compared to the last model. The writing style and vocab is Victorian and almost every sentence is grammatically correct with proper punctuation. And again this is trained from scratch so it sticks to 1800's subjects. ",
        "translatedContent": "이전 모델에 비해 상당히 개선되었습니다. 문체와 어휘가 빅토리아 시대 스타일이고, 거의 모든 문장이 문법적으로 맞고 구두점도 적절합니다. 그리고 이 모델은 처음부터 학습했기 때문에 1800년대 주제에만 집중합니다."
      },
      {
        "row": 13,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 14,
        "rowsha": "8DhXpgpVtg05XdyplRHf49EFOQNCJVzXA9RpmJQ+y9U=",
        "originContent": "![TimeLockLLM Sample Output](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)",
        "translatedContent": "![TimeLockLLM 샘플 출력](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)"
      },
      {
        "row": 15,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 16,
        "rowsha": "YgdLunUOAWWHsq+eAfyWcDz1fCc0rCcobnRpEXHoK94=",
        "originContent": "There are a lot of factual hallucinations. A lot (like 100%) of the details (dates, events, historical figures)  are made up. Also the sentences don't really have connections to each other, sometimes maybe 2 sentences will relate to each other but beyond that they dont. Another issue is sometimes a stray “Digitized by Google” footer shows up, so the next time I train I really have to make sure the texts are cleaned well. Overall I'm very happy with the results, it's nowhere near an LLM yet but definitely a sentence generator. ",
        "translatedContent": "사실과 다른 환각이 많습니다. 세부사항(날짜, 사건, 역사적 인물 등) 대부분(100%)이 만들어진 내용입니다. 또한 문장들끼리 연결이 잘 안 되고, 가끔 2개 문장이 서로 연관되어 있지만 그 이상은 없습니다. 또 다른 문제로는 \"Digitized by Google\"과 같은 하단 문구가 종종 출력되는데, 다음 학습 때는 텍스트를 더 철저히 정리해야 할 것 같습니다. 전반적으로 결과에 매우 만족하며, 아직 LLM 수준은 아니지만 문장 생성기로서의 역할은 충분합니다."
      },
      {
        "row": 17,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 18,
        "rowsha": "V44Ne2sN8v7ZVpRZ5vo7B2aUZFjGppYthg0fjsQAqd0=",
        "originContent": "I'm learning a lot and will start figuring out what I need to do better in the coming weeks. I will upload files soon! ",
        "translatedContent": "많은 것을 배우고 있고, 앞으로 몇 주 동안 더 나은 방법을 고민해볼 예정입니다. 곧 파일도 업로드하겠습니다!"
      },
      {
        "row": 19,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 20,
        "rowsha": "iMvQyl++GAiLpFMT+58v3e9/hb7zXnIJauchG5p986Y=",
        "originContent": "# Upcoming Plans ",
        "translatedContent": "# 앞으로의 계획"
      },
      {
        "row": 21,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 22,
        "rowsha": "OIt/QAbIp2G7kt87dbAIf6Ec0U3HnwjOW6LYRmdf68I=",
        "originContent": "(Completed) I'm going to start work on version 0.5 , instead of training using 50 books, I'll train using ideally 500-600. Right now I'm training nanoGPT using books from 1800-1850 and specifically from London. There is some challeneges like making sure the books I find are not updated or have modern interpretations but untouched books published withtin my chosen time period.",
        "translatedContent": "(완료) 0.5 버전 작업을 시작할 예정이며, 50권 대신 500~600권의 책을 활용해 학습할 계획입니다. 현재는 1800~1850년 런던에서 출판된 책들로 nanoGPT를 학습 중입니다. 도전 과제는 찾은 책들이 현대적으로 수정되었거나 해석된 버전이 아닌, 제가 정한 기간에 출판된 원본임을 확실히 해야 한다는 점입니다."
      },
      {
        "row": 23,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 24,
        "rowsha": "13sW7eekt8pSDdOxr/oKukwKEOVmw/M8xiAJC1VFXZc=",
        "originContent": "I want to train a new model (v1) with a much larger corpus, maybe 5-10x larger than the one I used for v0.5. My goal is to see if I can get reasoning abilities to emerge from Selective Temporal Training alone, this will be a more difficult task and I'm not even entirely sure if it's possible due to the fact that there is historical data limitations. In the upcoming weeks I will try to curate enough data for a 5-10GB corpus. I believe If I can get clean clean high quality data and rent a GPU, there will be progress.",
        "translatedContent": "더 큰 말뭉치(코퍼스)로 새로운 모델(v1)을 학습하고 싶습니다. v0.5에 사용한 데이터보다 5~10배 많은 분량이 목표입니다. Selective Temporal Training만으로 추론 능력이 생기는지 실험해보고 싶고, 이는 역사적 데이터의 한계 때문에 어려울 수 있지만 도전해볼 생각입니다. 앞으로 몇 주간 5~10GB의 데이터를 큐레이션하려고 합니다. 깨끗하고 고품질의 데이터를 확보하고 GPU만 대여하면 진전이 있을 것이라 믿습니다."
      },
      {
        "row": 25,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 26,
        "rowsha": "sO+voevLpUtEbqum6Gntntle+nPVa66c5GATAMLrgf0=",
        "originContent": "# How to Use This Project ",
        "translatedContent": "# 프로젝트 사용법"
      },
      {
        "row": 27,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 28,
        "rowsha": "XVoXr9uzZwN09vboETojEQJe057RBzcMUjXmQRCB/jo=",
        "originContent": "This project focuses mostly on curating historical data, preparing it for training and building a tokenizer. I am not going to cover the full LLM training process, for that refer to nanoGPT by Andrej Karpathy.",
        "translatedContent": "이 프로젝트는 주로 역사적 데이터를 큐레이션하고, 학습용으로 준비하며, 토크나이저를 만드는 과정에 초점을 둡니다. 전체 LLM 학습 과정은 다루지 않으니, 해당 부분은 Andrej Karpathy의 nanoGPT를 참고하세요."
      },
      {
        "row": 29,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 30,
        "rowsha": "kDK7XtqFkTiZvD804yYG4VEojvLrRbdEhmHEBzDAQz4=",
        "originContent": "# Step 1: Gather and Prepare Historical Texts ",
        "translatedContent": "# 1단계: 역사적 텍스트 수집 및 준비"
      },
      {
        "row": 31,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 32,
        "rowsha": "oRoOYaG+mx3SqTnhGcOKeH7W3W4wSRQmhZS4jPGMH8s=",
        "originContent": "Collect .txt files of public domain books, documents, etc from your chosen time period (e.g., London 1800-1850)",
        "translatedContent": "선택한 기간(예: 1800~1850년 런던)의 퍼블릭 도메인 책, 문서 등의 .txt 파일을 수집합니다."
      },
      {
        "row": 33,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 34,
        "rowsha": "EBDhZvTogrL4hqRuH095O7/tXXoy2OEskLQlang/pOA=",
        "originContent": "You can use download_texts_improved.py to download books for you if you need to.",
        "translatedContent": "필요하다면 download_texts_improved.py를 사용하여 책을 다운로드할 수 있습니다."
      },
      {
        "row": 35,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 36,
        "rowsha": "q5OO8x+9kIbzaQRWimI/Vo9ZowzVBtCX+TodObLsQoY=",
        "originContent": "Clean the text files using a script or manually remove headers/footer from Project Gutenberg, Modern annotations or things like OCR errors.",
        "translatedContent": "스크립트로 또는 수작업으로 Project Gutenberg의 머리말/꼬리말, 현대 주석, OCR 오류 등을 제거하여 텍스트 파일을 정리합니다."
      },
      {
        "row": 37,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 38,
        "rowsha": "N4wsIjC0LRlClodmfCtYX3qswttJnk0psU28/mlCRTw=",
        "originContent": "prepare_dataset.py should work fine.",
        "translatedContent": "prepare_dataset.py를 사용하면 충분합니다."
      },
      {
        "row": 39,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 40,
        "rowsha": "jDM2lr7pP+MT0pt+L0cd5nBXI83IPoT27NzIgplt7R8=",
        "originContent": "# Step 2: Build a Custom Tokenizer",
        "translatedContent": "# 2단계: 커스텀 토크나이저 빌드"
      },
      {
        "row": 41,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 42,
        "rowsha": "+90EorsgO/X2bFK2pYJw+vZIpjIbuMm4QR6W/xfj8C8=",
        "originContent": "Run train_tokenizer.py or train_tokenizer_hf.py on the cleaned data.",
        "translatedContent": "정리한 데이터에 대해 train_tokenizer.py 또는 train_tokenizer_hf.py를 실행합니다."
      },
      {
        "row": 43,
        "rowsha": "tkP3Eg1rWphTQMNhN2yYg/1+AA1IdcXbGT96aRMpnwc=",
        "originContent": "This will give you vocab.json and merges.txt",
        "translatedContent": "이 과정을 통해 vocab.json과 merges.txt가 생성됩니다."
      },
      {
        "row": 44,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 45,
        "rowsha": "/wqxgtOu72+x3a2xi7q23jDkx+WQv2SHrJddzpvm1Ys=",
        "originContent": "Thes files define vocab and merge rules for your model",
        "translatedContent": "이 파일들은 모델의 어휘 및 병합 규칙을 정의합니다."
      },
      {
        "row": 46,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 47,
        "rowsha": "vKPAEsPxc9uYtzjTX9/rNADSDnxkKwYcYpX/aiAp8Hc=",
        "originContent": "# Step 3: Train Your Model (nanoGPT) ",
        "translatedContent": "# 3단계: 모델 학습(nanoGPT)"
      },
      {
        "row": 48,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 49,
        "rowsha": "tCDY5iXt+Z7YYeTPouMSYDX5uuFnGROxZMvHyTOIblY=",
        "originContent": "Refer to [nanoGPT by Andrej Karpathy](https://github.com/karpathy/nanoGPT) for the training process.",
        "translatedContent": "학습 과정은 [Andrej Karpathy의 nanoGPT](https://github.com/karpathy/nanoGPT)를 참고하세요."
      },
      {
        "row": 50,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 51,
        "rowsha": "bLpr9snECDSJ5ejrqDWYNT8VXSupCQKQxyLxvAmQ2Kc=",
        "originContent": "You can train a different LLM if you want, but I used nanoGPT ",
        "translatedContent": "원한다면 다른 LLM을 써도 되지만 저는 nanoGPT를 사용했습니다."
      },
      {
        "row": 52,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 53,
        "rowsha": "OoCxyGfPN5TmdzAkaPphtPx303MJJ7vpfWbKrufGH5g=",
        "originContent": "# FAQ",
        "translatedContent": "# FAQ"
      },
      {
        "row": 54,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 55,
        "rowsha": "+5dDgPw4ILEotxso4tjjjz1cxwUei16yNQPDUKbgxoo=",
        "originContent": "## What is Selective Temporal Training ?",
        "translatedContent": "## Selective Temporal Training이란?"
      },
      {
        "row": 56,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 57,
        "rowsha": "hooEARKH4r/sDPh7JUtZAZ6TYMvBkTLZIcfw3g83xos=",
        "originContent": "Selective Temporal Training (STT) is a machine learning methodology where all training data is specifically curated to fall within a specific historical time period. It's done in order to model the language and knowledge of that era without influence from modern concepts. For example, the current model I have now (v0.5) is trained on data exclusively from 1800-1875, it's not fine tuned but trained from scratch resulting in output that reflects the linguistic style and historical context of that time period.",
        "translatedContent": "Selective Temporal Training(STT)은 모든 학습 데이터를 특정한 역사적 시기에만 한정해 엄선하는 기계학습 방법론입니다. 이는 현대 개념의 영향을 받지 않고 해당 시대의 언어와 지식을 모델링하기 위해서입니다. 예를 들어, 현재의 v0.5 모델은 1800~1875년 데이터만으로 처음부터 학습되었으며, 그 결과 해당 시대의 언어 스타일과 역사적 맥락을 반영합니다."
      },
      {
        "row": 58,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 59,
        "rowsha": "dVMKQ2mPI1Spc6x6r/jNG0PIR5YKpalU4MXx9JmKp/I=",
        "originContent": "## Why not just use fine-tuning or LoRA?",
        "translatedContent": "## 그냥 파인튜닝이나 LoRA를 쓰지 않는 이유는?"
      },
      {
        "row": 60,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 61,
        "rowsha": "oNvWlJHtQSyq1TwlqJyGtMzk4Z4mBIn8AW2SudzvUYs=",
        "originContent": "For this project I'm trying to create a language model that is unclouded from modern bias. If I fine-tune something like GPT-2, it's already pre-trained and that information won't go away. If I train from scratch the language model won't pretend to be old, it just will be. The Goal for this project right now is to create something can reason exclusively using knowledge from London books published between 1800 and 1850.",
        "translatedContent": "이 프로젝트의 목표는 현대적 편견에서 벗어난 언어모델을 만드는 것입니다. 예를 들어 GPT-2를 파인튜닝하면 이미 사전 학습된 정보는 지워지지 않습니다. 처음부터 학습하면 언어 모델이 옛것인 척하는 것이 아니라 실제로 옛날 모델이 되는 것입니다. 지금의 목표는 1800~1850년 런던에서 출판된 책의 지식만으로 추론할 수 있는 모델을 만드는 것입니다."
      },
      {
        "row": 62,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 63,
        "rowsha": "ByP4WlNmMoG6WIiLJNd6b080/DSciCgWmj9aYSJjAF0=",
        "originContent": "## What kind of data did you use for training?",
        "translatedContent": "## 학습에 사용한 데이터는 어떤 종류입니까?"
      },
      {
        "row": 64,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 65,
        "rowsha": "Kj6EF7wZdUrAFg4ErGmJuh9Q5Xujmb+tunpfssPKXkA=",
        "originContent": "I'm using books, legal documents, newspapers, and other writings from 1800–1850 London. The list I linked has like 200 but for the first training I just used 50 files about ~187 MB. You can view a list of the documents:",
        "translatedContent": "1800~1850년 런던의 책, 법률 문서, 신문, 기타 문서를 사용하고 있습니다. 링크된 목록에는 약 200개가 있지만 첫 학습에는 50개 파일, 약 187MB만 사용했습니다. 문서 목록은 아래에서 볼 수 있습니다:"
      },
      {
        "row": 66,
        "rowsha": "0mxyGiLJxzp9JPCg1oA+nbIwAKJbEC4ei9kSV3Gp84Y=",
        "originContent": "https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt",
        "translatedContent": "https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt"
      },
      {
        "row": 67,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 68,
        "rowsha": "RiLgksbH2sYLZRyKsFhVyfjC6nNxLsxWsc7XnFS061A=",
        "originContent": "## How large is the Version 0 model ?",
        "translatedContent": "## 0버전 모델의 크기는?"
      },
      {
        "row": 69,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 70,
        "rowsha": "55Ce1mBEzOreC728R5HRfuMIC/nZRg3Zcr87GVkjVVY=",
        "originContent": "This model is very small right now, I'm just doing this for fun and following a strict training rule of no modern sources. It has almost 16 million parameters but I'm gonna start gathering more old texts to begin another model training. Will give updates as I go.",
        "translatedContent": "이 모델은 아직 매우 작으며, 재미 삼아 현대 소스 없이 엄격한 학습 규칙을 따르고 있습니다. 현재 파라미터 수는 약 1,600만 개이며, 더 많은 옛날 텍스트를 수집해 다음 모델 학습을 시작할 예정입니다. 진행 상황은 계속 업데이트하겠습니다."
      },
      {
        "row": 71,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 72,
        "rowsha": "A40eQ7ZJiqr+bs9cl0Cb4QKKuS9z7/PA1ZaGn1TSehI=",
        "originContent": "## Training Specs ? ",
        "translatedContent": "## 학습 사양?"
      },
      {
        "row": 73,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 74,
        "rowsha": "EH8H1HW/C4Tb7LfJgVUnVGsk4pF9l40Rlev8tAkKhjI=",
        "originContent": "GPU: Geforce rtx 4060",
        "translatedContent": "GPU: Geforce rtx 4060"
      },
      {
        "row": 75,
        "rowsha": "vo3FdN37kY6VUB7PruRKfBPJDgsVJyBHIUCn/g8mt68=",
        "originContent": "CPU: i5-13400F ",
        "translatedContent": "CPU: i5-13400F"
      },
      {
        "row": 76,
        "rowsha": "W8fXPiQKUkoNso0PPfTvjYMy0IYo85j+gNXmB0aERO4=",
        "originContent": "Ram: 16GB DDR5.",
        "translatedContent": "Ram: 16GB DDR5."
      },
      {
        "row": 77,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  }
]