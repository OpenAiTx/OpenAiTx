<div align="right">
  <details>
    <summary >🌐 اللغة</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">简体中文</a>
        | <a href="#" title="Coming soon">繁體中文 (قريباً)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">한국어</a>
        | <a href="#" title="Coming soon">हिन्दी (قريباً)</a> |
        | <a href="#" title="Coming soon">ไทย (قريباً)</a> |
        | <a href="#" title="Coming soon">Français (قريباً)</a>
        | <a href="#" title="Coming soon">Deutsch (قريباً)</a>
        | <a href="#" title="Coming soon">Español (قريباً)</a>
        | <a href="#" title="Coming soon">Italiano (قريباً)</a>
        | <a href="#" title="Coming soon">Русский (قريباً)</a>
        | <a href="#" title="Coming soon">Português (قريباً)</a>
        | <a href="#" title="Coming soon">Nederlands (قريباً)</a>
        | <a href="#" title="Coming soon">Polski (قريباً)</a>
        | <a href="#" title="Coming soon">العربية (قريباً)</a>
        | <a href="#" title="Coming soon">فارسی (قريباً)</a>
        | <a href="#" title="Coming soon">Türkçe (قريباً)</a>
        | <a href="#" title="Coming soon">Tiếng Việt (قريباً)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (قريباً)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM
نموذج لغة كبير تم تدريبه فقط على بيانات من فترات زمنية معينة لتقليل الانحياز الحديث.

تخيل لو أن نموذج الذكاء الاصطناعي لم يكتفِ بادعاء كونه تاريخياً بل كان فعلاً كذلك.

تم البناء على [nanoGPT بواسطة أندريه كارباتي](https://github.com/karpathy/nanoGPT) النصوص الأساسية للتدريب وبنية النموذج هي من عمله.

# أهداف المشروع

مشروع TimeCapsule LLM هو مشروع تجريبي سيتم تدريبه فقط على نصوص مكتوبة في فترات زمنية معينة. الهدف هو محاكاة رؤية العالم واللغة في عصور تاريخية محددة.

# لماذا التخصيص (fine tuning) غير كافٍ

إذا قمت فقط بتخصيص نموذج مدرب مسبقاً، فإن نموذجك سيظل يعرف المفاهيم الحديثة. بالطبع تحقيق انعدام الانحياز الحديث بالكامل أمر صعب، لكنني أريد الاقتراب قدر الإمكان من ذلك. الوصول لعدم وجود انحياز حديث يتطلب تدريب نموذج من الصفر.

# النتائج المتوقعة

آمل عند الانتهاء أن لا يعرف هذا النموذج المفاهيم الحديثة ولا يستطيع الاستدلال بما يتجاوز ما تم تدريبه عليه. يجب ألا يتعرف على المفاهيم أو المفردات الحديثة وآمل ألا "يهلوس" بمعرفة حديثة.

# تحديثات التقدم

## 9 يوليو 2025

حددت الفترة الزمنية من 1800 إلى 1850 والمنطقة: لندن

جمعت قائمة نصوص وكتب ووثائق

حتى الآن حصلت على 50 ملف نصي وسأبدأ قريباً في تدريب NanoGPT

سأواصل تحديث هذا القسم طالما هناك تقدم

## 13 يوليو 2025

تم تدريب nanoGPT على 187 ميجابايت من بيانات نصوص تاريخية.

## 15 يوليو 2025

بدأت بتنزيل نصوص للجولة التدريبية الثانية. أقوم بجمع كل شيء من Internet Archive وقد وسعت الفترة الزمنية إلى 1800-1875. للحصول على مجموعة متنوعة من النصوص، يمكنك استخدام فلاتر الموضوع والبحث لموقع النشر والفترة الزمنية والمواضيع على Internet Archive.

![Search Filters](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## 16 يوليو 2025

قمت بتنزيل حوالي 500 ملف نصي من Internet Archive وبعد تنظيفها (فقط بحذف المسافات الزائدة، رؤوس جوتنبرج، إلخ) أصبح لدي حوالي 500 ميجابايت من البيانات. إنها مجموعة بيانات صغيرة جداً لكن آخر مرة دربت على 187 ميجابايت لذا يجب أن يكون هناك فرق ملحوظ في المخرجات بعد تدريب النموذج الثاني. آمل أن يتمكن هذا النموذج على الأقل من إنتاج جمل أكثر ترابطاً منطقياً. بالطبع ليس ذلك مضموناً لأن المجموعة لا تزال صغيرة جداً، لكنها أكثر مما استخدمته في المرة السابقة.

هذا يمكن إنجازه على جهازي الخاص، وهذا جيد لأنني قد ألاحظ بعض التحسينات قبل الانتقال لمجموعة بيانات أكبر تتطلب مني استئجار بطاقة رسومية. لا تقلقوا ما زلت أخطط لاستئجار GPU قريباً، لكن قبل ذلك أريد أن أتأكد من أن بياناتي منتقاة ونظيفة قدر الإمكان. إحدى المشاكل التي أواجهها هي التنظيف، كثير من هذه الملفات تحتوي على نصوص غير مفهومة. السكربتات التي استخدمتها للتنظيف تعمل لكنها ليست فعالة 100%.

سأقوم بتدريب هذه المجموعة اليوم ويجب أن يستغرق الأمر من 4 إلى 5 ساعات. عندما أنتهي وأختبره، سأعطيكم التحديثات. شكراً مرة أخرى لكل من يتابع مشروعي، حتى أن بعض الأشخاص أرسلوا لي روابط لموارد OCR فشكراً لكم! آمل أن يجرب المزيد من الناس هذا ويختبروا مجموعاتهم الخاصة من البيانات.


### تحديث التدريب

بدأت التدريب على مجموعة بحجم 435 ميجابايت (108 مليون رمز)، الأمور تسير بسلاسة حالياً. انخفضت خسارة التدريب من 10.9 إلى 4.9 في أول 2800 تكرار. أتوقع أن يكتمل خلال 8 أو 9 ساعات. سأضع تحديثاً آخر عند الانتهاء.

## 17 يوليو 2025

انتهى تدريب النموذج الثاني، استغرق الأمر على جهاز 4060 الخاص بي حوالي 8 ساعات و40 دقيقة (3900 تكرار/ساعة) لـ 33,000 تكرار (5 دورات). الخسارة النهائية للتدريب كانت 3.73. النتائج كانت جيدة بشكل مفاجئ، أصبح ينتج جمل بأسلوب القرن التاسع عشر بشكل مترابط حقاً.

## 28 يوليو 2025

قمت برفع الإصدار 0.5 على Hugging Face، [تفقدوه هنا](https://huggingface.co/haykgrigorian/TimeCapsuleLLM) إذا أحببتم. يمكنكم الآن تنزيل المستودع وتشغيله محلياً. للأسف nanoGPT لا يعمل بشكل مباشر مع HuggingFace، لذا ستحتاجون لتنزيل النموذج وتشغيله محلياً.

أيضاً سأبدأ في انتقاء بيانات للجولة التدريبية التالية، أعتقد أنني سأحتاج ربما 5-10 أضعاف البيانات لتحقيق قدرات استدلالية.

## 2 أغسطس 2025

سأبدأ قريباً العمل على الإصدار 1. سأحتاج للانتقال من بنية nanoGPT إلى شيء أكثر حداثة. لدي عدة هياكل LLM مفتوحة المصدر في ذهني، منها: OpenLLaMA v3, Phi-2 و Qwen 1.5B. ولدعم الانتقال إلى الإصدار 1، سأحتاج لتنقيح مجموعة بيانات أكبر بكثير وأكثر تنوعاً. سأحتاج على الأقل 5 جيجابايت من بيانات التدريب النظيفة.


# سلوك النموذج V0 والقيود

العينات المبكرة تُظهر استجابة النموذج بلغة وسلوك من القرن التاسع عشر. على سبيل المثال، قمت بتجربته بسؤال "من هو هنري؟" فأجاب "أعرف ذلك الرجل، لم أفعل ذلك بالسواد، العاصفة." وبالطبع هذه الجملة بلا معنى لكن النموذج يتعرف أنني أسأل عن شخص.

![عينة من مخرجات TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

لا يوجد ذكر للمفاهيم الحديثة، والمخرجات تحتوي في الغالب على كلمات وتراكيب لغوية من القرن التاسع عشر.

لا يزال بحاجة للكثير من العمل، فالتدريب على 187 ميجابايت فقط لن يعطيك نموذجاً ينتج نصوصاً ذات منطق معقد.

حالياً، ينتج النموذج جُملاً تفتقر للبنية الكاملة وغالباً بلا معنى، لكن هذا أمر طبيعي لحجم البيانات المستخدمة.

# سلوك النموذج V0.5 والقيود

هذه النسخة تمثل تحسناً جيداً مقارنة بالنموذج السابق. أسلوب الكتابة والمفردات فيكتورية وكل جملة تقريباً صحيحة نحويًا مع علامات الترقيم المناسبة. ومجدداً، تم تدريبه من الصفر لذا يلتزم بمواضيع القرن التاسع عشر.

![عينة من مخرجات TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

هناك الكثير من الهلوسة في الحقائق. معظم (بنسبة 100%) التفاصيل (تواريخ، أحداث، شخصيات تاريخية) مختلقة. كذلك الجمل غالباً غير مترابطة، أحياناً فقط جملتان قد تكونان مرتبطتين لكن أبعد من ذلك لا يوجد ترابط. مشكلة أخرى هي ظهور تذييل "Digitized by Google" أحياناً، لذا في التدريب القادم يجب التأكد من تنظيف النصوص جيداً. عموماً أنا سعيد جداً بالنتائج، النموذج ليس نموذج لغة بعد لكنه بالتأكيد مُولّد جمل.

أتعلم الكثير وسأبدأ قريباً في تحديد ما يجب تحسينه. سأرفع الملفات قريباً!

# الخطط القادمة

(تم الانتهاء) سأبدأ العمل على النسخة 0.5، وبدلاً من التدريب باستخدام 50 كتاباً، سأستخدم 500-600 كتاب إن أمكن. حالياً أقوم بتدريب nanoGPT باستخدام كتب من 1800-1850 وتحديداً من لندن. هناك بعض التحديات مثل التأكد من أن الكتب ليست مُحدثة أو بها تفسيرات حديثة بل كتب أصلية نُشرت في الفترة الزمنية المختارة.

أرغب في تدريب نموذج جديد (v1) بقاعدة بيانات أكبر بكثير، ربما أكبر 5-10 مرات من المستخدمة في v0.5. هدفي هو معرفة ما إذا كان من الممكن ظهور قدرات استدلالية من خلال التدريب الزمني الانتقائي فقط، ستكون مهمة أصعب ولست متأكداً إذا كانت ممكنة بسبب محدودية البيانات التاريخية. في الأسابيع القادمة سأحاول جمع بيانات كافية لمجموعة بيانات بحجم 5-10 جيجابايت. أعتقد أنه إذا حصلت على بيانات نظيفة عالية الجودة واستأجرت بطاقة رسوميات، سيكون هناك تقدم.

# كيفية استخدام هذا المشروع

يركز هذا المشروع بشكل أساسي على جمع البيانات التاريخية، تجهيزها للتدريب وبناء أداة تقسيم النصوص (tokenizer). لن أغطي عملية تدريب النموذج اللغوي بالكامل، لذلك راجع nanoGPT بواسطة Andrej Karpathy.

# الخطوة 1: جمع وتجهيز النصوص التاريخية

اجمع ملفات .txt من كتب ودراسات ووثائق في الملكية العامة من الفترة الزمنية التي تختارها (مثلاً، لندن 1800-1850)

يمكنك استخدام download_texts_improved.py لتنزيل الكتب إذا احتجت لذلك.

نظف ملفات النصوص باستخدام سكريبت أو احذف رؤوس وتذييلات Project Gutenberg يدوياً، والتعليقات الحديثة أو أخطاء OCR.

سكريبت prepare_dataset.py يجب أن يعمل بشكل جيد.

# الخطوة 2: بناء أداة تقسيم النصوص المخصصة

شغل train_tokenizer.py أو train_tokenizer_hf.py على البيانات المنظفة.
سيعطيك ذلك vocab.json و merges.txt

هذه الملفات تُعرّف المفردات وقواعد الدمج لنموذجك

# الخطوة 3: درّب نموذجك (nanoGPT)

راجع [nanoGPT بواسطة Andrej Karpathy](https://github.com/karpathy/nanoGPT) لعملية التدريب.

يمكنك تدريب نموذج لغوي آخر إذا أردت، لكنني استخدمت nanoGPT

# الأسئلة الشائعة

## ما هو التدريب الزمني الانتقائي؟

التدريب الزمني الانتقائي (STT) هو منهجية تعلم آلي يتم فيها اختيار جميع بيانات التدريب لتكون ضمن فترة زمنية تاريخية محددة. الهدف هو نمذجة لغة ومعرفة تلك الحقبة بدون أي تأثير من مفاهيم حديثة. على سبيل المثال، النموذج الحالي (v0.5) تم تدريبه على بيانات من 1800-1875 فقط، لم يتم تحسينه بل تدريب من الصفر، مما أدى إلى مخرجات تعكس الأسلوب اللغوي والسياق التاريخي لتلك الفترة.

## لماذا لا أستخدم فقط التحسين أو LoRA؟

في هذا المشروع أحاول إنشاء نموذج لغوي غير متأثر بالتحيزات الحديثة. إذا قمت بتحسين نموذج مثل GPT-2، فإنه بالفعل مدرب مسبقًا وهذه المعلومات لن تزول. إذا دربت من الصفر، لن يتظاهر النموذج بأنه قديم، بل سيكون كذلك بالفعل. الهدف الآن هو إنشاء نموذج يمكنه الاستدلال حصريًا باستخدام معرفة من كتب لندن المنشورة بين 1800 و1850.

## ما نوع البيانات التي استخدمتها في التدريب؟

أستخدم كتباً، وثائق قانونية، صحفاً وكتابات أخرى من لندن بين 1800–1850. القائمة التي أرفقتها بها حوالي 200 مصدر لكن في أول تدريب استخدمت فقط 50 ملفاً بحجم ~187 ميجابايت. يمكنك الاطلاع على قائمة المستندات:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## ما هو حجم نموذج النسخة 0؟

هذا النموذج صغير جداً حالياً، أعمل عليه للمتعة وأتبع قاعدة تدريب صارمة بدون مصادر حديثة. يحتوي تقريباً على 16 مليون معامل لكنني سأبدأ بجمع المزيد من النصوص القديمة لبدء تدريب نموذج آخر. سأقدم التحديثات تباعاً.

## مواصفات التدريب؟

GPU: Geforce rtx 4060
CPU: i5-13400F
رام: 16 جيجابايت DDR5.



---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-08-07

---