
<div align="right">
  <details>
    <summary >🌐 اللغة</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">简体中文</a>
        | <a href="#" title="قريباً">繁體中文 (قريباً)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">한국어</a>
        | <a href="#" title="قريباً">हिन्दी (قريباً)</a> |
        | <a href="#" title="قريباً">ไทย (قريباً)</a> |
        | <a href="#" title="قريباً">Français (قريباً)</a>
        | <a href="#" title="قريباً">Deutsch (قريباً)</a>
        | <a href="#" title="قريباً">Español (قريباً)</a>
        | <a href="#" title="قريباً">Italiano (قريباً)</a>
        | <a href="#" title="قريباً">Русский (قريباً)</a>
        | <a href="#" title="قريباً">Português (قريباً)</a>
        | <a href="#" title="قريباً">Nederlands (قريباً)</a>
        | <a href="#" title="قريباً">Polski (قريباً)</a>
        | <a href="#" title="قريباً">العربية (قريباً)</a>
        | <a href="#" title="قريباً">فارسی (قريباً)</a>
        | <a href="#" title="قريباً">Türkçe (قريباً)</a>
        | <a href="#" title="قريباً">Tiếng Việt (قريباً)</a>
        | <a href="#" title="قريباً">Bahasa Indonesia (قريباً)</a>

      </div>
    </div>
  </details>
</div>

# تايم كابسول LLM

*نموذج لغوي تم تدريبه **من الصفر** حصرياً على بيانات من أماكن وفترات زمنية محددة بهدف تقليل التحيزات الحديثة ومحاكاة أسلوب ومفردات ورؤية العصر.*

تخيل لو أن نموذج الذكاء الاصطناعي لم يكن فقط يتظاهر بأنه تاريخي بل كان كذلك فعلاً.

الإصدار v0 و v0.5 تم بناؤهما اعتماداً على [nanoGPT بواسطة Andrej Karpathy](https://github.com/karpathy/nanoGPT) نصوص التدريب الأساسية وبنية النموذج هي من عمله. 

v1 تم بناؤه على [Phi 1.5 بواسطة Microsoft](https://huggingface.co/microsoft/phi-1_5)

[رابط Hugging Face](https://huggingface.co/haykgrigorian/TimeCapsuleLLM)

##  سلوك النموذج والقيود

### **v0**  

المطالبات المبكرة تظهر النموذج يرد بلغة وسلوك القرن التاسع عشر. 
مثال: الطلب: "من هو هنري؟" ورد: "أنا أعرف ذلك الرجل، لم أفعل شيئًا أسودًا، العاصفة."

![مخرج عينة TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

- لا ذكر للمفاهيم الحديثة  
- المفردات دقيقة للعصر في الغالب  
- الجمل غير مترابطة في الغالب (متوقع لحجم بيانات تدريب ~187 ميغابايت)

### **v0.5** 

تحسن كبير على الإصدار v0.  
- أسلوب كتابة فيكتوري، علامات ترقيم صحيحة، جمل نحوية في الغالب  
- معدل الهلوسة الواقعية لا يزال مرتفعًا  
- ضوضاء OCR ("Digitized by Google") لا تزال تظهر في النتائج

![مخرج عينة TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

### **v1**

أول نموذج يسترجع ويربط حدث تاريخي حقيقي بشخصية فعلية من مجموعة البيانات.

مثال: الطلب: "كان ذلك في سنة ربنا 1834"

الناتج: "كان ذلك في سنة ربنا 1834 وكانت شوارع لندن مليئة بالاحتجاجات والعرائض. السبب، كما روى الكثيرون، لم يكن مرتبطًا بالخصوصية، بل تم تناوله في نفس اليوم في يوم اللورد بالمرستون، سيتلقى الجمهور بيانًا موجزًا عن الصعوبات التي وصل إليها القانون. ومن المؤسف أن الأحداث الحالية في تاريخ العالم واضحة، وبالتالي ستعرف. ليس صحيحًا أن الرجال الذين أسسوا الإنجيل في القدس كان لديهم سجل واسع ومثير للاهتمام عن الرخاء والازدهار"

في البداية افترضت أن احتجاجًا قد حدث بالمصادفة في نفس السنة ولكن انظر لهذا: ![1834protest](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)

### لماذا هذا مهم:

هذا هو أول مثال على أحد نماذجي يربط سنة بحدث تاريخي حقيقي وشخصية حقيقية مرتبطة بذلك الحدث (اللورد بالمرستون). النماذج السابقة (v0 و v0.5) كانت تستطيع تقليد أساليب الكتابة في القرن التاسع عشر ولكنها كانت تهلوس دائمًا الأحداث والأشخاص والحقائق. هذا يُظهر أن النموذج بدأ يتذكر أشياء من مجموعة البيانات

## الخطط القادمة

- هناك ما يقرب من 175,000 نص منشور في لندن بين عامي 1800-1875 على أرشيف الإنترنت
- أخطط لتوسيع مجموعة البيانات وتنظيفها بشكل أكبر لتحسين قدرات الاستدلال
- التوسع إلى مناطق وفترات زمنية مختلفة لنماذج تاريخية أكثر

## كيفية الاستخدام

يركز هذا المشروع في الغالب على تنسيق البيانات التاريخية وتحضيرها للتدريب وبناء معالج الرموز. لن أغطي عملية تدريب النموذج اللغوي الكبير بالكامل، لذلك يُرجى الرجوع إلى nanoGPT بواسطة أندريه كارباتي.

### الخطوة 1: جمع وتحضير النصوص التاريخية

- جمع ملفات .txt من الكتب والمستندات ضمن الملكية العامة من الفترة الزمنية التي تختارها (مثلاً: لندن 1800-1850)
- احتفظ بها ضمن نافذة الزمان/المكان المختارة
- نظّف ملفات النصوص باستخدام سكربت أو يدوياً لإزالة رؤوس/ذيول جوتنبرغ، أو التعليقات الحديثة أو أخطاء OCR.

### الخطوة 2: بناء معالج الرموز المخصص

- شغل train_tokenizer.py أو train_tokenizer_hf.py على البيانات المنظفة.
- سيعطيك هذا vocab.json و merges.txt
- هذه الملفات تعرف المفردات وقواعد الدمج لنموذجك

### الخطوة 3: درّب نموذجك

- راجع [nanoGPT بواسطة أندريه كارباتي](https://github.com/karpathy/nanoGPT) لعملية التدريب أو وثائق البنية التي اخترتها.

# الأسئلة الشائعة

## ما هو التدريب الزمني الانتقائي؟

التدريب الزمني الانتقائي (STT) هو منهجية تعلم آلي حيث يتم اختيار كل بيانات التدريب لتكون ضمن فترة زمنية تاريخية محددة. يتم ذلك بهدف نمذجة اللغة والمعرفة لتلك الحقبة دون تأثير من المفاهيم الحديثة. على سبيل المثال، النموذج الحالي الذي أملكه الآن (الإصدار 0.5) مدرب فقط على بيانات بين 1800-1875، لم يتم ضبطه بدقة بل تم تدريبه من الصفر، مما ينتج عنه مخرجات تعكس الأسلوب اللغوي والسياق التاريخي لتلك الفترة.

## لماذا لا أستخدم فقط الضبط الدقيق أو LoRA؟

في هذا المشروع أهدف إلى إنشاء نموذج لغوي خالٍ من التحيزات الحديثة. إذا قمت بضبط نموذج مثل GPT-2، فهو مدرب مسبقاً وتلك المعلومات لن تختفي. إذا دربت من الصفر، لن يتظاهر النموذج بأنه قديم، بل سيكون كذلك فعلاً. الهدف من هذا المشروع حالياً هو إنشاء شيء يمكنه الاستدلال حصرياً باستخدام معرفة من كتب لندن المنشورة بين 1800 و1875.

## ما نوع البيانات التي استخدمتها في التدريب؟



أنا أستخدم كتبًا، وثائق قانونية، صحفًا، وكتابات أخرى من لندن في الفترة 1800–1875. القائمة التي قمتُ بربطها (لإصدار v0) تحتوي على حوالي 200 ملف، لكن في أول تدريب استخدمت فقط 50 ملفًا بحجم حوالي ~187 ميجابايت. يمكنك عرض قائمة الوثائق على الرابط:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

أحجام مجموعات البيانات:
v0: ~187 ميجابايت
v0.5: ~435 ميجابايت
v1: ~6.25 جيجابايت

## ما حجم النماذج؟

V0: 16 مليون معلمة

V0.5: 123 مليون معلمة

V1: 700 مليون معلمة

# مواصفات التدريب؟

# V0/V0.5
GPU: Geforce rtx 4060
CPU: i5-13400F
الرام: 16GB DDR5.

# V1
GPU: A100 مستأجرة

















---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-09-30

---