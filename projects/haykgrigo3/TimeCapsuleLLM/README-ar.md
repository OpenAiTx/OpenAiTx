<div align="right">
  <details>
    <summary >🌐 اللغة</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">简体中文</a>
        | <a href="#" title="قريباً">繁體中文 (قريباً)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">한국어</a>
        | <a href="#" title="قريباً">हिन्दी (قريباً)</a> |
        | <a href="#" title="قريباً">ไทย (قريباً)</a> |
        | <a href="#" title="قريباً">Français (قريباً)</a>
        | <a href="#" title="قريباً">Deutsch (قريباً)</a>
        | <a href="#" title="قريباً">Español (قريباً)</a>
        | <a href="#" title="قريباً">Italiano (قريباً)</a>
        | <a href="#" title="قريباً">Русский (قريباً)</a>
        | <a href="#" title="قريباً">Português (قريباً)</a>
        | <a href="#" title="قريباً">Nederlands (قريباً)</a>
        | <a href="#" title="قريباً">Polski (قريباً)</a>
        | <a href="#" title="قريباً">العربية (قريباً)</a>
        | <a href="#" title="قريباً">فارسی (قريباً)</a>
        | <a href="#" title="قريباً">Türkçe (قريباً)</a>
        | <a href="#" title="قريباً">Tiếng Việt (قريباً)</a>
        | <a href="#" title="قريباً">Bahasa Indonesia (قريباً)</a>

      </div>
    </div>
  </details>
</div>

# تايم كابسول LLM
نموذج لغوي ضخم (LLM) تم تدريبه فقط على بيانات من فترات زمنية معينة لتقليل الانحياز العصري.

تخيل لو أن نموذج الذكاء الاصطناعي لم يكتفِ بادعاء التاريخية بل كان كذلك فعلاً.

تم بناؤه على [nanoGPT بواسطة أندريه كارباتي](https://github.com/karpathy/nanoGPT) سكريبتات التدريب الأساسية وبنية النموذج هي عمله.

# أهداف المشروع

تايم كابسول LLM هو مشروع تجريبي سيتم تدريبه فقط على نصوص مكتوبة خلال فترات زمنية معينة. الهدف هو محاكاة وجهة نظر ولغة حقب تاريخية محددة.

# لماذا التخصيص وحده لا يكفي

إذا قمت فقط بتخصيص نموذج مدرب مسبقاً، سيظل النموذج يعرف المفاهيم الحديثة. بالطبع تحقيق انعدام الانحياز العصري بالكامل أمر صعب لكني أريد الاقتراب من هذا الهدف قدر الإمكان. الحصول على نموذج بلا انحياز عصري يتطلب التدريب من الصفر.

# النتائج المتوقعة

آمل عند الانتهاء أن هذا النموذج لن يعرف المفاهيم الحديثة ولن يكون قادراً على الاستدلال خارج ما تم تدريبه عليه. يجب ألا يتعرف على المفاهيم أو المفردات الحديثة وأتمنى ألا يهلوس بمعرفة عصرية.

# تحديثات التقدم

## 9 يوليو 2025

لقد حددت فترتي الزمنية بين 1800-1850 والمنطقة: لندن

جمعت قائمة من النصوص والكتب والوثائق

حتى الآن حصلت على 50 ملف نصي وسأبدأ قريباً تدريب NanoGPT

سأواصل تحديث هذا القسم طالما هناك تقدم

## 13 يوليو 2025

تم تدريب nanoGPT على 187 ميغابايت من بيانات نصوص تاريخية.

## 15 يوليو 2025

بدأت في تحميل نصوص للتدريب الثاني. أحصل على كل شيء من Internet Archive وقد وسعت الفترة الزمنية إلى 1800-1875. للحصول على مجموعة متنوعة من النصوص، يمكنك استخدام الفلاتر للموضوع وفترة النشر والموقع في Internet Archive.

![Search Filters](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## 16 يوليو 2025

قمت بتحميل حوالي 500 ملف نصي من Internet Archive وبعد تنظيفها (حذف المسافات البيضاء، رؤوس جوتنبرج، إلخ) أصبح لدي حوالي 500 ميغابايت من البيانات. إنها مجموعة بيانات صغيرة لكن آخر مرة تدربت على 187 ميغابايت لذا يجب أن يكون هناك فرق ملحوظ على الأقل في النتائج بعد تدريب النموذج الثاني. آمل أن يتمكن هذا النموذج على الأقل من إنتاج جمل أكثر اتساقاً وتبدو منطقية نوعاً ما. بالطبع هذا ليس مضموناً بعد لأن المجموعة لا تزال صغيرة جداً، لكنها أكثر مما استخدمته في السابق.

يجب أن يكون هذا ممكناً على جهازي الخاص، وهذا أمر جيد لأنني آمل أن أرى بعض التحسينات قبل الانتقال إلى مجموعة بيانات أكبر ستتطلب مني استئجار وحدة معالجة رسومات. لكن لا تقلق لا أزال أخطط لاستئجار GPU قريباً، لكن قبل ذلك أريد التأكد من أن بياناتي نظيفة ومنتقاة قدر الإمكان. إحدى المشكلات التي أواجهها هي التنظيف، فالكثير من هذه الملفات النصية تحتوي على هراء مختلط. السكريبتات التي استخدمتها للتنظيف تعمل لكنها ليست فعالة بنسبة 100%.

سأقوم بتدريب هذه البيانات اليوم ويجب أن يستغرق الأمر حوالي 4-5 ساعات. بمجرد الانتهاء واختبار النتائج سأقدم التحديثات. شكراً مرة أخرى لكل من يتابع مشروعي، حتى أن بعض الأشخاص أرسلوا لي روابط لموارد OCR فشكراً لكم! آمل أن يجرب المزيد من الأشخاص هذا المشروع ويجربوا مجموعاتهم الخاصة من البيانات.

## 28 يوليو 2025

لقد قمت برفع النسخة v0.5 على Hugging Face، [تفضل بالاطلاع](https://huggingface.co/haykgrigorian/TimeCapsuleLLM) إذا أحببت. يمكنك الآن تحميل المستودع وتشغيله محلياً. للأسف nanoGPT لا يعمل مباشرة مع HuggingFace، لذا عليك تحميل وتشغيل النموذج محلياً.

كذلك سأبدأ في تنقية البيانات لجولة التدريب التالية، أعتقد أنني سأحتاج ربما 5-10 أضعاف البيانات لتحقيق قدرات استدلال.

### تحديث التدريب

بدأت التدريب على مجموعة بيانات 435 ميغابايت (108 مليون رمز)، الأمور تسير بسلاسة الآن. انخفضت خسارة التدريب من 10.9 إلى 4.9 في أول 2800 دورة تدريبية. أتوقع أن يستغرق الأمر حوالي 8 أو 9 ساعات حتى يكتمل. سأضع تحديثاً آخر بمجرد الانتهاء.

## 17 يوليو 2025 الساعة 2:13 صباحاً

انتهى تدريب النموذج الثاني، استغرق الأمر حوالي 8 ساعات و40 دقيقة على بطاقتي 4060 (3900 دورة/ساعة) لـ 33000 دورة (5 عصور). الخسارة النهائية كانت 3.73. النتائج كانت مفاجئة إذ ينتج جمل متماسكة بأسلوب القرن التاسع عشر الآن.

# سلوك النموذج V0 والقيود

العينات المبكرة تظهر استجابة النموذج بلغة وسلوك القرن التاسع عشر. على سبيل المثال، طلبت منه "Who art Henry?" فأجاب: "I know that man, I have did not a black, the storm." نعم هذه الجملة ليست منطقية لكن النموذج يتعرف أني أسأل عن شخص.

![TimeLockLLM Sample Output](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)


لا يوجد ذكر للمفاهيم الحديثة، والمخرجات تحتوي في الغالب على كلمات وتراكيب من القرن التاسع عشر.

لا يزال يحتاج إلى الكثير من العمل، فالتدريب على 187 ميجابايت لن يعطيك نموذجاً ينتج نصوصاً ذات تفكير معقد.

حالياً، ينتج النموذج جمل تفتقر إلى الهيكل الكامل للجملة وبشكل عام لا معنى لها، لكن هذا طبيعي لحجم البيانات المستخدمة في التدريب.

# سلوك النموذج V0.5 والقيود

هذا تحسن جيد مقارنة بالنموذج السابق. أسلوب الكتابة والمفردات فيكتورية وكل جملة تقريباً صحيحة نحوياً وبعلامات ترقيم مناسبة. ومرة أخرى، تم تدريبه من الصفر لذا يلتزم بمواضيع القرن التاسع عشر.

![مخرجات نموذج TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

هناك الكثير من الهلوسات الواقعية. الكثير (حوالي 100%) من التفاصيل (تواريخ، أحداث، شخصيات تاريخية) مختلقة. أيضاً الجمل لا ترتبط ببعضها، أحياناً ربما جملتان فقط ترتبطان لكن غير ذلك لا يوجد ترابط. مشكلة أخرى أحياناً يظهر تذييل "Digitized by Google" بشكل عشوائي، لذا في التدريب القادم يجب أن أتأكد من تنظيف النصوص جيداً. بشكل عام أنا سعيد جداً بالنتائج، فهو ليس نموذج لغة كبير بعد لكنه بالتأكيد مولد للجمل.

أتعلم الكثير وسأبدأ بتحديد ما يجب علي تحسينه في الأسابيع القادمة. سأقوم برفع الملفات قريباً!

# الخطط القادمة

(تم الإنجاز) سأبدأ العمل على النسخة 0.5، وبدلاً من التدريب باستخدام 50 كتاباً، سأقوم بالتدريب باستخدام 500-600 كتاب إن أمكن. حالياً أدرب nanoGPT باستخدام كتب من 1800-1850 ومن لندن تحديداً. هناك تحديات مثل التأكد من أن الكتب التي أجدها ليست محدثة أو تحوي تفسيرات حديثة بل كتب أصلية من الفترة الزمنية المختارة.

أرغب في تدريب نموذج جديد (v1) بقاعدة بيانات أكبر بكثير، ربما أكبر بـ 5-10 مرات من تلك التي استخدمتها في v0.5. هدفي هو معرفة ما إذا كان بإمكاني جعل قدرات الاستدلال تظهر من خلال التدريب الزمني الانتقائي فقط، وهذه مهمة أصعب ولست متأكداً تماماً من إمكانية ذلك بسبب محدودية البيانات التاريخية. في الأسابيع المقبلة سأحاول جمع بيانات كافية لقاعدة بيانات بحجم 5-10 جيجابايت. أعتقد أنه إذا حصلت على بيانات نظيفة وعالية الجودة واستأجرت معالج رسومي، سيكون هناك تقدم.

# كيفية استخدام هذا المشروع

يركز هذا المشروع بشكل أساسي على جمع البيانات التاريخية وتحضيرها للتدريب وبناء محلل ترميز tokenizer. لن أغطي عملية تدريب النموذج اللغوي بالكامل، لهذا راجع nanoGPT من أندريه كارباتي.

# الخطوة 1: جمع وتحضير النصوص التاريخية

اجمع ملفات .txt لكتب أو مستندات من الملكية العامة من الفترة الزمنية التي تختارها (مثلاً لندن 1800-1850)

يمكنك استخدام download_texts_improved.py لتنزيل الكتب إذا كنت بحاجة لذلك.

نظف ملفات النصوص باستخدام سكريبت أو يدوياً لإزالة الرؤوس/التذييلات من Project Gutenberg أو التفسيرات الحديثة أو أخطاء OCR.

prepare_dataset.py يجب أن يعمل بشكل جيد.

# الخطوة 2: بناء محلل ترميز مخصص

شغل train_tokenizer.py أو train_tokenizer_hf.py على البيانات المنظفة.
سيعطيك هذا vocab.json و merges.txt

هذه الملفات تعرف المفردات وقواعد الدمج لنموذجك

# الخطوة 3: تدريب النموذج الخاص بك (nanoGPT)

راجع [nanoGPT من أندريه كارباتي](https://github.com/karpathy/nanoGPT) لعملية التدريب.

يمكنك تدريب نموذج لغة آخر إذا رغبت، لكنني استخدمت nanoGPT

# الأسئلة الشائعة

## ما هو التدريب الزمني الانتقائي؟

التدريب الزمني الانتقائي (STT) هو منهجية تعلم آلي حيث يتم اختيار جميع بيانات التدريب لتكون ضمن فترة زمنية تاريخية محددة. يتم ذلك لنمذجة اللغة والمعرفة لتلك الحقبة دون تأثير المفاهيم الحديثة. على سبيل المثال، النموذج الحالي (v0.5) تم تدريبه على بيانات من 1800-1875 فقط، لم يتم تحسينه بل تم تدريبه من الصفر مما جعله يعكس أسلوب اللغة والسياق التاريخي لتلك الفترة.

## لماذا لا أستخدم فقط الضبط الدقيق أو LoRA؟

في هذا المشروع أحاول إنشاء نموذج لغوي غير متأثر بالتحيز الحديث. إذا قمت بتحسين نموذج مثل GPT-2، فهو مدرب مسبقاً ولن تزول تلك المعلومات. إذا دربت من الصفر، لن يتظاهر النموذج بأنه قديم، بل سيكون كذلك فعلاً. الهدف حالياً هو إنشاء شيء يمكنه الاستدلال باستخدام معرفة مستمدة حصراً من كتب لندن المنشورة بين 1800 و1850.

## ما نوع البيانات التي استخدمتها في التدريب؟

أستخدم كتباً، مستندات قانونية، صحفاً، وكتابات أخرى من لندن بين 1800–1850. القائمة التي أرفقتها تحوي حوالي 200 ملف لكن في التدريب الأول استخدمت فقط 50 ملفاً بحجم ~187 ميجابايت. يمكنك عرض قائمة الوثائق:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## ما حجم النموذج الإصدار 0؟

هذا النموذج صغير جداً حالياً، أعمل عليه للمتعة وأتبع قاعدة صارمة بعدم استخدام مصادر حديثة. يحتوي على حوالي 16 مليون معامل وسأبدأ بجمع المزيد من النصوص القديمة لبدء تدريب نموذج آخر. سأقدم التحديثات مع التقدم.

## مواصفات التدريب؟

GPU: Geforce rtx 4060
CPU: i5-13400F
الذاكرة: 16 جيجابايت DDR5.


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-29

---