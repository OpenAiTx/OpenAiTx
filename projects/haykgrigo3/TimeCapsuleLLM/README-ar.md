
<div align="right">
  <details>
    <summary >🌐 اللغة</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">简体中文</a>
        | <a href="#" title="قريباً">繁體中文 (قريباً)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">한국어</a>
        | <a href="#" title="قريباً">हिन्दी (قريباً)</a> |
        | <a href="#" title="قريباً">ไทย (قريباً)</a> |
        | <a href="#" title="قريباً">Français (قريباً)</a>
        | <a href="#" title="قريباً">Deutsch (قريباً)</a>
        | <a href="#" title="قريباً">Español (قريباً)</a>
        | <a href="#" title="قريباً">Italiano (قريباً)</a>
        | <a href="#" title="قريباً">Русский (قريباً)</a>
        | <a href="#" title="قريباً">Português (قريباً)</a>
        | <a href="#" title="قريباً">Nederlands (قريباً)</a>
        | <a href="#" title="قريباً">Polski (قريباً)</a>
        | <a href="#" title="قريباً">العربية (قريباً)</a>
        | <a href="#" title="قريباً">فارسی (قريباً)</a>
        | <a href="#" title="قريباً">Türkçe (قريباً)</a>
        | <a href="#" title="قريباً">Tiếng Việt (قريباً)</a>
        | <a href="#" title="قريباً">Bahasa Indonesia (قريباً)</a>

      </div>
    </div>
  </details>
</div>

# تايم كابسول LLM

*نموذج لغوي تم تدريبه **من الصفر** حصرياً على بيانات من أماكن وفترات زمنية محددة بهدف تقليل التحيزات الحديثة ومحاكاة أسلوب ومفردات ورؤية العصر.*

تخيل لو أن نموذج الذكاء الاصطناعي لم يكن فقط يتظاهر بأنه تاريخي بل كان كذلك فعلاً.

الإصدار v0 و v0.5 تم بناؤهما اعتماداً على [nanoGPT بواسطة Andrej Karpathy](https://github.com/karpathy/nanoGPT) نصوص التدريب الأساسية وبنية النموذج هي من عمله. 

v1 مبني على [Phi 1.5 من مايكروسوفت](https://huggingface.co/microsoft/phi-1_5)

## سلوك النموذج والقيود

### **الإصدار 0**

النصوص المبكرة تظهر النموذج يرد بلغة وسلوك من القرن التاسع عشر.
مثال: النص: "من هو هنري؟" ورد النموذج: "أعرف ذلك الرجل، لم أفعل شيئاً أسوداً، العاصفة."

![نموذج إخراج TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

- لا ذكر للمفاهيم الحديثة  
- المفردات غالباً دقيقة للعصر  
- الجمل في الغالب غير مترابطة (متوقع لمجموعة تدريب بحجم ~187MB)

### **الإصدار 0.5**

تحسن كبير عن الإصدار 0.  
- أسلوب كتابة فيكتوري، علامات ترقيم صحيحة، وجمل معظمها نحوية  
- لا يزال معدل الهلوسة الواقعية مرتفع  
- ضوضاء OCR ("Digitized by Google") لا تزال تظهر في النتائج

![نموذج إخراج TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

### **الإصدار 1**

أول نموذج يسترجع ويربط حدث تاريخي حقيقي بشخصية حقيقية من مجموعة البيانات.

مثال: النص: "كان ذلك في عام ربنا 1834"

النتيجة: "كان ذلك في عام ربنا 1834 وكانت شوارع لندن مليئة بالاحتجاج والعريضة. السبب، كما روى كثيرون، لم يكن مرتبطاً بالخصوصية، بل تم تناول نفس اليوم في يوم اللورد بالمرستون، سيتلقى الجمهور بياناً مختصراً عن الصعوبات التي وصلت إليها حالة القانون لدينا. إنه أمر مؤسف للغاية أن الأحداث الحالية في تاريخ العالم واضحة، وبالتالي ستكون معروفة. ليس صحيحاً أن الرجال الذين استقروا أولاً في الإنجيل في القدس يجب أن يكون لديهم سجلاً واسعاً ومثيراً للازدهار والازدهار"

في البداية افترضت أن الاحتجاج ربما حدث صدفة في نفس العام ولكن انظر إلى هذا: ![1834protest](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)

### لماذا هذا مهم:

هذا أول مثال لأحد نماذجي يربط سنة بحدث تاريخي حقيقي وشخص حقيقي مرتبط بذلك الحدث (اللورد بالمرستون). النماذج السابقة (الإصدار 0 و0.5) كانت تقلد أساليب الكتابة في القرن التاسع عشر لكنها كانت تهلوس دائماً الأحداث والأشخاص والحقائق. هذا يظهر أن النموذج بدأ يتذكر أشياء من مجموعة البيانات

## الخطط القادمة


- هناك ما يقرب من 175,000 نص منشور في لندن من 1800-1875 على موقع Internet Archive
- أخطط لتوسيع مجموعة البيانات وتنظيفها أكثر لتحسين قدرات الاستدلال
- التوسع ليشمل مناطق وفترات زمنية مختلفة لإنشاء نماذج تاريخية إضافية

## كيفية الاستخدام

يركز هذا المشروع بشكل أساسي على جمع البيانات التاريخية وتحضيرها للتدريب وبناء المرمّز. لن أغطي عملية تدريب النموذج اللغوي بالكامل، ولذلك يرجى الرجوع إلى nanoGPT بواسطة Andrej Karpathy.

### الخطوة 1: جمع وتحضير النصوص التاريخية

- اجمع ملفات .txt من الكتب والوثائق في الملكية العامة من الفترة الزمنية التي اخترتها (مثلاً، لندن 1800-1850)
- احتفظ بها ضمن نافذة الوقت/المكان التي اخترتها
- نظف ملفات النصوص باستخدام سكريبت أو قم بإزالة رؤوس/تذييلات مشروع غوتنبرغ يدوياً، أو الشروح الحديثة أو أخطاء الـ OCR.

### الخطوة 2: بناء مرمّز مخصص

- شغّل train_tokenizer.py أو train_tokenizer_hf.py على البيانات المنظفة.
- سيعطيك ذلك vocab.json و merges.txt
- هذه الملفات تحدد مفردات وقواعد دمج الرموز لنموذجك

### الخطوة 3: درّب نموذجك

- ارجع إلى [nanoGPT بواسطة Andrej Karpathy](https://github.com/karpathy/nanoGPT) لعملية التدريب أو وثائق بنية النموذج التي اخترتها.

# الأسئلة الشائعة

## ما هو التدريب الزمني الانتقائي؟

التدريب الزمني الانتقائي (STT) هو منهجية تعلم آلي حيث يتم اختيار كل بيانات التدريب بحيث تقع ضمن فترة زمنية تاريخية محددة. يتم ذلك بهدف نمذجة لغة ومعرفة ذلك العصر دون التأثر بالمفاهيم الحديثة. على سبيل المثال، النموذج الحالي الذي أملكه (v0.5) تم تدريبه حصرياً على بيانات من 1800-1875، ولم يتم تحسينه بل تم تدريبه من البداية، مما يؤدي إلى مخرجات تعكس الأسلوب اللغوي والسياق التاريخي لتلك الفترة.

## لماذا لا تستخدم فقط الضبط الدقيق أو LoRA؟

في هذا المشروع أحاول إنشاء نموذج لغوي خالٍ من التحيزات الحديثة. إذا قمت بضبط نموذج مثل GPT-2، فهو مدرّب مسبقاً ولن تختفي تلك المعلومات. إذا دربت من البداية، لن يتظاهر النموذج بأنه قديم، بل سيكون كذلك فعلاً. الهدف من هذا المشروع حالياً هو إنشاء نموذج يستطيع الاستدلال حصرياً باستخدام المعرفة من كتب لندن المنشورة بين 1800 و 1875.

## ما نوع البيانات التي استخدمتها في التدريب؟

أستخدم كتباً، وثائق قانونية، صحفاً، وكتابات أخرى من لندن بين 1800–1875. القائمة التي أرفقتها (لـ v0) تحتوي على حوالي 200، ولكن لأول تدريب استخدمت فقط 50 ملفاً بحجم حوالي ~187 ميغابايت. يمكنك رؤية قائمة الوثائق على الرابط:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt


حجم مجموعات البيانات:
v0: حوالي 187 ميغابايت
v0.5: حوالي 435 ميغابايت
v1: حوالي 6.25 غيغابايت

## ما هو حجم النماذج؟

V0: 16 مليون معامل

V0.5: 123 مليون معامل

V1: 700 مليون معامل

# مواصفات التدريب؟

# V0/V0.5
بطاقة الرسومات: Geforce rtx 4060
المعالج: i5-13400F
الذاكرة العشوائية: 16GB DDR5.

# V1
بطاقة الرسومات: A100 مستأجرة














---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-08-19

---