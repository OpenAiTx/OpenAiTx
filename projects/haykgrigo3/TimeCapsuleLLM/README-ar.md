
<div align="right">
  <details>
    <summary >🌐 اللغة</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">简体中文</a>
        | <a href="#" title="قريباً">繁體中文 (قريباً)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">한국어</a>
        | <a href="#" title="قريباً">हिन्दी (قريباً)</a> |
        | <a href="#" title="قريباً">ไทย (قريباً)</a> |
        | <a href="#" title="قريباً">Français (قريباً)</a>
        | <a href="#" title="قريباً">Deutsch (قريباً)</a>
        | <a href="#" title="قريباً">Español (قريباً)</a>
        | <a href="#" title="قريباً">Italiano (قريباً)</a>
        | <a href="#" title="قريباً">Русский (قريباً)</a>
        | <a href="#" title="قريباً">Português (قريباً)</a>
        | <a href="#" title="قريباً">Nederlands (قريباً)</a>
        | <a href="#" title="قريباً">Polski (قريباً)</a>
        | <a href="#" title="قريباً">العربية (قريباً)</a>
        | <a href="#" title="قريباً">فارسی (قريباً)</a>
        | <a href="#" title="قريباً">Türkçe (قريباً)</a>
        | <a href="#" title="قريباً">Tiếng Việt (قريباً)</a>
        | <a href="#" title="قريباً">Bahasa Indonesia (قريباً)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM
نموذج لغة كبير تم تدريبه فقط على بيانات من فترات زمنية محددة لتقليل الانحياز الحديث.

تخيل لو أن نموذج الذكاء الاصطناعي لم يكن يتظاهر فقط بأنه تاريخي، بل كان بالفعل كذلك.

مبني على [nanoGPT بواسطة أندريه كارباتي](https://github.com/karpathy/nanoGPT) نصوص التدريب الأساسية وهيكل النموذج هي من عمله.

# أهداف المشروع
تايم كابسول LLM هو مشروع تجريبي سيتم تدريبه فقط على النصوص المكتوبة خلال فترات زمنية محددة. الهدف هو محاكاة الرؤية للعالم واللغة لعصور تاريخية معينة.

# لماذا التخصيص ليس كافياً

إذا قمت فقط بتخصيص نموذج مدرب مسبقاً، سيظل نموذج LLM الخاص بك يعرف المفاهيم الحديثة. بالطبع تحقيق انعدام التحيز الحديث أمر صعب، لكنني أريد الاقتراب قدر الإمكان من ذلك. الحصول على انعدام كامل للتحيز الحديث يتطلب تدريب نموذج من الصفر.

# النتائج المتوقعة

آمل عند الانتهاء أن هذا النموذج لن يعرف المفاهيم الحديثة ولن يستطيع الاستدلال بما يتجاوز ما تم تدريبه عليه. يجب ألا يتعرف على المفاهيم أو المفردات الحديثة ولا يجب أن يهلوس بمعرفة حديثة.

# تحديثات التقدم

## 9 يوليو 2025

حددت الفترة الزمنية بين 1800-1850 والمنطقة: لندن

جمعت قائمة بالنصوص والكتب والمستندات

حتى الآن حصلت على 50 ملف نصي وسأبدأ تدريب NanoGPT قريباً

سأواصل تحديث هذا طالما هناك تقدم

## 13 يوليو 2025

تم تدريب nanoGPT على 187 ميجابايت من بيانات النصوص التاريخية.

## 15 يوليو 2025

بدأت بتنزيل نصوص للجولة التدريبية الثانية. أحصل على كل شيء من أرشيف الإنترنت ووسعت الفترة الزمنية إلى 1800-1875. للحصول على مجموعة متنوعة من النصوص، يمكنك استخدام فلاتر الموضوع والبحث عن مكان النشر والفترة الزمنية والمواضيع في أرشيف الإنترنت.

![فلاتر البحث](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## 16 يوليو 2025

قمت بتنزيل حوالي 500 ملف نصي من أرشيف الإنترنت وبعد تنظيفها (فقط حذف المسافات البيضاء، رؤوس غوتنبرغ، إلخ) أصبح لدي حوالي 500 ميجابايت من البيانات. إنها مجموعة بيانات صغيرة لكن آخر مرة تدربت على 187 ميجابايت لذا يجب أن يكون هناك فرق ملحوظ في الناتج بعد تدريب النموذج الثاني. آمل أن يتمكن هذا النموذج على الأقل من إنتاج جمل أكثر ترابطاً وتبدو منطقية نوعاً ما. بالطبع لا يوجد ضمان لأن هذه ما تزال مجموعة بيانات صغيرة جداً، لكنها أكثر مما استخدمته في المرة السابقة.

يجب أن يكون هذا قابلاً للتنفيذ على جهازي الخاص، وهذا جيد أيضاً لأنني آمل أن أرى بعض التحسينات قبل أن أنتقل إلى مجموعة بيانات أكبر مما سيتطلب مني استئجار وحدة معالجة رسومات GPU. لكن لا تقلق، ما زلت أخطط لاستئجار GPU قريباً، لكن قبل أن أفعل ذلك أريد التأكد من أن مجموعة البيانات الخاصة بي منظمة ونظيفة قدر الإمكان. إحدى المشكلات التي أواجهها هي التنظيف، العديد من هذه الملفات النصية تحتوي على رموز غير مفهومة. البرامج النصية التي استخدمتها للتنظيف تعمل لكن ليست فعالة بنسبة 100%.

سأتدرب على هذه المجموعة اليوم ويجب أن يستغرق ذلك حوالي 4-5 ساعات. بمجرد الانتهاء والاختبار، سأعطي التحديثات. شكراً مرة أخرى لكل من يتابع مشروعي، لقد أرسل لي بعض الأشخاص حتى روابط لموارد OCR فشكراً لكم! آمل أن يجرب المزيد من الناس ذلك ويجربوا مجموعاتهم الخاصة من البيانات.


## 12 أغسطس 2025

الإصدار 1 مكتمل، هذا الإصدار يستخدم Phi 1.5 (700 مليون معلمة) وتم تدريبه على مجموعة بيانات تحتوي على ما يقارب 7000 نص. التحسينات الرئيسية في هذا النموذج هي قدرته أحيانًا على الإشارة إلى أسماء حقيقية وأحداث تاريخية. لم يتمكن الإصدار 0 و0.5 من فعل ذلك، فعندما كنت أوجه تلك النماذج بشيء مثل "في عام 1821" كان الناتج مجرد هلوسات، أشياء لم تحدث أبدًا. مثال على ذلك مع الإصدار 1:

مطلبي: كان عام ربنا 1834،

الناتج: "كان عام ربنا 1834 وكانت شوارع لندن مليئة بالاحتجاجات والعرائض. السبب، كما رواه الكثيرون، لم يكن مرتبطًا بالأمر الخاص، بل تم تناوله في نفس اليوم في يوم اللورد بالمرستون، سيستلم العامة بيانًا موجزًا عن الصعوبات التي وصل إليها يوم القانون. إنه لأمر مؤسف للغاية، أن الأحداث الحالية في تاريخ العالم واضحة، وبالتالي ستعرف. ليس صحيحًا أن الرجال الذين استقروا أولاً في الإنجيل في القدس يجب أن يكون لديهم سجل واسع ومثير للاهتمام عن الرخاء والازدهار"

في البداية اعتقدت أن هذا قد يكون صدفة لكن انظر إلى هذا: ![1834protest](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)


### تحديث التدريب

بدأت التدريب على مجموعة بيانات بحجم 435 ميجابايت (108 مليون رمز)، يسير الأمر بسلاسة الآن. انخفض الفقد التدريبي من 10.9 إلى 4.9 في أول 2800 تكرار. أتوقع أن الأمر سيستغرق حوالي 8 أو 9 ساعات للانتهاء. سأقوم بنشر تحديث آخر عند الانتهاء.

## 17 يوليو 2025

انتهى التدريب للنموذج الثاني، استغرق الأمر من بطاقة 4060 حوالي 8 ساعات و40 دقيقة (3,900 تكرار/ساعة) لـ 33,000 تكرار (5 دورات). كان الفقد النهائي للتدريب 3.73. كانت النتائج جيدة بشكل مفاجئ، حيث ينتج الآن جمل متماسكة بأسلوب القرن التاسع عشر فعلاً.

## 28 يوليو 2025

قمت برفع الإصدار 0.5 على Hugging Face، [اطلع عليه هنا](https://huggingface.co/haykgrigorian/TimeCapsuleLLM) إذا رغبت بذلك. يمكنك الآن تحميل المستودع وتشغيله محليًا. للأسف nanoGPT لا يعمل بشكل أصلي مع HuggingFace، لذا عليك تحميل النموذج وتشغيله محليًا.

سأبدأ أيضًا في جمع البيانات لجولة التدريب التالية، أعتقد أنني سأحتاج ربما 5-10 أضعاف البيانات الحالية لتحقيق قدرات الاستدلال.

## 2 أغسطس 2025

سأبدأ العمل على الإصدار 1 قريبًا. سأحتاج للانتقال من معمارية nanoGPT إلى شيء أكثر حداثة. لدي عدة معماريات LLM مفتوحة المصدر في بالي، منها: OpenLLaMA v3، Phi-2 و Qwen 1.5B. ولدعم الانتقال إلى V1، سيتعين علي اختيار مجموعة بيانات أكبر وأكثر تنوعًا بعناية. سأحتاج إلى ما لا يقل عن 5 جيجابايت من بيانات التدريب النظيفة.

# سلوك وقيود نموذج V0

العروض الأولية تظهر النموذج يستجيب بلغة وسلوك القرن التاسع عشر. على سبيل المثال، وجهته بسؤال "من هو هنري؟" وأجاب "أعرف ذلك الرجل، لم أكن أسود، العاصفة." نعم هذه الجملة لا معنى لها لكن النموذج يفهم أنني أسأل عن شخص.

![TimeLockLLM Sample Output](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

لا يوجد ذكر للمفاهيم الحديثة، والنواتج تحتوي غالبًا على كلمات وتراكيب من القرن التاسع عشر.

لا يزال بحاجة للكثير من العمل، التدريب على 187 ميجابايت لن يمنحك نموذجًا ينتج نصوصًا باستدلالات معقدة.

حالياً، ينتج النموذج جمل تفتقر إلى البنية الكاملة للجملة وبشكل عام غير مفهومة، لكن هذا أمر طبيعي لحجم التدريب الحالي.

# سلوك النموذج V0.5 والقيود

هذا تحسن جيد مقارنة بالنموذج السابق. أسلوب الكتابة والمفردات من العصر الفيكتوري ومعظم الجمل صحيحة نحويًا مع علامات ترقيم مناسبة. ومرة أخرى، تم تدريب هذا النموذج من الصفر لذا فهو يلتزم بمواضيع القرن التاسع عشر.

![TimeLockLLM Sample Output](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

هناك الكثير من الهلوسات الواقعية. الكثير (تقريبًا 100٪) من التفاصيل (التواريخ، الأحداث، الشخصيات التاريخية) مختلقة. أيضًا الجمل ليس بينها ترابط حقيقي، أحيانًا قد ترتبط جملتين ببعضهما لكن غالبًا لا يوجد ترابط. مشكلة أخرى هي ظهور تذييل "Digitized by Google" أحيانًا، لذلك في التدريب القادم يجب أن أتأكد من تنظيف النصوص جيدًا. بشكل عام أنا سعيد جدًا بالنتائج، لا يزال بعيدًا عن كونه نموذج لغة كبير لكن بالتأكيد مولد جمل.

أتعلم الكثير وسأبدأ في معرفة ما يجب أن أفعله بشكل أفضل في الأسابيع القادمة. سأرفع الملفات قريبًا!

# سلوك النموذج V1 والقيود

سأرفع بعض الأمثلة قريبًا وأجري مقارنات بين النماذج الثلاثة باستخدام نفس المحفز. أيضًا سأرفع V1 على huggingface مثلما فعلت مع الإصدار السابق، يمكنك العثور على حسابي في huggingface هنا: https://huggingface.co/haykgrigorian/TimeCapsuleLLM

# الخطط القادمة

(تم الإنجاز) سأبدأ العمل على الإصدار 0.5، بدلًا من التدريب باستخدام 50 كتابًا، سأدرب باستخدام 500-600 كتابًا. حاليًا أقوم بتدريب nanoGPT باستخدام كتب من 1800-1850 ومن لندن تحديدًا. هناك تحديات مثل التأكد من أن الكتب التي أجدها ليست محدثة أو لها تفسيرات حديثة بل كتب غير معدلة تم نشرها ضمن الفترة الزمنية المختارة.

أرغب في تدريب نموذج جديد (v1) باستخدام مجموعة بيانات أكبر بكثير، ربما 5-10 أضعاف ما استخدمته في v0.5. هدفي هو معرفة ما إذا كان بإمكاني تحقيق قدرات استنتاج من التدريب الزمني الانتقائي وحده، ستكون مهمة أصعب ولا أضمن إمكانية ذلك بسبب محدودية البيانات التاريخية. في الأسابيع القادمة سأحاول جمع بيانات كافية لمجموعة بيانات بحجم 5-10 جيجابايت. أعتقد أنه إذا تمكنت من الحصول على بيانات نظيفة عالية الجودة واستئجار GPU، سيكون هناك تقدم.

# كيفية استخدام هذا المشروع

يركز هذا المشروع بشكل أساسي على جمع البيانات التاريخية، تجهيزها للتدريب وبناء المرمز. لن أشرح عملية تدريب نموذج اللغة الكبير بالكامل، لذلك راجع nanoGPT بواسطة Andrej Karpathy.

# الخطوة 1: جمع وتجهيز النصوص التاريخية

اجمع ملفات .txt لكتب أو وثائق ضمن الملكية العامة من الفترة الزمنية المختارة (مثال: لندن 1800-1850)

يمكنك استخدام download_texts_improved.py لتنزيل الكتب إذا احتجت لذلك.

نظف ملفات النص باستخدام سكريبت أو بإزالة رؤوس/تذييلات Project Gutenberg يدويًا، أو الشروح الحديثة أو الأخطاء الناتجة عن OCR.

يجب أن يعمل prepare_dataset.py بشكل جيد.

# الخطوة 2: بناء مرمز مخصص

شغل train_tokenizer.py أو train_tokenizer_hf.py على البيانات النظيفة.
سيعطيك vocab.json و merges.txt
هذه الملفات تحدد المفردات وقواعد الدمج لنموذجك

# الخطوة 3: درب نموذجك (nanoGPT)

راجع [nanoGPT بواسطة Andrej Karpathy](https://github.com/karpathy/nanoGPT) لعملية التدريب.

يمكنك تدريب نموذج لغوي كبير مختلف إذا أردت، لكنني استخدمت nanoGPT

# الأسئلة الشائعة

## ما هو التدريب الزمني الانتقائي؟

التدريب الزمني الانتقائي (STT) هو منهجية تعلم آلي يتم فيها اختيار جميع بيانات التدريب بحيث تقع ضمن فترة زمنية تاريخية محددة. يتم ذلك من أجل نمذجة اللغة والمعرفة لتلك الحقبة دون تأثير من المفاهيم الحديثة. على سبيل المثال، النموذج الحالي الذي أملكه (v0.5) تم تدريبه على بيانات من الفترة 1800-1875 حصريًا، ولم يتم تحسينه بل تم تدريبه من الصفر مما أدى إلى إخراج يعكس الأسلوب اللغوي والسياق التاريخي لتلك الفترة.

## لماذا لا تستخدم فقط التحسين أو LoRA؟

في هذا المشروع أحاول إنشاء نموذج لغوي غير متأثر بالتحيزات الحديثة. إذا قمت بتحسين نموذج مثل GPT-2، فهو مدرّب مسبقًا ولن تختفي تلك المعلومات. إذا دربت النموذج من الصفر فلن يتظاهر النموذج اللغوي بأنه قديم، بل سيكون كذلك فعلاً. الهدف من هذا المشروع حالياً هو إنشاء شيء يمكنه الاستدلال حصريًا باستخدام المعرفة من كتب لندن المنشورة بين عامي 1800 و 1850.

## ما نوع البيانات التي استخدمتها في التدريب؟

أستخدم كتباً ووثائق قانونية وصحف وكتابات أخرى من لندن بين 1800–1850. القائمة التي أرفقتها تحتوي على حوالي 200، لكن في أول تدريب استخدمت فقط 50 ملفًا بحجم إجمالي حوالي 187 ميغابايت. يمكنك عرض قائمة الوثائق:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## كم حجم النماذج؟

الإصدار V0: عدد المعلمات 16 مليون

الإصدار V0.5: عدد المعلمات 123 مليون

الإصدار V1: عدد المعلمات 700 مليون

# مواصفات التدريب؟

#V0/V0.5
GPU: Geforce rtx 4060
CPU: i5-13400F
Ram: 16GB DDR5.

#V1

GPU: A100 rented









---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-08-12

---