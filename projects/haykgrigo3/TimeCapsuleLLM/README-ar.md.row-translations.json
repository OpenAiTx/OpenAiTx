[
  {
    "row": 1,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 2,
    "rowsha": "cWgam+tnnXudu7i74+ahMEGk/A9dQS+EwWLAIfi3dHk=",
    "originContent": "<div align=\"right\">",
    "translatedContent": "<div align=\"right\">"
  },
  {
    "row": 3,
    "rowsha": "orOcu5ARna/hb3RUkj6dBI8pHTM3WHeTvby17l5E0h0=",
    "originContent": "  <details>",
    "translatedContent": "  <details>"
  },
  {
    "row": 4,
    "rowsha": "TtgkLzblnvP0q9aAIVXt6s2LczXjy5k+QvHKcU0/5Ms=",
    "originContent": "    <summary >🌐 Language</summary>",
    "translatedContent": "    <summary >🌐 اللغة</summary>"
  },
  {
    "row": 5,
    "rowsha": "fZtk4rPTAJEEslnbhSVkHEcPlsctYSzAV7CDPL3rJmA=",
    "originContent": "    <div>",
    "translatedContent": "    <div>"
  },
  {
    "row": 6,
    "rowsha": "9KQxOeJSigvTmGWO+mtnl8kZY9zQfueoy8sk4lYm09Q=",
    "originContent": "      <div align=\"center\">",
    "translatedContent": "      <div align=\"center\">"
  },
  {
    "row": 7,
    "rowsha": "CeOhdpchZBoZSEUDtSE417JEcMBSZw18jeJuHJBKB2Y=",
    "originContent": "        <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en\">English</a>",
    "translatedContent": "        <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en\">English</a>"
  },
  {
    "row": 8,
    "rowsha": "ToO7MFa3QrNNljdQWIagsnOPxe8cXuuA2m5msIm+Kbs=",
    "originContent": "        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN\">简体中文</a>",
    "translatedContent": "        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN\">简体中文</a>"
  },
  {
    "row": 9,
    "rowsha": "MRATmWdRMRw0JU4u9h5pMb6GU17lQFgG9v/bpGLr9pM=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">繁體中文 (coming soon)</a> |",
    "translatedContent": "        | <a href=\"#\" title=\"قريباً\">繁體中文 (قريباً)</a> |"
  },
  {
    "row": 10,
    "rowsha": "GY7LXxG3rk5eFh9itcqM0cTtmHybyjLTf1icB3jN31I=",
    "originContent": "        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja\">日本語</a>",
    "translatedContent": "        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja\">日本語</a>"
  },
  {
    "row": 11,
    "rowsha": "b5TwunGJh+gsAe7aQU3dkfobXF/nknCEta1msDa7XBU=",
    "originContent": "        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko\">한국어</a>",
    "translatedContent": "        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko\">한국어</a>"
  },
  {
    "row": 12,
    "rowsha": "1/HCgPsVh2ChqMY+k/VVxEWHPRRmWWCjy5nDRibi3mM=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">हिन्दी (coming soon)</a> |",
    "translatedContent": "        | <a href=\"#\" title=\"قريباً\">हिन्दी (قريباً)</a> |"
  },
  {
    "row": 13,
    "rowsha": "3lfEHT+5HYFEvbE5cl+xujQPYjtVmzTifT37iqPTWII=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">ไทย (coming soon)</a> |",
    "translatedContent": "        | <a href=\"#\" title=\"قريباً\">ไทย (قريباً)</a> |"
  },
  {
    "row": 14,
    "rowsha": "KmG3P0px2E3bt1lU/w3eGop+zeA1j8xL0k280Zd9m2s=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">Français (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"قريباً\">Français (قريباً)</a>"
  },
  {
    "row": 15,
    "rowsha": "CSdHSEXgIs3M2Q/6zIIJ8NbKkZWhydhBqNus94qrPvg=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">Deutsch (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"قريباً\">Deutsch (قريباً)</a>"
  },
  {
    "row": 16,
    "rowsha": "8wz7pDuXc3dk+ZcqZ1jmmh8zh6xN3Wb6qWbCjxAj7dA=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">Español (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"قريباً\">Español (قريباً)</a>"
  },
  {
    "row": 17,
    "rowsha": "op/NqIZs7OjCSpNgpXk8RnqDnTegVPyWUQhuQxvTR7U=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">Italiano (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"قريباً\">Italiano (قريباً)</a>"
  },
  {
    "row": 18,
    "rowsha": "tAvlfwut/Ad9q1huxc8EREZGv7vYHbrEujzUS8xoaQo=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">Русский (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"قريباً\">Русский (قريباً)</a>"
  },
  {
    "row": 19,
    "rowsha": "WhhSpeeCUUAqJiVTS4Fvyc6A2c+24Jnj3MW7XLQuIcI=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">Português (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"قريباً\">Português (قريباً)</a>"
  },
  {
    "row": 20,
    "rowsha": "0yPXPrWh+Vzc6FBE9iiciw5HwpOSmo05HNe36wfTWCI=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">Nederlands (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"قريباً\">Nederlands (قريباً)</a>"
  },
  {
    "row": 21,
    "rowsha": "mdW6YUUXf5KzI4CwZxrE08ofaLonUOMnJpN3vPR7Y2A=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">Polski (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"قريباً\">Polski (قريباً)</a>"
  },
  {
    "row": 22,
    "rowsha": "sw1AXxAGQNvn4eSG9enTWNkwKH0yr6LlVtXBH1j9z8s=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">العربية (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"قريباً\">العربية (قريباً)</a>"
  },
  {
    "row": 23,
    "rowsha": "I8dh9zmXisU0+CpddA55QQgvujH03J/dEnXgj5aFtQM=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">فارسی (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"قريباً\">فارسی (قريباً)</a>"
  },
  {
    "row": 24,
    "rowsha": "7VFv8o6de72ciJrbh3mctfrEgCJhNvuKGWJNOmCaPdM=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">Türkçe (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"قريباً\">Türkçe (قريباً)</a>"
  },
  {
    "row": 25,
    "rowsha": "C+XRvFz/D3o9/JyPqwitsxtskFZleJC/oFUr4SEeiHA=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">Tiếng Việt (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"قريباً\">Tiếng Việt (قريباً)</a>"
  },
  {
    "row": 26,
    "rowsha": "ntGI5B+n9x96pV3ZG5GG83nmocQbxTJjKY7VVwa6Rq8=",
    "originContent": "        | <a href=\"#\" title=\"Coming soon\">Bahasa Indonesia (coming soon)</a>",
    "translatedContent": "        | <a href=\"#\" title=\"قريباً\">Bahasa Indonesia (قريباً)</a>"
  },
  {
    "row": 27,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 28,
    "rowsha": "0OM5wNEm0TO56MEBvQzL7AUZM7/3OpgIeqRf2zFre3Q=",
    "originContent": "      </div>",
    "translatedContent": "      </div>"
  },
  {
    "row": 29,
    "rowsha": "fcjTfY+fs8YnY5slBs1sZvWPAqEQR7tzaBDO54skkGQ=",
    "originContent": "    </div>",
    "translatedContent": "    </div>"
  },
  {
    "row": 30,
    "rowsha": "+fQNH2ldI7UM/rqRscP3hUSWAmw1HvQ2wEKDN8JagT0=",
    "originContent": "  </details>",
    "translatedContent": "  </details>"
  },
  {
    "row": 31,
    "rowsha": "qsMmUbEPVnxGG5tPJV1vsfpoWbU2jYvZpRr5IKshzyM=",
    "originContent": "</div>",
    "translatedContent": "</div>"
  },
  {
    "row": 32,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 33,
    "rowsha": "VRGjp0FtfvQ89lbX/wJLis2ypCRtNJwe8ViIi29+Rko=",
    "originContent": "# TimeCapsule LLM",
    "translatedContent": "# TimeCapsule LLM"
  },
  {
    "row": 34,
    "rowsha": "XGlykErifWX9oIzV4ZXDc4AUnsuesz8LvpruG76e6uY=",
    "originContent": "An LLM trained only on data from certain time periods to reduce modern bias.",
    "translatedContent": "نموذج لغة كبير تم تدريبه فقط على بيانات من فترات زمنية محددة لتقليل الانحياز الحديث."
  },
  {
    "row": 35,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 36,
    "rowsha": "06wDXO9Un3ot9kUKAGg7CaRsIVSkfS1d2m+EQ6HOFog=",
    "originContent": "Imagine if an AI model didnt just pretend to be historical but actually was.",
    "translatedContent": "تخيل لو أن نموذج الذكاء الاصطناعي لم يكن يتظاهر فقط بأنه تاريخي، بل كان بالفعل كذلك."
  },
  {
    "row": 37,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 38,
    "rowsha": "2773v/qIXSAsW4pN2HtYcVltfzG1vzgVbHgfjStBQIY=",
    "originContent": "Built on [nanoGPT by Andrej Karpathy](https://github.com/karpathy/nanoGPT) Core training scripts and model architecture are his work. ",
    "translatedContent": "مبني على [nanoGPT بواسطة أندريه كارباتي](https://github.com/karpathy/nanoGPT) نصوص التدريب الأساسية وهيكل النموذج هي من عمله."
  },
  {
    "row": 39,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 40,
    "rowsha": "wITJJBD/4abiy4E37iMdOcGmifkmz4dALLyk6AhA1kc=",
    "originContent": "# Project Goals ",
    "translatedContent": "# أهداف المشروع"
  },
  {
    "row": 41,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "تايم كابسول LLM هو مشروع تجريبي سيتم تدريبه فقط على النصوص المكتوبة خلال فترات زمنية محددة. الهدف هو محاكاة الرؤية للعالم واللغة لعصور تاريخية معينة."
  },
  {
    "row": 42,
    "rowsha": "LlW7r/H8NhftFgGAHce1f4KThGgsoT8aJ88/IsiLntc=",
    "originContent": "TimeCapsule LLM is an expirimental project that will only be trained on texts written during certain time periods. The goal is to simulate the worldview and language of specific historical eras.",
    "translatedContent": ""
  },
  {
    "row": 43,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "# لماذا التخصيص ليس كافياً"
  },
  {
    "row": 44,
    "rowsha": "obYFMCTDj8qHZGo0BQtA2AlwA8JgNcjDK9WlMRI4eq8=",
    "originContent": "# Why fine tuning isn't enough ",
    "translatedContent": ""
  },
  {
    "row": 45,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "إذا قمت فقط بتخصيص نموذج مدرب مسبقاً، سيظل نموذج LLM الخاص بك يعرف المفاهيم الحديثة. بالطبع تحقيق انعدام التحيز الحديث أمر صعب، لكنني أريد الاقتراب قدر الإمكان من ذلك. الحصول على انعدام كامل للتحيز الحديث يتطلب تدريب نموذج من الصفر."
  },
  {
    "row": 46,
    "rowsha": "yNEBOKV/RnG7CvDjiWhkXKK6vqbwki9QKC+Zs+8PzbM=",
    "originContent": "If you just fine tune a pre-trained model, your LLM is still gonna know modern concepts. Of course achieving zero modern bias is difficult but I want to get as close as possible to this. Getting no modern bias requires training a model from scratch.",
    "translatedContent": ""
  },
  {
    "row": 47,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "# النتائج المتوقعة"
  },
  {
    "row": 48,
    "rowsha": "SdJkrN/DUD4+aOCh9lfDM3AAqMxlyukDfye/nzXzxN0=",
    "originContent": "# Expected outcomes ",
    "translatedContent": ""
  },
  {
    "row": 49,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "آمل عند الانتهاء أن هذا النموذج لن يعرف المفاهيم الحديثة ولن يستطيع الاستدلال بما يتجاوز ما تم تدريبه عليه. يجب ألا يتعرف على المفاهيم أو المفردات الحديثة ولا يجب أن يهلوس بمعرفة حديثة."
  },
  {
    "row": 50,
    "rowsha": "gEYZXexiMGI/47WQffUoZrmXtyl2Y/NvOPHilNrWEvg=",
    "originContent": "Hopefully when finished, this model will not know modern concepts and will not be able to reason beyond what it's been trained on. It shouldnt recognize modern concepts/vocab and should not hallucinate modern knowledge.",
    "translatedContent": ""
  },
  {
    "row": 51,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "# تحديثات التقدم"
  },
  {
    "row": 52,
    "rowsha": "8EWRxPaogE2BaXxVJE1VFNAXNdS6KUYPLDFN8xlQ9LE=",
    "originContent": "# Progress Updates",
    "translatedContent": ""
  },
  {
    "row": 53,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "## 9 يوليو 2025"
  },
  {
    "row": 54,
    "rowsha": "oq91hnNV5WwmrEF0amya8kSN7gu21MN5nOcR2dPRBZ0=",
    "originContent": "## July 9th, 2025",
    "translatedContent": ""
  },
  {
    "row": 55,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "حددت الفترة الزمنية بين 1800-1850 والمنطقة: لندن"
  },
  {
    "row": 56,
    "rowsha": "yU3u8taDdAaD23tJB9+n/2wmry0GfF+KXqADT4YLuJ8=",
    "originContent": "I've set my time period for 1800-1850 and region: London ",
    "translatedContent": ""
  },
  {
    "row": 57,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "جمعت قائمة بالنصوص والكتب والمستندات"
  },
  {
    "row": 58,
    "rowsha": "uQe95shOfOi8NA0M2/CQCXlOjNsiSmGrt5dZbeP4ANs=",
    "originContent": "I've gathered a list of texts, books, documents ",
    "translatedContent": ""
  },
  {
    "row": 59,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "حتى الآن حصلت على 50 ملف نصي وسأبدأ تدريب NanoGPT قريباً"
  },
  {
    "row": 60,
    "rowsha": "i9Kzka7MMa5yKjfdslauZFzKk+gAcnyILwscFoaepYs=",
    "originContent": "So far I've gotten 50 as txt files and will begin training NanoGPT soon ",
    "translatedContent": ""
  },
  {
    "row": 61,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "سأواصل تحديث هذا طالما هناك تقدم"
  },
  {
    "row": 62,
    "rowsha": "Wov5RgnyTA0P0gtJKLL0GTcSf7t8WrgFcAzDufE5Xh4=",
    "originContent": "Will update this as long as progress is made",
    "translatedContent": ""
  },
  {
    "row": 63,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "## 13 يوليو 2025"
  },
  {
    "row": 64,
    "rowsha": "FSOOW2G6pyPozg+3u76To6E4Pthd9lRoZE396fwY2I4=",
    "originContent": "## July 13th, 2025",
    "translatedContent": ""
  },
  {
    "row": 65,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "تم تدريب nanoGPT على 187 ميجابايت من بيانات النصوص التاريخية."
  },
  {
    "row": 66,
    "rowsha": "GszU76q+4dgDkDO4uNZpx/9WhQTTCZlq4VYIt0ZdgD0=",
    "originContent": "Trained nanoGPT with 187MB of historial text data. ",
    "translatedContent": ""
  },
  {
    "row": 67,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "## 15 يوليو 2025"
  },
  {
    "row": 68,
    "rowsha": "pGPL3z/t2hDtXa67ubNgmRC/+b4O2Yp/0ff3R/9mraE=",
    "originContent": "## July 15th, 2025",
    "translatedContent": ""
  },
  {
    "row": 69,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "بدأت بتنزيل نصوص للجولة التدريبية الثانية. أحصل على كل شيء من أرشيف الإنترنت ووسعت الفترة الزمنية إلى 1800-1875. للحصول على مجموعة متنوعة من النصوص، يمكنك استخدام فلاتر الموضوع والبحث عن مكان النشر والفترة الزمنية والمواضيع في أرشيف الإنترنت."
  },
  {
    "row": 70,
    "rowsha": "+jwjqBw9Cr+lRnmxUzCQ0SnVBfXYeJDycuYf/p0JJgg=",
    "originContent": "I started downloading texts for the second training run. I'm getting everything from Internet Archive and I've expanded the time period to 1800-1875. To get a diverse range of texts, you can use subject and search filters for publication location, time period and subjects on Internet Archive. ",
    "translatedContent": ""
  },
  {
    "row": 71,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "![فلاتر البحث](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)"
  },
  {
    "row": 72,
    "rowsha": "XE9Xts6Q8wsZVZHg8uD/1ZXBQ/j2uFLsR9HwsiaqMds=",
    "originContent": "![Search Filters](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)",
    "translatedContent": ""
  },
  {
    "row": 73,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "## 16 يوليو 2025"
  },
  {
    "row": 74,
    "rowsha": "iREIjirFxs+ic0QmjQG1FQYKHC5brQaZ5JjPaEto+lU=",
    "originContent": "## July 16th, 2025",
    "translatedContent": ""
  },
  {
    "row": 75,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "قمت بتنزيل حوالي 500 ملف نصي من أرشيف الإنترنت وبعد تنظيفها (فقط حذف المسافات البيضاء، رؤوس غوتنبرغ، إلخ) أصبح لدي حوالي 500 ميجابايت من البيانات. إنها مجموعة بيانات صغيرة لكن آخر مرة تدربت على 187 ميجابايت لذا يجب أن يكون هناك فرق ملحوظ في الناتج بعد تدريب النموذج الثاني. آمل أن يتمكن هذا النموذج على الأقل من إنتاج جمل أكثر ترابطاً وتبدو منطقية نوعاً ما. بالطبع لا يوجد ضمان لأن هذه ما تزال مجموعة بيانات صغيرة جداً، لكنها أكثر مما استخدمته في المرة السابقة."
  },
  {
    "row": 76,
    "rowsha": "c1Ww8CUkqpg5TNm17QY7m130dQycuSFaAia2gfx/uLw=",
    "originContent": "I downloaded around 500 txt files from Internet Archive and after cleaning them (just deleting whitespaces, Gutenberg headers, etc) I have around 500MB of data. It's a tiny dataset but last time I trained off of 187MB so there should be at least some kind of noticable difference in the output after I train the second model. I'm hoping this model can at least produce more coherent sentences that kind of make sense. It's not a guarantee of course since this is still a tiny tiny dataset, but it's more than what I used last time. ",
    "translatedContent": ""
  },
  {
    "row": 77,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "يجب أن يكون هذا قابلاً للتنفيذ على جهازي الخاص، وهذا جيد أيضاً لأنني آمل أن أرى بعض التحسينات قبل أن أنتقل إلى مجموعة بيانات أكبر مما سيتطلب مني استئجار وحدة معالجة رسومات GPU. لكن لا تقلق، ما زلت أخطط لاستئجار GPU قريباً، لكن قبل أن أفعل ذلك أريد التأكد من أن مجموعة البيانات الخاصة بي منظمة ونظيفة قدر الإمكان. إحدى المشكلات التي أواجهها هي التنظيف، العديد من هذه الملفات النصية تحتوي على رموز غير مفهومة. البرامج النصية التي استخدمتها للتنظيف تعمل لكن ليست فعالة بنسبة 100%."
  },
  {
    "row": 78,
    "rowsha": "h/hyxvgOlOm5er9sn3CL2wmktMoq2q+qZi5Vi7upXGI=",
    "originContent": "This should be doable on my own hardware, it's good too because I can hopefully see some kind of improvements before I jump to a bigger dataset which would require me to rent a GPU. But don't worry I still plan on renting a GPU soon, but before I do that I wanna make sure my dataset is as curated and clean as possible. One of the issues I have is cleaning, a lot of these txt files have gibberish mixed in. The scripts I've used for cleaning do work but they're not 100% effective. ",
    "translatedContent": ""
  },
  {
    "row": 79,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "سأتدرب على هذه المجموعة اليوم ويجب أن يستغرق ذلك حوالي 4-5 ساعات. بمجرد الانتهاء والاختبار، سأعطي التحديثات. شكراً مرة أخرى لكل من يتابع مشروعي، لقد أرسل لي بعض الأشخاص حتى روابط لموارد OCR فشكراً لكم! آمل أن يجرب المزيد من الناس ذلك ويجربوا مجموعاتهم الخاصة من البيانات."
  },
  {
    "row": 80,
    "rowsha": "8LYxYjHwrXnU59N+13Fd9m653Tgyom3yQQAWGIrSPSc=",
    "originContent": "I will train this dataset today and it should take around 4-5 hours. Once it's done and I test it, I will give updates. Thank you again to everyone whos checking out my project, I've even had some people even giving me links to OCR resources so Thank you! I hope more people try this out and expirement with they're own datasets. ",
    "translatedContent": ""
  },
  {
    "row": 81,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 82,
    "rowsha": "ih02CyPS6D3hq8065ysVT4Teh0oimcSXOr3a0ZlSDUc=",
    "originContent": "## August 12, 2025",
    "translatedContent": "## 12 أغسطس 2025"
  },
  {
    "row": 83,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 84,
    "rowsha": "Vu3bDLQeTGUJ45FQ5g9LkghhutbMWGXdzzPrlGb0Jzc=",
    "originContent": "Version 1 is complete, this version uses Phi 1.5 (700M parameters) and is trained on a dataset that has almost 7000 texts. The main improvements with this model is it's ability to sometimes reference real names and historical events. Version 0 and 0.5 could not do this, whenever I prompted those models with something like \"In the year 1821\" the output would just be hallucinations, stuff that never happened. An example of this with Version 1:",
    "translatedContent": "الإصدار 1 مكتمل، هذا الإصدار يستخدم Phi 1.5 (700 مليون معلمة) وتم تدريبه على مجموعة بيانات تحتوي على ما يقارب 7000 نص. التحسينات الرئيسية في هذا النموذج هي قدرته أحيانًا على الإشارة إلى أسماء حقيقية وأحداث تاريخية. لم يتمكن الإصدار 0 و0.5 من فعل ذلك، فعندما كنت أوجه تلك النماذج بشيء مثل \"في عام 1821\" كان الناتج مجرد هلوسات، أشياء لم تحدث أبدًا. مثال على ذلك مع الإصدار 1:"
  },
  {
    "row": 85,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 86,
    "rowsha": "9IdQmw3qQSL7u/5HdEkUTrA3mB3XinzAhLuryah0cD0=",
    "originContent": "My prompt: It was the year of our Lord 1834, ",
    "translatedContent": "مطلبي: كان عام ربنا 1834،"
  },
  {
    "row": 87,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 88,
    "rowsha": "zvm/1tcaMFjcQmuN7y0hczAbbgAwaKRtjBpah5/+kb8=",
    "originContent": "The output: \"It was the year of our Lord 1834 and the streets of London were filled with protest and petition. The cause, as many re counted, was not bound in the way of private, but having taken up the same day in the day of Lord Palmerston, the public will receive a short statement of the difficulties under which the day of law has reached us. It is a matter of deep regret, that the present events in the history of the world are clear, and consequently will be'known. It is not true that the very men who first settled in the Gospel at Jerusalem should have so extensive and so interesting a record of the prosperity and prosperity\" ",
    "translatedContent": "الناتج: \"كان عام ربنا 1834 وكانت شوارع لندن مليئة بالاحتجاجات والعرائض. السبب، كما رواه الكثيرون، لم يكن مرتبطًا بالأمر الخاص، بل تم تناوله في نفس اليوم في يوم اللورد بالمرستون، سيستلم العامة بيانًا موجزًا عن الصعوبات التي وصل إليها يوم القانون. إنه لأمر مؤسف للغاية، أن الأحداث الحالية في تاريخ العالم واضحة، وبالتالي ستعرف. ليس صحيحًا أن الرجال الذين استقروا أولاً في الإنجيل في القدس يجب أن يكون لديهم سجل واسع ومثير للاهتمام عن الرخاء والازدهار\""
  },
  {
    "row": 89,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 90,
    "rowsha": "D65A3DA1psBrWR2f/2JT9alA5NzUEjFgRJ2C66XhdLE=",
    "originContent": "At first I assumed this might be a coincidence but take a look at this: ![1834protest](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)",
    "translatedContent": "في البداية اعتقدت أن هذا قد يكون صدفة لكن انظر إلى هذا: ![1834protest](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)"
  },
  {
    "row": 91,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 92,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 93,
    "rowsha": "mNjt3ebwxfwyPM2/AK7E/GDTFM6i95HG8GZ/8TZs9EA=",
    "originContent": "### Training Update ",
    "translatedContent": "### تحديث التدريب"
  },
  {
    "row": 94,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 95,
    "rowsha": "RxsWSGTgM12Md9v3+GBLg3PyoUxQ9kl1AglpanRmZqE=",
    "originContent": "I started training on a 435MB (108 M tokens) corpus, it's going pretty smooth right now. Train loss dropped from 10.9 to 4.9 in the first 2800 iterations. I expect it'll take around 8 or 9 hours to complete. I'll post another update once it's done.",
    "translatedContent": "بدأت التدريب على مجموعة بيانات بحجم 435 ميجابايت (108 مليون رمز)، يسير الأمر بسلاسة الآن. انخفض الفقد التدريبي من 10.9 إلى 4.9 في أول 2800 تكرار. أتوقع أن الأمر سيستغرق حوالي 8 أو 9 ساعات للانتهاء. سأقوم بنشر تحديث آخر عند الانتهاء."
  },
  {
    "row": 96,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 97,
    "rowsha": "+KGe6w5lIW875VBdo8dnngqlaSzfoN0zFImo+SuVZ3k=",
    "originContent": "## July 17th, 2025",
    "translatedContent": "## 17 يوليو 2025"
  },
  {
    "row": 98,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 99,
    "rowsha": "Q0uM34dBNqytALNUZSPxoBQZT3LxqlwyioEi3nTshXQ=",
    "originContent": "The training is done for the second model, it took my 4060 around 8 hours and 40 minutes (3,900 iters/hr) for 33,000 iters (5 epochs). Final train loss was 3.73. The outputs were suprisingly good it genuinely generates coherent 19th century style sentences now. ",
    "translatedContent": "انتهى التدريب للنموذج الثاني، استغرق الأمر من بطاقة 4060 حوالي 8 ساعات و40 دقيقة (3,900 تكرار/ساعة) لـ 33,000 تكرار (5 دورات). كان الفقد النهائي للتدريب 3.73. كانت النتائج جيدة بشكل مفاجئ، حيث ينتج الآن جمل متماسكة بأسلوب القرن التاسع عشر فعلاً."
  },
  {
    "row": 100,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 101,
    "rowsha": "13xzxThQO3iEGB5PU51V1kYgK85//yEFbCkwtu07a2U=",
    "originContent": "## July 28th, 2025 ",
    "translatedContent": "## 28 يوليو 2025"
  },
  {
    "row": 102,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 103,
    "rowsha": "R6HmXRpR+7izGCE6kI9wyELs4kYvAtLMWO75nOXAf1M=",
    "originContent": "I've gone ahead and uploaded v0.5 to Hugging Face, [Check it out](https://huggingface.co/haykgrigorian/TimeCapsuleLLM) if youd like. You can now download my repo and run it locally. Unfortunately nanoGPT doesn't work natively with HuggingFace, so you'll have to download and run the model locally. ",
    "translatedContent": "قمت برفع الإصدار 0.5 على Hugging Face، [اطلع عليه هنا](https://huggingface.co/haykgrigorian/TimeCapsuleLLM) إذا رغبت بذلك. يمكنك الآن تحميل المستودع وتشغيله محليًا. للأسف nanoGPT لا يعمل بشكل أصلي مع HuggingFace، لذا عليك تحميل النموذج وتشغيله محليًا."
  },
  {
    "row": 104,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 105,
    "rowsha": "3/uUOOHuCoxdKkQ6c+1J96AfmOsQ59aZxo/FYGK9OlI=",
    "originContent": "Also I will begin curating data for my next training run, I believe I'll need maybe 5-10x more data to achieve reasoning capabilities. ",
    "translatedContent": "سأبدأ أيضًا في جمع البيانات لجولة التدريب التالية، أعتقد أنني سأحتاج ربما 5-10 أضعاف البيانات الحالية لتحقيق قدرات الاستدلال."
  },
  {
    "row": 106,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 107,
    "rowsha": "UXyL9heEocaX5oRp2rVKQgcRB28wVdVl+Kl7/DqbKLc=",
    "originContent": "## August 2nd, 2025",
    "translatedContent": "## 2 أغسطس 2025"
  },
  {
    "row": 108,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 109,
    "rowsha": "PTcIrGtJ33LlUO6TFsGwvV/mCnbcxmmcYIasCuuLx40=",
    "originContent": "I'm going to start work on Version 1 soon. I will need to transition from nanoGPT's architecture to soemthing more modern. I have several open-source LLM archictectures in mind, including: OpenLLaMA v3, Phi-2 and Qwen 1.5B. And to support the jump to V1, I'll need to carefully curate a much bigger and diverse dataset. I'll need at least 5GB of clean training data.",
    "translatedContent": "سأبدأ العمل على الإصدار 1 قريبًا. سأحتاج للانتقال من معمارية nanoGPT إلى شيء أكثر حداثة. لدي عدة معماريات LLM مفتوحة المصدر في بالي، منها: OpenLLaMA v3، Phi-2 و Qwen 1.5B. ولدعم الانتقال إلى V1، سيتعين علي اختيار مجموعة بيانات أكبر وأكثر تنوعًا بعناية. سأحتاج إلى ما لا يقل عن 5 جيجابايت من بيانات التدريب النظيفة."
  },
  {
    "row": 110,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 111,
    "rowsha": "YbYVsndAe75CgxPU/J34HsXOaqcQlIjTCm06o4eVQIg=",
    "originContent": "# V0 Model Behavior & Limitations ",
    "translatedContent": "# سلوك وقيود نموذج V0"
  },
  {
    "row": 112,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 113,
    "rowsha": "OCh97kyOITXqFpKyZsol6voS+nrzc9n9flpDpNyf35w=",
    "originContent": "Early prompts show the model responding with 1800's language and behavior. For example, I prompted it with \"Who art Henry?\" and it replied \"I know that man, I have did not a black, the storm.\" and yeah that sentence makes no sense but the LLM is recognizing I'm asking about a person. ",
    "translatedContent": "العروض الأولية تظهر النموذج يستجيب بلغة وسلوك القرن التاسع عشر. على سبيل المثال، وجهته بسؤال \"من هو هنري؟\" وأجاب \"أعرف ذلك الرجل، لم أكن أسود، العاصفة.\" نعم هذه الجملة لا معنى لها لكن النموذج يفهم أنني أسأل عن شخص."
  },
  {
    "row": 114,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 115,
    "rowsha": "yKIR0teTc66wVDG+jdIyNmAzItXb2JH2ld3D7tm4qnM=",
    "originContent": "![TimeLockLLM Sample Output](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)",
    "translatedContent": "![TimeLockLLM Sample Output](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)"
  },
  {
    "row": 116,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 117,
    "rowsha": "/GR84OQ/Xp3d+Yv3/vVIIQFyl8ExKQLgjT1Go5rPplE=",
    "originContent": "There is no mention of modern concetps, outputs contain mostly words and phrasing from the 1800's.",
    "translatedContent": "لا يوجد ذكر للمفاهيم الحديثة، والنواتج تحتوي غالبًا على كلمات وتراكيب من القرن التاسع عشر."
  },
  {
    "row": 118,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 119,
    "rowsha": "aojQk3KjkX8shkLnJSZvUZBo0vk4tKDT9aysMfY9yK4=",
    "originContent": "It still needs alot of work, training off of 187MB will not give you a model that produces text with complex reasoning. ",
    "translatedContent": "لا يزال بحاجة للكثير من العمل، التدريب على 187 ميجابايت لن يمنحك نموذجًا ينتج نصوصًا باستدلالات معقدة."
  },
  {
    "row": 120,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 121,
    "rowsha": "SzQ3fs08W+JGBPGw6+aEzLsm/vGfQNDJ6YZnfFE0F4I=",
    "originContent": "Right now it produces sentences that lack full sentence structure and overall just make no sense but this is normal for the training size. ",
    "translatedContent": "حالياً، ينتج النموذج جمل تفتقر إلى البنية الكاملة للجملة وبشكل عام غير مفهومة، لكن هذا أمر طبيعي لحجم التدريب الحالي."
  },
  {
    "row": 122,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 123,
    "rowsha": "eeaXlcgOJhwk4liDhaA6/6S4URmAdV8vj9hlqGr3LFw=",
    "originContent": "# V0.5 Model Behavior & Limitations",
    "translatedContent": "# سلوك النموذج V0.5 والقيود"
  },
  {
    "row": 124,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 125,
    "rowsha": "tII9OX/M7KpPoQKPIphmQBWtsSVucUevvTnzVsTXcaI=",
    "originContent": "This is a nice improvement compared to the last model. The writing style and vocab is Victorian and almost every sentence is grammatically correct with proper punctuation. And again this is trained from scratch so it sticks to 1800's subjects. ",
    "translatedContent": "هذا تحسن جيد مقارنة بالنموذج السابق. أسلوب الكتابة والمفردات من العصر الفيكتوري ومعظم الجمل صحيحة نحويًا مع علامات ترقيم مناسبة. ومرة أخرى، تم تدريب هذا النموذج من الصفر لذا فهو يلتزم بمواضيع القرن التاسع عشر."
  },
  {
    "row": 126,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 127,
    "rowsha": "8DhXpgpVtg05XdyplRHf49EFOQNCJVzXA9RpmJQ+y9U=",
    "originContent": "![TimeLockLLM Sample Output](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)",
    "translatedContent": "![TimeLockLLM Sample Output](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)"
  },
  {
    "row": 128,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 129,
    "rowsha": "YgdLunUOAWWHsq+eAfyWcDz1fCc0rCcobnRpEXHoK94=",
    "originContent": "There are a lot of factual hallucinations. A lot (like 100%) of the details (dates, events, historical figures)  are made up. Also the sentences don't really have connections to each other, sometimes maybe 2 sentences will relate to each other but beyond that they dont. Another issue is sometimes a stray “Digitized by Google” footer shows up, so the next time I train I really have to make sure the texts are cleaned well. Overall I'm very happy with the results, it's nowhere near an LLM yet but definitely a sentence generator. ",
    "translatedContent": "هناك الكثير من الهلوسات الواقعية. الكثير (تقريبًا 100٪) من التفاصيل (التواريخ، الأحداث، الشخصيات التاريخية) مختلقة. أيضًا الجمل ليس بينها ترابط حقيقي، أحيانًا قد ترتبط جملتين ببعضهما لكن غالبًا لا يوجد ترابط. مشكلة أخرى هي ظهور تذييل \"Digitized by Google\" أحيانًا، لذلك في التدريب القادم يجب أن أتأكد من تنظيف النصوص جيدًا. بشكل عام أنا سعيد جدًا بالنتائج، لا يزال بعيدًا عن كونه نموذج لغة كبير لكن بالتأكيد مولد جمل."
  },
  {
    "row": 130,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 131,
    "rowsha": "V44Ne2sN8v7ZVpRZ5vo7B2aUZFjGppYthg0fjsQAqd0=",
    "originContent": "I'm learning a lot and will start figuring out what I need to do better in the coming weeks. I will upload files soon! ",
    "translatedContent": "أتعلم الكثير وسأبدأ في معرفة ما يجب أن أفعله بشكل أفضل في الأسابيع القادمة. سأرفع الملفات قريبًا!"
  },
  {
    "row": 132,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 133,
    "rowsha": "KYAZ5TGCcW3BtPPWPqj6LyktFuBuQHXKdjySkeqqliY=",
    "originContent": "# V1 Model Behavior & Limitations ",
    "translatedContent": "# سلوك النموذج V1 والقيود"
  },
  {
    "row": 134,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 135,
    "rowsha": "5ERqUEB22vCoQvNkDV/y8ULDJA4ly6K7xDVXDOYkAW4=",
    "originContent": "I will upload some example output soon and also do comparisons between the 3 models with the same prompt. I will also upload V1 to huggingface like I did with my last verison, you can find my huggingface account here: https://huggingface.co/haykgrigorian/TimeCapsuleLLM",
    "translatedContent": "سأرفع بعض الأمثلة قريبًا وأجري مقارنات بين النماذج الثلاثة باستخدام نفس المحفز. أيضًا سأرفع V1 على huggingface مثلما فعلت مع الإصدار السابق، يمكنك العثور على حسابي في huggingface هنا: https://huggingface.co/haykgrigorian/TimeCapsuleLLM"
  },
  {
    "row": 136,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 137,
    "rowsha": "iMvQyl++GAiLpFMT+58v3e9/hb7zXnIJauchG5p986Y=",
    "originContent": "# Upcoming Plans ",
    "translatedContent": "# الخطط القادمة"
  },
  {
    "row": 138,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 139,
    "rowsha": "OIt/QAbIp2G7kt87dbAIf6Ec0U3HnwjOW6LYRmdf68I=",
    "originContent": "(Completed) I'm going to start work on version 0.5 , instead of training using 50 books, I'll train using ideally 500-600. Right now I'm training nanoGPT using books from 1800-1850 and specifically from London. There is some challeneges like making sure the books I find are not updated or have modern interpretations but untouched books published withtin my chosen time period.",
    "translatedContent": "(تم الإنجاز) سأبدأ العمل على الإصدار 0.5، بدلًا من التدريب باستخدام 50 كتابًا، سأدرب باستخدام 500-600 كتابًا. حاليًا أقوم بتدريب nanoGPT باستخدام كتب من 1800-1850 ومن لندن تحديدًا. هناك تحديات مثل التأكد من أن الكتب التي أجدها ليست محدثة أو لها تفسيرات حديثة بل كتب غير معدلة تم نشرها ضمن الفترة الزمنية المختارة."
  },
  {
    "row": 140,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 141,
    "rowsha": "13sW7eekt8pSDdOxr/oKukwKEOVmw/M8xiAJC1VFXZc=",
    "originContent": "I want to train a new model (v1) with a much larger corpus, maybe 5-10x larger than the one I used for v0.5. My goal is to see if I can get reasoning abilities to emerge from Selective Temporal Training alone, this will be a more difficult task and I'm not even entirely sure if it's possible due to the fact that there is historical data limitations. In the upcoming weeks I will try to curate enough data for a 5-10GB corpus. I believe If I can get clean clean high quality data and rent a GPU, there will be progress.",
    "translatedContent": "أرغب في تدريب نموذج جديد (v1) باستخدام مجموعة بيانات أكبر بكثير، ربما 5-10 أضعاف ما استخدمته في v0.5. هدفي هو معرفة ما إذا كان بإمكاني تحقيق قدرات استنتاج من التدريب الزمني الانتقائي وحده، ستكون مهمة أصعب ولا أضمن إمكانية ذلك بسبب محدودية البيانات التاريخية. في الأسابيع القادمة سأحاول جمع بيانات كافية لمجموعة بيانات بحجم 5-10 جيجابايت. أعتقد أنه إذا تمكنت من الحصول على بيانات نظيفة عالية الجودة واستئجار GPU، سيكون هناك تقدم."
  },
  {
    "row": 142,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 143,
    "rowsha": "sO+voevLpUtEbqum6Gntntle+nPVa66c5GATAMLrgf0=",
    "originContent": "# How to Use This Project ",
    "translatedContent": "# كيفية استخدام هذا المشروع"
  },
  {
    "row": 144,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 145,
    "rowsha": "XVoXr9uzZwN09vboETojEQJe057RBzcMUjXmQRCB/jo=",
    "originContent": "This project focuses mostly on curating historical data, preparing it for training and building a tokenizer. I am not going to cover the full LLM training process, for that refer to nanoGPT by Andrej Karpathy.",
    "translatedContent": "يركز هذا المشروع بشكل أساسي على جمع البيانات التاريخية، تجهيزها للتدريب وبناء المرمز. لن أشرح عملية تدريب نموذج اللغة الكبير بالكامل، لذلك راجع nanoGPT بواسطة Andrej Karpathy."
  },
  {
    "row": 146,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 147,
    "rowsha": "kDK7XtqFkTiZvD804yYG4VEojvLrRbdEhmHEBzDAQz4=",
    "originContent": "# Step 1: Gather and Prepare Historical Texts ",
    "translatedContent": "# الخطوة 1: جمع وتجهيز النصوص التاريخية"
  },
  {
    "row": 148,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 149,
    "rowsha": "oRoOYaG+mx3SqTnhGcOKeH7W3W4wSRQmhZS4jPGMH8s=",
    "originContent": "Collect .txt files of public domain books, documents, etc from your chosen time period (e.g., London 1800-1850)",
    "translatedContent": "اجمع ملفات .txt لكتب أو وثائق ضمن الملكية العامة من الفترة الزمنية المختارة (مثال: لندن 1800-1850)"
  },
  {
    "row": 150,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 151,
    "rowsha": "EBDhZvTogrL4hqRuH095O7/tXXoy2OEskLQlang/pOA=",
    "originContent": "You can use download_texts_improved.py to download books for you if you need to.",
    "translatedContent": "يمكنك استخدام download_texts_improved.py لتنزيل الكتب إذا احتجت لذلك."
  },
  {
    "row": 152,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 153,
    "rowsha": "q5OO8x+9kIbzaQRWimI/Vo9ZowzVBtCX+TodObLsQoY=",
    "originContent": "Clean the text files using a script or manually remove headers/footer from Project Gutenberg, Modern annotations or things like OCR errors.",
    "translatedContent": "نظف ملفات النص باستخدام سكريبت أو بإزالة رؤوس/تذييلات Project Gutenberg يدويًا، أو الشروح الحديثة أو الأخطاء الناتجة عن OCR."
  },
  {
    "row": 154,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 155,
    "rowsha": "N4wsIjC0LRlClodmfCtYX3qswttJnk0psU28/mlCRTw=",
    "originContent": "prepare_dataset.py should work fine.",
    "translatedContent": "يجب أن يعمل prepare_dataset.py بشكل جيد."
  },
  {
    "row": 156,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 157,
    "rowsha": "jDM2lr7pP+MT0pt+L0cd5nBXI83IPoT27NzIgplt7R8=",
    "originContent": "# Step 2: Build a Custom Tokenizer",
    "translatedContent": "# الخطوة 2: بناء مرمز مخصص"
  },
  {
    "row": 158,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 159,
    "rowsha": "+90EorsgO/X2bFK2pYJw+vZIpjIbuMm4QR6W/xfj8C8=",
    "originContent": "Run train_tokenizer.py or train_tokenizer_hf.py on the cleaned data.",
    "translatedContent": "شغل train_tokenizer.py أو train_tokenizer_hf.py على البيانات النظيفة."
  },
  {
    "row": 160,
    "rowsha": "tkP3Eg1rWphTQMNhN2yYg/1+AA1IdcXbGT96aRMpnwc=",
    "originContent": "This will give you vocab.json and merges.txt",
    "translatedContent": "سيعطيك vocab.json و merges.txt"
  },
  {
    "row": 161,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "هذه الملفات تحدد المفردات وقواعد الدمج لنموذجك"
  },
  {
    "row": 162,
    "rowsha": "/wqxgtOu72+x3a2xi7q23jDkx+WQv2SHrJddzpvm1Ys=",
    "originContent": "Thes files define vocab and merge rules for your model",
    "translatedContent": ""
  },
  {
    "row": 163,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "# الخطوة 3: درب نموذجك (nanoGPT)"
  },
  {
    "row": 164,
    "rowsha": "vKPAEsPxc9uYtzjTX9/rNADSDnxkKwYcYpX/aiAp8Hc=",
    "originContent": "# Step 3: Train Your Model (nanoGPT) ",
    "translatedContent": ""
  },
  {
    "row": 165,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "راجع [nanoGPT بواسطة Andrej Karpathy](https://github.com/karpathy/nanoGPT) لعملية التدريب."
  },
  {
    "row": 166,
    "rowsha": "tCDY5iXt+Z7YYeTPouMSYDX5uuFnGROxZMvHyTOIblY=",
    "originContent": "Refer to [nanoGPT by Andrej Karpathy](https://github.com/karpathy/nanoGPT) for the training process.",
    "translatedContent": ""
  },
  {
    "row": 167,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "يمكنك تدريب نموذج لغوي كبير مختلف إذا أردت، لكنني استخدمت nanoGPT"
  },
  {
    "row": 168,
    "rowsha": "bLpr9snECDSJ5ejrqDWYNT8VXSupCQKQxyLxvAmQ2Kc=",
    "originContent": "You can train a different LLM if you want, but I used nanoGPT ",
    "translatedContent": ""
  },
  {
    "row": 169,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "# الأسئلة الشائعة"
  },
  {
    "row": 170,
    "rowsha": "OoCxyGfPN5TmdzAkaPphtPx303MJJ7vpfWbKrufGH5g=",
    "originContent": "# FAQ",
    "translatedContent": ""
  },
  {
    "row": 171,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "## ما هو التدريب الزمني الانتقائي؟"
  },
  {
    "row": 172,
    "rowsha": "+5dDgPw4ILEotxso4tjjjz1cxwUei16yNQPDUKbgxoo=",
    "originContent": "## What is Selective Temporal Training ?",
    "translatedContent": ""
  },
  {
    "row": 173,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "التدريب الزمني الانتقائي (STT) هو منهجية تعلم آلي يتم فيها اختيار جميع بيانات التدريب بحيث تقع ضمن فترة زمنية تاريخية محددة. يتم ذلك من أجل نمذجة اللغة والمعرفة لتلك الحقبة دون تأثير من المفاهيم الحديثة. على سبيل المثال، النموذج الحالي الذي أملكه (v0.5) تم تدريبه على بيانات من الفترة 1800-1875 حصريًا، ولم يتم تحسينه بل تم تدريبه من الصفر مما أدى إلى إخراج يعكس الأسلوب اللغوي والسياق التاريخي لتلك الفترة."
  },
  {
    "row": 174,
    "rowsha": "hooEARKH4r/sDPh7JUtZAZ6TYMvBkTLZIcfw3g83xos=",
    "originContent": "Selective Temporal Training (STT) is a machine learning methodology where all training data is specifically curated to fall within a specific historical time period. It's done in order to model the language and knowledge of that era without influence from modern concepts. For example, the current model I have now (v0.5) is trained on data exclusively from 1800-1875, it's not fine tuned but trained from scratch resulting in output that reflects the linguistic style and historical context of that time period.",
    "translatedContent": ""
  },
  {
    "row": 175,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "## لماذا لا تستخدم فقط التحسين أو LoRA؟"
  },
  {
    "row": 176,
    "rowsha": "dVMKQ2mPI1Spc6x6r/jNG0PIR5YKpalU4MXx9JmKp/I=",
    "originContent": "## Why not just use fine-tuning or LoRA?",
    "translatedContent": ""
  },
  {
    "row": 177,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "في هذا المشروع أحاول إنشاء نموذج لغوي غير متأثر بالتحيزات الحديثة. إذا قمت بتحسين نموذج مثل GPT-2، فهو مدرّب مسبقًا ولن تختفي تلك المعلومات. إذا دربت النموذج من الصفر فلن يتظاهر النموذج اللغوي بأنه قديم، بل سيكون كذلك فعلاً. الهدف من هذا المشروع حالياً هو إنشاء شيء يمكنه الاستدلال حصريًا باستخدام المعرفة من كتب لندن المنشورة بين عامي 1800 و 1850."
  },
  {
    "row": 178,
    "rowsha": "oNvWlJHtQSyq1TwlqJyGtMzk4Z4mBIn8AW2SudzvUYs=",
    "originContent": "For this project I'm trying to create a language model that is unclouded from modern bias. If I fine-tune something like GPT-2, it's already pre-trained and that information won't go away. If I train from scratch the language model won't pretend to be old, it just will be. The Goal for this project right now is to create something can reason exclusively using knowledge from London books published between 1800 and 1850.",
    "translatedContent": ""
  },
  {
    "row": 179,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "## ما نوع البيانات التي استخدمتها في التدريب؟"
  },
  {
    "row": 180,
    "rowsha": "ByP4WlNmMoG6WIiLJNd6b080/DSciCgWmj9aYSJjAF0=",
    "originContent": "## What kind of data did you use for training?",
    "translatedContent": ""
  },
  {
    "row": 181,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "أستخدم كتباً ووثائق قانونية وصحف وكتابات أخرى من لندن بين 1800–1850. القائمة التي أرفقتها تحتوي على حوالي 200، لكن في أول تدريب استخدمت فقط 50 ملفًا بحجم إجمالي حوالي 187 ميغابايت. يمكنك عرض قائمة الوثائق:"
  },
  {
    "row": 182,
    "rowsha": "Kj6EF7wZdUrAFg4ErGmJuh9Q5Xujmb+tunpfssPKXkA=",
    "originContent": "I'm using books, legal documents, newspapers, and other writings from 1800–1850 London. The list I linked has like 200 but for the first training I just used 50 files about ~187 MB. You can view a list of the documents:",
    "translatedContent": "https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt"
  },
  {
    "row": 183,
    "rowsha": "0mxyGiLJxzp9JPCg1oA+nbIwAKJbEC4ei9kSV3Gp84Y=",
    "originContent": "https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt",
    "translatedContent": ""
  },
  {
    "row": 184,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "## كم حجم النماذج؟"
  },
  {
    "row": 185,
    "rowsha": "rrsRMYLHKsjbkzYRKilseYJKKApHSgo9buiBkZFSvlY=",
    "originContent": "## How large are the models ?",
    "translatedContent": ""
  },
  {
    "row": 186,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "الإصدار V0: عدد المعلمات 16 مليون"
  },
  {
    "row": 187,
    "rowsha": "MVeUG+6lIlH89E699W2WfE+eQmKEul25KNMIY8AtB6M=",
    "originContent": "V0: 16M Parameters",
    "translatedContent": ""
  },
  {
    "row": 188,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "الإصدار V0.5: عدد المعلمات 123 مليون"
  },
  {
    "row": 189,
    "rowsha": "hQIpZXf/UvJi3QFJEM6jrjG7vnb9cvTFg/cvKv0buqM=",
    "originContent": "V0.5 123M Parameters",
    "translatedContent": ""
  },
  {
    "row": 190,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "الإصدار V1: عدد المعلمات 700 مليون"
  },
  {
    "row": 191,
    "rowsha": "26WDqv9Q/x8glaWZTkjUWL5VNwuuXgUuXs51BhKEskk=",
    "originContent": "V1: 700M Parameters",
    "translatedContent": ""
  },
  {
    "row": 192,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "# مواصفات التدريب؟"
  },
  {
    "row": 193,
    "rowsha": "zaP9HMP5oBcc9MGTkbFGzMO2XxeWWl+QlS9Yz9nfNXc=",
    "originContent": "# Training Specs ? ",
    "translatedContent": ""
  },
  {
    "row": 194,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "#V0/V0.5"
  },
  {
    "row": 195,
    "rowsha": "F/31V18ZJZVpqdqexHnDrgTPaWCFGcintqPNuIkiXuE=",
    "originContent": "#V0/V0.5",
    "translatedContent": "GPU: Geforce rtx 4060"
  },
  {
    "row": 196,
    "rowsha": "EH8H1HW/C4Tb7LfJgVUnVGsk4pF9l40Rlev8tAkKhjI=",
    "originContent": "GPU: Geforce rtx 4060",
    "translatedContent": "CPU: i5-13400F"
  },
  {
    "row": 197,
    "rowsha": "vo3FdN37kY6VUB7PruRKfBPJDgsVJyBHIUCn/g8mt68=",
    "originContent": "CPU: i5-13400F ",
    "translatedContent": "Ram: 16GB DDR5."
  },
  {
    "row": 198,
    "rowsha": "W8fXPiQKUkoNso0PPfTvjYMy0IYo85j+gNXmB0aERO4=",
    "originContent": "Ram: 16GB DDR5.",
    "translatedContent": ""
  },
  {
    "row": 199,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": "#V1"
  },
  {
    "row": 200,
    "rowsha": "F5uxw2t7Mttv7FTw1w1bsQQrYW79NPWp2wnmKItLVk0=",
    "originContent": "#V1",
    "translatedContent": ""
  },
  {
    "row": 201,
    "rowsha": "cBfLeLi5ORliO9eNHlkjuWbV5U2y03sn/wFBoChMpoc=",
    "originContent": "GPU: A100 rented",
    "translatedContent": "GPU: A100 rented"
  },
  {
    "row": 202,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 203,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 204,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 205,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 206,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 207,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 208,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 209,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  }
]