<div align="right">
  <details>
    <summary >ğŸŒ NgÃ´n ngá»¯</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ç®€ä½“ä¸­æ–‡</a>
        | <a href="#" title="Coming soon">ç¹é«”ä¸­æ–‡ (sáº¯p ra máº¯t)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">æ—¥æœ¬èª</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">í•œêµ­ì–´</a>
        | <a href="#" title="Coming soon">à¤¹à¤¿à¤¨à¥à¤¦à¥€ (sáº¯p ra máº¯t)</a> |
        | <a href="#" title="Coming soon">à¹„à¸—à¸¢ (sáº¯p ra máº¯t)</a> |
        | <a href="#" title="Coming soon">FranÃ§ais (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Coming soon">Deutsch (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Coming soon">EspaÃ±ol (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Coming soon">Italiano (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Coming soon">Ğ ÑƒÑÑĞºĞ¸Ğ¹ (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Coming soon">PortuguÃªs (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Coming soon">Nederlands (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Coming soon">Polski (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Coming soon">Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Coming soon">ÙØ§Ø±Ø³ÛŒ (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Coming soon">TÃ¼rkÃ§e (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Coming soon">Tiáº¿ng Viá»‡t (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (sáº¯p ra máº¯t)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM
Má»™t mÃ´ hÃ¬nh LLM chá»‰ Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn dá»¯ liá»‡u tá»« cÃ¡c giai Ä‘oáº¡n thá»i gian nháº¥t Ä‘á»‹nh nháº±m giáº£m thiÃªn kiáº¿n hiá»‡n Ä‘áº¡i.

HÃ£y tÆ°á»Ÿng tÆ°á»£ng náº¿u má»™t mÃ´ hÃ¬nh AI khÃ´ng chá»‰ giáº£ vá» lÃ  lá»‹ch sá»­ mÃ  thá»±c sá»± lÃ  nhÆ° váº­y.

Dá»±a trÃªn [nanoGPT cá»§a Andrej Karpathy](https://github.com/karpathy/nanoGPT). CÃ¡c script huáº¥n luyá»‡n cá»‘t lÃµi vÃ  kiáº¿n trÃºc mÃ´ hÃ¬nh lÃ  cÃ´ng viá»‡c cá»§a Ã´ng áº¥y.

# Má»¥c tiÃªu dá»± Ã¡n

TimeCapsule LLM lÃ  má»™t dá»± Ã¡n thá»­ nghiá»‡m chá»‰ Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn cÃ¡c vÄƒn báº£n Ä‘Æ°á»£c viáº¿t trong má»™t sá»‘ giai Ä‘oáº¡n lá»‹ch sá»­ nháº¥t Ä‘á»‹nh. Má»¥c tiÃªu lÃ  mÃ´ phá»ng tháº¿ giá»›i quan vÃ  ngÃ´n ngá»¯ cá»§a cÃ¡c thá»i Ä‘áº¡i lá»‹ch sá»­ cá»¥ thá»ƒ.

# Táº¡i sao tinh chá»‰nh lÃ  chÆ°a Ä‘á»§

Náº¿u báº¡n chá»‰ tinh chá»‰nh má»™t mÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n trÆ°á»›c, LLM cá»§a báº¡n váº«n sáº½ biáº¿t cÃ¡c khÃ¡i niá»‡m hiá»‡n Ä‘áº¡i. ÄÆ°Æ¡ng nhiÃªn viá»‡c Ä‘áº¡t Ä‘áº¿n má»©c khÃ´ng cÃ³ thiÃªn kiáº¿n hiá»‡n Ä‘áº¡i lÃ  ráº¥t khÃ³ nhÆ°ng tÃ´i muá»‘n cá»‘ gáº¯ng Ä‘áº¡t gáº§n nháº¥t cÃ³ thá»ƒ. Äá»ƒ khÃ´ng cÃ³ thiÃªn kiáº¿n hiá»‡n Ä‘áº¡i thÃ¬ cáº§n huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh tá»« Ä‘áº§u.

# Káº¿t quáº£ ká»³ vá»ng

Hy vá»ng sau khi hoÃ n thÃ nh, mÃ´ hÃ¬nh nÃ y sáº½ khÃ´ng biáº¿t cÃ¡c khÃ¡i niá»‡m hiá»‡n Ä‘áº¡i vÃ  sáº½ khÃ´ng thá»ƒ suy luáº­n vÆ°á»£t quÃ¡ nhá»¯ng gÃ¬ nÃ³ Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n. NÃ³ khÃ´ng nÃªn nháº­n biáº¿t cÃ¡c khÃ¡i niá»‡m/tá»« vá»±ng hiá»‡n Ä‘áº¡i vÃ  tÃ´i hy vá»ng nÃ³ khÃ´ng bá»‹a Ä‘áº·t kiáº¿n thá»©c hiá»‡n Ä‘áº¡i.

# Cáº­p nháº­t tiáº¿n Ä‘á»™

## NgÃ y 9 thÃ¡ng 7, 2025

TÃ´i Ä‘Ã£ chá»n giai Ä‘oáº¡n thá»i gian tá»« 1800-1850 vÃ  khu vá»±c: London

TÃ´i Ä‘Ã£ táº­p há»£p má»™t danh sÃ¡ch vÄƒn báº£n, sÃ¡ch, tÃ i liá»‡u

Hiá»‡n táº¡i tÃ´i Ä‘Ã£ cÃ³ Ä‘Æ°á»£c 50 file txt vÃ  sáº½ báº¯t Ä‘áº§u huáº¥n luyá»‡n NanoGPT sá»›m

Sáº½ cáº­p nháº­t má»¥c nÃ y náº¿u cÃ³ tiáº¿n triá»ƒn

## NgÃ y 13 thÃ¡ng 7, 2025

ÄÃ£ huáº¥n luyá»‡n nanoGPT vá»›i 187MB dá»¯ liá»‡u vÄƒn báº£n lá»‹ch sá»­.

## NgÃ y 15 thÃ¡ng 7, 2025

TÃ´i báº¯t Ä‘áº§u táº£i vá» cÃ¡c vÄƒn báº£n cho láº§n huáº¥n luyá»‡n thá»© hai. TÃ´i Ä‘ang láº¥y táº¥t cáº£ tá»« Internet Archive vÃ  Ä‘Ã£ má»Ÿ rá»™ng giai Ä‘oáº¡n thá»i gian lÃªn 1800-1875. Äá»ƒ cÃ³ Ä‘Æ°á»£c nhiá»u loáº¡i vÄƒn báº£n Ä‘a dáº¡ng, báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng cÃ¡c bá»™ lá»c chá»§ Ä‘á» vÃ  tÃ¬m kiáº¿m theo Ä‘á»‹a Ä‘iá»ƒm xuáº¥t báº£n, khoáº£ng thá»i gian vÃ  chá»§ Ä‘á» trÃªn Internet Archive.

![Bá»™ lá»c tÃ¬m kiáº¿m](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## NgÃ y 16 thÃ¡ng 7, 2025

TÃ´i Ä‘Ã£ táº£i vá» khoáº£ng 500 file txt tá»« Internet Archive vÃ  sau khi lÃ m sáº¡ch (chá»‰ xÃ³a khoáº£ng tráº¯ng, tiÃªu Ä‘á» Gutenberg, v.v.) tÃ´i cÃ³ khoáº£ng 500MB dá»¯ liá»‡u. ÄÃ¢y lÃ  má»™t bá»™ dá»¯ liá»‡u ráº¥t nhá» nhÆ°ng láº§n trÆ°á»›c tÃ´i chá»‰ huáº¥n luyá»‡n vá»›i 187MB nÃªn láº§n nÃ y hy vá»ng sáº½ tháº¥y sá»± khÃ¡c biá»‡t rÃµ rá»‡t trong Ä‘áº§u ra sau khi huáº¥n luyá»‡n mÃ´ hÃ¬nh thá»© hai. TÃ´i hy vá»ng mÃ´ hÃ¬nh nÃ y cÃ³ thá»ƒ táº¡o ra cÃ¡c cÃ¢u hoÃ n chá»‰nh hÆ¡n, cÃ³ Ã½ nghÄ©a hÆ¡n. Táº¥t nhiÃªn Ä‘iá»u nÃ y khÃ´ng Ä‘áº£m báº£o vÃ¬ bá»™ dá»¯ liá»‡u váº«n ráº¥t nhá», nhÆ°ng váº«n nhiá»u hÆ¡n láº§n trÆ°á»›c.

Äiá»u nÃ y cÃ³ thá»ƒ thá»±c hiá»‡n Ä‘Æ°á»£c trÃªn pháº§n cá»©ng cÃ¡ nhÃ¢n, cÅ©ng tá»‘t vÃ¬ tÃ´i cÃ³ thá»ƒ tháº¥y Ä‘Æ°á»£c má»™t sá»‘ cáº£i thiá»‡n trÆ°á»›c khi chuyá»ƒn sang bá»™ dá»¯ liá»‡u lá»›n hÆ¡n vÃ  cáº§n thuÃª GPU. NhÆ°ng Ä‘á»«ng lo, tÃ´i váº«n dá»± Ä‘á»‹nh sáº½ thuÃª GPU sá»›m, nhÆ°ng trÆ°á»›c Ä‘Ã³ tÃ´i muá»‘n cháº¯c cháº¯n bá»™ dá»¯ liá»‡u cá»§a mÃ¬nh Ä‘Æ°á»£c chá»n lá»c vÃ  lÃ m sáº¡ch tá»‘t nháº¥t cÃ³ thá»ƒ. Má»™t trong nhá»¯ng váº¥n Ä‘á» tÃ´i gáº·p pháº£i lÃ  lÃ m sáº¡ch, ráº¥t nhiá»u file txt cÃ³ láº«n kÃ½ tá»± vÃ´ nghÄ©a. CÃ¡c script tÃ´i dÃ¹ng Ä‘á»ƒ lÃ m sáº¡ch cÃ³ hiá»‡u quáº£ nhÆ°ng khÃ´ng pháº£i 100%.

TÃ´i sáº½ huáº¥n luyá»‡n bá»™ dá»¯ liá»‡u nÃ y hÃ´m nay vÃ  dá»± kiáº¿n máº¥t khoáº£ng 4-5 tiáº¿ng. Khi xong vÃ  tÃ´i kiá»ƒm tra, tÃ´i sáº½ cáº­p nháº­t. Cáº£m Æ¡n má»i ngÆ°á»i Ä‘Ã£ quan tÃ¢m dá»± Ã¡n cá»§a tÃ´i, tháº­m chÃ­ cÃ³ ngÆ°á»i cÃ²n gá»­i cho tÃ´i cÃ¡c liÃªn káº¿t tÃ i nguyÃªn OCR nÃªn xin cáº£m Æ¡n! TÃ´i hy vá»ng sáº½ cÃ³ nhiá»u ngÆ°á»i thá»­ nghiá»‡m vá»›i bá»™ dá»¯ liá»‡u riÃªng cá»§a mÃ¬nh.

### Cáº­p nháº­t huáº¥n luyá»‡n

TÃ´i Ä‘Ã£ báº¯t Ä‘áº§u huáº¥n luyá»‡n trÃªn táº­p dá»¯ liá»‡u 435MB (108 triá»‡u tokens), quÃ¡ trÃ¬nh hiá»‡n táº¡i khÃ¡ suÃ´n sáº». Train loss giáº£m tá»« 10.9 xuá»‘ng 4.9 trong 2800 vÃ²ng láº·p Ä‘áº§u tiÃªn. TÃ´i dá»± kiáº¿n sáº½ máº¥t khoáº£ng 8 hoáº·c 9 tiáº¿ng Ä‘á»ƒ hoÃ n thÃ nh. TÃ´i sáº½ Ä‘Äƒng cáº­p nháº­t ná»¯a khi xong.

## NgÃ y 17 thÃ¡ng 7, 2025 2:13AM

ÄÃ£ hoÃ n thÃ nh huáº¥n luyá»‡n mÃ´ hÃ¬nh thá»© hai, 4060 cá»§a tÃ´i máº¥t khoáº£ng 8 tiáº¿ng 40 phÃºt (3.900 vÃ²ng/giá») cho 33.000 vÃ²ng (5 epochs). Train loss cuá»‘i cÃ¹ng lÃ  3.73. Káº¿t quáº£ Ä‘áº§u ra khÃ¡ báº¥t ngá», mÃ´ hÃ¬nh thá»±c sá»± táº¡o ra cÃ¢u vÄƒn kiá»ƒu tháº¿ ká»· 19 khÃ¡ máº¡ch láº¡c.

# HÃ nh vi & Háº¡n cháº¿ cá»§a MÃ´ hÃ¬nh V0

CÃ¡c prompt ban Ä‘áº§u cho tháº¥y mÃ´ hÃ¬nh tráº£ lá»i vá»›i ngÃ´n ngá»¯ vÃ  hÃ nh vi cá»§a nhá»¯ng nÄƒm 1800. VÃ­ dá»¥, tÃ´i nháº­p "Who art Henry?" vÃ  nÃ³ tráº£ lá»i "I know that man, I have did not a black, the storm." vÃ  vÃ¢ng, cÃ¢u nÃ y khÃ´ng cÃ³ Ã½ nghÄ©a nhÆ°ng LLM nháº­n biáº¿t tÃ´i Ä‘ang há»i vá» má»™t ngÆ°á»i.

![TimeLockLLM Sample Output](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

KhÃ´ng cÃ³ Ä‘á» cáº­p Ä‘áº¿n cÃ¡c khÃ¡i niá»‡m hiá»‡n Ä‘áº¡i, Ä‘áº§u ra chá»§ yáº¿u chá»©a tá»« ngá»¯ vÃ  cÃ¡ch diá»…n Ä‘áº¡t cá»§a nhá»¯ng nÄƒm 1800.

MÃ´ hÃ¬nh váº«n cáº§n cáº£i thiá»‡n nhiá»u, huáº¥n luyá»‡n vá»›i 187MB sáº½ khÃ´ng cho ra mÃ´ hÃ¬nh sinh vÄƒn báº£n cÃ³ suy luáº­n phá»©c táº¡p.

Hiá»‡n táº¡i nÃ³ táº¡o ra cÃ¡c cÃ¢u thiáº¿u cáº¥u trÃºc hoÃ n chá»‰nh vÃ  nhÃ¬n chung lÃ  khÃ´ng há»£p lÃ½ nhÆ°ng Ä‘iá»u nÃ y lÃ  bÃ¬nh thÆ°á»ng vá»›i kÃ­ch thÆ°á»›c dá»¯ liá»‡u huáº¥n luyá»‡n nhÆ° váº­y.

# HÃ nh Vi & Giá»›i Háº¡n cá»§a MÃ´ HÃ¬nh V0.5

ÄÃ¢y lÃ  má»™t cáº£i tiáº¿n tá»‘t so vá»›i mÃ´ hÃ¬nh trÆ°á»›c. Phong cÃ¡ch viáº¿t vÃ  tá»« vá»±ng mang tÃ­nh thá»i Victoria vÃ  háº§u nhÆ° má»i cÃ¢u Ä‘á»u Ä‘Ãºng ngá»¯ phÃ¡p vá»›i dáº¥u cÃ¢u chuáº©n. VÃ  má»™t láº§n ná»¯a, mÃ´ hÃ¬nh nÃ y Ä‘Æ°á»£c huáº¥n luyá»‡n tá»« Ä‘áº§u nÃªn chá»‰ táº­p trung vÃ o cÃ¡c chá»§ Ä‘á» cá»§a tháº¿ ká»· 19.

![TimeLockLLM Sample Output](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

CÃ³ ráº¥t nhiá»u thÃ´ng tin khÃ´ng chÃ­nh xÃ¡c. Ráº¥t nhiá»u (gáº§n nhÆ° 100%) cÃ¡c chi tiáº¿t (ngÃ y thÃ¡ng, sá»± kiá»‡n, nhÃ¢n váº­t lá»‹ch sá»­) Ä‘á»u lÃ  bá»‹a Ä‘áº·t. NgoÃ i ra, cÃ¡c cÃ¢u khÃ´ng tháº­t sá»± liÃªn káº¿t vá»›i nhau, Ä‘Ã´i lÃºc chá»‰ cÃ³ 2 cÃ¢u liÃªn quan nhÆ°ng thÆ°á»ng thÃ¬ khÃ´ng. Má»™t váº¥n Ä‘á» khÃ¡c lÃ  Ä‘Ã´i khi xuáº¥t hiá»‡n footer â€œDigitized by Googleâ€ láº¡c lÃµng, nÃªn láº§n huáº¥n luyá»‡n tiáº¿p theo tÃ´i pháº£i Ä‘áº£m báº£o lÃ m sáº¡ch vÄƒn báº£n ká»¹ hÆ¡n. NhÃ¬n chung tÃ´i ráº¥t hÃ i lÃ²ng vá»›i káº¿t quáº£, tuy chÆ°a Ä‘áº¡t Ä‘áº¿n má»©c LLM nhÆ°ng cháº¯c cháº¯n lÃ  má»™t bá»™ sinh cÃ¢u tá»‘t.

TÃ´i Ä‘ang há»c há»i ráº¥t nhiá»u vÃ  sáº½ báº¯t Ä‘áº§u xÃ¡c Ä‘á»‹nh nhá»¯ng gÃ¬ cáº§n cáº£i thiá»‡n trong vÃ i tuáº§n tá»›i. TÃ´i sáº½ sá»›m táº£i lÃªn cÃ¡c tá»‡p!

# Káº¿ Hoáº¡ch Sáº¯p Tá»›i

(ÄÃ£ hoÃ n thÃ nh) TÃ´i sáº½ báº¯t Ä‘áº§u lÃ m viá»‡c vá»›i phiÃªn báº£n 0.5, thay vÃ¬ huáº¥n luyá»‡n vá»›i 50 cuá»‘n sÃ¡ch, tÃ´i sáº½ huáº¥n luyá»‡n vá»›i khoáº£ng 500-600 cuá»‘n. Hiá»‡n táº¡i tÃ´i Ä‘ang huáº¥n luyá»‡n nanoGPT báº±ng sÃ¡ch tá»« nÄƒm 1800-1850, Ä‘áº·c biá»‡t lÃ  táº¡i London. CÃ³ má»™t sá»‘ thÃ¡ch thá»©c nhÆ° Ä‘áº£m báº£o nhá»¯ng cuá»‘n sÃ¡ch tÃ¬m Ä‘Æ°á»£c khÃ´ng bá»‹ cáº­p nháº­t hoáº·c giáº£i thÃ­ch hiá»‡n Ä‘áº¡i mÃ  lÃ  sÃ¡ch gá»‘c xuáº¥t báº£n trong khoáº£ng thá»i gian tÃ´i chá»n.

TÃ´i muá»‘n huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh má»›i (v1) vá»›i táº­p dá»¯ liá»‡u lá»›n hÆ¡n nhiá»u, cÃ³ thá»ƒ gáº¥p 5-10 láº§n so vá»›i v0.5. Má»¥c tiÃªu lÃ  xem liá»‡u cÃ³ thá»ƒ xuáº¥t hiá»‡n kháº£ nÄƒng suy luáº­n chá»‰ nhá» Huáº¥n Luyá»‡n Thá»i Gian CÃ³ Chá»n Lá»c hay khÃ´ng, Ä‘Ã¢y sáº½ lÃ  má»™t nhiá»‡m vá»¥ khÃ³ hÆ¡n vÃ  tÃ´i cÅ©ng khÃ´ng cháº¯c liá»‡u cÃ³ kháº£ thi khÃ´ng do háº¡n cháº¿ vá» dá»¯ liá»‡u lá»‹ch sá»­. Trong vÃ i tuáº§n tá»›i tÃ´i sáº½ cá»‘ gáº¯ng chá»n lá»c Ä‘á»§ dá»¯ liá»‡u cho má»™t táº­p 5-10GB. TÃ´i tin ráº±ng náº¿u cÃ³ dá»¯ liá»‡u sáº¡ch cháº¥t lÆ°á»£ng cao vÃ  thuÃª Ä‘Æ°á»£c GPU, sáº½ cÃ³ tiáº¿n triá»ƒn.

# HÆ°á»›ng Dáº«n Sá»­ Dá»¥ng Dá»± Ãn NÃ y

Dá»± Ã¡n nÃ y chá»§ yáº¿u táº­p trung vÃ o viá»‡c thu tháº­p dá»¯ liá»‡u lá»‹ch sá»­, chuáº©n bá»‹ cho huáº¥n luyá»‡n vÃ  xÃ¢y dá»±ng tokenizer. TÃ´i sáº½ khÃ´ng Ä‘á» cáº­p toÃ n bá»™ quy trÃ¬nh huáº¥n luyá»‡n LLM, báº¡n hÃ£y tham kháº£o nanoGPT cá»§a Andrej Karpathy.

# BÆ°á»›c 1: Thu Tháº­p vÃ  Chuáº©n Bá»‹ VÄƒn Báº£n Lá»‹ch Sá»­

Thu tháº­p cÃ¡c tá»‡p .txt cá»§a sÃ¡ch, tÃ i liá»‡u, v.v. thuá»™c pháº¡m vi cÃ´ng cá»™ng trong khoáº£ng thá»i gian báº¡n chá»n (vÃ­ dá»¥: London 1800-1850)

Báº¡n cÃ³ thá»ƒ dÃ¹ng download_texts_improved.py Ä‘á»ƒ táº£i sÃ¡ch náº¿u cáº§n.

LÃ m sáº¡ch tá»‡p vÄƒn báº£n báº±ng script hoáº·c thá»§ cÃ´ng Ä‘á»ƒ loáº¡i bá» header/footer cá»§a Project Gutenberg, chÃº thÃ­ch hiá»‡n Ä‘áº¡i hoáº·c lá»—i OCR.

prepare_dataset.py nÃªn hoáº¡t Ä‘á»™ng tá»‘t.

# BÆ°á»›c 2: XÃ¢y Dá»±ng Tokenizer TÃ¹y Chá»‰nh

Cháº¡y train_tokenizer.py hoáº·c train_tokenizer_hf.py trÃªn dá»¯ liá»‡u Ä‘Ã£ lÃ m sáº¡ch.
Báº¡n sáº½ nháº­n Ä‘Æ°á»£c vocab.json vÃ  merges.txt

CÃ¡c tá»‡p nÃ y xÃ¡c Ä‘á»‹nh tá»« vá»±ng vÃ  quy táº¯c gá»™p cho mÃ´ hÃ¬nh cá»§a báº¡n

# BÆ°á»›c 3: Huáº¥n Luyá»‡n MÃ´ HÃ¬nh (nanoGPT)

Tham kháº£o [nanoGPT cá»§a Andrej Karpathy](https://github.com/karpathy/nanoGPT) Ä‘á»ƒ biáº¿t quy trÃ¬nh huáº¥n luyá»‡n.

Báº¡n cÃ³ thá»ƒ huáº¥n luyá»‡n LLM khÃ¡c náº¿u muá»‘n, nhÆ°ng tÃ´i dÃ¹ng nanoGPT

# Há»i ÄÃ¡p

## Huáº¥n Luyá»‡n Thá»i Gian CÃ³ Chá»n Lá»c lÃ  gÃ¬?

Huáº¥n Luyá»‡n Thá»i Gian CÃ³ Chá»n Lá»c (Selective Temporal Training - STT) lÃ  phÆ°Æ¡ng phÃ¡p há»c mÃ¡y trong Ä‘Ã³ toÃ n bá»™ dá»¯ liá»‡u huáº¥n luyá»‡n Ä‘Æ°á»£c tuyá»ƒn chá»n Ä‘á»ƒ náº±m trong má»™t giai Ä‘oáº¡n lá»‹ch sá»­ nháº¥t Ä‘á»‹nh. Äiá»u nÃ y nháº±m mÃ´ phá»ng ngÃ´n ngá»¯ vÃ  kiáº¿n thá»©c cá»§a thá»i ká»³ Ä‘Ã³ mÃ  khÃ´ng bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi cÃ¡c khÃ¡i niá»‡m hiá»‡n Ä‘áº¡i. VÃ­ dá»¥, mÃ´ hÃ¬nh hiá»‡n táº¡i cá»§a tÃ´i (v0.5) Ä‘Æ°á»£c huáº¥n luyá»‡n hoÃ n toÃ n báº±ng dá»¯ liá»‡u tá»« 1800-1875, khÃ´ng tinh chá»‰nh mÃ  huáº¥n luyá»‡n tá»« Ä‘áº§u nÃªn Ä‘áº§u ra pháº£n Ã¡nh Ä‘Ãºng phong cÃ¡ch ngÃ´n ngá»¯ vÃ  bá»‘i cáº£nh lá»‹ch sá»­ thá»i Ä‘Ã³.

## Táº¡i sao khÃ´ng chá»‰ tinh chá»‰nh hoáº·c dÃ¹ng LoRA?

Trong dá»± Ã¡n nÃ y, tÃ´i muá»‘n táº¡o má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ khÃ´ng bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi thÃ nh kiáº¿n hiá»‡n Ä‘áº¡i. Náº¿u tÃ´i tinh chá»‰nh má»™t mÃ´ hÃ¬nh nhÆ° GPT-2, nÃ³ Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n sáºµn vÃ  thÃ´ng tin Ä‘Ã³ khÃ´ng thá»ƒ máº¥t Ä‘i. Náº¿u huáº¥n luyá»‡n tá»« Ä‘áº§u, mÃ´ hÃ¬nh sáº½ khÃ´ng giáº£ vá» cá»• xÆ°a, nÃ³ thá»±c sá»± lÃ  nhÆ° váº­y. Má»¥c tiÃªu dá»± Ã¡n lÃºc nÃ y lÃ  táº¡o ra má»™t thá»© cÃ³ thá»ƒ suy luáº­n chá»‰ dá»±a trÃªn kiáº¿n thá»©c tá»« cÃ¡c sÃ¡ch London xuáº¥t báº£n trong khoáº£ng 1800-1850.

## Báº¡n Ä‘Ã£ dÃ¹ng dá»¯ liá»‡u gÃ¬ Ä‘á»ƒ huáº¥n luyá»‡n?

TÃ´i dÃ¹ng sÃ¡ch, tÃ i liá»‡u phÃ¡p lÃ½, bÃ¡o chÃ­ vÃ  cÃ¡c bÃ i viáº¿t khÃ¡c tá»« London 1800â€“1850. Danh sÃ¡ch tÃ´i Ä‘Ã£ liÃªn káº¿t cÃ³ khoáº£ng 200 tÃ i liá»‡u nhÆ°ng huáº¥n luyá»‡n Ä‘áº§u tiÃªn tÃ´i chá»‰ dÃ¹ng 50 tá»‡p khoáº£ng ~187 MB. Báº¡n cÃ³ thá»ƒ xem danh sÃ¡ch tÃ i liá»‡u táº¡i:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## MÃ´ HÃ¬nh PhiÃªn Báº£n 0 lá»›n cá»¡ nÃ o?

MÃ´ hÃ¬nh nÃ y hiá»‡n ráº¥t nhá», tÃ´i chá»‰ lÃ m cho vui vÃ  tuÃ¢n thá»§ nghiÃªm ngáº·t quy táº¯c khÃ´ng dÃ¹ng nguá»“n hiá»‡n Ä‘áº¡i. NÃ³ cÃ³ gáº§n 16 triá»‡u tham sá»‘ nhÆ°ng tÃ´i sáº½ báº¯t Ä‘áº§u thu tháº­p thÃªm vÄƒn báº£n cá»• Ä‘á»ƒ báº¯t Ä‘áº§u huáº¥n luyá»‡n mÃ´ hÃ¬nh má»›i. Sáº½ cáº­p nháº­t tiáº¿n Ä‘á»™ sau.

## ThÃ´ng Sá»‘ Huáº¥n Luyá»‡n?

GPU: Geforce rtx 4060
CPU: i5-13400F
Ram: 16GB DDR5.



---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-21

---