<div align="right">
  <details>
    <summary >ğŸŒ NgÃ´n ngá»¯</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ç®€ä½“ä¸­æ–‡</a>
        | <a href="#" title="Coming soon">ç¹é«”ä¸­æ–‡ (sáº¯p ra máº¯t)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">æ—¥æœ¬èª</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">í•œêµ­ì–´</a>
        | <a href="#" title="Coming soon">à¤¹à¤¿à¤¨à¥à¤¦à¥€ (sáº¯p ra máº¯t)</a> |
        | <a href="#" title="Coming soon">à¹„à¸—à¸¢ (sáº¯p ra máº¯t)</a> |
        | <a href="#" title="Coming soon">FranÃ§ais (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Coming soon">Deutsch (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Coming soon">EspaÃ±ol (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Coming soon">Italiano (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Coming soon">Ğ ÑƒÑÑĞºĞ¸Ğ¹ (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Coming soon">PortuguÃªs (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Coming soon">Nederlands (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Coming soon">Polski (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Coming soon">Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Coming soon">ÙØ§Ø±Ø³ÛŒ (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Coming soon">TÃ¼rkÃ§e (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Coming soon">Tiáº¿ng Viá»‡t (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (sáº¯p ra máº¯t)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM
Má»™t mÃ´ hÃ¬nh LLM chá»‰ Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn dá»¯ liá»‡u tá»« cÃ¡c thá»i ká»³ nháº¥t Ä‘á»‹nh Ä‘á»ƒ giáº£m thiÃªn kiáº¿n hiá»‡n Ä‘áº¡i.

HÃ£y tÆ°á»Ÿng tÆ°á»£ng náº¿u má»™t mÃ´ hÃ¬nh AI khÃ´ng chá»‰ giáº£ vá» lÃ  lá»‹ch sá»­ mÃ  thá»±c sá»± lÃ  nhÆ° váº­y.

ÄÆ°á»£c xÃ¢y dá»±ng trÃªn [nanoGPT bá»Ÿi Andrej Karpathy](https://github.com/karpathy/nanoGPT) CÃ¡c táº­p lá»‡nh huáº¥n luyá»‡n cá»‘t lÃµi vÃ  kiáº¿n trÃºc mÃ´ hÃ¬nh lÃ  cá»§a Ã´ng áº¥y.

# Má»¥c tiÃªu dá»± Ã¡n

TimeCapsule LLM lÃ  má»™t dá»± Ã¡n thá»­ nghiá»‡m chá»‰ Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn cÃ¡c vÄƒn báº£n Ä‘Æ°á»£c viáº¿t trong nhá»¯ng thá»i ká»³ nháº¥t Ä‘á»‹nh. Má»¥c tiÃªu lÃ  mÃ´ phá»ng tháº¿ giá»›i quan vÃ  ngÃ´n ngá»¯ cá»§a cÃ¡c thá»i Ä‘áº¡i lá»‹ch sá»­ cá»¥ thá»ƒ.

# Táº¡i sao chá»‰ tinh chá»‰nh lÃ  chÆ°a Ä‘á»§

Náº¿u báº¡n chá»‰ tinh chá»‰nh má»™t mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c, LLM cá»§a báº¡n váº«n sáº½ biáº¿t cÃ¡c khÃ¡i niá»‡m hiá»‡n Ä‘áº¡i. DÄ© nhiÃªn Ä‘áº¡t Ä‘Æ°á»£c má»©c Ä‘á»™ khÃ´ng cÃ³ thiÃªn kiáº¿n hiá»‡n Ä‘áº¡i lÃ  ráº¥t khÃ³ nhÆ°ng tÃ´i muá»‘n Ä‘áº¡t Ä‘Æ°á»£c gáº§n nháº¥t cÃ³ thá»ƒ. Äá»ƒ khÃ´ng cÃ³ thiÃªn kiáº¿n hiá»‡n Ä‘áº¡i Ä‘Ã²i há»i pháº£i huáº¥n luyá»‡n mÃ´ hÃ¬nh tá»« Ä‘áº§u.

# Káº¿t quáº£ ká»³ vá»ng

Hy vá»ng khi hoÃ n thÃ nh, mÃ´ hÃ¬nh nÃ y sáº½ khÃ´ng biáº¿t cÃ¡c khÃ¡i niá»‡m hiá»‡n Ä‘áº¡i vÃ  khÃ´ng thá»ƒ suy luáº­n vÆ°á»£t quÃ¡ nhá»¯ng gÃ¬ nÃ³ Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n. NÃ³ khÃ´ng nÃªn nháº­n biáº¿t Ä‘Æ°á»£c khÃ¡i niá»‡m/tá»« vá»±ng hiá»‡n Ä‘áº¡i vÃ  tÃ´i hy vá»ng nÃ³ khÃ´ng bá»‹a Ä‘áº·t kiáº¿n thá»©c hiá»‡n Ä‘áº¡i.

# Cáº­p nháº­t tiáº¿n Ä‘á»™

## NgÃ y 9 thÃ¡ng 7, 2025

TÃ´i Ä‘Ã£ Ä‘áº·t thá»i ká»³ cho dá»± Ã¡n lÃ  1800-1850 vÃ  khu vá»±c: London

TÃ´i Ä‘Ã£ thu tháº­p danh sÃ¡ch cÃ¡c vÄƒn báº£n, sÃ¡ch, tÃ i liá»‡u

Hiá»‡n tÃ´i Ä‘Ã£ cÃ³ Ä‘Æ°á»£c 50 tá»‡p dáº¡ng txt vÃ  sáº½ báº¯t Ä‘áº§u huáº¥n luyá»‡n NanoGPT sá»›m

Sáº½ tiáº¿p tá»¥c cáº­p nháº­t náº¿u cÃ³ tiáº¿n triá»ƒn

## NgÃ y 13 thÃ¡ng 7, 2025

ÄÃ£ huáº¥n luyá»‡n nanoGPT vá»›i 187MB dá»¯ liá»‡u vÄƒn báº£n lá»‹ch sá»­.

## NgÃ y 15 thÃ¡ng 7, 2025

TÃ´i báº¯t Ä‘áº§u táº£i vá» cÃ¡c vÄƒn báº£n cho láº§n huáº¥n luyá»‡n thá»© hai. TÃ´i láº¥y má»i thá»© tá»« Internet Archive vÃ  Ä‘Ã£ má»Ÿ rá»™ng thá»i ká»³ Ä‘áº¿n 1800-1875. Äá»ƒ cÃ³ pháº¡m vi vÄƒn báº£n Ä‘a dáº¡ng, báº¡n cÃ³ thá»ƒ dÃ¹ng bá»™ lá»c chá»§ Ä‘á» vÃ  tÃ¬m kiáº¿m theo vá»‹ trÃ­ xuáº¥t báº£n, thá»i ká»³ vÃ  chá»§ Ä‘á» trÃªn Internet Archive.

![Bá»™ lá»c tÃ¬m kiáº¿m](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## NgÃ y 16 thÃ¡ng 7, 2025

TÃ´i Ä‘Ã£ táº£i xuá»‘ng khoáº£ng 500 tá»‡p txt tá»« Internet Archive vÃ  sau khi lÃ m sáº¡ch (chá»‰ xÃ³a khoáº£ng tráº¯ng, tiÃªu Ä‘á» Gutenberg, v.v.) tÃ´i cÃ³ khoáº£ng 500MB dá»¯ liá»‡u. ÄÃ¢y lÃ  má»™t táº­p dá»¯ liá»‡u nhá» nhÆ°ng láº§n trÆ°á»›c tÃ´i chá»‰ huáº¥n luyá»‡n vá»›i 187MB nÃªn Ã­t nháº¥t cÅ©ng sáº½ cÃ³ sá»± khÃ¡c biá»‡t Ä‘Ã¡ng chÃº Ã½ vá» Ä‘áº§u ra sau khi tÃ´i huáº¥n luyá»‡n mÃ´ hÃ¬nh thá»© hai. TÃ´i hy vá»ng mÃ´ hÃ¬nh nÃ y Ã­t nháº¥t cÃ³ thá»ƒ táº¡o ra cÃ¡c cÃ¢u máº¡ch láº¡c hÆ¡n, cÃ³ Ã½ nghÄ©a. DÄ© nhiÃªn khÃ´ng pháº£i Ä‘áº£m báº£o vÃ¬ Ä‘Ã¢y váº«n lÃ  táº­p dá»¯ liá»‡u ráº¥t nhá», nhÆ°ng váº«n hÆ¡n láº§n trÆ°á»›c.

Äiá»u nÃ y cÃ³ thá»ƒ thá»±c hiá»‡n trÃªn pháº§n cá»©ng cÃ¡ nhÃ¢n cá»§a tÃ´i, Ä‘iá»u nÃ y cÅ©ng tá»‘t vÃ¬ tÃ´i cÃ³ thá»ƒ hy vá»ng tháº¥y Ä‘Æ°á»£c má»™t sá»‘ cáº£i thiá»‡n trÆ°á»›c khi chuyá»ƒn sang táº­p dá»¯ liá»‡u lá»›n hÆ¡n cáº§n thuÃª GPU. NhÆ°ng Ä‘á»«ng lo, tÃ´i váº«n dá»± Ä‘á»‹nh thuÃª GPU sá»›m, nhÆ°ng trÆ°á»›c Ä‘Ã³ tÃ´i muá»‘n Ä‘áº£m báº£o táº­p dá»¯ liá»‡u cá»§a mÃ¬nh Ä‘Æ°á»£c chá»n lá»c vÃ  sáº¡ch nháº¥t cÃ³ thá»ƒ. Má»™t trong nhá»¯ng váº¥n Ä‘á» tÃ´i gáº·p pháº£i lÃ  lÃ m sáº¡ch, nhiá»u tá»‡p txt nÃ y bá»‹ láº«n chá»¯ linh tinh. CÃ¡c script tÃ´i dÃ¹ng Ä‘á»ƒ lÃ m sáº¡ch hoáº¡t Ä‘á»™ng Ä‘Æ°á»£c nhÆ°ng khÃ´ng hiá»‡u quáº£ 100%.

TÃ´i sáº½ huáº¥n luyá»‡n táº­p dá»¯ liá»‡u nÃ y hÃ´m nay vÃ  máº¥t khoáº£ng 4-5 giá». Khi xong vÃ  kiá»ƒm tra, tÃ´i sáº½ cáº­p nháº­t. Cáº£m Æ¡n má»i ngÆ°á»i Ä‘Ã£ quan tÃ¢m Ä‘áº¿n dá»± Ã¡n cá»§a tÃ´i, tháº­m chÃ­ cÃ³ vÃ i ngÆ°á»i gá»­i tÃ´i cáº£ cÃ¡c tÃ i nguyÃªn OCR nÃªn xin cáº£m Æ¡n! Hy vá»ng sáº½ cÃ³ nhiá»u ngÆ°á»i thá»­ lÃ m Ä‘iá»u nÃ y vÃ  thá»­ nghiá»‡m vá»›i bá»™ dá»¯ liá»‡u cá»§a riÃªng há».


### Cáº­p nháº­t huáº¥n luyá»‡n

TÃ´i báº¯t Ä‘áº§u huáº¥n luyá»‡n trÃªn táº­p há»£p 435MB (108 triá»‡u token), hiá»‡n táº¡i khÃ¡ suÃ´n sáº». Äá»™ máº¥t mÃ¡t huáº¥n luyá»‡n giáº£m tá»« 10.9 xuá»‘ng 4.9 sau 2800 láº§n láº·p Ä‘áº§u tiÃªn. TÃ´i dá»± Ä‘oÃ¡n sáº½ máº¥t khoáº£ng 8 hoáº·c 9 giá» Ä‘á»ƒ hoÃ n thÃ nh. Sáº½ Ä‘Äƒng cáº­p nháº­t tiáº¿p khi xong.

## NgÃ y 17 thÃ¡ng 7, 2025

ÄÃ£ hoÃ n thÃ nh huáº¥n luyá»‡n cho mÃ´ hÃ¬nh thá»© hai, chiáº¿c 4060 cá»§a tÃ´i máº¥t khoáº£ng 8 giá» 40 phÃºt (3.900 vÃ²ng láº·p/giá») cho 33.000 vÃ²ng láº·p (5 epoch). Äá»™ máº¥t mÃ¡t huáº¥n luyá»‡n cuá»‘i cÃ¹ng lÃ  3.73. Äáº§u ra tháº­t sá»± tá»‘t, giá» Ä‘Ã¢y mÃ´ hÃ¬nh táº¡o ra cÃ¡c cÃ¢u theo phong cÃ¡ch tháº¿ ká»· 19 ráº¥t máº¡ch láº¡c.

## NgÃ y 28 thÃ¡ng 7, 2025

TÃ´i Ä‘Ã£ táº£i lÃªn báº£n v0.5 lÃªn Hugging Face, [Xem táº¡i Ä‘Ã¢y](https://huggingface.co/haykgrigorian/TimeCapsuleLLM) náº¿u báº¡n muá»‘n. Báº¡n cÃ³ thá»ƒ táº£i repo cá»§a tÃ´i vá» vÃ  cháº¡y cá»¥c bá»™. ÄÃ¡ng tiáº¿c nanoGPT khÃ´ng hoáº¡t Ä‘á»™ng trá»±c tiáº¿p vá»›i HuggingFace, nÃªn báº¡n pháº£i táº£i vÃ  cháº¡y mÃ´ hÃ¬nh cá»¥c bá»™.

TÃ´i cÅ©ng sáº½ báº¯t Ä‘áº§u chá»n lá»c dá»¯ liá»‡u cho láº§n huáº¥n luyá»‡n tiáº¿p theo, tÃ´i nghÄ© sáº½ cáº§n khoáº£ng 5-10 láº§n dá»¯ liá»‡u Ä‘á»ƒ Ä‘áº¡t kháº£ nÄƒng suy luáº­n.

## NgÃ y 2 thÃ¡ng 8, 2025

TÃ´i sáº½ sá»›m báº¯t Ä‘áº§u lÃ m viá»‡c cho PhiÃªn báº£n 1. TÃ´i sáº½ cáº§n chuyá»ƒn Ä‘á»•i tá»« kiáº¿n trÃºc cá»§a nanoGPT sang má»™t thá»© hiá»‡n Ä‘áº¡i hÆ¡n. TÃ´i Ä‘Ã£ cÃ¢n nháº¯c vÃ i kiáº¿n trÃºc LLM mÃ£ nguá»“n má»Ÿ, gá»“m: OpenLLaMA v3, Phi-2 vÃ  Qwen 1.5B. VÃ  Ä‘á»ƒ há»— trá»£ chuyá»ƒn sang V1, tÃ´i sáº½ pháº£i chá»n lá»c má»™t táº­p dá»¯ liá»‡u lá»›n vÃ  Ä‘a dáº¡ng hÆ¡n nhiá»u. TÃ´i sáº½ cáº§n Ã­t nháº¥t 5GB dá»¯ liá»‡u huáº¥n luyá»‡n sáº¡ch.


# HÃ nh Vi & Giá»›i Háº¡n Cá»§a MÃ´ HÃ¬nh V0

CÃ¡c prompt ban Ä‘áº§u cho tháº¥y mÃ´ hÃ¬nh pháº£n há»“i báº±ng ngÃ´n ngá»¯ vÃ  hÃ nh vi cá»§a nhá»¯ng nÄƒm 1800. VÃ­ dá»¥, tÃ´i Ä‘Ã£ nháº­p "Who art Henry?" vÃ  nÃ³ tráº£ lá»i "I know that man, I have did not a black, the storm." vÃ  vÃ¢ng, cÃ¢u Ä‘Ã³ khÃ´ng cÃ³ nghÄ©a gÃ¬ nhÆ°ng LLM nháº­n ra tÃ´i Ä‘ang há»i vá» má»™t ngÆ°á»i.

![Káº¿t Quáº£ Máº«u TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

KhÃ´ng cÃ³ Ä‘á» cáº­p Ä‘áº¿n cÃ¡c khÃ¡i niá»‡m hiá»‡n Ä‘áº¡i, káº¿t quáº£ chá»§ yáº¿u chá»©a tá»« ngá»¯ vÃ  cÃ¡ch diá»…n Ä‘áº¡t cá»§a nhá»¯ng nÄƒm 1800.

NÃ³ váº«n cáº§n ráº¥t nhiá»u cáº£i thiá»‡n, viá»‡c huáº¥n luyá»‡n tá»« 187MB sáº½ khÃ´ng thá»ƒ giÃºp báº¡n cÃ³ má»™t mÃ´ hÃ¬nh táº¡o ra vÄƒn báº£n vá»›i kháº£ nÄƒng suy luáº­n phá»©c táº¡p.

Hiá»‡n táº¡i nÃ³ táº¡o ra cÃ¡c cÃ¢u thiáº¿u cáº¥u trÃºc Ä‘áº§y Ä‘á»§ vÃ  nhÃ¬n chung khÃ´ng cÃ³ Ã½ nghÄ©a gÃ¬, nhÆ°ng Ä‘iá»u nÃ y lÃ  bÃ¬nh thÆ°á»ng vá»›i kÃ­ch thÆ°á»›c dá»¯ liá»‡u huáº¥n luyá»‡n nhÆ° váº­y.

# HÃ nh Vi & Giá»›i Háº¡n Cá»§a MÃ´ HÃ¬nh V0.5

ÄÃ¢y lÃ  má»™t sá»± cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ so vá»›i mÃ´ hÃ¬nh trÆ°á»›c. Phong cÃ¡ch viáº¿t vÃ  tá»« vá»±ng mang tÃ­nh cháº¥t thá»i Victoria vÃ  gáº§n nhÆ° má»i cÃ¢u Ä‘á»u Ä‘Ãºng ngá»¯ phÃ¡p vá»›i dáº¥u cÃ¢u Ä‘áº§y Ä‘á»§. VÃ  má»™t láº§n ná»¯a, Ä‘Ã¢y lÃ  mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n tá»« Ä‘áº§u nÃªn nÃ³ bÃ¡m sÃ¡t cÃ¡c chá»§ Ä‘á» cá»§a nhá»¯ng nÄƒm 1800.

![Káº¿t Quáº£ Máº«u TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

CÃ³ ráº¥t nhiá»u thÃ´ng tin tÆ°á»Ÿng tÆ°á»£ng. Ráº¥t nhiá»u (gáº§n nhÆ° 100%) chi tiáº¿t (ngÃ y thÃ¡ng, sá»± kiá»‡n, nhÃ¢n váº­t lá»‹ch sá»­) Ä‘á»u lÃ  bá»‹a Ä‘áº·t. NgoÃ i ra, cÃ¡c cÃ¢u khÃ´ng thá»±c sá»± liÃªn káº¿t vá»›i nhau, Ä‘Ã´i khi cÃ³ thá»ƒ 2 cÃ¢u cÃ³ liÃªn quan nhÆ°ng ngoÃ i ra thÃ¬ khÃ´ng. Má»™t váº¥n Ä‘á» ná»¯a lÃ  Ä‘Ã´i khi xuáº¥t hiá»‡n dÃ²ng footer â€œDigitized by Googleâ€ láº¡c lÃµng, nÃªn láº§n huáº¥n luyá»‡n tá»›i tÃ´i pháº£i Ä‘áº£m báº£o lÃ m sáº¡ch dá»¯ liá»‡u tháº­t ká»¹. NhÃ¬n chung tÃ´i ráº¥t hÃ i lÃ²ng vá»›i káº¿t quáº£, nÃ³ chÆ°a pháº£i LLM nhÆ°ng cháº¯c cháº¯n lÃ  trÃ¬nh táº¡o cÃ¢u.

TÃ´i há»c Ä‘Æ°á»£c ráº¥t nhiá»u vÃ  sáº½ báº¯t Ä‘áº§u xÃ¡c Ä‘á»‹nh nhá»¯ng gÃ¬ cáº§n lÃ m tá»‘t hÆ¡n trong vÃ i tuáº§n tá»›i. TÃ´i sáº½ sá»›m táº£i lÃªn cÃ¡c file!

# Káº¿ Hoáº¡ch Sáº¯p Tá»›i

(ÄÃ£ hoÃ n thÃ nh) TÃ´i sáº½ báº¯t Ä‘áº§u lÃ m viá»‡c trÃªn phiÃªn báº£n 0.5, thay vÃ¬ huáº¥n luyá»‡n vá»›i 50 cuá»‘n sÃ¡ch, tÃ´i sáº½ huáº¥n luyá»‡n vá»›i khoáº£ng 500-600 cuá»‘n. Hiá»‡n táº¡i tÃ´i Ä‘ang huáº¥n luyá»‡n nanoGPT vá»›i cÃ¡c sÃ¡ch tá»« 1800-1850 vÃ  cá»¥ thá»ƒ lÃ  tá»« London. CÃ³ nhá»¯ng thÃ¡ch thá»©c nhÆ° Ä‘áº£m báº£o cÃ¡c cuá»‘n sÃ¡ch tÃ´i tÃ¬m khÃ´ng bá»‹ chá»‰nh sá»­a hay thÃªm cÃ¡c diá»…n giáº£i hiá»‡n Ä‘áº¡i mÃ  lÃ  sÃ¡ch gá»‘c xuáº¥t báº£n trong khoáº£ng thá»i gian tÃ´i chá»n.

TÃ´i muá»‘n huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh má»›i (v1) vá»›i táº­p dá»¯ liá»‡u lá»›n hÆ¡n nhiá»u, cÃ³ thá»ƒ gáº¥p 5-10 láº§n cÃ¡i tÃ´i dÃ¹ng cho v0.5. Má»¥c tiÃªu cá»§a tÃ´i lÃ  xem liá»‡u kháº£ nÄƒng suy luáº­n cÃ³ thá»ƒ xuáº¥t hiá»‡n chá»‰ tá»« QuÃ¡ TrÃ¬nh Huáº¥n Luyá»‡n Theo Thá»i Gian Chá»n Lá»c hay khÃ´ng, Ä‘Ã¢y sáº½ lÃ  nhiá»‡m vá»¥ khÃ³ hÆ¡n vÃ  tÃ´i cÅ©ng chÆ°a cháº¯c cÃ³ thá»ƒ do háº¡n cháº¿ dá»¯ liá»‡u lá»‹ch sá»­. Trong vÃ i tuáº§n tá»›i, tÃ´i sáº½ cá»‘ gáº¯ng chá»n lá»c Ä‘á»§ dá»¯ liá»‡u cho má»™t táº­p há»£p 5-10GB. TÃ´i tin ráº±ng náº¿u cÃ³ dá»¯ liá»‡u sáº¡ch, cháº¥t lÆ°á»£ng cao vÃ  thuÃª Ä‘Æ°á»£c GPU, sáº½ cÃ³ tiáº¿n triá»ƒn.

# CÃ¡ch Sá»­ Dá»¥ng Dá»± Ãn NÃ y

Dá»± Ã¡n nÃ y chá»§ yáº¿u táº­p trung vÃ o viá»‡c chá»n lá»c dá»¯ liá»‡u lá»‹ch sá»­, chuáº©n bá»‹ cho huáº¥n luyá»‡n vÃ  xÃ¢y dá»±ng tokenizer. TÃ´i sáº½ khÃ´ng hÆ°á»›ng dáº«n toÃ n bá»™ quÃ¡ trÃ¬nh huáº¥n luyá»‡n LLM, tham kháº£o nanoGPT cá»§a Andrej Karpathy Ä‘á»ƒ biáº¿t chi tiáº¿t.

# BÆ°á»›c 1: Thu Tháº­p & Chuáº©n Bá»‹ VÄƒn Báº£n Lá»‹ch Sá»­

Thu tháº­p cÃ¡c file .txt cá»§a cÃ¡c cuá»‘n sÃ¡ch, tÃ i liá»‡u, v.v. thuá»™c pháº¡m vi cÃ´ng cá»™ng tá»« thá»i ká»³ báº¡n chá»n (vÃ­ dá»¥: London 1800-1850)

Báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng download_texts_improved.py Ä‘á»ƒ táº£i sÃ¡ch náº¿u cáº§n.

LÃ m sáº¡ch cÃ¡c file vÄƒn báº£n báº±ng script hoáº·c xÃ³a thá»§ cÃ´ng pháº§n header/footer tá»« Project Gutenberg, chÃº thÃ­ch hiá»‡n Ä‘áº¡i hay lá»—i OCR.

prepare_dataset.py nÃªn hoáº¡t Ä‘á»™ng tá»‘t.

# BÆ°á»›c 2: XÃ¢y Dá»±ng Tokenizer TÃ¹y Chá»‰nh

Cháº¡y train_tokenizer.py hoáº·c train_tokenizer_hf.py trÃªn dá»¯ liá»‡u Ä‘Ã£ lÃ m sáº¡ch.
Äiá»u nÃ y sáº½ táº¡o ra vocab.json vÃ  merges.txt

CÃ¡c file nÃ y Ä‘á»‹nh nghÄ©a tá»« vá»±ng vÃ  quy táº¯c gá»™p cho mÃ´ hÃ¬nh cá»§a báº¡n

# BÆ°á»›c 3: Huáº¥n Luyá»‡n MÃ´ HÃ¬nh (nanoGPT)

Tham kháº£o [nanoGPT cá»§a Andrej Karpathy](https://github.com/karpathy/nanoGPT) Ä‘á»ƒ biáº¿t quy trÃ¬nh huáº¥n luyá»‡n.

Báº¡n cÃ³ thá»ƒ huáº¥n luyá»‡n LLM khÃ¡c náº¿u muá»‘n, nhÆ°ng tÃ´i Ä‘Ã£ dÃ¹ng nanoGPT

# FAQ

## QuÃ¡ TrÃ¬nh Huáº¥n Luyá»‡n Theo Thá»i Gian Chá»n Lá»c lÃ  gÃ¬?

QuÃ¡ TrÃ¬nh Huáº¥n Luyá»‡n Theo Thá»i Gian Chá»n Lá»c (Selective Temporal Training - STT) lÃ  má»™t phÆ°Æ¡ng phÃ¡p há»c mÃ¡y mÃ  toÃ n bá»™ dá»¯ liá»‡u huáº¥n luyá»‡n Ä‘á»u Ä‘Æ°á»£c chá»n lá»c Ä‘á»ƒ náº±m trong má»™t khoáº£ng thá»i gian lá»‹ch sá»­ cá»¥ thá»ƒ. Má»¥c tiÃªu lÃ  mÃ´ hÃ¬nh hÃ³a ngÃ´n ngá»¯ vÃ  tri thá»©c cá»§a thá»i ká»³ Ä‘Ã³ mÃ  khÃ´ng chá»‹u áº£nh hÆ°á»Ÿng tá»« cÃ¡c khÃ¡i niá»‡m hiá»‡n Ä‘áº¡i. VÃ­ dá»¥, mÃ´ hÃ¬nh hiá»‡n táº¡i cá»§a tÃ´i (v0.5) Ä‘Æ°á»£c huáº¥n luyá»‡n chá»‰ tá»« dá»¯ liá»‡u 1800-1875, khÃ´ng tinh chá»‰nh mÃ  huáº¥n luyá»‡n tá»« Ä‘áº§u nÃªn Ä‘áº§u ra pháº£n Ã¡nh phong cÃ¡ch ngÃ´n ngá»¯ vÃ  bá»‘i cáº£nh lá»‹ch sá»­ thá»i Ä‘Ã³.

## Táº¡i sao khÃ´ng chá»‰ tinh chá»‰nh hay dÃ¹ng LoRA?

Vá»›i dá»± Ã¡n nÃ y tÃ´i muá»‘n táº¡o ra má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ khÃ´ng bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi thiÃªn kiáº¿n hiá»‡n Ä‘áº¡i. Náº¿u tÃ´i tinh chá»‰nh GPT-2 cháº³ng háº¡n, nÃ³ Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c vÃ  thÃ´ng tin Ä‘Ã³ khÃ´ng thá»ƒ máº¥t Ä‘i. Náº¿u tÃ´i huáº¥n luyá»‡n tá»« Ä‘áº§u thÃ¬ mÃ´ hÃ¬nh sáº½ khÃ´ng "giáº£ vá»" lÃ  cá»• xÆ°a, mÃ  nÃ³ thá»±c sá»± nhÆ° váº­y. Má»¥c tiÃªu dá»± Ã¡n hiá»‡n táº¡i lÃ  táº¡o ra thá»© gÃ¬ Ä‘Ã³ cÃ³ thá»ƒ suy luáº­n chá»‰ dá»±a trÃªn kiáº¿n thá»©c tá»« cÃ¡c sÃ¡ch London xuáº¥t báº£n giá»¯a 1800 vÃ  1850.

## Báº¡n Ä‘Ã£ dÃ¹ng loáº¡i dá»¯ liá»‡u nÃ o Ä‘á»ƒ huáº¥n luyá»‡n?

TÃ´i dÃ¹ng sÃ¡ch, tÃ i liá»‡u phÃ¡p lÃ½, bÃ¡o chÃ­ vÃ  cÃ¡c vÄƒn báº£n khÃ¡c tá»« London 1800â€“1850. Danh sÃ¡ch tÃ´i Ä‘Ã£ dáº«n cÃ³ khoáº£ng 200 tÃ i liá»‡u nhÆ°ng cho láº§n huáº¥n luyá»‡n Ä‘áº§u tiÃªn tÃ´i chá»‰ dÃ¹ng 50 file khoáº£ng ~187 MB. Báº¡n cÃ³ thá»ƒ xem danh sÃ¡ch tÃ i liá»‡u táº¡i:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## MÃ´ hÃ¬nh PhiÃªn báº£n 0 lá»›n cá»¡ nÃ o?

MÃ´ hÃ¬nh nÃ y hiá»‡n ráº¥t nhá», tÃ´i chá»‰ lÃ m cho vui vÃ  tuÃ¢n thá»§ nghiÃªm ngáº·t quy táº¯c huáº¥n luyá»‡n khÃ´ng dÃ¹ng nguá»“n hiá»‡n Ä‘áº¡i. NÃ³ cÃ³ gáº§n 16 triá»‡u tham sá»‘ nhÆ°ng tÃ´i sáº½ báº¯t Ä‘áº§u thu tháº­p thÃªm cÃ¡c vÄƒn báº£n cá»• Ä‘á»ƒ chuáº©n bá»‹ cho láº§n huáº¥n luyá»‡n mÃ´ hÃ¬nh tiáº¿p theo. Sáº½ cáº­p nháº­t thÃªm khi cÃ³ tiáº¿n triá»ƒn.

## ThÃ´ng Sá»‘ Huáº¥n Luyá»‡n?

GPU: Geforce rtx 4060
CPU: i5-13400F
Ram: 16GB DDR5.



---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-08-07

---