
<div align="right">
  <details>
    <summary >ğŸŒ NgÃ´n ngá»¯</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ç®€ä½“ä¸­æ–‡</a>
        | <a href="#" title="Coming soon">ç¹é«”ä¸­æ–‡ (sáº¯p ra máº¯t)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">æ—¥æœ¬èª</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">í•œêµ­ì–´</a>
        | <a href="#" title="Coming soon">à¤¹à¤¿à¤¨à¥à¤¦à¥€ (sáº¯p ra máº¯t)</a> |
        | <a href="#" title="Coming soon">à¹„à¸—à¸¢ (sáº¯p ra máº¯t)</a> |
        | <a href="#" title="Coming soon">FranÃ§ais (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Coming soon">Deutsch (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Coming soon">EspaÃ±ol (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Coming soon">Italiano (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Coming soon">Ğ ÑƒÑÑĞºĞ¸Ğ¹ (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Coming soon">PortuguÃªs (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Coming soon">Nederlands (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Coming soon">Polski (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Coming soon">Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Coming soon">ÙØ§Ø±Ø³ÛŒ (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Coming soon">TÃ¼rkÃ§e (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Coming soon">Tiáº¿ng Viá»‡t (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (sáº¯p ra máº¯t)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM
Má»™t mÃ´ hÃ¬nh LLM chá»‰ Ä‘Æ°á»£c huáº¥n luyá»‡n tá»« dá»¯ liá»‡u thuá»™c cÃ¡c giai Ä‘oáº¡n thá»i gian nháº¥t Ä‘á»‹nh nháº±m giáº£m thiÃªn vá»‹ hiá»‡n Ä‘áº¡i.

HÃ£y tÆ°á»Ÿng tÆ°á»£ng má»™t mÃ´ hÃ¬nh AI khÃ´ng chá»‰ giáº£ vá» lÃ  lá»‹ch sá»­ mÃ  thá»±c sá»± nhÆ° váº­y.

ÄÆ°á»£c xÃ¢y dá»±ng trÃªn [nanoGPT cá»§a Andrej Karpathy](https://github.com/karpathy/nanoGPT) CÃ¡c script huáº¥n luyá»‡n lÃµi vÃ  kiáº¿n trÃºc mÃ´ hÃ¬nh lÃ  cá»§a Ã´ng áº¥y.

# Má»¥c tiÃªu Dá»± Ã¡n

TimeCapsule LLM lÃ  má»™t dá»± Ã¡n thá»­ nghiá»‡m chá»‰ Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn cÃ¡c vÄƒn báº£n Ä‘Æ°á»£c viáº¿t trong nhá»¯ng giai Ä‘oáº¡n lá»‹ch sá»­ nháº¥t Ä‘á»‹nh. Má»¥c tiÃªu lÃ  mÃ´ phá»ng tháº¿ giá»›i quan vÃ  ngÃ´n ngá»¯ cá»§a cÃ¡c thá»i ká»³ lá»‹ch sá»­ cá»¥ thá»ƒ.

# Táº¡i sao chá»‰ tinh chá»‰nh lÃ  chÆ°a Ä‘á»§

Náº¿u báº¡n chá»‰ tinh chá»‰nh má»™t mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c, LLM cá»§a báº¡n váº«n sáº½ biáº¿t cÃ¡c khÃ¡i niá»‡m hiá»‡n Ä‘áº¡i. Táº¥t nhiÃªn Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c má»©c khÃ´ng cÃ³ thiÃªn vá»‹ hiá»‡n Ä‘áº¡i lÃ  ráº¥t khÃ³ nhÆ°ng tÃ´i muá»‘n cá»‘ gáº¯ng gáº§n nháº¥t vá»›i Ä‘iá»u nÃ y. Muá»‘n loáº¡i bá» hoÃ n toÃ n thiÃªn vá»‹ hiá»‡n Ä‘áº¡i thÃ¬ pháº£i huáº¥n luyá»‡n mÃ´ hÃ¬nh tá»« Ä‘áº§u.

# Káº¿t quáº£ ká»³ vá»ng

Hy vá»ng khi hoÃ n thÃ nh, mÃ´ hÃ¬nh nÃ y sáº½ khÃ´ng biáº¿t cÃ¡c khÃ¡i niá»‡m hiá»‡n Ä‘áº¡i vÃ  khÃ´ng thá»ƒ suy luáº­n ngoÃ i pháº¡m vi Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n. NÃ³ khÃ´ng nÃªn nháº­n biáº¿t cÃ¡c khÃ¡i niá»‡m/tá»« vá»±ng hiá»‡n Ä‘áº¡i vÃ  tÃ´i hy vá»ng nÃ³ khÃ´ng tá»± bá»‹a ra kiáº¿n thá»©c hiá»‡n Ä‘áº¡i.

# Cáº­p nháº­t Tiáº¿n Ä‘á»™

## 9 thÃ¡ng 7, 2025

TÃ´i Ä‘Ã£ xÃ¡c Ä‘á»‹nh giai Ä‘oáº¡n thá»i gian lÃ  1800-1850 vÃ  khu vá»±c: London

TÃ´i Ä‘Ã£ thu tháº­p Ä‘Æ°á»£c danh sÃ¡ch vÄƒn báº£n, sÃ¡ch, tÃ i liá»‡u

Hiá»‡n táº¡i tÃ´i Ä‘Ã£ cÃ³ 50 file txt vÃ  sáº½ báº¯t Ä‘áº§u huáº¥n luyá»‡n NanoGPT sá»›m

Sáº½ cáº­p nháº­t táº¡i Ä‘Ã¢y miá»…n lÃ  cÃ²n tiáº¿n triá»ƒn

## 13 thÃ¡ng 7, 2025

ÄÃ£ huáº¥n luyá»‡n nanoGPT vá»›i 187MB dá»¯ liá»‡u vÄƒn báº£n lá»‹ch sá»­.

## 15 thÃ¡ng 7, 2025

TÃ´i Ä‘Ã£ báº¯t Ä‘áº§u táº£i vá» vÄƒn báº£n cho láº§n huáº¥n luyá»‡n thá»© hai. TÃ´i láº¥y táº¥t cáº£ tá»« Internet Archive vÃ  Ä‘Ã£ má»Ÿ rá»™ng giai Ä‘oáº¡n lÃªn 1800-1875. Äá»ƒ cÃ³ Ä‘Æ°á»£c nhiá»u loáº¡i vÄƒn báº£n Ä‘a dáº¡ng, báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng bá»™ lá»c chá»§ Ä‘á», vá»‹ trÃ­ xuáº¥t báº£n, thá»i ká»³ vÃ  chá»§ Ä‘á» trÃªn Internet Archive.

![Bá»™ lá»c tÃ¬m kiáº¿m](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## 16 thÃ¡ng 7, 2025

TÃ´i Ä‘Ã£ táº£i vá» khoáº£ng 500 file txt tá»« Internet Archive vÃ  sau khi lÃ m sáº¡ch (chá»‰ xÃ³a khoáº£ng tráº¯ng, tiÃªu Ä‘á» Gutenberg, v.v.) tÃ´i cÃ³ khoáº£ng 500MB dá»¯ liá»‡u. ÄÃ¢y lÃ  má»™t táº­p dá»¯ liá»‡u nhá» nhÆ°ng láº§n trÆ°á»›c tÃ´i chá»‰ huáº¥n luyá»‡n vá»›i 187MB nÃªn láº§n nÃ y cháº¯c cháº¯n sáº½ cÃ³ sá»± khÃ¡c biá»‡t Ä‘Ã¡ng chÃº Ã½ trong Ä‘áº§u ra sau khi huáº¥n luyá»‡n mÃ´ hÃ¬nh thá»© hai. TÃ´i hy vá»ng mÃ´ hÃ¬nh nÃ y Ã­t nháº¥t cÃ³ thá»ƒ sinh ra nhá»¯ng cÃ¢u rÃµ rÃ ng hÆ¡n, há»£p lÃ½ hÆ¡n. Táº¥t nhiÃªn khÃ´ng Ä‘áº£m báº£o vÃ¬ váº«n lÃ  táº­p dá»¯ liá»‡u ráº¥t nhá», nhÆ°ng váº«n nhiá»u hÆ¡n láº§n trÆ°á»›c.

Viá»‡c nÃ y cÃ³ thá»ƒ thá»±c hiá»‡n Ä‘Æ°á»£c trÃªn pháº§n cá»©ng cá»§a tÃ´i, Ä‘iá»u nÃ y cÅ©ng tá»‘t vÃ¬ tÃ´i hy vá»ng sáº½ tháº¥y Ä‘Æ°á»£c má»™t sá»‘ cáº£i tiáº¿n trÆ°á»›c khi chuyá»ƒn sang táº­p dá»¯ liá»‡u lá»›n hÆ¡n cáº§n thuÃª GPU. NhÆ°ng Ä‘á»«ng lo tÃ´i váº«n dá»± Ä‘á»‹nh sáº½ thuÃª GPU sá»›m, chá»‰ lÃ  trÆ°á»›c khi lÃ m váº­y tÃ´i muá»‘n cháº¯c cháº¯n táº­p dá»¯ liá»‡u cá»§a mÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c tuyá»ƒn chá»n vÃ  lÃ m sáº¡ch tá»‘t nháº¥t cÃ³ thá»ƒ. Má»™t váº¥n Ä‘á» tÃ´i gáº·p pháº£i lÃ  lÃ m sáº¡ch, nhiá»u file txt nÃ y láº«n lá»™n kÃ­ tá»± vÃ´ nghÄ©a. CÃ¡c script tÃ´i dÃ¹ng Ä‘á»ƒ lÃ m sáº¡ch cÃ³ hiá»‡u quáº£ nhÆ°ng khÃ´ng hoÃ n toÃ n. 

TÃ´i sáº½ huáº¥n luyá»‡n táº­p dá»¯ liá»‡u nÃ y hÃ´m nay vÃ  sáº½ máº¥t khoáº£ng 4-5 giá». Sau khi xong vÃ  kiá»ƒm tra, tÃ´i sáº½ cáº­p nháº­t tiáº¿p. Cáº£m Æ¡n má»i ngÆ°á»i Ä‘Ã£ quan tÃ¢m Ä‘áº¿n dá»± Ã¡n cá»§a tÃ´i, tháº­m chÃ­ cÃ³ ngÆ°á»i gá»­i tÃ´i cÃ¡c liÃªn káº¿t tÃ i nguyÃªn OCR nÃªn cáº£m Æ¡n ráº¥t nhiá»u! Hy vá»ng sáº½ cÃ³ thÃªm nhiá»u ngÆ°á»i thá»­ vÃ  thá»­ nghiá»‡m vá»›i bá»™ dá»¯ liá»‡u riÃªng cá»§a há».

## 28 thÃ¡ng 7, 2025

TÃ´i Ä‘Ã£ táº£i lÃªn báº£n v0.5 lÃªn Hugging Face, [Xem táº¡i Ä‘Ã¢y](https://huggingface.co/haykgrigorian/TimeCapsuleLLM) náº¿u báº¡n muá»‘n. Báº¡n cÃ³ thá»ƒ táº£i repo cá»§a tÃ´i vÃ  cháº¡y nÃ³ cá»¥c bá»™. Ráº¥t tiáº¿c nanoGPT khÃ´ng hoáº¡t Ä‘á»™ng trá»±c tiáº¿p vá»›i HuggingFace, nÃªn báº¡n sáº½ pháº£i táº£i vá» vÃ  cháº¡y mÃ´ hÃ¬nh cá»¥c bá»™.

NgoÃ i ra tÃ´i sáº½ báº¯t Ä‘áº§u chá»n lá»c dá»¯ liá»‡u cho láº§n huáº¥n luyá»‡n tiáº¿p theo, tÃ´i nghÄ© tÃ´i sáº½ cáº§n nhiá»u dá»¯ liá»‡u hÆ¡n ná»¯a, gáº¥p 5-10 láº§n Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c kháº£ nÄƒng suy luáº­n.

### Cáº­p nháº­t huáº¥n luyá»‡n

TÃ´i Ä‘Ã£ báº¯t Ä‘áº§u huáº¥n luyá»‡n trÃªn má»™t táº­p dá»¯ liá»‡u 435MB (108 triá»‡u tokens), hiá»‡n táº¡i má»i thá»© diá»…n ra khÃ¡ suÃ´n sáº». Train loss giáº£m tá»« 10.9 xuá»‘ng 4.9 chá»‰ sau 2800 vÃ²ng láº·p Ä‘áº§u tiÃªn. TÃ´i dá»± Ä‘oÃ¡n sáº½ máº¥t khoáº£ng 8 hoáº·c 9 tiáº¿ng Ä‘á»ƒ hoÃ n thÃ nh. Sáº½ Ä‘Äƒng cáº­p nháº­t khi hoÃ n thÃ nh.

## 17 thÃ¡ng 7, 2025 2:13AM

ÄÃ£ huáº¥n luyá»‡n xong cho mÃ´ hÃ¬nh thá»© hai, chiáº¿c 4060 cá»§a tÃ´i máº¥t khoáº£ng 8 tiáº¿ng 40 phÃºt (3.900 vÃ²ng/giá») cho 33.000 vÃ²ng (5 epoch). Train loss cuá»‘i cÃ¹ng lÃ  3.73. Káº¿t quáº£ Ä‘áº§u ra khÃ¡ tá»‘t, thá»±c sá»± sinh ra nhá»¯ng cÃ¢u vÄƒn kiá»ƒu tháº¿ ká»· 19 cÃ³ nghÄ©a.

# HÃ nh vi & Giá»›i háº¡n cá»§a MÃ´ hÃ¬nh V0

Nhá»¯ng gá»£i Ã½ ban Ä‘áº§u cho tháº¥y mÃ´ hÃ¬nh pháº£n há»“i vá»›i ngÃ´n ngá»¯ vÃ  hÃ nh vi cá»§a tháº­p niÃªn 1800. VÃ­ dá»¥, tÃ´i thá»­ vá»›i "Who art Henry?" vÃ  nÃ³ tráº£ lá»i "I know that man, I have did not a black, the storm." vÃ  vÃ¢ng cÃ¢u Ä‘Ã³ khÃ´ng cÃ³ nghÄ©a nhÆ°ng LLM nháº­n ra tÃ´i Ä‘ang há»i vá» má»™t ngÆ°á»i.

![TimeLockLLM Máº«u Ä‘áº§u ra](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

KhÃ´ng cÃ³ Ä‘á» cáº­p Ä‘áº¿n cÃ¡c khÃ¡i niá»‡m hiá»‡n Ä‘áº¡i, Ä‘áº§u ra chá»§ yáº¿u chá»©a cÃ¡c tá»« ngá»¯ vÃ  cÃ¡ch diá»…n Ä‘áº¡t tá»« nhá»¯ng nÄƒm 1800.

Váº«n cÃ²n ráº¥t nhiá»u viá»‡c cáº§n lÃ m, huáº¥n luyá»‡n tá»« 187MB sáº½ khÃ´ng cho báº¡n má»™t mÃ´ hÃ¬nh táº¡o ra vÄƒn báº£n vá»›i lÃ½ luáº­n phá»©c táº¡p.

Hiá»‡n táº¡i nÃ³ táº¡o ra cÃ¡c cÃ¢u thiáº¿u cáº¥u trÃºc cÃ¢u Ä‘áº§y Ä‘á»§ vÃ  nhÃ¬n chung lÃ  khÃ´ng cÃ³ Ã½ nghÄ©a, nhÆ°ng Ä‘iá»u nÃ y lÃ  bÃ¬nh thÆ°á»ng Ä‘á»‘i vá»›i kÃ­ch thÆ°á»›c táº­p huáº¥n luyá»‡n nÃ y.

# HÃ nh Vi & Giá»›i Háº¡n cá»§a MÃ´ HÃ¬nh V0.5

ÄÃ¢y lÃ  má»™t sá»± cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ so vá»›i mÃ´ hÃ¬nh trÆ°á»›c Ä‘Ã³. Phong cÃ¡ch viáº¿t vÃ  vá»‘n tá»« lÃ  thá»i Victoria vÃ  háº§u nhÆ° má»i cÃ¢u Ä‘á»u Ä‘Ãºng ngá»¯ phÃ¡p vá»›i dáº¥u cÃ¢u Ä‘áº§y Ä‘á»§. VÃ  má»™t láº§n ná»¯a, mÃ´ hÃ¬nh nÃ y Ä‘Æ°á»£c huáº¥n luyá»‡n tá»« Ä‘áº§u nÃªn chá»‰ táº­p trung vÃ o cÃ¡c chá»§ Ä‘á» cá»§a tháº¿ ká»· 19.

![VÃ­ dá»¥ Ä‘áº§u ra TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

CÃ³ ráº¥t nhiá»u áº£o giÃ¡c thá»±c táº¿. Ráº¥t nhiá»u (gáº§n nhÆ° 100%) chi tiáº¿t (ngÃ y thÃ¡ng, sá»± kiá»‡n, nhÃ¢n váº­t lá»‹ch sá»­) Ä‘á»u lÃ  bá»‹a Ä‘áº·t. NgoÃ i ra cÃ¡c cÃ¢u khÃ´ng thá»±c sá»± liÃªn káº¿t vá»›i nhau, Ä‘Ã´i khi chá»‰ cÃ³ 2 cÃ¢u liÃªn quan nhÆ°ng ngoÃ i ra thÃ¬ khÃ´ng. Má»™t váº¥n Ä‘á» khÃ¡c lÃ  Ä‘Ã´i khi xuáº¥t hiá»‡n dÃ²ng chÃ¢n trang â€œDigitized by Googleâ€, nÃªn láº§n huáº¥n luyá»‡n tá»›i tÃ´i pháº£i cháº¯c cháº¯n lÃ m sáº¡ch vÄƒn báº£n ká»¹ hÆ¡n. NhÃ¬n chung tÃ´i ráº¥t hÃ i lÃ²ng vá»›i káº¿t quáº£, nÃ³ chÆ°a pháº£i lÃ  má»™t LLM nhÆ°ng cháº¯c cháº¯n lÃ  má»™t trÃ¬nh táº¡o cÃ¢u.

TÃ´i Ä‘ang há»c há»i ráº¥t nhiá»u vÃ  sáº½ báº¯t Ä‘áº§u tÃ¬m ra nhá»¯ng gÃ¬ mÃ¬nh cáº§n lÃ m tá»‘t hÆ¡n trong vÃ i tuáº§n tá»›i. TÃ´i sáº½ sá»›m táº£i lÃªn cÃ¡c tá»‡p!

# Káº¿ Hoáº¡ch Sáº¯p Tá»›i

(ÄÃ£ hoÃ n thÃ nh) TÃ´i sáº½ báº¯t Ä‘áº§u lÃ m viá»‡c vá»›i phiÃªn báº£n 0.5, thay vÃ¬ huáº¥n luyá»‡n vá»›i 50 cuá»‘n sÃ¡ch, tÃ´i sáº½ huáº¥n luyá»‡n vá»›i lÃ½ tÆ°á»Ÿng lÃ  500-600 cuá»‘n. Hiá»‡n táº¡i tÃ´i Ä‘ang huáº¥n luyá»‡n nanoGPT vá»›i sÃ¡ch tá»« 1800-1850 vÃ  cá»¥ thá»ƒ lÃ  tá»« London. CÃ³ má»™t sá»‘ thÃ¡ch thá»©c nhÆ° Ä‘áº£m báº£o cÃ¡c sÃ¡ch tÃ´i tÃ¬m khÃ´ng bá»‹ cáº­p nháº­t hoáº·c cÃ³ diá»…n giáº£i hiá»‡n Ä‘áº¡i mÃ  lÃ  cÃ¡c sÃ¡ch gá»‘c xuáº¥t báº£n trong giai Ä‘oáº¡n tÃ´i chá»n.

TÃ´i muá»‘n huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh má»›i (v1) vá»›i táº­p dá»¯ liá»‡u lá»›n hÆ¡n nhiá»u, cÃ³ thá»ƒ lá»›n hÆ¡n 5-10 láº§n so vá»›i v0.5. Má»¥c tiÃªu lÃ  xem liá»‡u tÃ´i cÃ³ thá»ƒ lÃ m xuáº¥t hiá»‡n kháº£ nÄƒng lÃ½ luáº­n chá»‰ tá»« Huáº¥n Luyá»‡n Thá»i Gian Chá»n Lá»c hay khÃ´ng, Ä‘Ã¢y sáº½ lÃ  nhiá»‡m vá»¥ khÃ³ hÆ¡n vÃ  tÃ´i cÅ©ng khÃ´ng cháº¯c cÃ³ thá»ƒ vÃ¬ dá»¯ liá»‡u lá»‹ch sá»­ bá»‹ giá»›i háº¡n. Trong vÃ i tuáº§n tá»›i tÃ´i sáº½ cá»‘ gáº¯ng tá»•ng há»£p Ä‘á»§ dá»¯ liá»‡u cho táº­p dá»¯ liá»‡u 5-10GB. TÃ´i tin ráº±ng náº¿u tÃ´i cÃ³ dá»¯ liá»‡u sáº¡ch, cháº¥t lÆ°á»£ng cao vÃ  thuÃª Ä‘Æ°á»£c GPU, sáº½ cÃ³ tiáº¿n triá»ƒn.

# CÃ¡ch Sá»­ Dá»¥ng Dá»± Ãn NÃ y

Dá»± Ã¡n nÃ y chá»§ yáº¿u táº­p trung vÃ o viá»‡c thu tháº­p dá»¯ liá»‡u lá»‹ch sá»­, chuáº©n bá»‹ cho huáº¥n luyá»‡n vÃ  xÃ¢y dá»±ng tokenizer. TÃ´i sáº½ khÃ´ng trÃ¬nh bÃ y toÃ n bá»™ quÃ¡ trÃ¬nh huáº¥n luyá»‡n LLM, cho viá»‡c Ä‘Ã³ hÃ£y tham kháº£o nanoGPT cá»§a Andrej Karpathy.

# BÆ°á»›c 1: Thu Tháº­p vÃ  Chuáº©n Bá»‹ VÄƒn Báº£n Lá»‹ch Sá»­

Thu tháº­p cÃ¡c tá»‡p .txt cá»§a sÃ¡ch, tÃ i liá»‡u, v.v. thuá»™c pháº¡m vi cÃ´ng cá»™ng tá»« giai Ä‘oáº¡n báº¡n chá»n (vÃ­ dá»¥: London 1800-1850)

Báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng download_texts_improved.py Ä‘á»ƒ táº£i sÃ¡ch náº¿u cáº§n.

LÃ m sáº¡ch cÃ¡c tá»‡p vÄƒn báº£n báº±ng script hoáº·c loáº¡i bá» thá»§ cÃ´ng header/footer tá»« Project Gutenberg, chÃº thÃ­ch hiá»‡n Ä‘áº¡i hoáº·c lá»—i OCR.

prepare_dataset.py nÃªn hoáº¡t Ä‘á»™ng tá»‘t.

# BÆ°á»›c 2: XÃ¢y Dá»±ng Tokenizer TÃ¹y Chá»‰nh

Cháº¡y train_tokenizer.py hoáº·c train_tokenizer_hf.py trÃªn dá»¯ liá»‡u Ä‘Ã£ lÃ m sáº¡ch.
Báº¡n sáº½ nháº­n Ä‘Æ°á»£c vocab.json vÃ  merges.txt

CÃ¡c tá»‡p nÃ y Ä‘á»‹nh nghÄ©a tá»« vá»±ng vÃ  quy táº¯c gá»™p cho mÃ´ hÃ¬nh cá»§a báº¡n

# BÆ°á»›c 3: Huáº¥n Luyá»‡n MÃ´ HÃ¬nh (nanoGPT)

Tham kháº£o [nanoGPT cá»§a Andrej Karpathy](https://github.com/karpathy/nanoGPT) Ä‘á»ƒ biáº¿t quy trÃ¬nh huáº¥n luyá»‡n.

Báº¡n cÃ³ thá»ƒ huáº¥n luyá»‡n LLM khÃ¡c náº¿u muá»‘n, nhÆ°ng tÃ´i dÃ¹ng nanoGPT

# FAQ

## Huáº¥n Luyá»‡n Thá»i Gian Chá»n Lá»c lÃ  gÃ¬?

Huáº¥n Luyá»‡n Thá»i Gian Chá»n Lá»c (Selective Temporal Training - STT) lÃ  má»™t phÆ°Æ¡ng phÃ¡p há»c mÃ¡y mÃ  toÃ n bá»™ dá»¯ liá»‡u huáº¥n luyá»‡n Ä‘á»u Ä‘Æ°á»£c chá»n lá»c náº±m trong má»™t thá»i ká»³ lá»‹ch sá»­ cá»¥ thá»ƒ. Viá»‡c nÃ y nháº±m mÃ´ phá»ng ngÃ´n ngá»¯ vÃ  tri thá»©c cá»§a thá»i ká»³ Ä‘Ã³ mÃ  khÃ´ng bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi cÃ¡c khÃ¡i niá»‡m hiá»‡n Ä‘áº¡i. VÃ­ dá»¥, mÃ´ hÃ¬nh hiá»‡n táº¡i tÃ´i cÃ³ (v0.5) Ä‘Æ°á»£c huáº¥n luyá»‡n hoÃ n toÃ n tá»« dá»¯ liá»‡u 1800-1875, khÃ´ng tinh chá»‰nh mÃ  huáº¥n luyá»‡n tá»« Ä‘áº§u nÃªn Ä‘áº§u ra pháº£n Ã¡nh phong cÃ¡ch ngÃ´n ngá»¯ vÃ  bá»‘i cáº£nh lá»‹ch sá»­ thá»i Ä‘Ã³.

## Táº¡i sao khÃ´ng chá»‰ sá»­ dá»¥ng fine-tuning hoáº·c LoRA?

Vá»›i dá»± Ã¡n nÃ y tÃ´i muá»‘n táº¡o má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ khÃ´ng bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi thiÃªn kiáº¿n hiá»‡n Ä‘áº¡i. Náº¿u tÃ´i fine-tune nhÆ° GPT-2, nÃ³ Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c vÃ  thÃ´ng tin Ä‘Ã³ sáº½ khÃ´ng máº¥t Ä‘i. Náº¿u tÃ´i huáº¥n luyá»‡n tá»« Ä‘áº§u, mÃ´ hÃ¬nh sáº½ khÃ´ng giáº£ vá» lÃ  cÅ©, mÃ  nÃ³ sáº½ lÃ  nhÆ° váº­y. Má»¥c tiÃªu hiá»‡n táº¡i cá»§a dá»± Ã¡n nÃ y lÃ  táº¡o ra thá»© gÃ¬ Ä‘Ã³ chá»‰ cÃ³ thá»ƒ lÃ½ luáº­n dá»±a trÃªn tri thá»©c tá»« sÃ¡ch London xuáº¥t báº£n trong giai Ä‘oáº¡n 1800-1850.

## Báº¡n Ä‘Ã£ sá»­ dá»¥ng dá»¯ liá»‡u gÃ¬ Ä‘á»ƒ huáº¥n luyá»‡n?

TÃ´i sá»­ dá»¥ng sÃ¡ch, tÃ i liá»‡u phÃ¡p lÃ½, bÃ¡o chÃ­ vÃ  cÃ¡c báº£n viáº¿t khÃ¡c tá»« London 1800â€“1850. Danh sÃ¡ch tÃ´i liÃªn káº¿t cÃ³ khoáº£ng 200 tÃ i liá»‡u nhÆ°ng láº§n Ä‘áº§u huáº¥n luyá»‡n tÃ´i chá»‰ dÃ¹ng 50 tá»‡p khoáº£ng ~187 MB. Báº¡n cÃ³ thá»ƒ xem danh sÃ¡ch tÃ i liá»‡u:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## MÃ´ hÃ¬nh PhiÃªn báº£n 0 lá»›n cá»¡ nÃ o?

MÃ´ hÃ¬nh nÃ y ráº¥t nhá» hiá»‡n táº¡i, tÃ´i chá»‰ lÃ m cho vui vÃ  tuÃ¢n thá»§ nghiÃªm ngáº·t quy táº¯c khÃ´ng sá»­ dá»¥ng nguá»“n hiá»‡n Ä‘áº¡i. NÃ³ cÃ³ gáº§n 16 triá»‡u tham sá»‘ nhÆ°ng tÃ´i sáº½ báº¯t Ä‘áº§u thu tháº­p thÃªm vÄƒn báº£n cá»• Ä‘á»ƒ báº¯t Ä‘áº§u huáº¥n luyá»‡n mÃ´ hÃ¬nh khÃ¡c. Sáº½ cáº­p nháº­t khi cÃ³ tiáº¿n triá»ƒn.

## ThÃ´ng sá»‘ Huáº¥n Luyá»‡n?

GPU: Geforce rtx 4060
CPU: i5-13400F
Ram: 16GB DDR5.


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-29

---