
<div align="right">
  <details>
    <summary >ğŸŒ NgÃ´n ngá»¯</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ç®€ä½“ä¸­æ–‡</a>
        | <a href="#" title="Sáº¯p ra máº¯t">ç¹é«”ä¸­æ–‡ (sáº¯p ra máº¯t)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">æ—¥æœ¬èª</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">í•œêµ­ì–´</a>
        | <a href="#" title="Sáº¯p ra máº¯t">à¤¹à¤¿à¤¨à¥à¤¦à¥€ (sáº¯p ra máº¯t)</a> |
        | <a href="#" title="Sáº¯p ra máº¯t">à¹„à¸—à¸¢ (sáº¯p ra máº¯t)</a> |
        | <a href="#" title="Sáº¯p ra máº¯t">FranÃ§ais (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Sáº¯p ra máº¯t">Deutsch (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Sáº¯p ra máº¯t">EspaÃ±ol (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Sáº¯p ra máº¯t">Italiano (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Sáº¯p ra máº¯t">Ğ ÑƒÑÑĞºĞ¸Ğ¹ (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Sáº¯p ra máº¯t">PortuguÃªs (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Sáº¯p ra máº¯t">Nederlands (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Sáº¯p ra máº¯t">Polski (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Sáº¯p ra máº¯t">Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Sáº¯p ra máº¯t">ÙØ§Ø±Ø³ÛŒ (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Sáº¯p ra máº¯t">TÃ¼rkÃ§e (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Sáº¯p ra máº¯t">Tiáº¿ng Viá»‡t (sáº¯p ra máº¯t)</a>
        | <a href="#" title="Sáº¯p ra máº¯t">Bahasa Indonesia (sáº¯p ra máº¯t)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM
Má»™t mÃ´ hÃ¬nh LLM Ä‘Æ°á»£c huáº¥n luyá»‡n chá»‰ trÃªn dá»¯ liá»‡u tá»« cÃ¡c giai Ä‘oáº¡n thá»i gian nháº¥t Ä‘á»‹nh Ä‘á»ƒ giáº£m thiÃªn vá»‹ hiá»‡n Ä‘áº¡i.

HÃ£y tÆ°á»Ÿng tÆ°á»£ng náº¿u má»™t mÃ´ hÃ¬nh AI khÃ´ng chá»‰ giáº£ vá» lÃ  lá»‹ch sá»­ mÃ  thá»±c sá»± Ä‘Ãºng nhÆ° váº­y.

ÄÆ°á»£c xÃ¢y dá»±ng trÃªn [nanoGPT cá»§a Andrej Karpathy](https://github.com/karpathy/nanoGPT) CÃ¡c táº­p lá»‡nh huáº¥n luyá»‡n cá»‘t lÃµi vÃ  kiáº¿n trÃºc mÃ´ hÃ¬nh lÃ  cá»§a Ã´ng áº¥y.

# Má»¥c tiÃªu dá»± Ã¡n
TimeCapsule LLM lÃ  má»™t dá»± Ã¡n thá»­ nghiá»‡m chá»‰ Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn cÃ¡c vÄƒn báº£n Ä‘Æ°á»£c viáº¿t trong nhá»¯ng khoáº£ng thá»i gian nháº¥t Ä‘á»‹nh. Má»¥c tiÃªu lÃ  mÃ´ phá»ng tháº¿ giá»›i quan vÃ  ngÃ´n ngá»¯ cá»§a cÃ¡c thá»i ká»³ lá»‹ch sá»­ cá»¥ thá»ƒ.

# Táº¡i sao tinh chá»‰nh thÃ´i lÃ  chÆ°a Ä‘á»§

Náº¿u báº¡n chá»‰ tinh chá»‰nh má»™t mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c, LLM cá»§a báº¡n váº«n sáº½ biáº¿t cÃ¡c khÃ¡i niá»‡m hiá»‡n Ä‘áº¡i. Táº¥t nhiÃªn, Ä‘áº¡t Ä‘Æ°á»£c má»©c Ä‘á»™ khÃ´ng thiÃªn vá»‹ hiá»‡n Ä‘áº¡i lÃ  ráº¥t khÃ³ nhÆ°ng tÃ´i muá»‘n tiáº¿n gáº§n nháº¥t cÃ³ thá»ƒ Ä‘áº¿n Ä‘iá»u nÃ y. Äá»ƒ khÃ´ng cÃ³ thiÃªn vá»‹ hiá»‡n Ä‘áº¡i thÃ¬ cáº§n pháº£i huáº¥n luyá»‡n mÃ´ hÃ¬nh tá»« Ä‘áº§u.

# Káº¿t quáº£ ká»³ vá»ng

Hy vá»ng khi hoÃ n thÃ nh, mÃ´ hÃ¬nh nÃ y sáº½ khÃ´ng biáº¿t cÃ¡c khÃ¡i niá»‡m hiá»‡n Ä‘áº¡i vÃ  khÃ´ng thá»ƒ suy luáº­n vÆ°á»£t quÃ¡ nhá»¯ng gÃ¬ Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n. NÃ³ khÃ´ng nÃªn nháº­n ra cÃ¡c tá»« vá»±ng/khÃ¡i niá»‡m hiá»‡n Ä‘áº¡i vÃ  khÃ´ng nÃªn táº¡o ra kiáº¿n thá»©c hiá»‡n Ä‘áº¡i giáº£ tÆ°á»Ÿng.

# Cáº­p nháº­t tiáº¿n Ä‘á»™

## NgÃ y 9 thÃ¡ng 7 nÄƒm 2025

TÃ´i Ä‘Ã£ chá»n thá»i ká»³ 1800-1850 vÃ  khu vá»±c: London

TÃ´i Ä‘Ã£ thu tháº­p danh sÃ¡ch cÃ¡c vÄƒn báº£n, sÃ¡ch, tÃ i liá»‡u

Hiá»‡n táº¡i tÃ´i Ä‘Ã£ cÃ³ 50 tá»‡p txt vÃ  sáº½ báº¯t Ä‘áº§u huáº¥n luyá»‡n NanoGPT sá»›m

Sáº½ cáº­p nháº­t tiáº¿p khi cÃ³ tiáº¿n triá»ƒn

## NgÃ y 13 thÃ¡ng 7 nÄƒm 2025

ÄÃ£ huáº¥n luyá»‡n nanoGPT vá»›i 187MB dá»¯ liá»‡u vÄƒn báº£n lá»‹ch sá»­.

## NgÃ y 15 thÃ¡ng 7 nÄƒm 2025

TÃ´i báº¯t Ä‘áº§u táº£i vá» cÃ¡c vÄƒn báº£n cho láº§n huáº¥n luyá»‡n thá»© hai. TÃ´i láº¥y táº¥t cáº£ tá»« Internet Archive vÃ  Ä‘Ã£ má»Ÿ rá»™ng thá»i ká»³ sang 1800-1875. Äá»ƒ cÃ³ Ä‘Æ°á»£c nhiá»u loáº¡i vÄƒn báº£n Ä‘a dáº¡ng, báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng bá»™ lá»c chá»§ Ä‘á» vÃ  tÃ¬m kiáº¿m theo vá»‹ trÃ­ xuáº¥t báº£n, thá»i ká»³ vÃ  chá»§ Ä‘á» trÃªn Internet Archive.

![Bá»™ lá»c tÃ¬m kiáº¿m](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## NgÃ y 16 thÃ¡ng 7 nÄƒm 2025

TÃ´i Ä‘Ã£ táº£i vá» khoáº£ng 500 tá»‡p txt tá»« Internet Archive vÃ  sau khi lÃ m sáº¡ch chÃºng (chá»‰ xÃ³a khoáº£ng tráº¯ng, tiÃªu Ä‘á» Gutenberg, v.v) tÃ´i cÃ³ khoáº£ng 500MB dá»¯ liá»‡u. ÄÃ¢y lÃ  má»™t bá»™ dá»¯ liá»‡u ráº¥t nhá» nhÆ°ng láº§n trÆ°á»›c tÃ´i Ä‘Ã£ huáº¥n luyá»‡n vá»›i 187MB nÃªn cháº¯c cháº¯n sáº½ cÃ³ sá»± khÃ¡c biá»‡t rÃµ rá»‡t nÃ o Ä‘Ã³ trong Ä‘áº§u ra sau khi tÃ´i huáº¥n luyá»‡n mÃ´ hÃ¬nh thá»© hai. TÃ´i hy vá»ng mÃ´ hÃ¬nh nÃ y Ã­t nháº¥t cÃ³ thá»ƒ táº¡o ra cÃ¡c cÃ¢u máº¡ch láº¡c hÆ¡n, cÃ³ Ã½ nghÄ©a hÆ¡n. DÄ© nhiÃªn khÃ´ng cÃ³ gÃ¬ Ä‘áº£m báº£o vÃ¬ Ä‘Ã¢y váº«n lÃ  bá»™ dá»¯ liá»‡u cá»±c nhá», nhÆ°ng váº«n nhiá»u hÆ¡n láº§n trÆ°á»›c.

Äiá»u nÃ y cÃ³ thá»ƒ thá»±c hiá»‡n Ä‘Æ°á»£c trÃªn pháº§n cá»©ng cá»§a tÃ´i, cÅ©ng tá»‘t vÃ¬ tÃ´i hy vá»ng sáº½ tháº¥y má»™t sá»‘ cáº£i tiáº¿n trÆ°á»›c khi chuyá»ƒn sang bá»™ dá»¯ liá»‡u lá»›n hÆ¡n, cÃ¡i mÃ  sáº½ buá»™c tÃ´i pháº£i thuÃª GPU. NhÆ°ng Ä‘á»«ng lo, tÃ´i váº«n dá»± Ä‘á»‹nh sáº½ thuÃª GPU sá»›m, tuy nhiÃªn trÆ°á»›c khi lÃ m váº­y tÃ´i muá»‘n Ä‘áº£m báº£o bá»™ dá»¯ liá»‡u cá»§a mÃ¬nh Ä‘Æ°á»£c tuyá»ƒn chá»n vÃ  sáº¡ch nháº¥t cÃ³ thá»ƒ. Má»™t trong nhá»¯ng váº¥n Ä‘á» tÃ´i gáº·p pháº£i lÃ  khÃ¢u lÃ m sáº¡ch, ráº¥t nhiá»u tá»‡p txt cÃ³ láº«n dá»¯ liá»‡u vÃ´ nghÄ©a. CÃ¡c script tÃ´i dÃ¹ng Ä‘á»ƒ lÃ m sáº¡ch cÃ³ hiá»‡u quáº£ nhÆ°ng khÃ´ng hoÃ n toÃ n 100%.

TÃ´i sáº½ huáº¥n luyá»‡n bá»™ dá»¯ liá»‡u nÃ y hÃ´m nay vÃ  quÃ¡ trÃ¬nh sáº½ máº¥t khoáº£ng 4-5 tiáº¿ng. Khi hoÃ n thÃ nh vÃ  tÃ´i kiá»ƒm tra, tÃ´i sáº½ cáº­p nháº­t tiáº¿n Ä‘á»™. Má»™t láº§n ná»¯a cáº£m Æ¡n táº¥t cáº£ nhá»¯ng ai Ä‘ang quan tÃ¢m tá»›i dá»± Ã¡n cá»§a tÃ´i, tháº­m chÃ­ cÃ³ ngÆ°á»i gá»­i tÃ´i cÃ¡c link tÃ i nguyÃªn OCR nÃªn xin cáº£m Æ¡n! TÃ´i hy vá»ng nhiá»u ngÆ°á»i sáº½ thá»­ lÃ m Ä‘iá»u nÃ y vÃ  thá»­ nghiá»‡m vá»›i bá»™ dá»¯ liá»‡u cá»§a chÃ­nh há».


## NgÃ y 12 thÃ¡ng 8 nÄƒm 2025

PhiÃªn báº£n 1 Ä‘Ã£ hoÃ n thÃ nh, phiÃªn báº£n nÃ y sá»­ dá»¥ng Phi 1.5 (700 triá»‡u tham sá»‘) vÃ  Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn bá»™ dá»¯ liá»‡u gáº§n 7000 vÄƒn báº£n. Cáº£i tiáº¿n chÃ­nh cá»§a mÃ´ hÃ¬nh nÃ y lÃ  kháº£ nÄƒng Ä‘Ã´i khi tham chiáº¿u Ä‘áº¿n tÃªn tháº­t vÃ  cÃ¡c sá»± kiá»‡n lá»‹ch sá»­. PhiÃªn báº£n 0 vÃ  0.5 khÃ´ng thá»ƒ lÃ m Ä‘Æ°á»£c Ä‘iá»u nÃ y, báº¥t cá»© khi nÃ o tÃ´i yÃªu cáº§u nhá»¯ng mÃ´ hÃ¬nh Ä‘Ã³ báº±ng má»™t cÃ¢u nhÆ° "VÃ o nÄƒm 1821" thÃ¬ káº¿t quáº£ chá»‰ lÃ  cÃ¡c tÆ°á»Ÿng tÆ°á»£ng, nhá»¯ng thá»© chÆ°a tá»«ng xáº£y ra. Má»™t vÃ­ dá»¥ vá» Ä‘iá»u nÃ y vá»›i PhiÃªn báº£n 1:

Lá»i nháº¯c cá»§a tÃ´i: ÄÃ³ lÃ  nÄƒm ChÃºa chÃºng ta 1834,

Káº¿t quáº£: "ÄÃ³ lÃ  nÄƒm ChÃºa chÃºng ta 1834 vÃ  cÃ¡c con phá»‘ London Ä‘áº§y nhá»¯ng cuá»™c biá»ƒu tÃ¬nh vÃ  kiáº¿n nghá»‹. LÃ½ do, nhÆ° nhiá»u ngÆ°á»i ká»ƒ láº¡i, khÃ´ng bá»‹ rÃ ng buá»™c á»Ÿ phÆ°Æ¡ng diá»‡n cÃ¡ nhÃ¢n, nhÆ°ng Ä‘Ã£ Ä‘Æ°á»£c tiáº¿p nháº­n trong cÃ¹ng ngÃ y cá»§a ngÃ y ChÃºa Palmerston, cÃ´ng chÃºng sáº½ nháº­n Ä‘Æ°á»£c má»™t tuyÃªn bá»‘ ngáº¯n vá» nhá»¯ng khÃ³ khÄƒn mÃ  ngÃ y luáº­t phÃ¡p Ä‘Ã£ Ä‘áº¿n vá»›i chÃºng ta. ÄÃ³ lÃ  Ä‘iá»u Ä‘Ã¡ng tiáº¿c sÃ¢u sáº¯c, ráº±ng cÃ¡c sá»± kiá»‡n hiá»‡n táº¡i trong lá»‹ch sá»­ tháº¿ giá»›i ráº¥t rÃµ rÃ ng, vÃ  sáº½ Ä‘Æ°á»£c biáº¿t Ä‘áº¿n. KhÃ´ng Ä‘Ãºng khi nhá»¯ng ngÆ°á»i Ä‘áº§u tiÃªn truyá»n giÃ¡o táº¡i Jerusalem láº¡i cÃ³ má»™t ghi chÃ©p rá»™ng lá»›n vÃ  thÃº vá»‹ Ä‘áº¿n tháº¿ vá» thá»‹nh vÆ°á»£ng vÃ  phÃ¡t triá»ƒn" 

Ban Ä‘áº§u tÃ´i nghÄ© Ä‘Ã¢y cÃ³ thá»ƒ lÃ  sá»± trÃ¹ng há»£p, nhÆ°ng hÃ£y nhÃ¬n vÃ o Ä‘Ã¢y: ![1834protest](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)


### Cáº­p nháº­t quÃ¡ trÃ¬nh huáº¥n luyá»‡n 

TÃ´i báº¯t Ä‘áº§u huáº¥n luyá»‡n trÃªn má»™t táº­p dá»¯ liá»‡u 435MB (108 triá»‡u token), má»i thá»© hiá»‡n Ä‘ang diá»…n ra khÃ¡ suÃ´n sáº». Äá»™ máº¥t mÃ¡t huáº¥n luyá»‡n giáº£m tá»« 10.9 xuá»‘ng 4.9 chá»‰ sau 2800 lÆ°á»£t láº·p Ä‘áº§u tiÃªn. TÃ´i dá»± Ä‘oÃ¡n sáº½ máº¥t khoáº£ng 8 hoáº·c 9 giá» Ä‘á»ƒ hoÃ n thÃ nh. TÃ´i sáº½ Ä‘Äƒng cáº­p nháº­t má»›i khi hoÃ n táº¥t.

## NgÃ y 17 thÃ¡ng 7 nÄƒm 2025

Viá»‡c huáº¥n luyá»‡n cho mÃ´ hÃ¬nh thá»© hai Ä‘Ã£ hoÃ n thÃ nh, chiáº¿c 4060 cá»§a tÃ´i máº¥t khoáº£ng 8 giá» 40 phÃºt (3.900 lÆ°á»£t láº·p/giá») cho 33.000 lÆ°á»£t láº·p (5 epoch). Äá»™ máº¥t mÃ¡t huáº¥n luyá»‡n cuá»‘i cÃ¹ng lÃ  3.73. Káº¿t quáº£ Ä‘áº§u ra tháº­t sá»± ráº¥t tá»‘t, mÃ´ hÃ¬nh táº¡o ra cÃ¡c cÃ¢u theo phong cÃ¡ch tháº¿ ká»· 19 ráº¥t máº¡ch láº¡c.

## NgÃ y 28 thÃ¡ng 7 nÄƒm 2025 

TÃ´i Ä‘Ã£ táº£i lÃªn v0.5 lÃªn Hugging Face, [Xem táº¡i Ä‘Ã¢y](https://huggingface.co/haykgrigorian/TimeCapsuleLLM) náº¿u báº¡n muá»‘n. Giá» Ä‘Ã¢y báº¡n cÃ³ thá»ƒ táº£i repo cá»§a tÃ´i vÃ  cháº¡y nÃ³ táº¡i mÃ¡y cÃ¡ nhÃ¢n. ÄÃ¡ng tiáº¿c nanoGPT khÃ´ng hoáº¡t Ä‘á»™ng trá»±c tiáº¿p vá»›i HuggingFace, nÃªn báº¡n sáº½ pháº£i táº£i vá» vÃ  cháº¡y mÃ´ hÃ¬nh táº¡i mÃ¡y.

NgoÃ i ra tÃ´i sáº½ báº¯t Ä‘áº§u chá»n lá»c dá»¯ liá»‡u cho láº§n huáº¥n luyá»‡n tiáº¿p theo, tÃ´i tin lÃ  sáº½ cáº§n nhiá»u hÆ¡n khoáº£ng 5-10 láº§n dá»¯ liá»‡u Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c kháº£ nÄƒng suy luáº­n.

## NgÃ y 2 thÃ¡ng 8 nÄƒm 2025

TÃ´i sáº½ báº¯t Ä‘áº§u lÃ m viá»‡c trÃªn PhiÃªn báº£n 1 sá»›m thÃ´i. TÃ´i sáº½ cáº§n chuyá»ƒn tá»« kiáº¿n trÃºc cá»§a nanoGPT sang má»™t thá»© hiá»‡n Ä‘áº¡i hÆ¡n. TÃ´i cÃ³ vÃ i kiáº¿n trÃºc LLM mÃ£ nguá»“n má»Ÿ Ä‘ang cÃ¢n nháº¯c, bao gá»“m: OpenLLaMA v3, Phi-2 vÃ  Qwen 1.5B. VÃ  Ä‘á»ƒ há»— trá»£ cho bÆ°á»›c tiáº¿n lÃªn V1, tÃ´i cáº§n chá»n lá»c má»™t bá»™ dá»¯ liá»‡u huáº¥n luyá»‡n lá»›n hÆ¡n vÃ  Ä‘a dáº¡ng hÆ¡n nhiá»u. TÃ´i sáº½ cáº§n Ã­t nháº¥t 5GB dá»¯ liá»‡u huáº¥n luyá»‡n sáº¡ch.

# HÃ nh vi & Giá»›i háº¡n cá»§a mÃ´ hÃ¬nh V0

CÃ¡c lá»i nháº¯c ban Ä‘áº§u cho tháº¥y mÃ´ hÃ¬nh pháº£n há»“i báº±ng ngÃ´n ngá»¯ vÃ  hÃ nh vi cá»§a nhá»¯ng nÄƒm 1800. VÃ­ dá»¥, tÃ´i há»i "Who art Henry?" vÃ  nÃ³ tráº£ lá»i "I know that man, I have did not a black, the storm." vÃ  vÃ¢ng, cÃ¢u nÃ y khÃ´ng cÃ³ nghÄ©a gÃ¬ nhÆ°ng LLM nháº­n ra tÃ´i Ä‘ang há»i vá» má»™t ngÆ°á»i.

![TimeLockLLM Sample Output](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

KhÃ´ng cÃ³ Ä‘á» cáº­p Ä‘áº¿n cÃ¡c khÃ¡i niá»‡m hiá»‡n Ä‘áº¡i, káº¿t quáº£ Ä‘áº§u ra chá»§ yáº¿u lÃ  tá»« ngá»¯ vÃ  cÃ¡ch diá»…n Ä‘áº¡t cá»§a nhá»¯ng nÄƒm 1800.

NÃ³ váº«n cÃ²n cáº§n ráº¥t nhiá»u cáº£i thiá»‡n, huáº¥n luyá»‡n trÃªn 187MB sáº½ khÃ´ng táº¡o ra mÃ´ hÃ¬nh sinh vÄƒn báº£n vá»›i kháº£ nÄƒng suy luáº­n phá»©c táº¡p. 

Hiá»‡n táº¡i mÃ´ hÃ¬nh táº¡o ra cÃ¡c cÃ¢u thiáº¿u cáº¥u trÃºc Ä‘áº§y Ä‘á»§ vÃ  nhÃ¬n chung lÃ  khÃ´ng cÃ³ Ã½ nghÄ©a, nhÆ°ng Ä‘iá»u nÃ y lÃ  bÃ¬nh thÆ°á»ng vá»›i kÃ­ch thÆ°á»›c dá»¯ liá»‡u huáº¥n luyá»‡n.

# HÃ nh Vi & Giá»›i Háº¡n cá»§a MÃ´ HÃ¬nh V0.5

ÄÃ¢y lÃ  má»™t cáº£i tiáº¿n Ä‘Ã¡ng ká»ƒ so vá»›i mÃ´ hÃ¬nh trÆ°á»›c. Phong cÃ¡ch viáº¿t vÃ  tá»« vá»±ng mang mÃ u sáº¯c thá»i Victorian, vÃ  háº§u nhÆ° má»i cÃ¢u Ä‘á»u Ä‘Ãºng ngá»¯ phÃ¡p vá»›i dáº¥u cÃ¢u chuáº©n. Má»™t láº§n ná»¯a, mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n tá»« Ä‘áº§u nÃªn ná»™i dung váº«n táº­p trung vÃ o cÃ¡c chá»§ Ä‘á» cá»§a tháº¿ ká»· 19.

![VÃ­ dá»¥ Ä‘áº§u ra TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

CÃ³ ráº¥t nhiá»u thÃ´ng tin tÆ°á»Ÿng tÆ°á»£ng. Ráº¥t nhiá»u chi tiáº¿t (ngÃ y thÃ¡ng, sá»± kiá»‡n, nhÃ¢n váº­t lá»‹ch sá»­) Ä‘á»u lÃ  bá»‹a Ä‘áº·t. NgoÃ i ra, cÃ¡c cÃ¢u khÃ´ng thá»±c sá»± liÃªn káº¿t vá»›i nhau, Ä‘Ã´i khi chá»‰ cÃ³ 2 cÃ¢u liÃªn quan Ä‘áº¿n nhau nhÆ°ng ngoÃ i ra thÃ¬ khÃ´ng. Má»™t váº¥n Ä‘á» ná»¯a lÃ  Ä‘Ã´i khi xuáº¥t hiá»‡n dÃ²ng chÃ¢n trang â€œDigitized by Googleâ€, nÃªn láº§n huáº¥n luyá»‡n tá»›i tÃ´i tháº­t sá»± pháº£i Ä‘áº£m báº£o vÄƒn báº£n Ä‘Æ°á»£c lÃ m sáº¡ch ká»¹. NhÃ¬n chung tÃ´i ráº¥t hÃ i lÃ²ng vá»›i káº¿t quáº£, dÃ¹ chÆ°a thá»ƒ gá»i lÃ  LLM nhÆ°ng cháº¯c cháº¯n lÃ  má»™t trÃ¬nh táº¡o cÃ¢u.

TÃ´i Ä‘ang há»c há»i ráº¥t nhiá»u vÃ  sáº½ báº¯t Ä‘áº§u xÃ¡c Ä‘á»‹nh Ä‘iá»u cáº§n cáº£i thiá»‡n trong vÃ i tuáº§n tá»›i. TÃ´i sáº½ sá»›m táº£i lÃªn cÃ¡c file!

# HÃ nh Vi & Giá»›i Háº¡n cá»§a MÃ´ HÃ¬nh V1

TÃ´i sáº½ sá»›m táº£i lÃªn má»™t sá»‘ Ä‘áº§u ra vÃ­ dá»¥ vÃ  so sÃ¡nh giá»¯a 3 mÃ´ hÃ¬nh vá»›i cÃ¹ng má»™t Ä‘á» bÃ i. TÃ´i cÅ©ng sáº½ táº£i lÃªn V1 lÃªn huggingface nhÆ° phiÃªn báº£n trÆ°á»›c, báº¡n cÃ³ thá»ƒ tÃ¬m tÃ i khoáº£n huggingface cá»§a tÃ´i táº¡i Ä‘Ã¢y: https://huggingface.co/haykgrigorian/TimeCapsuleLLM

# Káº¿ Hoáº¡ch Sáº¯p Tá»›i

(ÄÃ£ hoÃ n thÃ nh) TÃ´i sáº½ báº¯t Ä‘áº§u phÃ¡t triá»ƒn phiÃªn báº£n 0.5, thay vÃ¬ huáº¥n luyá»‡n vá»›i 50 cuá»‘n sÃ¡ch, tÃ´i sáº½ huáº¥n luyá»‡n vá»›i 500-600 cuá»‘n. Hiá»‡n táº¡i tÃ´i Ä‘ang huáº¥n luyá»‡n nanoGPT vá»›i cÃ¡c sÃ¡ch tá»« nÄƒm 1800-1850 vÃ  cá»¥ thá»ƒ lÃ  tá»« London. CÃ³ má»™t sá»‘ thÃ¡ch thá»©c nhÆ° Ä‘áº£m báº£o sÃ¡ch tÃ´i tÃ¬m khÃ´ng bá»‹ cáº­p nháº­t hoáº·c cÃ³ diá»…n giáº£i hiá»‡n Ä‘áº¡i, mÃ  lÃ  sÃ¡ch gá»‘c xuáº¥t báº£n Ä‘Ãºng thá»i gian tÃ´i chá»n.

TÃ´i muá»‘n huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh má»›i (v1) vá»›i táº­p dá»¯ liá»‡u lá»›n hÆ¡n nhiá»u, cÃ³ thá»ƒ lá»›n hÆ¡n 5-10 láº§n so vá»›i v0.5. Má»¥c tiÃªu cá»§a tÃ´i lÃ  xem liá»‡u kháº£ nÄƒng suy luáº­n cÃ³ xuáº¥t hiá»‡n chá»‰ tá»« QuÃ¡ TrÃ¬nh Huáº¥n Luyá»‡n Thá»i Gian CÃ³ Chá»n Lá»c hay khÃ´ng; Ä‘Ã¢y sáº½ lÃ  nhiá»‡m vá»¥ khÃ³ hÆ¡n vÃ  tÃ´i cÅ©ng chÆ°a cháº¯c kháº£ thi do háº¡n cháº¿ dá»¯ liá»‡u lá»‹ch sá»­. Trong vÃ i tuáº§n tá»›i tÃ´i sáº½ cá»‘ gáº¯ng tá»•ng há»£p Ä‘á»§ dá»¯ liá»‡u cho táº­p 5-10GB. TÃ´i tin náº¿u cÃ³ dá»¯ liá»‡u sáº¡ch, cháº¥t lÆ°á»£ng cao vÃ  thuÃª Ä‘Æ°á»£c GPU, sáº½ cÃ³ tiáº¿n triá»ƒn.

# CÃ¡ch Sá»­ Dá»¥ng Dá»± Ãn NÃ y

Dá»± Ã¡n nÃ y chá»§ yáº¿u táº­p trung vÃ o viá»‡c thu tháº­p dá»¯ liá»‡u lá»‹ch sá»­, chuáº©n bá»‹ cho huáº¥n luyá»‡n vÃ  xÃ¢y dá»±ng tokenizer. TÃ´i sáº½ khÃ´ng Ä‘á» cáº­p Ä‘áº§y Ä‘á»§ quÃ¡ trÃ¬nh huáº¥n luyá»‡n LLM, báº¡n cÃ³ thá»ƒ tham kháº£o nanoGPT cá»§a Andrej Karpathy.

# BÆ°á»›c 1: Thu Tháº­p vÃ  Chuáº©n Bá»‹ VÄƒn Báº£n Lá»‹ch Sá»­

Thu tháº­p cÃ¡c file .txt cá»§a sÃ¡ch, tÃ i liá»‡u trong pháº¡m vi cÃ´ng cá»™ng tá»« thá»i gian báº¡n chá»n (vÃ­ dá»¥: London 1800-1850)

Báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng download_texts_improved.py Ä‘á»ƒ táº£i sÃ¡ch náº¿u cáº§n.

LÃ m sáº¡ch cÃ¡c file vÄƒn báº£n báº±ng script hoáº·c thá»§ cÃ´ng Ä‘á»ƒ loáº¡i bá» Ä‘áº§u trang/cuá»‘i trang tá»« Project Gutenberg, chÃº thÃ­ch hiá»‡n Ä‘áº¡i hoáº·c cÃ¡c lá»—i OCR.

prepare_dataset.py nÃªn hoáº¡t Ä‘á»™ng tá»‘t.

# BÆ°á»›c 2: XÃ¢y Dá»±ng Tokenizer TÃ¹y Chá»‰nh

Cháº¡y train_tokenizer.py hoáº·c train_tokenizer_hf.py trÃªn dá»¯ liá»‡u Ä‘Ã£ lÃ m sáº¡ch.
Viá»‡c nÃ y sáº½ táº¡o ra vocab.json vÃ  merges.txt
CÃ¡c tá»‡p nÃ y Ä‘á»‹nh nghÄ©a tá»« vá»±ng vÃ  quy táº¯c ghÃ©p cho mÃ´ hÃ¬nh cá»§a báº¡n

# BÆ°á»›c 3: Huáº¥n luyá»‡n MÃ´ hÃ¬nh cá»§a báº¡n (nanoGPT)

Tham kháº£o [nanoGPT cá»§a Andrej Karpathy](https://github.com/karpathy/nanoGPT) cho quÃ¡ trÃ¬nh huáº¥n luyá»‡n.

Báº¡n cÃ³ thá»ƒ huáº¥n luyá»‡n má»™t LLM khÃ¡c náº¿u muá»‘n, nhÆ°ng tÃ´i Ä‘Ã£ dÃ¹ng nanoGPT

# CÃ¢u há»i thÆ°á»ng gáº·p

## ÄÃ o táº¡o Thá»i ká»³ Chá»n lá»c (Selective Temporal Training) lÃ  gÃ¬?

Selective Temporal Training (STT) lÃ  má»™t phÆ°Æ¡ng phÃ¡p há»c mÃ¡y mÃ  toÃ n bá»™ dá»¯ liá»‡u huáº¥n luyá»‡n Ä‘Æ°á»£c chá»n lá»c Ä‘á»ƒ rÆ¡i vÃ o má»™t khoáº£ng thá»i gian lá»‹ch sá»­ cá»¥ thá»ƒ. Viá»‡c nÃ y nháº±m mÃ´ phá»ng ngÃ´n ngá»¯ vÃ  kiáº¿n thá»©c cá»§a thá»i Ä‘áº¡i Ä‘Ã³ mÃ  khÃ´ng bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi cÃ¡c khÃ¡i niá»‡m hiá»‡n Ä‘áº¡i. VÃ­ dá»¥, mÃ´ hÃ¬nh hiá»‡n táº¡i tÃ´i Ä‘ang cÃ³ (v0.5) Ä‘Æ°á»£c huáº¥n luyá»‡n hoÃ n toÃ n trÃªn dá»¯ liá»‡u tá»« nÄƒm 1800-1875, khÃ´ng tinh chá»‰nh mÃ  huáº¥n luyá»‡n tá»« Ä‘áº§u nÃªn káº¿t quáº£ pháº£n Ã¡nh phong cÃ¡ch ngÃ´n ngá»¯ vÃ  bá»‘i cáº£nh lá»‹ch sá»­ thá»i ká»³ Ä‘Ã³.

## Táº¡i sao khÃ´ng chá»‰ dÃ¹ng tinh chá»‰nh (fine-tuning) hoáº·c LoRA?

Vá»›i dá»± Ã¡n nÃ y tÃ´i muá»‘n táº¡o má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ khÃ´ng bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi cÃ¡c thÃ nh kiáº¿n hiá»‡n Ä‘áº¡i. Náº¿u tÃ´i tinh chá»‰nh thá»© nhÆ° GPT-2, nÃ³ Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c vÃ  thÃ´ng tin Ä‘Ã³ sáº½ khÃ´ng máº¥t Ä‘i. Náº¿u tÃ´i huáº¥n luyá»‡n tá»« Ä‘áº§u thÃ¬ mÃ´ hÃ¬nh ngÃ´n ngá»¯ sáº½ khÃ´ng giáº£ vá» lÃ  cá»• xÆ°a, nÃ³ thá»±c sá»± sáº½ nhÆ° váº­y. Má»¥c tiÃªu hiá»‡n táº¡i lÃ  táº¡o ra thá»© gÃ¬ Ä‘Ã³ chá»‰ cÃ³ thá»ƒ suy luáº­n dá»±a trÃªn kiáº¿n thá»©c tá»« cÃ¡c sÃ¡ch London xuáº¥t báº£n trong giai Ä‘oáº¡n 1800 Ä‘áº¿n 1850.

## Loáº¡i dá»¯ liá»‡u nÃ o báº¡n dÃ¹ng Ä‘á»ƒ huáº¥n luyá»‡n?

TÃ´i sá»­ dá»¥ng sÃ¡ch, tÃ i liá»‡u phÃ¡p luáº­t, bÃ¡o chÃ­ vÃ  cÃ¡c bÃ i viáº¿t khÃ¡c tá»« London giai Ä‘oáº¡n 1800â€“1850. Danh sÃ¡ch tÃ´i Ä‘Ã­nh kÃ¨m cÃ³ khoáº£ng 200 tÃ i liá»‡u nhÆ°ng cho láº§n huáº¥n luyá»‡n Ä‘áº§u tÃ´i chá»‰ dÃ¹ng 50 tá»‡p khoáº£ng ~187 MB. Báº¡n cÃ³ thá»ƒ xem danh sÃ¡ch tÃ i liá»‡u:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## MÃ´ hÃ¬nh cÃ³ kÃ­ch thÆ°á»›c bao nhiÃªu?

V0: 16M Tham sá»‘

V0.5: 123M Tham sá»‘

V1: 700M Tham sá»‘

# ThÃ´ng sá»‘ huáº¥n luyá»‡n?

#V0/V0.5
GPU: Geforce rtx 4060
CPU: i5-13400F
Ram: 16GB DDR5.

#V1

GPU: A100 rented









---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-08-12

---