
<div align="right">
  <details>
    <summary >ğŸŒ NgÃ´n ngá»¯</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ç®€ä½“ä¸­æ–‡</a>
        | <a href="#" title="Coming soon">ç¹é«”ä¸­æ–‡ (coming soon)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">æ—¥æœ¬èª</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">í•œêµ­ì–´</a>
        | <a href="#" title="Coming soon">à¤¹à¤¿à¤¨à¥à¤¦à¥€ (coming soon)</a> |
        | <a href="#" title="Coming soon">à¹„à¸—à¸¢ (coming soon)</a> |
        | <a href="#" title="Coming soon">FranÃ§ais (coming soon)</a>
        | <a href="#" title="Coming soon">Deutsch (coming soon)</a>
        | <a href="#" title="Coming soon">EspaÃ±ol (coming soon)</a>
        | <a href="#" title="Coming soon">Italiano (coming soon)</a>
        | <a href="#" title="Coming soon">Ğ ÑƒÑÑĞºĞ¸Ğ¹ (coming soon)</a>
        | <a href="#" title="Coming soon">PortuguÃªs (coming soon)</a>
        | <a href="#" title="Coming soon">Nederlands (coming soon)</a>
        | <a href="#" title="Coming soon">Polski (coming soon)</a>
        | <a href="#" title="Coming soon">Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© (coming soon)</a>
        | <a href="#" title="Coming soon">ÙØ§Ø±Ø³ÛŒ (coming soon)</a>
        | <a href="#" title="Coming soon">TÃ¼rkÃ§e (coming soon)</a>
        | <a href="#" title="Coming soon">Tiáº¿ng Viá»‡t (coming soon)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (coming soon)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM
Má»™t mÃ´ hÃ¬nh LLM Ä‘Æ°á»£c huáº¥n luyá»‡n chá»‰ trÃªn dá»¯ liá»‡u tá»« cÃ¡c giai Ä‘oáº¡n thá»i gian nháº¥t Ä‘á»‹nh nháº±m giáº£m thiÃªn vá»‹ hiá»‡n Ä‘áº¡i.

HÃ£y tÆ°á»Ÿng tÆ°á»£ng náº¿u má»™t mÃ´ hÃ¬nh AI khÃ´ng chá»‰ giáº£ vá» lÃ  lá»‹ch sá»­ mÃ  thá»±c sá»± lÃ  nhÆ° váº­y.

Dá»±a trÃªn [nanoGPT cá»§a Andrej Karpathy](https://github.com/karpathy/nanoGPT) CÃ¡c script huáº¥n luyá»‡n cá»‘t lÃµi vÃ  kiáº¿n trÃºc mÃ´ hÃ¬nh lÃ  cá»§a Ã´ng áº¥y.

# Má»¥c tiÃªu dá»± Ã¡n

TimeCapsule LLM lÃ  má»™t dá»± Ã¡n thá»­ nghiá»‡m chá»‰ Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn cÃ¡c vÄƒn báº£n viáº¿t trong nhá»¯ng giai Ä‘oáº¡n lá»‹ch sá»­ nháº¥t Ä‘á»‹nh. Má»¥c tiÃªu lÃ  mÃ´ phá»ng tháº¿ giá»›i quan vÃ  ngÃ´n ngá»¯ cá»§a cÃ¡c thá»i Ä‘áº¡i lá»‹ch sá»­ cá»¥ thá»ƒ.

# Táº¡i sao chá»‰ tinh chá»‰nh lÃ  chÆ°a Ä‘á»§

Náº¿u báº¡n chá»‰ tinh chá»‰nh má»™t mÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n trÆ°á»›c, LLM cá»§a báº¡n váº«n sáº½ biáº¿t cÃ¡c khÃ¡i niá»‡m hiá»‡n Ä‘áº¡i. Táº¥t nhiÃªn, Ä‘áº¡t Ä‘Æ°á»£c má»©c khÃ´ng thiÃªn vá»‹ hiá»‡n Ä‘áº¡i lÃ  ráº¥t khÃ³ nhÆ°ng tÃ´i muá»‘n tiáº¿n gáº§n nháº¥t cÃ³ thá»ƒ tá»›i Ä‘iá»u nÃ y. KhÃ´ng cÃ³ thiÃªn vá»‹ hiá»‡n Ä‘áº¡i Ä‘Ã²i há»i pháº£i huáº¥n luyá»‡n mÃ´ hÃ¬nh tá»« Ä‘áº§u.

# Káº¿t quáº£ ká»³ vá»ng

Hy vá»ng khi hoÃ n thÃ nh, mÃ´ hÃ¬nh nÃ y sáº½ khÃ´ng biáº¿t cÃ¡c khÃ¡i niá»‡m hiá»‡n Ä‘áº¡i vÃ  sáº½ khÃ´ng thá»ƒ suy luáº­n ngoÃ i nhá»¯ng gÃ¬ nÃ³ Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n. NÃ³ khÃ´ng nÃªn nháº­n biáº¿t cÃ¡c tá»« vá»±ng/khÃ¡i niá»‡m hiá»‡n Ä‘áº¡i vÃ  tÃ´i hy vá»ng nÃ³ khÃ´ng táº¡o ra kiáº¿n thá»©c hiá»‡n Ä‘áº¡i tÆ°á»Ÿng tÆ°á»£ng.

# Cáº­p nháº­t tiáº¿n Ä‘á»™

## NgÃ y 9 thÃ¡ng 7, 2025

TÃ´i Ä‘Ã£ chá»n giai Ä‘oáº¡n thá»i gian 1800-1850 vÃ  khu vá»±c: London

TÃ´i Ä‘Ã£ táº­p há»£p má»™t danh sÃ¡ch vÄƒn báº£n, sÃ¡ch, tÃ i liá»‡u

Cho Ä‘áº¿n nay tÃ´i Ä‘Ã£ cÃ³ 50 file dáº¡ng txt vÃ  sáº½ báº¯t Ä‘áº§u huáº¥n luyá»‡n NanoGPT sá»›m

Sáº½ cáº­p nháº­t pháº§n nÃ y miá»…n lÃ  cÃ²n tiáº¿n triá»ƒn

## NgÃ y 13 thÃ¡ng 7, 2025

ÄÃ£ huáº¥n luyá»‡n nanoGPT vá»›i 187MB dá»¯ liá»‡u vÄƒn báº£n lá»‹ch sá»­.

## NgÃ y 15 thÃ¡ng 7, 2025

TÃ´i báº¯t Ä‘áº§u táº£i vá» cÃ¡c vÄƒn báº£n cho Ä‘á»£t huáº¥n luyá»‡n thá»© hai. TÃ´i láº¥y má»i thá»© tá»« Internet Archive vÃ  tÃ´i Ä‘Ã£ má»Ÿ rá»™ng khoáº£ng thá»i gian tá»›i 1800-1875. Äá»ƒ cÃ³ Ä‘Æ°á»£c dáº£i vÄƒn báº£n Ä‘a dáº¡ng, báº¡n cÃ³ thá»ƒ dÃ¹ng cÃ¡c bá»™ lá»c chá»§ Ä‘á» vÃ  tÃ¬m kiáº¿m cho Ä‘á»‹a Ä‘iá»ƒm xuáº¥t báº£n, khoáº£ng thá»i gian vÃ  chá»§ Ä‘á» trÃªn Internet Archive.

![Search Filters](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## NgÃ y 16 thÃ¡ng 7, 2025

TÃ´i Ä‘Ã£ táº£i vá» khoáº£ng 500 file txt tá»« Internet Archive vÃ  sau khi dá»n dáº¹p (chá»‰ xÃ³a khoáº£ng tráº¯ng, tiÃªu Ä‘á» Gutenberg, vv) tÃ´i cÃ³ khoáº£ng 500MB dá»¯ liá»‡u. ÄÃ¢y lÃ  má»™t bá»™ dá»¯ liá»‡u ráº¥t nhá» nhÆ°ng láº§n trÆ°á»›c tÃ´i chá»‰ huáº¥n luyá»‡n vá»›i 187MB nÃªn cháº¯c cháº¯n sáº½ cÃ³ sá»± khÃ¡c biá»‡t cÃ³ thá»ƒ nháº­n tháº¥y trong Ä‘áº§u ra sau khi tÃ´i huáº¥n luyá»‡n mÃ´ hÃ¬nh thá»© hai. TÃ´i hy vá»ng mÃ´ hÃ¬nh nÃ y Ã­t nháº¥t cÃ³ thá»ƒ táº¡o ra cÃ¡c cÃ¢u máº¡ch láº¡c hÆ¡n, há»£p lÃ½ hÆ¡n. Táº¥t nhiÃªn khÃ´ng cÃ³ gÃ¬ Ä‘áº£m báº£o vÃ¬ bá»™ dá»¯ liá»‡u váº«n cá»±c ká»³ nhá», nhÆ°ng cÅ©ng Ä‘Ã£ nhiá»u hÆ¡n láº§n trÆ°á»›c.

Viá»‡c nÃ y cÃ³ thá»ƒ thá»±c hiá»‡n trÃªn pháº§n cá»©ng cá»§a tÃ´i, Ä‘iá»u nÃ y cÅ©ng tá»‘t vÃ¬ tÃ´i hy vá»ng sáº½ tháº¥y má»™t sá»‘ cáº£i thiá»‡n trÆ°á»›c khi chuyá»ƒn sang bá»™ dá»¯ liá»‡u lá»›n hÆ¡n mÃ  sáº½ buá»™c tÃ´i pháº£i thuÃª GPU. NhÆ°ng Ä‘á»«ng lo tÃ´i váº«n dá»± Ä‘á»‹nh thuÃª GPU sá»›m, nhÆ°ng trÆ°á»›c khi lÃ m váº­y tÃ´i muá»‘n cháº¯c cháº¯n bá»™ dá»¯ liá»‡u cá»§a tÃ´i Ä‘Æ°á»£c chá»n lá»c vÃ  sáº¡ch nháº¥t cÃ³ thá»ƒ. Má»™t trong cÃ¡c váº¥n Ä‘á» tÃ´i gáº·p pháº£i lÃ  lÃ m sáº¡ch, nhiá»u file txt nÃ y cÃ³ láº«n chá»¯ vÃ´ nghÄ©a. CÃ¡c script tÃ´i dÃ¹ng Ä‘á»ƒ lÃ m sáº¡ch cÃ³ hiá»‡u quáº£ nhÆ°ng khÃ´ng pháº£i lÃºc nÃ o cÅ©ng Ä‘áº¡t 100%.

TÃ´i sáº½ huáº¥n luyá»‡n bá»™ dá»¯ liá»‡u nÃ y hÃ´m nay vÃ  dá»± kiáº¿n sáº½ máº¥t khoáº£ng 4-5 giá». Khi hoÃ n thÃ nh vÃ  tÃ´i kiá»ƒm tra nÃ³, tÃ´i sáº½ cáº­p nháº­t. Cáº£m Æ¡n má»i ngÆ°á»i Ä‘Ã£ quan tÃ¢m dá»± Ã¡n cá»§a tÃ´i, tháº­m chÃ­ cÃ³ ngÆ°á»i cÃ²n gá»­i link tÃ i nguyÃªn OCR cho tÃ´i nÃªn cáº£m Æ¡n ráº¥t nhiá»u! TÃ´i hy vá»ng nhiá»u ngÆ°á»i sáº½ thá»­ nghiá»‡m vÃ  thá»­ vá»›i bá»™ dá»¯ liá»‡u cá»§a riÃªng mÃ¬nh.


### Cáº­p nháº­t huáº¥n luyá»‡n

TÃ´i Ä‘Ã£ báº¯t Ä‘áº§u huáº¥n luyá»‡n trÃªn táº­p dá»¯ liá»‡u 435MB (108 triá»‡u tokens), hiá»‡n táº¡i khÃ¡ suÃ´n sáº». Máº¥t mÃ¡t huáº¥n luyá»‡n giáº£m tá»« 10.9 xuá»‘ng 4.9 trong 2800 vÃ²ng láº·p Ä‘áº§u. TÃ´i dá»± Ä‘oÃ¡n sáº½ máº¥t khoáº£ng 8 hoáº·c 9 giá» Ä‘á»ƒ hoÃ n thÃ nh. TÃ´i sáº½ Ä‘Äƒng cáº­p nháº­t ná»¯a khi xong.

## NgÃ y 17 thÃ¡ng 7, 2025 2:13AM

Viá»‡c huáº¥n luyá»‡n cho mÃ´ hÃ¬nh thá»© hai Ä‘Ã£ hoÃ n thÃ nh, chiáº¿c 4060 cá»§a tÃ´i máº¥t khoáº£ng 8 tiáº¿ng 40 phÃºt (3.900 láº·p/giá») cho 33.000 láº·p (5 epoch). Máº¥t mÃ¡t huáº¥n luyá»‡n cuá»‘i lÃ  3.73. Äáº§u ra ráº¥t báº¥t ngá», nÃ³ thá»±c sá»± táº¡o ra nhá»¯ng cÃ¢u vÄƒn phong tháº¿ ká»· 19 máº¡ch láº¡c.

## NgÃ y 28 thÃ¡ng 7, 2025

TÃ´i Ä‘Ã£ tiáº¿n hÃ nh táº£i lÃªn phiÃªn báº£n v0.5 lÃªn Hugging Face, [Xem táº¡i Ä‘Ã¢y](https://huggingface.co/haykgrigorian/TimeCapsuleLLM) náº¿u báº¡n muá»‘n. Giá» báº¡n cÃ³ thá»ƒ táº£i repo cá»§a tÃ´i vÃ  cháº¡y nÃ³ cá»¥c bá»™. ÄÃ¡ng tiáº¿c nanoGPT khÃ´ng lÃ m viá»‡c trá»±c tiáº¿p vá»›i HuggingFace, nÃªn báº¡n sáº½ pháº£i táº£i vÃ  cháº¡y mÃ´ hÃ¬nh cá»¥c bá»™.

TÃ´i cÅ©ng sáº½ báº¯t Ä‘áº§u chá»n lá»c dá»¯ liá»‡u cho láº§n huáº¥n luyá»‡n tiáº¿p theo, tÃ´i tin mÃ¬nh cáº§n gáº¥p 5-10 láº§n dá»¯ liá»‡u Ä‘á»ƒ Ä‘áº¡t kháº£ nÄƒng suy luáº­n.


# HÃ nh vi & háº¡n cháº¿ cá»§a mÃ´ hÃ¬nh V0

CÃ¡c prompt ban Ä‘áº§u cho tháº¥y mÃ´ hÃ¬nh pháº£n há»“i báº±ng ngÃ´n ngá»¯ vÃ  hÃ nh vi cá»§a nhá»¯ng nÄƒm 1800. VÃ­ dá»¥, tÃ´i há»i "Who art Henry?" vÃ  nÃ³ tráº£ lá»i "I know that man, I have did not a black, the storm." vÃ  vÃ¢ng, cÃ¢u Ä‘Ã³ khÃ´ng cÃ³ nghÄ©a gÃ¬ nhÆ°ng LLM Ä‘ang nháº­n biáº¿t tÃ´i Ä‘ang há»i vá» má»™t ngÆ°á»i.

![Káº¿t quáº£ máº«u TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

KhÃ´ng cÃ³ Ä‘á» cáº­p Ä‘áº¿n cÃ¡c khÃ¡i niá»‡m hiá»‡n Ä‘áº¡i, Ä‘áº§u ra chá»§ yáº¿u sá»­ dá»¥ng tá»« ngá»¯ vÃ  cÃ¡ch diá»…n Ä‘áº¡t cá»§a tháº­p niÃªn 1800.

Váº«n cÃ²n ráº¥t nhiá»u viá»‡c pháº£i lÃ m, huáº¥n luyá»‡n vá»›i 187MB sáº½ khÃ´ng táº¡o ra má»™t mÃ´ hÃ¬nh cÃ³ kháº£ nÄƒng láº­p luáº­n phá»©c táº¡p.

Hiá»‡n táº¡i nÃ³ táº¡o ra cÃ¡c cÃ¢u thiáº¿u cáº¥u trÃºc hoÃ n chá»‰nh vÃ  nhÃ¬n chung khÃ¡ khÃ³ hiá»ƒu, nhÆ°ng Ä‘iá»u nÃ y lÃ  bÃ¬nh thÆ°á»ng vá»›i kÃ­ch thÆ°á»›c dá»¯ liá»‡u huáº¥n luyá»‡n nÃ y.

# HÃ nh vi & Giá»›i háº¡n cá»§a MÃ´ hÃ¬nh V0.5

ÄÃ¢y lÃ  má»™t cáº£i tiáº¿n Ä‘Ã¡ng ká»ƒ so vá»›i mÃ´ hÃ¬nh trÆ°á»›c. Phong cÃ¡ch viáº¿t vÃ  tá»« vá»±ng mang Ä‘áº­m cháº¥t thá»i Victoria vÃ  gáº§n nhÆ° má»i cÃ¢u Ä‘á»u Ä‘Ãºng ngá»¯ phÃ¡p vá»›i dáº¥u cÃ¢u thÃ­ch há»£p. VÃ  má»™t láº§n ná»¯a, mÃ´ hÃ¬nh nÃ y Ä‘Æ°á»£c huáº¥n luyá»‡n tá»« Ä‘áº§u nÃªn chá»‰ táº­p trung vÃ o cÃ¡c chá»§ Ä‘á» cá»§a tháº­p niÃªn 1800.

![Káº¿t quáº£ máº«u TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

CÃ³ ráº¥t nhiá»u "áº£o giÃ¡c thá»±c táº¿". Ráº¥t nhiá»u (gáº§n nhÆ° 100%) chi tiáº¿t (ngÃ y thÃ¡ng, sá»± kiá»‡n, nhÃ¢n váº­t lá»‹ch sá»­) lÃ  bá»‹a Ä‘áº·t. NgoÃ i ra, cÃ¡c cÃ¢u khÃ´ng thá»±c sá»± liÃªn káº¿t vá»›i nhau, thá»‰nh thoáº£ng cÃ³ 2 cÃ¢u liÃªn quan nhÆ°ng ngoÃ i ra thÃ¬ khÃ´ng. Má»™t váº¥n Ä‘á» khÃ¡c lÃ  Ä‘Ã´i khi xuáº¥t hiá»‡n dÃ²ng footer â€œDigitized by Googleâ€ khÃ´ng mong muá»‘n, nÃªn láº§n huáº¥n luyá»‡n tá»›i tÃ´i pháº£i Ä‘áº£m báº£o lÃ m sáº¡ch vÄƒn báº£n tháº­t ká»¹. NhÃ¬n chung tÃ´i ráº¥t hÃ i lÃ²ng vá»›i káº¿t quáº£ nÃ y, váº«n cÃ²n xa má»›i thÃ nh LLM nhÆ°ng cháº¯c cháº¯n Ä‘Ã£ lÃ  má»™t trÃ¬nh táº¡o cÃ¢u.

TÃ´i Ä‘ang há»c há»i ráº¥t nhiá»u vÃ  sáº½ báº¯t Ä‘áº§u tÃ¬m hiá»ƒu nhá»¯ng gÃ¬ cáº§n cáº£i thiá»‡n trong vÃ i tuáº§n tá»›i. TÃ´i sáº½ sá»›m táº£i lÃªn cÃ¡c tá»‡p!

# Káº¿ hoáº¡ch Sáº¯p tá»›i

(ÄÃ£ hoÃ n thÃ nh) TÃ´i sáº½ báº¯t Ä‘áº§u lÃ m viá»‡c trÃªn phiÃªn báº£n 0.5, thay vÃ¬ huáº¥n luyá»‡n vá»›i 50 quyá»ƒn sÃ¡ch, tÃ´i sáº½ huáº¥n luyá»‡n vá»›i lÃ½ tÆ°á»Ÿng lÃ  500-600 quyá»ƒn. Hiá»‡n táº¡i tÃ´i Ä‘ang huáº¥n luyá»‡n nanoGPT báº±ng cÃ¡c cuá»‘n sÃ¡ch tá»« 1800-1850 vÃ  cá»¥ thá»ƒ lÃ  á»Ÿ London. CÃ³ má»™t sá»‘ thÃ¡ch thá»©c nhÆ° Ä‘áº£m báº£o nhá»¯ng cuá»‘n sÃ¡ch tÃ´i tÃ¬m khÃ´ng bá»‹ chá»‰nh sá»­a hoáº·c cÃ³ giáº£i thÃ­ch hiá»‡n Ä‘áº¡i mÃ  lÃ  sÃ¡ch gá»‘c xuáº¥t báº£n trong thá»i gian tÃ´i chá»n.

TÃ´i muá»‘n huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh má»›i (v1) vá»›i táº­p dá»¯ liá»‡u lá»›n hÆ¡n nhiá»u, cÃ³ thá»ƒ gáº¥p 5-10 láº§n so vá»›i v0.5. Má»¥c tiÃªu cá»§a tÃ´i lÃ  xem liá»‡u cÃ³ thá»ƒ khiáº¿n kháº£ nÄƒng láº­p luáº­n xuáº¥t hiá»‡n chá»‰ tá»« Huáº¥n luyá»‡n Thá»i gian Chá»n lá»c hay khÃ´ng, Ä‘Ã¢y sáº½ lÃ  má»™t nhiá»‡m vá»¥ khÃ³ khÄƒn hÆ¡n vÃ  tÃ´i cÅ©ng khÃ´ng cháº¯c liá»‡u cÃ³ kháº£ thi khÃ´ng vÃ¬ cÃ³ giá»›i háº¡n vá» dá»¯ liá»‡u lá»‹ch sá»­. Trong vÃ i tuáº§n tá»›i tÃ´i sáº½ cá»‘ gáº¯ng thu tháº­p Ä‘á»§ dá»¯ liá»‡u cho má»™t táº­p 5-10GB. TÃ´i tin ráº±ng náº¿u cÃ³ dá»¯ liá»‡u sáº¡ch, cháº¥t lÆ°á»£ng cao vÃ  thuÃª Ä‘Æ°á»£c GPU, sáº½ cÃ³ tiáº¿n triá»ƒn.

# CÃ¡ch sá»­ dá»¥ng dá»± Ã¡n nÃ y

Dá»± Ã¡n nÃ y chá»§ yáº¿u táº­p trung vÃ o viá»‡c thu tháº­p dá»¯ liá»‡u lá»‹ch sá»­, chuáº©n bá»‹ cho huáº¥n luyá»‡n vÃ  xÃ¢y dá»±ng tokenizer. TÃ´i sáº½ khÃ´ng hÆ°á»›ng dáº«n toÃ n bá»™ quÃ¡ trÃ¬nh huáº¥n luyá»‡n LLM, cho viá»‡c Ä‘Ã³ báº¡n hÃ£y tham kháº£o nanoGPT cá»§a Andrej Karpathy.

# BÆ°á»›c 1: Thu tháº­p vÃ  Chuáº©n bá»‹ VÄƒn báº£n Lá»‹ch sá»­

Thu tháº­p cÃ¡c tá»‡p .txt cá»§a sÃ¡ch, tÃ i liá»‡u, v.v. thuá»™c pháº¡m vi cÃ´ng cá»™ng tá»« thá»i ká»³ báº¡n chá»n (vÃ­ dá»¥, London 1800-1850)

Báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng download_texts_improved.py Ä‘á»ƒ táº£i sÃ¡ch náº¿u cáº§n.

LÃ m sáº¡ch cÃ¡c tá»‡p vÄƒn báº£n báº±ng script hoáº·c xÃ³a thá»§ cÃ´ng header/footer cá»§a Project Gutenberg, chÃº thÃ­ch hiá»‡n Ä‘áº¡i hay cÃ¡c lá»—i OCR.

prepare_dataset.py nÃªn hoáº¡t Ä‘á»™ng tá»‘t.

# BÆ°á»›c 2: XÃ¢y dá»±ng Tokenizer TÃ¹y chá»‰nh

Cháº¡y train_tokenizer.py hoáº·c train_tokenizer_hf.py trÃªn dá»¯ liá»‡u Ä‘Ã£ lÃ m sáº¡ch.
Báº¡n sáº½ nháº­n Ä‘Æ°á»£c vocab.json vÃ  merges.txt

CÃ¡c tá»‡p nÃ y Ä‘á»‹nh nghÄ©a tá»« vá»±ng vÃ  quy táº¯c gá»™p tá»« cho mÃ´ hÃ¬nh cá»§a báº¡n

# BÆ°á»›c 3: Huáº¥n luyá»‡n MÃ´ hÃ¬nh (nanoGPT)

Tham kháº£o [nanoGPT cá»§a Andrej Karpathy](https://github.com/karpathy/nanoGPT) cho quy trÃ¬nh huáº¥n luyá»‡n.

Báº¡n cÃ³ thá»ƒ huáº¥n luyá»‡n má»™t LLM khÃ¡c náº¿u muá»‘n, nhÆ°ng tÃ´i dÃ¹ng nanoGPT

# CÃ¢u há»i thÆ°á»ng gáº·p

## Huáº¥n luyá»‡n Thá»i gian Chá»n lá»c lÃ  gÃ¬?

Huáº¥n luyá»‡n Thá»i gian Chá»n lá»c (Selective Temporal Training - STT) lÃ  má»™t phÆ°Æ¡ng phÃ¡p há»c mÃ¡y mÃ  táº¥t cáº£ dá»¯ liá»‡u huáº¥n luyá»‡n Ä‘Æ°á»£c chá»n lá»c Ä‘á»ƒ náº±m hoÃ n toÃ n trong má»™t giai Ä‘oáº¡n lá»‹ch sá»­ nháº¥t Ä‘á»‹nh. Má»¥c Ä‘Ã­ch lÃ  Ä‘á»ƒ mÃ´ phá»ng ngÃ´n ngá»¯ vÃ  tri thá»©c cá»§a thá»i ká»³ Ä‘Ã³ mÃ  khÃ´ng bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi cÃ¡c khÃ¡i niá»‡m hiá»‡n Ä‘áº¡i. VÃ­ dá»¥, mÃ´ hÃ¬nh hiá»‡n táº¡i cá»§a tÃ´i (v0.5) Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»™c quyá»n trÃªn dá»¯ liá»‡u tá»« 1800-1875, khÃ´ng tinh chá»‰nh mÃ  huáº¥n luyá»‡n tá»« Ä‘áº§u, cho ra Ä‘áº§u ra pháº£n Ã¡nh phong cÃ¡ch ngÃ´n ngá»¯ vÃ  bá»‘i cáº£nh lá»‹ch sá»­ cá»§a thá»i ká»³ Ä‘Ã³.

## Táº¡i sao khÃ´ng chá»‰ tinh chá»‰nh hoáº·c dÃ¹ng LoRA?

Vá»›i dá»± Ã¡n nÃ y tÃ´i muá»‘n táº¡o má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ khÃ´ng bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi thiÃªn kiáº¿n hiá»‡n Ä‘áº¡i. Náº¿u tÃ´i tinh chá»‰nh nhÆ° GPT-2, nÃ³ Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c vÃ  thÃ´ng tin Ä‘Ã³ sáº½ khÃ´ng máº¥t Ä‘i. Náº¿u huáº¥n luyá»‡n tá»« Ä‘áº§u, mÃ´ hÃ¬nh sáº½ khÃ´ng giáº£ vá» cá»• xÆ°a, mÃ  nÃ³ thá»±c sá»± nhÆ° tháº¿. Má»¥c tiÃªu hiá»‡n táº¡i lÃ  táº¡o ra má»™t mÃ´ hÃ¬nh cÃ³ thá»ƒ láº­p luáº­n chá»‰ báº±ng kiáº¿n thá»©c tá»« sÃ¡ch London xuáº¥t báº£n giai Ä‘oáº¡n 1800-1850.

## Báº¡n Ä‘Ã£ dÃ¹ng loáº¡i dá»¯ liá»‡u nÃ o Ä‘á»ƒ huáº¥n luyá»‡n?

TÃ´i sá»­ dá»¥ng sÃ¡ch, tÃ i liá»‡u phÃ¡p lÃ½, bÃ¡o chÃ­ vÃ  cÃ¡c tÃ¡c pháº©m khÃ¡c tá»« London 1800â€“1850. Danh sÃ¡ch tÃ´i Ä‘Ã£ liÃªn káº¿t cÃ³ khoáº£ng 200 tÃ i liá»‡u nhÆ°ng láº§n huáº¥n luyá»‡n Ä‘áº§u tÃ´i chá»‰ dÃ¹ng 50 tá»‡p, khoáº£ng ~187 MB. Báº¡n cÃ³ thá»ƒ xem danh sÃ¡ch tÃ i liá»‡u táº¡i:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## MÃ´ hÃ¬nh PhiÃªn báº£n 0 lá»›n cá»¡ nÃ o?

MÃ´ hÃ¬nh nÃ y hiá»‡n ráº¥t nhá», tÃ´i chá»‰ lÃ m cho vui vÃ  tuÃ¢n thá»§ nghiÃªm ngáº·t quy táº¯c khÃ´ng dÃ¹ng nguá»“n hiá»‡n Ä‘áº¡i. NÃ³ cÃ³ gáº§n 16 triá»‡u tham sá»‘ nhÆ°ng tÃ´i sáº½ báº¯t Ä‘áº§u thu tháº­p thÃªm tÃ i liá»‡u cÅ© Ä‘á»ƒ huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh má»›i. Sáº½ cáº­p nháº­t thÃ´ng tin sau.

## ThÃ´ng sá»‘ huáº¥n luyá»‡n?

GPU: Geforce rtx 4060
CPU: i5-13400F
Ram: 16GB DDR5.


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-08-02

---