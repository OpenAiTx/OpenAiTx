
<div align="right">
  <details>
    <summary >üåê Lingua</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="#" title="Coming soon">ÁπÅÈ´î‰∏≠Êñá (presto disponibile)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="#" title="Coming soon">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä (presto disponibile)</a> |
        | <a href="#" title="Coming soon">‡πÑ‡∏ó‡∏¢ (presto disponibile)</a> |
        | <a href="#" title="Coming soon">Fran√ßais (presto disponibile)</a>
        | <a href="#" title="Coming soon">Deutsch (presto disponibile)</a>
        | <a href="#" title="Coming soon">Espa√±ol (presto disponibile)</a>
        | <a href="#" title="Coming soon">Italiano (presto disponibile)</a>
        | <a href="#" title="Coming soon">–†—É—Å—Å–∫–∏–π (presto disponibile)</a>
        | <a href="#" title="Coming soon">Portugu√™s (presto disponibile)</a>
        | <a href="#" title="Coming soon">Nederlands (presto disponibile)</a>
        | <a href="#" title="Coming soon">Polski (presto disponibile)</a>
        | <a href="#" title="Coming soon">ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (presto disponibile)</a>
        | <a href="#" title="Coming soon">ŸÅÿßÿ±ÿ≥€å (presto disponibile)</a>
        | <a href="#" title="Coming soon">T√ºrk√ße (presto disponibile)</a>
        | <a href="#" title="Coming soon">Ti·∫øng Vi·ªát (presto disponibile)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (presto disponibile)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM
Un LLM addestrato solo su dati di determinati periodi storici per ridurre i bias moderni.

Immagina se un modello AI non si limitasse a fingere di essere storico, ma lo fosse davvero.

Basato su [nanoGPT di Andrej Karpathy](https://github.com/karpathy/nanoGPT). Gli script di training principali e l‚Äôarchitettura del modello sono opera sua.

# Obiettivi del Progetto

TimeCapsule LLM √® un progetto sperimentale che verr√† addestrato solo su testi scritti in determinati periodi storici. L‚Äôobiettivo √® simulare la visione del mondo e il linguaggio di specifiche epoche storiche.

# Perch√© il fine-tuning non basta

Se si esegue solo un fine-tuning su un modello pre-addestrato, il vostro LLM conoscer√† comunque concetti moderni. Ovviamente raggiungere un bias moderno pari a zero √® difficile, ma voglio avvicinarmi il pi√π possibile a questo obiettivo. Ottenere l‚Äôassenza di bias moderni richiede un addestramento del modello da zero.

# Risultati attesi

Spero che, una volta completato, questo modello non conosca concetti moderni e non sia in grado di ragionare oltre ci√≤ su cui √® stato addestrato. Non dovrebbe riconoscere vocaboli/conoscenze moderne e spero che non ‚Äúallucini‚Äù conoscenze moderne.

# Aggiornamenti sui progressi

## 9 luglio 2025

Ho fissato il mio periodo di riferimento tra il 1800 e il 1850 e la regione: Londra

Ho raccolto un elenco di testi, libri, documenti

Finora ne ho ottenuti 50 in formato txt e inizier√≤ presto il training di NanoGPT

Aggiorner√≤ questa sezione man mano che progredisco

## 13 luglio 2025

Ho addestrato nanoGPT con 187MB di testi storici.

## 15 luglio 2025

Ho iniziato a scaricare testi per la seconda sessione di training. Sto prendendo tutto da Internet Archive e ho ampliato il periodo a 1800-1875. Per ottenere una gamma diversificata di testi, puoi usare i filtri per soggetto e ricerca per luogo di pubblicazione, periodo e argomenti su Internet Archive.

![Filtri di Ricerca](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## 16 luglio 2025

Ho scaricato circa 500 file txt da Internet Archive e dopo averli ripuliti (eliminando solo spazi bianchi, header Gutenberg, ecc.) ho circa 500MB di dati. √à un dataset piccolo ma l‚Äôultima volta ho addestrato su 187MB quindi dovrei vedere almeno qualche differenza nell‚Äôoutput dopo aver addestrato il secondo modello. Spero che questo modello riesca almeno a produrre frasi pi√π coerenti che abbiano un senso. Ovviamente non √® garantito dato che il dataset √® ancora minuscolo, ma √® pi√π grande rispetto all‚Äôultima volta.

Questo dovrebbe essere fattibile sul mio hardware personale, il che √® positivo perch√© posso vedere qualche miglioramento prima di passare a un dataset pi√π grande che richiederebbe il noleggio di una GPU. Ma non preoccuparti, ho comunque intenzione di noleggiare presto una GPU, ma prima voglio assicurarmi che il mio dataset sia il pi√π possibile curato e pulito. Uno dei problemi √® la pulizia; molti di questi file txt hanno dentro del ‚Äúgibberish‚Äù. Gli script che ho usato per la pulizia funzionano, ma non sono efficaci al 100%.

Allener√≤ questo dataset oggi e dovrebbe richiedere circa 4-5 ore. Una volta terminato e testato, fornir√≤ aggiornamenti. Grazie ancora a tutti coloro che stanno seguendo il mio progetto, alcune persone mi hanno anche fornito link a risorse OCR quindi grazie! Spero che pi√π persone provino a sperimentare con i propri dataset.

## 28 luglio 2025

Ho caricato la versione v0.5 su Hugging Face, [Dai un‚Äôocchiata](https://huggingface.co/haykgrigorian/TimeCapsuleLLM) se vuoi. Ora puoi scaricare il mio repository ed eseguirlo localmente. Purtroppo nanoGPT non funziona nativamente con HuggingFace, quindi dovrai scaricare ed eseguire il modello in locale.

Inoltre inizier√≤ a curare i dati per la prossima sessione di training, credo mi serviranno forse 5-10 volte pi√π dati per raggiungere capacit√† di ragionamento.

### Aggiornamento Training

Ho iniziato il training su un corpus di 435MB (108 M di token), al momento sta andando piuttosto bene. La train loss √® scesa da 10.9 a 4.9 nelle prime 2800 iterazioni. Penso che ci vorranno circa 8 o 9 ore per completarlo. Pubblicher√≤ un altro aggiornamento quando sar√† finito.

## 17 luglio 2025, ore 2:13

Il training √® terminato per il secondo modello, la mia 4060 ha impiegato circa 8 ore e 40 minuti (3.900 iter/ora) per 33.000 iterazioni (5 epoche). La train loss finale √® stata 3.73. Gli output erano sorprendentemente buoni: ora genera frasi coerenti in stile XIX secolo.

# Comportamento & Limitazioni del Modello V0

I primi prompt mostrano che il modello risponde con linguaggio e comportamento da anni 1800. Ad esempio, ho chiesto "Who art Henry?" e ha risposto "I know that man, I have did not a black, the storm." e s√¨, quella frase non ha senso ma l‚ÄôLLM sta riconoscendo che sto chiedendo di una persona.

![TimeLockLLM Esempio Output](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)
Non c'√® alcun riferimento a concetti moderni, gli output contengono principalmente parole ed espressioni dell‚ÄôOttocento.

C'√® ancora molto lavoro da fare, addestrare su 187MB non ti dar√† un modello che produca testo con ragionamento complesso.

Al momento genera frasi che mancano di una struttura completa e in generale non hanno senso, ma questo √® normale per la dimensione del training.

# Comportamento e Limitazioni del Modello V0.5

Questo √® un bel miglioramento rispetto al modello precedente. Lo stile di scrittura e il vocabolario sono vittoriani e quasi tutte le frasi sono grammaticalmente corrette con la punteggiatura giusta. E anche questo √® addestrato da zero quindi si attiene ai temi dell‚ÄôOttocento.

![Esempio di Output TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

Ci sono molte allucinazioni fattuali. Molti (tipo il 100%) dei dettagli (date, eventi, personaggi storici) sono inventati. Inoltre le frasi non hanno davvero collegamento tra loro, a volte forse 2 frasi sono correlate ma oltre non lo sono. Un altro problema √® che a volte compare un footer ‚ÄúDigitized by Google‚Äù, quindi la prossima volta dovr√≤ assicurarmi di pulire bene i testi. Nel complesso sono molto contento dei risultati, non √® ancora un LLM ma sicuramente √® un generatore di frasi.

Sto imparando molto e nelle prossime settimane cercher√≤ di capire cosa devo migliorare. Caricher√≤ presto i file!

# Piani Futuri

(Completato) Inizier√≤ a lavorare sulla versione 0.5, invece di addestrare usando 50 libri, user√≤ idealmente 500-600. Al momento sto addestrando nanoGPT usando libri dal 1800 al 1850 e specificatamente da Londra. Ci sono delle sfide, come assicurarsi che i libri trovati non siano aggiornati o abbiano interpretazioni moderne ma siano libri intatti pubblicati nel periodo scelto.

Voglio addestrare un nuovo modello (v1) con un corpus molto pi√π grande, magari 5-10 volte pi√π grande di quello usato per la v0.5. Il mio obiettivo √® vedere se riesco a far emergere capacit√† di ragionamento dal Solo Addestramento Temporale Selettivo, sar√† un compito pi√π difficile e non sono nemmeno sicuro che sia possibile a causa dei limiti dei dati storici. Nelle prossime settimane cercher√≤ di curare abbastanza dati per un corpus di 5-10GB. Credo che se riuscir√≤ a ottenere dati puliti di alta qualit√† e noleggiare una GPU, ci saranno progressi.

# Come Usare Questo Progetto

Questo progetto si concentra principalmente sulla raccolta di dati storici, la loro preparazione per l‚Äôaddestramento e la costruzione di un tokenizer. Non tratter√≤ l‚Äôintero processo di addestramento LLM, per quello fai riferimento a nanoGPT di Andrej Karpathy.

# Passo 1: Raccogli e Prepara Testi Storici

Raccogli file .txt di libri di pubblico dominio, documenti, ecc dal periodo scelto (es: Londra 1800-1850)

Puoi usare download_texts_improved.py per scaricare libri se ne hai bisogno.

Pulisci i file di testo usando uno script o rimuovi manualmente intestazioni/footer di Project Gutenberg, annotazioni moderne o errori OCR.

prepare_dataset.py dovrebbe funzionare bene.

# Passo 2: Costruisci un Tokenizer Personalizzato

Esegui train_tokenizer.py o train_tokenizer_hf.py sui dati puliti.
Questo ti dar√† vocab.json e merges.txt

Questi file definiscono il vocabolario e le regole di unione per il tuo modello

# Passo 3: Addestra il Tuo Modello (nanoGPT)

Fai riferimento a [nanoGPT di Andrej Karpathy](https://github.com/karpathy/nanoGPT) per il processo di addestramento.

Puoi addestrare un altro LLM se vuoi, ma io ho usato nanoGPT

# FAQ

## Cos‚Äô√® il Selective Temporal Training?

Il Selective Temporal Training (STT) √® una metodologia di machine learning in cui tutti i dati di addestramento sono curati specificamente per rientrare in un determinato periodo storico. Si fa per modellare il linguaggio e la conoscenza di quell‚Äôepoca senza l‚Äôinfluenza di concetti moderni. Ad esempio, il modello che ho ora (v0.5) √® addestrato esclusivamente su dati dal 1800 al 1875, non √® fine tuned ma addestrato da zero, producendo output che riflettono lo stile linguistico e il contesto storico di quel periodo.

## Perch√© non usare solo fine-tuning o LoRA?

Per questo progetto sto cercando di creare un modello linguistico non influenzato da bias moderni. Se faccio fine-tuning su qualcosa come GPT-2, √® gi√† pre-addestrato e quell‚Äôinformazione non andr√† via. Se addestro da zero il modello non finger√† di essere antico, lo sar√† davvero. L‚Äôobiettivo per ora √® creare qualcosa che possa ragionare esclusivamente usando conoscenze tratte da libri londinesi pubblicati tra il 1800 e il 1850.

## Che tipo di dati hai usato per l‚Äôaddestramento?

Uso libri, documenti legali, giornali e altri scritti della Londra 1800‚Äì1850. La lista che ho linkato ne ha circa 200 ma per il primo addestramento ho usato solo 50 file per circa ~187 MB. Puoi vedere una lista dei documenti:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## Quanto √® grande la Versione 0 del modello?

Questo modello √® molto piccolo al momento, lo sto facendo solo per divertimento e seguendo una regola ferrea di nessuna fonte moderna. Ha quasi 16 milioni di parametri ma inizier√≤ a raccogliere altri testi antichi per iniziare un nuovo addestramento. Dar√≤ aggiornamenti strada facendo.

## Specifiche dell‚Äôaddestramento?

GPU: Geforce rtx 4060
CPU: i5-13400F 
Ram: 16GB DDR5.



---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-29

---