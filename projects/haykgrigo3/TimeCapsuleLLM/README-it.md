<div align="right">
  <details>
    <summary >üåê Lingua</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="#" title="Coming soon">ÁπÅÈ´î‰∏≠Êñá (coming soon)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="#" title="Coming soon">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä (coming soon)</a> |
        | <a href="#" title="Coming soon">‡πÑ‡∏ó‡∏¢ (coming soon)</a> |
        | <a href="#" title="Coming soon">Fran√ßais (coming soon)</a>
        | <a href="#" title="Coming soon">Deutsch (coming soon)</a>
        | <a href="#" title="Coming soon">Espa√±ol (coming soon)</a>
        | <a href="#" title="Coming soon">Italiano (coming soon)</a>
        | <a href="#" title="Coming soon">–†—É—Å—Å–∫–∏–π (coming soon)</a>
        | <a href="#" title="Coming soon">Portugu√™s (coming soon)</a>
        | <a href="#" title="Coming soon">Nederlands (coming soon)</a>
        | <a href="#" title="Coming soon">Polski (coming soon)</a>
        | <a href="#" title="Coming soon">ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (coming soon)</a>
        | <a href="#" title="Coming soon">ŸÅÿßÿ±ÿ≥€å (coming soon)</a>
        | <a href="#" title="Coming soon">T√ºrk√ße (coming soon)</a>
        | <a href="#" title="Coming soon">Ti·∫øng Vi·ªát (coming soon)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (coming soon)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM
Un LLM addestrato solo su dati di determinati periodi per ridurre i bias moderni.

Immagina se un modello AI non si limitasse a fingere di essere storico ma lo fosse davvero.

Basato su [nanoGPT di Andrej Karpathy](https://github.com/karpathy/nanoGPT). Gli script di addestramento principali e l'architettura del modello sono opera sua.

# Obiettivi del Progetto

TimeCapsule LLM √® un progetto sperimentale che verr√† addestrato solo su testi scritti in determinati periodi storici. L'obiettivo √® simulare la visione del mondo e il linguaggio di specifiche epoche storiche.

# Perch√© il fine tuning non basta

Se esegui solo un fine tuning su un modello pre-addestrato, il tuo LLM conoscer√† comunque concetti moderni. Ovviamente ottenere zero bias moderno √® difficile, ma voglio avvicinarmi il pi√π possibile a questo risultato. Per non avere bias moderni √® necessario addestrare un modello da zero.

# Risultati attesi

Si spera che, una volta terminato, questo modello non conosca concetti moderni e non sia in grado di ragionare oltre ci√≤ su cui √® stato addestrato. Non dovrebbe riconoscere concetti o vocabolario moderni e spero che non "allucini" conoscenze moderne.

# Aggiornamenti sui Progressi

## 9 luglio 2025

Ho fissato il mio periodo di interesse: 1800-1850 e regione: Londra

Ho raccolto un elenco di testi, libri, documenti

Finora ne ho ottenuti 50 in formato txt e inizier√≤ presto l‚Äôaddestramento di NanoGPT

Aggiorner√≤ questa sezione man mano che ci saranno progressi

## 13 luglio 2025

Ho addestrato nanoGPT con 187MB di dati testuali storici.

## 15 luglio 2025

Ho iniziato a scaricare testi per la seconda sessione di addestramento. Sto prendendo tutto da Internet Archive e ho ampliato il periodo a 1800-1875. Per ottenere una gamma diversificata di testi, puoi usare i filtri per soggetto e ricerca relativi al luogo di pubblicazione, periodo e argomenti su Internet Archive.

![Filtri di Ricerca](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## 16 luglio 2025

Ho scaricato circa 500 file txt da Internet Archive e dopo averli ripuliti (eliminando spazi bianchi, intestazioni Gutenberg, ecc.) ho circa 500MB di dati. √à un dataset piccolo ma la volta scorsa ho addestrato con 187MB quindi dovrei vedere almeno qualche differenza nell‚Äôoutput dopo aver addestrato il secondo modello. Spero che questo modello riesca almeno a produrre frasi pi√π coerenti che abbiano un certo senso. Ovviamente non √® garantito visto che il dataset √® ancora molto piccolo, ma √® pi√π di quanto usato la volta scorsa.

Dovrebbe essere fattibile con il mio hardware, ed √® anche positivo perch√© spero di vedere qualche miglioramento prima di passare a un dataset pi√π grande che richiederebbe di affittare una GPU. Ma non preoccupatevi, ho comunque intenzione di noleggiare una GPU a breve, ma prima voglio assicurarmi che il mio dataset sia il pi√π curato e pulito possibile. Uno dei problemi che ho √® la pulizia, molti di questi file txt contengono del testo insensato. Gli script che ho usato funzionano ma non sono efficaci al 100%.

Allener√≤ questo dataset oggi e dovrebbe richiedere circa 4-5 ore. Una volta completato e testato, fornir√≤ aggiornamenti. Grazie ancora a tutti quelli che stanno seguendo il mio progetto, alcune persone mi hanno anche segnalato risorse OCR quindi grazie! Spero che altri provino a fare esperimenti con i propri dataset.


### Aggiornamento Addestramento

Ho iniziato l‚Äôaddestramento su un corpus di 435MB (108 milioni di token), sta andando abbastanza bene al momento. La train loss √® scesa da 10.9 a 4.9 nelle prime 2800 iterazioni. Credo ci vorranno circa 8 o 9 ore per completare. Pubblicher√≤ un altro aggiornamento una volta terminato.

## 17 luglio 2025 2:13AM

L‚Äôaddestramento √® terminato per il secondo modello, la mia 4060 ha impiegato circa 8 ore e 40 minuti (3.900 iter/ora) per 33.000 iterazioni (5 epoche). La train loss finale √® stata 3.73. Gli output sono stati sorprendentemente buoni: ora genera davvero frasi in stile XIX secolo in modo coerente.

## 28 luglio 2025

Ho caricato la versione 0.5 su Hugging Face, [Dai un‚Äôocchiata](https://huggingface.co/haykgrigorian/TimeCapsuleLLM) se vuoi. Ora puoi scaricare la mia repo ed eseguirla localmente. Sfortunatamente nanoGPT non funziona nativamente con HuggingFace, quindi dovrai scaricare ed eseguire il modello localmente.

Inoltre inizier√≤ a curare i dati per la prossima sessione di addestramento. Credo che avr√≤ bisogno di almeno 5-10 volte pi√π dati per ottenere capacit√† di ragionamento.


# Comportamento e Limiti del Modello V0

I primi prompt mostrano che il modello risponde con linguaggio e comportamento da 1800. Ad esempio, l‚Äôho interrogato con ‚ÄúWho art Henry?‚Äù e ha risposto ‚ÄúI know that man, I have did not a black, the storm.‚Äù e s√¨, quella frase non ha senso ma l‚ÄôLLM riconosce che sto chiedendo di una persona.


![Esempio di Output TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

Non c'√® alcun riferimento a concetti moderni, gli output contengono per lo pi√π parole e formulazioni tipiche del 1800.

C'√® ancora molto lavoro da fare, addestrarsi su 187MB non permette di ottenere un modello capace di produrre testo con ragionamento complesso.

Attualmente produce frasi che mancano di una struttura completa e in generale non hanno senso, ma ci√≤ √® normale per la dimensione del dataset di addestramento.

# Comportamento e Limitazioni del Modello V0.5

Questo rappresenta un notevole miglioramento rispetto al modello precedente. Lo stile di scrittura e il vocabolario sono vittoriani e quasi ogni frase √® grammaticalmente corretta, con la punteggiatura appropriata. E di nuovo, essendo stato addestrato da zero, resta fedele agli argomenti dell‚Äô800.

![Esempio di Output TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

Sono presenti molte allucinazioni fattuali. Molti (come il 100%) dei dettagli (date, eventi, personaggi storici) sono inventati. Inoltre, le frasi spesso non sono collegate tra loro: a volte forse 2 frasi hanno relazione, ma oltre a ci√≤ non lo sono. Un altro problema √® che talvolta compare un footer errante ‚ÄúDigitized by Google‚Äù, quindi la prossima volta che addestro dovr√≤ davvero assicurarmi che i testi siano ben puliti. In generale, sono molto soddisfatto dei risultati: non √® ancora lontanamente un LLM ma sicuramente √® un generatore di frasi.

Sto imparando molto e presto cercher√≤ di capire cosa devo migliorare nelle prossime settimane. Caricher√≤ i file a breve!

# Piani Futuri

(Completato) Inizier√≤ a lavorare sulla versione 0.5; invece di usare 50 libri per l‚Äôaddestramento, ne user√≤ idealmente 500-600. Attualmente sto addestrando nanoGPT usando libri dal 1800-1850 e specificamente da Londra. Ci sono alcune sfide, come assicurarsi che i libri trovati non siano stati aggiornati o abbiano interpretazioni moderne, ma siano libri intatti pubblicati nel periodo scelto.

Voglio addestrare un nuovo modello (v1) con un corpus molto pi√π grande, magari 5-10 volte pi√π grande di quello usato per v0.5. Il mio obiettivo √® vedere se √® possibile far emergere capacit√† di ragionamento solo dal Selective Temporal Training, sar√† un compito pi√π difficile e non sono nemmeno sicuro che sia possibile a causa delle limitazioni dei dati storici. Nelle prossime settimane cercher√≤ di curare abbastanza dati per un corpus da 5-10GB. Credo che se riuscir√≤ a ottenere dati puliti e di alta qualit√† e affittare una GPU, ci saranno progressi.

# Come Utilizzare Questo Progetto

Questo progetto si concentra principalmente sulla raccolta di dati storici, sulla loro preparazione per l‚Äôaddestramento e sulla costruzione di un tokenizer. Non tratter√≤ l‚Äôintero processo di addestramento LLM; per quello, fare riferimento a nanoGPT di Andrej Karpathy.

# Passo 1: Raccogliere e Preparare Testi Storici

Raccogli file .txt di libri, documenti, ecc. di pubblico dominio dal periodo scelto (es: Londra 1800-1850)

Puoi usare download_texts_improved.py per scaricare i libri, se necessario.

Pulisci i file di testo usando uno script o rimuovi manualmente header/footer di Project Gutenberg, annotazioni moderne o errori OCR.

prepare_dataset.py dovrebbe funzionare correttamente.

# Passo 2: Costruisci un Tokenizer Personalizzato

Esegui train_tokenizer.py o train_tokenizer_hf.py sui dati puliti.
Questo ti dar√† vocab.json e merges.txt

Questi file definiscono il vocabolario e le regole di unione per il tuo modello

# Passo 3: Addestra il Tuo Modello (nanoGPT)

Fare riferimento a [nanoGPT di Andrej Karpathy](https://github.com/karpathy/nanoGPT) per il processo di addestramento.

Puoi addestrare un LLM diverso se vuoi, ma io ho usato nanoGPT

# FAQ

## Cos'√® il Selective Temporal Training?

Il Selective Temporal Training (STT) √® una metodologia di machine learning in cui tutti i dati di addestramento sono specificamente curati per ricadere in un determinato periodo storico. Serve a modellare il linguaggio e le conoscenze di quell‚Äôepoca senza influenze da concetti moderni. Per esempio, il modello attuale (v0.5) √® addestrato esclusivamente su dati dal 1800 al 1875, non √® fine-tuned ma addestrato da zero, producendo output che riflette lo stile linguistico e il contesto storico di quel periodo.

## Perch√© non usare semplicemente il fine-tuning o LoRA?

Per questo progetto sto cercando di creare un modello linguistico privo di bias moderni. Se faccio il fine-tuning di qualcosa come GPT-2, √® gi√† pre-addestrato e quell‚Äôinformazione non scompare. Se invece addestro da zero, il modello linguistico non finger√† di essere antico, lo sar√† davvero. L'obiettivo per ora √® creare qualcosa che possa ragionare esclusivamente usando conoscenze tratte da libri londinesi pubblicati tra il 1800 e il 1850.

## Che tipo di dati hai usato per l‚Äôaddestramento?

Sto usando libri, documenti legali, giornali e altri scritti da Londra 1800‚Äì1850. La lista che ho linkato ne contiene circa 200, ma per il primo addestramento ho usato solo 50 file per circa 187 MB. Puoi consultare la lista dei documenti:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## Quanto √® grande il modello Versione 0?

Questo modello √® molto piccolo al momento, lo sto facendo solo per divertimento e seguo una regola rigorosa di non includere fonti moderne. Ha quasi 16 milioni di parametri ma inizier√≤ a raccogliere altri testi antichi per cominciare un nuovo addestramento. Dar√≤ aggiornamenti man mano.

## Specifiche di Addestramento?

GPU: Geforce rtx 4060
CPU: i5-13400F
Ram: 16GB DDR5.


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-08-02

---