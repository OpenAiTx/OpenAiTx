
<div align="right">
  <details>
    <summary >üåê Lingua</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="#" title="Coming soon">ÁπÅÈ´î‰∏≠Êñá (presto disponibile)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="#" title="Coming soon">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä (presto disponibile)</a> |
        | <a href="#" title="Coming soon">‡πÑ‡∏ó‡∏¢ (presto disponibile)</a> |
        | <a href="#" title="Coming soon">Fran√ßais (presto disponibile)</a>
        | <a href="#" title="Coming soon">Deutsch (presto disponibile)</a>
        | <a href="#" title="Coming soon">Espa√±ol (presto disponibile)</a>
        | <a href="#" title="Coming soon">Italiano (presto disponibile)</a>
        | <a href="#" title="Coming soon">–†—É—Å—Å–∫–∏–π (presto disponibile)</a>
        | <a href="#" title="Coming soon">Portugu√™s (presto disponibile)</a>
        | <a href="#" title="Coming soon">Nederlands (presto disponibile)</a>
        | <a href="#" title="Coming soon">Polski (presto disponibile)</a>
        | <a href="#" title="Coming soon">ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (presto disponibile)</a>
        | <a href="#" title="Coming soon">ŸÅÿßÿ±ÿ≥€å (presto disponibile)</a>
        | <a href="#" title="Coming soon">T√ºrk√ße (presto disponibile)</a>
        | <a href="#" title="Coming soon">Ti·∫øng Vi·ªát (presto disponibile)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (presto disponibile)</a>

      </div>
    </div>
  </details>
</div>

# TimeCapsule LLM
Un LLM addestrato solo su dati provenienti da determinati periodi storici per ridurre il bias moderno.

Immagina se un modello di IA non solo fingesse di essere storico, ma lo fosse davvero.

Basato su [nanoGPT di Andrej Karpathy](https://github.com/karpathy/nanoGPT) Gli script principali di addestramento e l‚Äôarchitettura del modello sono opera sua.

# Obiettivi del Progetto

TimeCapsule LLM √® un progetto sperimentale che verr√† addestrato solo su testi scritti durante determinati periodi storici. L'obiettivo √® simulare la visione del mondo e il linguaggio di specifiche epoche storiche.

# Perch√© il fine tuning non √® sufficiente

Se si effettua solo il fine tuning di un modello preaddestrato, il tuo LLM conoscer√† comunque concetti moderni. Ovviamente ottenere una totale assenza di bias moderno √® difficile, ma voglio avvicinarmi il pi√π possibile a questo. Per non avere bias moderno √® necessario addestrare un modello da zero.

# Risultati attesi

Spero che, una volta terminato, questo modello non conosca concetti moderni e non sia in grado di ragionare oltre ci√≤ su cui √® stato addestrato. Non dovrebbe riconoscere vocaboli/conoscenze moderne e non dovrebbe "allucinare" informazioni contemporanee.

# Aggiornamenti sui progressi

## 9 luglio 2025

Ho fissato il mio periodo storico tra il 1800 e il 1850 e la regione: Londra

Ho raccolto una lista di testi, libri, documenti

Finora ne ho ottenuti 50 in formato txt e inizier√≤ presto l‚Äôaddestramento con NanoGPT

Aggiorner√≤ questa sezione man mano che ci saranno progressi

## 13 luglio 2025

Ho addestrato NanoGPT con 187MB di dati testuali storici.

## 15 luglio 2025

Ho iniziato a scaricare testi per la seconda sessione di addestramento. Sto prendendo tutto dall‚ÄôInternet Archive e ho ampliato il periodo a 1800-1875. Per ottenere una gamma diversificata di testi, si possono usare filtri per soggetto, luogo di pubblicazione, periodo storico e tematiche su Internet Archive.

![Filtri di ricerca](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg)

## 16 luglio 2025

Ho scaricato circa 500 file txt da Internet Archive e, dopo averli puliti (eliminando spazi bianchi, intestazioni Gutenberg, ecc.), ho circa 500MB di dati. √à un dataset piccolo ma l‚Äôultima volta ho addestrato con 187MB, quindi dovrei vedere almeno qualche differenza nel risultato dopo aver addestrato il secondo modello. Spero che questo modello riesca almeno a produrre frasi pi√π coerenti e sensate. Non √® garantito ovviamente, visto che il dataset √® ancora minuscolo, ma √® pi√π di quanto usato in precedenza.

Dovrebbe essere fattibile sull‚Äôhardware che ho, il che √® positivo perch√© spero di vedere qualche miglioramento prima di passare a un dataset pi√π grande, che mi costringerebbe a noleggiare una GPU. Non preoccupatevi, ho comunque intenzione di noleggiare una GPU presto, ma prima voglio assicurarmi che il mio dataset sia il pi√π curato e pulito possibile. Uno dei problemi principali √® la pulizia: molti di questi file txt hanno dentro del testo incomprensibile. Gli script che uso per pulire funzionano, ma non sono efficaci al 100%.

Oggi addestrer√≤ questo dataset e dovrebbe richiedere circa 4-5 ore. Una volta finito e testato, dar√≤ aggiornamenti. Grazie ancora a tutti quelli che stanno seguendo il mio progetto, alcune persone mi hanno persino inviato link a risorse OCR quindi grazie! Spero che pi√π persone provino questo esperimento e ci lavorino con i propri dataset.

## 12 Agosto 2025

La Versione 1 √® completa, questa versione utilizza Phi 1.5 (700M parametri) ed √® stata addestrata su un dataset di quasi 7000 testi. Il principale miglioramento di questo modello √® la capacit√† di riferirsi talvolta a nomi reali ed eventi storici. Le versioni 0 e 0.5 non riuscivano a farlo, quando chiedevo a quei modelli qualcosa come "Nell'anno 1821" il risultato erano solo allucinazioni, cose che non sono mai accadute. Un esempio con la Versione 1:

Il mio prompt: Era l‚Äôanno del nostro Signore 1834,

L‚Äôoutput: "Era l‚Äôanno del nostro Signore 1834 e le strade di Londra erano piene di proteste e petizioni. La causa, come molti raccontarono, non era legata a questioni private, ma era stata adottata lo stesso giorno dal Lord Palmerston; il pubblico ricever√† una breve dichiarazione delle difficolt√† sotto le quali il giorno della legge ci ha raggiunto. √à un motivo di profondo rammarico che gli eventi attuali nella storia del mondo siano chiari, e di conseguenza saranno conosciuti. Non √® vero che proprio gli uomini che si stabilirono per primi nel Vangelo a Gerusalemme abbiano una documentazione cos√¨ ampia e interessante della prosperit√† e prosperit√†"

All‚Äôinizio pensavo fosse una coincidenza, ma guarda qui: ![1834protest](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)


### Aggiornamento Addestramento

Ho iniziato l‚Äôaddestramento su un corpus da 435MB (108 M token), sta andando abbastanza bene. La train loss √® scesa da 10.9 a 4.9 nelle prime 2800 iterazioni. Penso che ci vorranno circa 8 o 9 ore per completare. Pubblicher√≤ un altro aggiornamento una volta terminato.

## 17 Luglio 2025

L‚Äôaddestramento √® terminato per il secondo modello, la mia 4060 ha impiegato circa 8 ore e 40 minuti (3.900 iteraz./ora) per 33.000 iterazioni (5 epoche). La train loss finale era 3.73. Gli output erano sorprendentemente buoni; ora genera frasi coerenti in stile XIX secolo.

## 28 Luglio 2025

Ho caricato v0.5 su Hugging Face, [Dai un‚Äôocchiata](https://huggingface.co/haykgrigorian/TimeCapsuleLLM) se vuoi. Ora puoi scaricare il mio repo ed eseguirlo localmente. Purtroppo nanoGPT non funziona nativamente con HuggingFace, quindi dovrai scaricare ed eseguire il modello in locale.

Inoltre inizier√≤ a selezionare dati per la prossima sessione di addestramento, credo che serviranno forse 5-10 volte pi√π dati per ottenere capacit√† di ragionamento.

## 2 Agosto 2025

Presto inizier√≤ a lavorare sulla Versione 1. Dovr√≤ passare dall‚Äôarchitettura nanoGPT a una pi√π moderna. Ho in mente diverse architetture LLM open-source, tra cui: OpenLLaMA v3, Phi-2 e Qwen 1.5B. E per supportare il passaggio a V1, dovr√≤ curare con molta attenzione un dataset molto pi√π grande e diversificato. Mi serviranno almeno 5GB di dati di addestramento puliti.

# Comportamento & Limitazioni del Modello V0

I primi prompt mostrano il modello che risponde con linguaggio e comportamento dell‚ÄôOttocento. Ad esempio, l‚Äôho interrogato con "Who art Henry?" e ha risposto "I know that man, I have did not a black, the storm." e s√¨, quella frase non ha senso ma il LLM sta riconoscendo che sto chiedendo di una persona.

![TimeLockLLM Sample Output](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)

Non c‚Äô√® menzione di concetti moderni, gli output contengono soprattutto parole e frasi dell‚ÄôOttocento.

C‚Äô√® ancora molto lavoro da fare, addestrare su 187MB non ti dar√† un modello che produce testo con ragionamento complesso. 

Al momento produce frasi che mancano di una struttura completa e in generale non hanno senso, ma questo √® normale per la dimensione dell'addestramento.

# Comportamento e Limitazioni del Modello V0.5

Questo √® un bel miglioramento rispetto al modello precedente. Lo stile di scrittura e il vocabolario sono vittoriani e quasi ogni frase √® grammaticalmente corretta con la punteggiatura appropriata. E ancora una volta questo √® stato addestrato da zero, quindi si attiene ai temi dell'Ottocento.

![Esempio di Output TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)

Ci sono molte allucinazioni fattuali. Molti (come il 100%) dei dettagli (date, eventi, personaggi storici) sono inventati. Inoltre le frasi non sono davvero collegate tra loro, a volte forse 2 frasi si riferiscono tra loro ma oltre a questo non lo fanno. Un altro problema √® che a volte compare un footer errante ‚ÄúDigitized by Google‚Äù, quindi la prossima volta che addestro devo davvero assicurarmi che i testi siano ben puliti. Nel complesso sono molto soddisfatto dei risultati, non √® ancora vicino a un LLM ma sicuramente √® un generatore di frasi.

Sto imparando molto e inizier√≤ a capire cosa devo migliorare nelle prossime settimane. Caricher√≤ presto dei file!

# Comportamento e Limitazioni del Modello V1

Caricher√≤ presto alcuni output di esempio e far√≤ anche confronti tra i 3 modelli con lo stesso prompt. Caricher√≤ anche la V1 su huggingface come ho fatto con la mia ultima versione, puoi trovare il mio account huggingface qui: https://huggingface.co/haykgrigorian/TimeCapsuleLLM

# Piani Futuri

(Completato) Inizier√≤ a lavorare sulla versione 0.5, invece di addestrare usando 50 libri, addestrer√≤ usando idealmente 500-600. Attualmente sto addestrando nanoGPT usando libri dal 1800-1850 e specificamente da Londra. Ci sono delle sfide come assicurarsi che i libri trovati non siano aggiornati o abbiano interpretazioni moderne ma siano libri intatti pubblicati nel periodo scelto.

Voglio addestrare un nuovo modello (v1) con un corpus molto pi√π grande, magari 5-10 volte pi√π grande di quello usato per v0.5. Il mio obiettivo √® vedere se posso far emergere capacit√† di ragionamento dal Solo Addestramento Temporale Selettivo, sar√† un compito pi√π difficile e non sono nemmeno sicuro che sia possibile dato che ci sono limitazioni nei dati storici. Nelle prossime settimane cercher√≤ di curare abbastanza dati per un corpus da 5-10GB. Credo che se riuscir√≤ a ottenere dati puliti di alta qualit√† e affittare una GPU, ci saranno progressi.

# Come Usare Questo Progetto

Questo progetto si concentra principalmente sulla cura dei dati storici, sulla loro preparazione per l'addestramento e sulla creazione di un tokenizer. Non coprir√≤ l'intero processo di addestramento LLM, per quello fai riferimento a nanoGPT di Andrej Karpathy.

# Passo 1: Raccogliere e Preparare Testi Storici

Raccogli file .txt di libri di pubblico dominio, documenti, ecc. dal periodo storico scelto (ad esempio, Londra 1800-1850)

Puoi usare download_texts_improved.py per scaricare i libri se ne hai bisogno.

Pulisci i file di testo usando uno script o rimuovi manualmente header/footer da Project Gutenberg, annotazioni moderne o errori OCR.

prepare_dataset.py dovrebbe funzionare bene.

# Passo 2: Costruire un Tokenizer Personalizzato

Esegui train_tokenizer.py o train_tokenizer_hf.py sui dati puliti.
Questo ti dar√† vocab.json e merges.txt

Questi file definiscono il vocabolario e le regole di fusione per il tuo modello

# Passo 3: Allena il Tuo Modello (nanoGPT)

Consulta [nanoGPT di Andrej Karpathy](https://github.com/karpathy/nanoGPT) per il processo di addestramento.

Puoi addestrare un diverso LLM se vuoi, ma io ho usato nanoGPT

# FAQ

## Cos'√® l'Addestramento Temporale Selettivo?

L'Addestramento Temporale Selettivo (STT) √® una metodologia di machine learning in cui tutti i dati di addestramento sono specificamente selezionati per appartenere a un periodo storico definito. Questo viene fatto per modellare la lingua e la conoscenza di quell'epoca senza influenze da concetti moderni. Ad esempio, il modello attuale che ho (v0.5) √® addestrato esclusivamente su dati dal 1800 al 1875, non √® stato ottimizzato ma addestrato da zero, producendo output che riflettono lo stile linguistico e il contesto storico di quel periodo.

## Perch√© non usare semplicemente il fine-tuning o LoRA?

Per questo progetto sto cercando di creare un modello linguistico privo di bias moderni. Se faccio il fine-tuning su qualcosa come GPT-2, √® gi√† pre-addestrato e quell'informazione non scomparir√†. Se invece addestro da zero, il modello linguistico non finger√† di essere antico, lo sar√† davvero. L'obiettivo attuale di questo progetto √® creare qualcosa che possa ragionare esclusivamente usando la conoscenza dei libri londinesi pubblicati tra il 1800 e il 1850.

## Che tipo di dati hai usato per l'addestramento?

Sto usando libri, documenti legali, giornali e altri scritti provenienti da Londra tra il 1800 e il 1850. La lista che ho linkato ne contiene circa 200, ma per il primo addestramento ho usato solo 50 file per circa ~187 MB. Puoi visualizzare la lista dei documenti:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt

## Quanto sono grandi i modelli?

V0: 16M Parametri

V0.5: 123M Parametri

V1: 700M Parametri

# Specifiche di Addestramento?

#V0/V0.5
GPU: Geforce rtx 4060
CPU: i5-13400F
Ram: 16GB DDR5.

#V1
GPU: A100 rented









---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-08-12

---