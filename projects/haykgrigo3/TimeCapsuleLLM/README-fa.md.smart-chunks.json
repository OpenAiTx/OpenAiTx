[
  {
    "Id": 1,
    "Content": "\n<div align=\"right\">\n  <details>\n    <summary >🌐 Language</summary>\n    <div>\n      <div align=\"center\">\n        <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en\">English</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN\">简体中文</a>\n        | <a href=\"#\" title=\"Coming soon\">繁體中文 (coming soon)</a> |\n        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja\">日本語</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko\">한국어</a>\n        | <a href=\"#\" title=\"Coming soon\">हिन्दी (coming soon)</a> |\n        | <a href=\"#\" title=\"Coming soon\">ไทย (coming soon)</a> |\n        | <a href=\"#\" title=\"Coming soon\">Français (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Deutsch (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Español (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Italiano (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Русский (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Português (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Nederlands (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Polski (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">العربية (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">فارسی (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Türkçe (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Tiếng Việt (coming soon)</a>\n        | <a href=\"#\" title=\"Coming soon\">Bahasa Indonesia (coming soon)</a>\n\n      </div>\n    </div>\n  </details>\n</div>\n\n# TimeCapsule LLM\n\n*A language model trained **from scratch** exclusively on data from certain places and time periods to reduce modern bias and emulate the voice, vocabulary, and worldview of the era.*\n\nImagine if an AI model didnt just pretend to be historical but actually was.\n\nv0 and v0.5 built on [nanoGPT by Andrej Karpathy](https://github.com/karpathy/nanoGPT) Core training scripts and model architecture are his work. \n",
    "ContentSha": "Nctf/KF+1qyjgomYTtfV5kKwO9Fg9lrGLFemM++WzgU=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "<div align=\"right\">\n  <details>\n    <summary >🌐 زبان</summary>\n    <div>\n      <div align=\"center\">\n        <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en\">English</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN\">简体中文</a>\n        | <a href=\"#\" title=\"Coming soon\">繁體中文 (به زودی)</a> |\n        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja\">日本語</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko\">한국어</a>\n        | <a href=\"#\" title=\"Coming soon\">हिन्दी (به زودی)</a> |\n        | <a href=\"#\" title=\"Coming soon\">ไทย (به زودی)</a> |\n        | <a href=\"#\" title=\"Coming soon\">Français (به زودی)</a>\n        | <a href=\"#\" title=\"Coming soon\">Deutsch (به زودی)</a>\n        | <a href=\"#\" title=\"Coming soon\">Español (به زودی)</a>\n        | <a href=\"#\" title=\"Coming soon\">Italiano (به زودی)</a>\n        | <a href=\"#\" title=\"Coming soon\">Русский (به زودی)</a>\n        | <a href=\"#\" title=\"Coming soon\">Português (به زودی)</a>\n        | <a href=\"#\" title=\"Coming soon\">Nederlands (به زودی)</a>\n        | <a href=\"#\" title=\"Coming soon\">Polski (به زودی)</a>\n        | <a href=\"#\" title=\"Coming soon\">العربية (به زودی)</a>\n        | <a href=\"#\" title=\"Coming soon\">فارسی (به زودی)</a>\n        | <a href=\"#\" title=\"Coming soon\">Türkçe (به زودی)</a>\n        | <a href=\"#\" title=\"Coming soon\">Tiếng Việt (به زودی)</a>\n        | <a href=\"#\" title=\"Coming soon\">Bahasa Indonesia (به زودی)</a>\n\n      </div>\n    </div>\n  </details>\n</div>\n\n# تایم‌کپسول ال‌ال‌ام\n\n*یک مدل زبانی که **از صفر** فقط با داده‌های مربوط به مکان‌ها و دوره‌های زمانی خاص آموزش داده شده تا سوگیری‌های مدرن را کاهش داده و صدای، واژگان و جهان‌بینی آن عصر را شبیه‌سازی کند.*\n\nتصور کنید اگر یک مدل هوش مصنوعی فقط وانمود به تاریخی بودن نکند، بلکه واقعا تاریخی باشد.\n\nنسخه‌های v0 و v0.5 بر پایه [nanoGPT توسط آندری کارپاتی](https://github.com/karpathy/nanoGPT) ساخته شده‌اند. اسکریپت‌های اصلی آموزش و معماری مدل متعلق به اوست.\n\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 2,
    "Content": "v1 built on [Phi 1.5 by Microsoft](https://huggingface.co/microsoft/phi-1_5)\n\n[Hugging Face Link](https://huggingface.co/haykgrigorian/TimeCapsuleLLM)\n\n\n##  Model Behavior & Limitations\n\n### **v0**  \n\nEarly prompts show the model responding with 1800's language and behavior. \nExample: Prompt: \"Who art Henry?\" and it replied \"I know that man, I have did not a black, the storm.\" \n\n![TimeLockLLM Sample Output](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)\n\n- No mention of modern concepts  \n- Mostly era-accurate vocabulary  \n- Sentences are mostly incoherent (expected for ~187MB training data)\n\n### **v0.5** \n\nA significant improvement over v0.  \n- Victorian writing style, proper punctuation, mostly grammatical sentences  \n- Still high factual hallucination rate  \n- OCR noise (“Digitized by Google”) still present in outputs\n\n![TimeLockLLM Sample Output](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)\n\n### **v1**\n\nFirst model to recall and connect a real historical event with an actual figure from the dataset.\n\nExample: Prompt: \"It was the year of our Lord 1834\" \n\nThe output: \"It was the year of our Lord 1834 and the streets of London were filled with protest and petition. The cause, as many re counted, was not bound in the way of private, but having taken up the same day in the day of Lord Palmerston, the public will receive a short statement of the difficulties under which the day of law has reached us. It is a matter of deep regret, that the present events in the history of the world are clear, and consequently will be'known. It is not true that the very men who first settled in the Gospel at Jerusalem should have so extensive and so interesting a record of the prosperity and prosperity\" \n\nAt first I assumed that a protest might have coincidentally taken place the same year but take a look at this: ![1834protest](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)\n\n### Why this matters:\n\nThis is the first example of one of my models connecting a year to both a real historical event and a real person tied to that event (Lord Palmerston). Earlier models (v0 and v0.5) could mimic writing styles of the 19th century but would always hallucinate events, people and facts. This shows the model is beggining to remember things from the dataset ",
    "ContentSha": "uiT3rRoD6Qv1qSIL749rEr+c4kH+VDeg6rL8gib2bzs=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "نسخه v1 بر پایه [Phi 1.5 توسط مایکروسافت](https://huggingface.co/microsoft/phi-1_5) ساخته شده است\n\n[لینک Hugging Face](https://huggingface.co/haykgrigorian/TimeCapsuleLLM)\n\n##  رفتار مدل و محدودیت‌ها\n\n### **نسخه v0**  \n\nپرامپت‌های اولیه نشان می‌دهند که مدل با زبان و رفتار دهه ۱۸۰۰ پاسخ می‌دهد.  \nمثال: پرامپت: \"چه کسی هنری است؟\" و پاسخ داد: \"من آن مرد را می‌شناسم، من نکرده‌ام سیاه، طوفان.\"  \n\n![نمونه خروجی TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)\n\n- بدون اشاره به مفاهیم مدرن  \n- عمدتاً واژگان مطابق با دوره زمانی  \n- جملات عمدتاً نامفهوم (انتظار می‌رفت با حدود ۱۸۷ مگابایت داده آموزشی)\n\n### **نسخه v0.5** \n\nبهبود قابل توجه نسبت به نسخه v0  \n- سبک نوشتار ویکتوریایی، نقطه‌گذاری صحیح، جملات عمدتاً دستوری  \n- هنوز نرخ بالای هذیان‌گویی واقعی  \n- نویز OCR (\"Digitized by Google\") همچنان در خروجی‌ها وجود دارد\n\n![نمونه خروجی TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)\n\n### **نسخه v1**\n\nاولین مدلی که توانست یک رویداد تاریخی واقعی را با یک شخصیت حقیقی از مجموعه داده مرتبط کند.\n\nمثال: پرامپت: \"در سال پروردگارمان ۱۸۳۴ بود\" \n\nخروجی: \"در سال پروردگارمان ۱۸۳۴ بود و خیابان‌های لندن پر از اعتراض و دادخواست بود. علت، همانطور که بسیاری نقل کردند، به شیوه خصوصی محدود نبود، بلکه همان روز در روز لرد پالمرستون مطرح شد، عموم مردم بیانیه کوتاهی از دشواری‌هایی که قانون به ما رسانده است دریافت خواهند کرد. این موضوعی است که جای تأسف عمیق دارد، که رویدادهای کنونی در تاریخ جهان واضح هستند و بنابراین شناخته خواهند شد. درست نیست که همان مردانی که برای اولین بار در انجیل در اورشلیم ساکن شدند، باید چنین سابقه گسترده و جالبی از رونق و رفاه داشته باشند\"\n\nدر ابتدا تصور می‌کردم که شاید به طور تصادفی اعتراضی در همان سال رخ داده باشد اما به این توجه کنید: ![1834protest](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)\n\n### چرا این موضوع اهمیت دارد:\n\nاین اولین نمونه‌ای است که یکی از مدل‌های من یک سال را هم به یک رویداد تاریخی واقعی و هم به یک فرد واقعی مرتبط با آن رویداد (لرد پالمرستون) متصل می‌کند. مدل‌های قبلی (v0 و v0.5) می‌توانستند سبک نوشتاری قرن نوزدهم را تقلید کنند اما همواره رویدادها، افراد و حقایق را هذیان می‌گفتند. این نشان می‌دهد که مدل شروع به به‌خاطر سپردن اطلاعات از مجموعه داده کرده است.\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "qJUbd5u/iOZOtuM+fYFPZOjgtKDl8iwUfAi1n3MVisI=",
        "originContent": "v1 built on [Phi 1.5 by Microsoft](https://huggingface.co/microsoft/phi-1_5)",
        "translatedContent": "نسخه v1 بر پایه [Phi 1.5 توسط مایکروسافت](https://huggingface.co/microsoft/phi-1_5) ساخته شده است"
      },
      {
        "row": 2,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 3,
        "rowsha": "0kza+J1J1rtVrJmQnk2qdxJsC7lvjDN6mSSj8nyd31Y=",
        "originContent": "[Hugging Face Link](https://huggingface.co/haykgrigorian/TimeCapsuleLLM)",
        "translatedContent": "[لینک Hugging Face](https://huggingface.co/haykgrigorian/TimeCapsuleLLM)"
      },
      {
        "row": 4,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 5,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "##  رفتار مدل و محدودیت‌ها"
      },
      {
        "row": 6,
        "rowsha": "S8kboQ4LnLUhavjolKrTgNgwVOMJrwpnaY1CWGr6epQ=",
        "originContent": "##  Model Behavior & Limitations",
        "translatedContent": ""
      },
      {
        "row": 7,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "### **نسخه v0**  "
      },
      {
        "row": 8,
        "rowsha": "1nqZ1MR/Rxg5du0+tZch+QesCAT+5qMlbhJftBOVqDI=",
        "originContent": "### **v0**  ",
        "translatedContent": ""
      },
      {
        "row": 9,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "پرامپت‌های اولیه نشان می‌دهند که مدل با زبان و رفتار دهه ۱۸۰۰ پاسخ می‌دهد.  "
      },
      {
        "row": 10,
        "rowsha": "hlawgmI5oFABN37xYPae7PebQq1TP5c6DMT8b78524g=",
        "originContent": "Early prompts show the model responding with 1800's language and behavior. ",
        "translatedContent": "مثال: پرامپت: \"چه کسی هنری است؟\" و پاسخ داد: \"من آن مرد را می‌شناسم، من نکرده‌ام سیاه، طوفان.\"  "
      },
      {
        "row": 11,
        "rowsha": "5eCkvYiGa3VxONgXmVAvuTajgTbveIO0PKy313G2fos=",
        "originContent": "Example: Prompt: \"Who art Henry?\" and it replied \"I know that man, I have did not a black, the storm.\" ",
        "translatedContent": ""
      },
      {
        "row": 12,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "![نمونه خروجی TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)"
      },
      {
        "row": 13,
        "rowsha": "yKIR0teTc66wVDG+jdIyNmAzItXb2JH2ld3D7tm4qnM=",
        "originContent": "![TimeLockLLM Sample Output](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true)",
        "translatedContent": ""
      },
      {
        "row": 14,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "- بدون اشاره به مفاهیم مدرن  "
      },
      {
        "row": 15,
        "rowsha": "iCVdbo9EUfepGTS1z1bOwgFn1wDAa9zhHoBLJxEw1w8=",
        "originContent": "- No mention of modern concepts  ",
        "translatedContent": "- عمدتاً واژگان مطابق با دوره زمانی  "
      },
      {
        "row": 16,
        "rowsha": "khHeh5VmbIuCnyoNCTDAXLsQOpnqbamm8vyOjvWVmLE=",
        "originContent": "- Mostly era-accurate vocabulary  ",
        "translatedContent": "- جملات عمدتاً نامفهوم (انتظار می‌رفت با حدود ۱۸۷ مگابایت داده آموزشی)"
      },
      {
        "row": 17,
        "rowsha": "UP5LBmH8evd8yPger3Fjvs1PXYiVAALQC0JWmcmlN08=",
        "originContent": "- Sentences are mostly incoherent (expected for ~187MB training data)",
        "translatedContent": ""
      },
      {
        "row": 18,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "### **نسخه v0.5** "
      },
      {
        "row": 19,
        "rowsha": "5p60D9yLuB40iACJ6apiWJq8lt1AqZAyvFkG70v4xco=",
        "originContent": "### **v0.5** ",
        "translatedContent": ""
      },
      {
        "row": 20,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "بهبود قابل توجه نسبت به نسخه v0  "
      },
      {
        "row": 21,
        "rowsha": "IPr+C8pVE68OvudYYmIOcbEumcoTmjBiq3HmXPghcvU=",
        "originContent": "A significant improvement over v0.  ",
        "translatedContent": "- سبک نوشتار ویکتوریایی، نقطه‌گذاری صحیح، جملات عمدتاً دستوری  "
      },
      {
        "row": 22,
        "rowsha": "VEokY2Qi826s5lAKKkVoOd5hju8hvOxG21P/L2LPz9k=",
        "originContent": "- Victorian writing style, proper punctuation, mostly grammatical sentences  ",
        "translatedContent": "- هنوز نرخ بالای هذیان‌گویی واقعی  "
      },
      {
        "row": 23,
        "rowsha": "hkitRVnW6jxtd+R18O5Olb5eqNIJGCKv4Yt2I+yLrWE=",
        "originContent": "- Still high factual hallucination rate  ",
        "translatedContent": "- نویز OCR (\"Digitized by Google\") همچنان در خروجی‌ها وجود دارد"
      },
      {
        "row": 24,
        "rowsha": "qlWBqyQVUBsL120iExHvs0PDrN2m0WeMIyRHeHJ1qhM=",
        "originContent": "- OCR noise (“Digitized by Google”) still present in outputs",
        "translatedContent": ""
      },
      {
        "row": 25,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "![نمونه خروجی TimeLockLLM](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)"
      },
      {
        "row": 26,
        "rowsha": "8DhXpgpVtg05XdyplRHf49EFOQNCJVzXA9RpmJQ+y9U=",
        "originContent": "![TimeLockLLM Sample Output](https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true)",
        "translatedContent": ""
      },
      {
        "row": 27,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "### **نسخه v1**"
      },
      {
        "row": 28,
        "rowsha": "mQ9MARMITxOHtqSNIP76nXvTlEM8Ope3qbQ/YG1h6/8=",
        "originContent": "### **v1**",
        "translatedContent": ""
      },
      {
        "row": 29,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "اولین مدلی که توانست یک رویداد تاریخی واقعی را با یک شخصیت حقیقی از مجموعه داده مرتبط کند."
      },
      {
        "row": 30,
        "rowsha": "sAlnUjWbUBk6czBOHW/IgxyE98WYhX0LrIYmZBr8P2Y=",
        "originContent": "First model to recall and connect a real historical event with an actual figure from the dataset.",
        "translatedContent": ""
      },
      {
        "row": 31,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "مثال: پرامپت: \"در سال پروردگارمان ۱۸۳۴ بود\" "
      },
      {
        "row": 32,
        "rowsha": "Cd0DPnkqItUnSk3fHx9ckSSN6MNcZnsEX3S44GZyxTw=",
        "originContent": "Example: Prompt: \"It was the year of our Lord 1834\" ",
        "translatedContent": ""
      },
      {
        "row": 33,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "خروجی: \"در سال پروردگارمان ۱۸۳۴ بود و خیابان‌های لندن پر از اعتراض و دادخواست بود. علت، همانطور که بسیاری نقل کردند، به شیوه خصوصی محدود نبود، بلکه همان روز در روز لرد پالمرستون مطرح شد، عموم مردم بیانیه کوتاهی از دشواری‌هایی که قانون به ما رسانده است دریافت خواهند کرد. این موضوعی است که جای تأسف عمیق دارد، که رویدادهای کنونی در تاریخ جهان واضح هستند و بنابراین شناخته خواهند شد. درست نیست که همان مردانی که برای اولین بار در انجیل در اورشلیم ساکن شدند، باید چنین سابقه گسترده و جالبی از رونق و رفاه داشته باشند\""
      },
      {
        "row": 34,
        "rowsha": "zvm/1tcaMFjcQmuN7y0hczAbbgAwaKRtjBpah5/+kb8=",
        "originContent": "The output: \"It was the year of our Lord 1834 and the streets of London were filled with protest and petition. The cause, as many re counted, was not bound in the way of private, but having taken up the same day in the day of Lord Palmerston, the public will receive a short statement of the difficulties under which the day of law has reached us. It is a matter of deep regret, that the present events in the history of the world are clear, and consequently will be'known. It is not true that the very men who first settled in the Gospel at Jerusalem should have so extensive and so interesting a record of the prosperity and prosperity\" ",
        "translatedContent": ""
      },
      {
        "row": 35,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "در ابتدا تصور می‌کردم که شاید به طور تصادفی اعتراضی در همان سال رخ داده باشد اما به این توجه کنید: ![1834protest](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)"
      },
      {
        "row": 36,
        "rowsha": "WfSfj2g4CByfYQWI59s4C3EwGLZuWfz1t4w49a49e+8=",
        "originContent": "At first I assumed that a protest might have coincidentally taken place the same year but take a look at this: ![1834protest](https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png)",
        "translatedContent": ""
      },
      {
        "row": 37,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "### چرا این موضوع اهمیت دارد:"
      },
      {
        "row": 38,
        "rowsha": "qlFf3SlVZpptxE7ExYx9wjs1lBnO/bsCjUl1Q4Sw2ds=",
        "originContent": "### Why this matters:",
        "translatedContent": ""
      },
      {
        "row": 39,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "این اولین نمونه‌ای است که یکی از مدل‌های من یک سال را هم به یک رویداد تاریخی واقعی و هم به یک فرد واقعی مرتبط با آن رویداد (لرد پالمرستون) متصل می‌کند. مدل‌های قبلی (v0 و v0.5) می‌توانستند سبک نوشتاری قرن نوزدهم را تقلید کنند اما همواره رویدادها، افراد و حقایق را هذیان می‌گفتند. این نشان می‌دهد که مدل شروع به به‌خاطر سپردن اطلاعات از مجموعه داده کرده است."
      },
      {
        "row": 40,
        "rowsha": "WdZfxluSvMeRhCl5u8NEkZI+F2o+eDrUIZyWPgHGv6w=",
        "originContent": "This is the first example of one of my models connecting a year to both a real historical event and a real person tied to that event (Lord Palmerston). Earlier models (v0 and v0.5) could mimic writing styles of the 19th century but would always hallucinate events, people and facts. This shows the model is beggining to remember things from the dataset ",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 3,
    "Content": "\n## Upcoming Plans \n\n- There are nearly 175,000 texts published in London from 1800-1875 on Internet Archive \n- I plan on expanding the corpus and cleaning it more for better reasoning abilities\n- Expanding to different regions and time periods for more historical models\n\n\n## How to Use\n\nThis project focuses mostly on curating historical data, preparing it for training and building a tokenizer. I am not going to cover the full LLM training process, for that refer to nanoGPT by Andrej Karpathy.\n\n### Step 1: Gather and Prepare Historical Texts \n\n- Collect .txt files of public domain books, documents, etc from your chosen time period (e.g., London 1800-1850) \n- Keep them within your chosen time/place window  \n- Clean the text files using a script or manually remove headers/footer from Project Gutenberg, Modern annotations or things like OCR errors.\n\n### Step 2: Build a Custom Tokenizer\n\n- Run train_tokenizer.py or train_tokenizer_hf.py on the cleaned data.\n- This will give you vocab.json and merges.txt\n- Thes files define vocab and merge rules for your model\n\n### Step 3: Train Your Model \n\n- Refer to [nanoGPT by Andrej Karpathy](https://github.com/karpathy/nanoGPT) for the training process or your chosen architecture’s docs.\n\n# FAQ\n\n## What is Selective Temporal Training ?\n\nSelective Temporal Training (STT) is a machine learning methodology where all training data is specifically curated to fall within a specific historical time period. It's done in order to model the language and knowledge of that era without influence from modern concepts. For example, the current model I have now (v0.5) is trained on data exclusively from 1800-1875, it's not fine tuned but trained from scratch resulting in output that reflects the linguistic style and historical context of that time period.\n\n## Why not just use fine-tuning or LoRA?\n\nFor this project I'm trying to create a language model that is unclouded from modern bias. If I fine-tune something like GPT-2, it's already pre-trained and that information won't go away. If I train from scratch the language model won't pretend to be old, it just will be. The Goal for this project right now is to create something can reason exclusively using knowledge from London books published between 1800 and 1875.\n\n## What kind of data did you use for training?\n",
    "ContentSha": "PVmS0gcv9I8wbtUt+qdoQ+UXB+m+tDn+XX5dLnLZNN4=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "\n## برنامه‌های آتی\n\n- تقریباً ۱۷۵,۰۰۰ متن منتشر شده در لندن از سال ۱۸۰۰ تا ۱۸۷۵ در Internet Archive وجود دارد\n- قصد دارم مجموعه داده را گسترش داده و آن را بیشتر پاک‌سازی کنم تا توانایی استدلال بهتری حاصل شود\n- گسترش به مناطق و دوره‌های زمانی مختلف برای مدل‌های تاریخی بیشتر\n\n## نحوه استفاده\n\nاین پروژه عمدتاً بر گردآوری داده‌های تاریخی، آماده‌سازی آن برای آموزش و ساخت توکنایزر تمرکز دارد. من فرآیند کامل آموزش LLM را پوشش نمی‌دهم؛ برای این منظور به nanoGPT اثر Andrej Karpathy مراجعه کنید.\n\n### مرحله ۱: گردآوری و آماده‌سازی متون تاریخی\n\n- فایل‌های .txt از کتاب‌ها، اسناد و... دامنه عمومی را از دوره زمانی انتخابی خود جمع‌آوری کنید (مثلاً لندن ۱۸۰۰-۱۸۵۰)\n- آن‌ها را در بازه زمانی/مکانی انتخابی خود نگه دارید\n- فایل‌های متنی را با یک اسکریپت یا به صورت دستی پاک‌سازی کنید و سرصفحه/پاصفحه پروژه گوتنبرگ، توضیحات مدرن یا خطاهای OCR را حذف کنید.\n\n### مرحله ۲: ساخت توکنایزر سفارشی\n\n- train_tokenizer.py یا train_tokenizer_hf.py را روی داده‌های پاک‌سازی شده اجرا کنید.\n- این کار vocab.json و merges.txt را به شما می‌دهد\n- این فایل‌ها واژگان و قواعد ادغام برای مدل شما را تعریف می‌کنند\n\n### مرحله ۳: آموزش مدل خود\n\n- برای فرآیند آموزش یا مستندات معماری انتخابی، به [nanoGPT اثر Andrej Karpathy](https://github.com/karpathy/nanoGPT) مراجعه کنید.\n\n# پرسش‌های متداول\n\n## آموزش زمانی گزینشی چیست؟\n\nآموزش زمانی گزینشی (STT) روشی در یادگیری ماشین است که تمام داده‌های آموزشی به طور خاص برای قرار گرفتن در یک دوره تاریخی مشخص انتخاب می‌شوند. هدف آن مدل‌سازی زبان و دانش آن دوره بدون تأثیر مفاهیم مدرن است. به عنوان مثال، مدل فعلی من (نسخه ۰.۵) فقط با داده‌های ۱۸۰۰ تا ۱۸۷۵ آموزش دیده است؛ از ابتدا آموزش دیده و نه فقط تنظیم دقیق شده، بنابراین خروجی آن بازتاب‌دهنده سبک زبانی و زمینه تاریخی همان دوره است.\n\n## چرا فقط از تنظیم دقیق یا LoRA استفاده نمی‌کنید؟\n\nبرای این پروژه، هدفم ساخت یک مدل زبانی بدون سوگیری مدرن است. اگر چیزی مثل GPT-2 را تنظیم دقیق کنم، آن قبلاً آموزش دیده و اطلاعات آن باقی خواهد ماند. اگر مدل را از ابتدا آموزش دهم، مدل زبانی وانمود به قدیمی بودن نمی‌کند، واقعاً قدیمی خواهد بود. هدف فعلی این پروژه ساخت مدلی است که بتواند منحصراً با دانش کتاب‌های لندن منتشر شده بین ۱۸۰۰ و ۱۸۷۵ استدلال کند.\n\n## از چه نوع داده‌ای برای آموزش استفاده کردید؟\n\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 2,
        "rowsha": "xt81v+AyDswOM8KVPmSYl1lfW4vISpujtQjRWm0mKbA=",
        "originContent": "## Upcoming Plans ",
        "translatedContent": "## برنامه‌های آتی"
      },
      {
        "row": 3,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 4,
        "rowsha": "CulYLHqIw5Hn1cLDvlekSWBe1moOd8QSIirjeM77tlk=",
        "originContent": "- There are nearly 175,000 texts published in London from 1800-1875 on Internet Archive ",
        "translatedContent": "- تقریباً ۱۷۵,۰۰۰ متن منتشر شده در لندن از سال ۱۸۰۰ تا ۱۸۷۵ در Internet Archive وجود دارد"
      },
      {
        "row": 5,
        "rowsha": "VWINtuHshK3Fs4MeBmv24/pZC0iMQ70HtMIowLnrsAI=",
        "originContent": "- I plan on expanding the corpus and cleaning it more for better reasoning abilities",
        "translatedContent": "- قصد دارم مجموعه داده را گسترش داده و آن را بیشتر پاک‌سازی کنم تا توانایی استدلال بهتری حاصل شود"
      },
      {
        "row": 6,
        "rowsha": "3b7jQ1J58CCfC/VV9FWJY23SOlvhzx/vIVth3ZBIPog=",
        "originContent": "- Expanding to different regions and time periods for more historical models",
        "translatedContent": "- گسترش به مناطق و دوره‌های زمانی مختلف برای مدل‌های تاریخی بیشتر"
      },
      {
        "row": 7,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 8,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## نحوه استفاده"
      },
      {
        "row": 9,
        "rowsha": "yJlGQKI3p2nVYC1uqNa6eKPSIzUW38uRiGu0PspqvUU=",
        "originContent": "## How to Use",
        "translatedContent": ""
      },
      {
        "row": 10,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "این پروژه عمدتاً بر گردآوری داده‌های تاریخی، آماده‌سازی آن برای آموزش و ساخت توکنایزر تمرکز دارد. من فرآیند کامل آموزش LLM را پوشش نمی‌دهم؛ برای این منظور به nanoGPT اثر Andrej Karpathy مراجعه کنید."
      },
      {
        "row": 11,
        "rowsha": "XVoXr9uzZwN09vboETojEQJe057RBzcMUjXmQRCB/jo=",
        "originContent": "This project focuses mostly on curating historical data, preparing it for training and building a tokenizer. I am not going to cover the full LLM training process, for that refer to nanoGPT by Andrej Karpathy.",
        "translatedContent": ""
      },
      {
        "row": 12,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "### مرحله ۱: گردآوری و آماده‌سازی متون تاریخی"
      },
      {
        "row": 13,
        "rowsha": "h3A3o0uEgrm0Te8o4iW7R0m1bNwx7jj9Q5iUhhV0iTI=",
        "originContent": "### Step 1: Gather and Prepare Historical Texts ",
        "translatedContent": ""
      },
      {
        "row": 14,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "- فایل‌های .txt از کتاب‌ها، اسناد و... دامنه عمومی را از دوره زمانی انتخابی خود جمع‌آوری کنید (مثلاً لندن ۱۸۰۰-۱۸۵۰)"
      },
      {
        "row": 15,
        "rowsha": "E60jdnnHtQ07Lgqtx5VBWz6jh3ysKu7LMtZHrIYaQiQ=",
        "originContent": "- Collect .txt files of public domain books, documents, etc from your chosen time period (e.g., London 1800-1850) ",
        "translatedContent": "- آن‌ها را در بازه زمانی/مکانی انتخابی خود نگه دارید"
      },
      {
        "row": 16,
        "rowsha": "/VflzaebH3p9kM3YtRzEWV0i/lVdD/pGby0wiJoXoco=",
        "originContent": "- Keep them within your chosen time/place window  ",
        "translatedContent": "- فایل‌های متنی را با یک اسکریپت یا به صورت دستی پاک‌سازی کنید و سرصفحه/پاصفحه پروژه گوتنبرگ، توضیحات مدرن یا خطاهای OCR را حذف کنید."
      },
      {
        "row": 17,
        "rowsha": "6P9O4yLTQNPt+lgfziVbtyNP+4+SBhj6UvzpJ+Br7PM=",
        "originContent": "- Clean the text files using a script or manually remove headers/footer from Project Gutenberg, Modern annotations or things like OCR errors.",
        "translatedContent": ""
      },
      {
        "row": 18,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "### مرحله ۲: ساخت توکنایزر سفارشی"
      },
      {
        "row": 19,
        "rowsha": "QHGXZGXfahA0jwtugXfFPK3RfnoD/MgGK7KZOEu7Yu0=",
        "originContent": "### Step 2: Build a Custom Tokenizer",
        "translatedContent": ""
      },
      {
        "row": 20,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "- train_tokenizer.py یا train_tokenizer_hf.py را روی داده‌های پاک‌سازی شده اجرا کنید."
      },
      {
        "row": 21,
        "rowsha": "hPnDq1u2xQSmYVNea/p6gaqpP4bMQD0lWroth4bud9A=",
        "originContent": "- Run train_tokenizer.py or train_tokenizer_hf.py on the cleaned data.",
        "translatedContent": "- این کار vocab.json و merges.txt را به شما می‌دهد"
      },
      {
        "row": 22,
        "rowsha": "ib4aP51Hu/ktXBqXfyiOtGRLdTUEhyEupPgINhe9JwA=",
        "originContent": "- This will give you vocab.json and merges.txt",
        "translatedContent": "- این فایل‌ها واژگان و قواعد ادغام برای مدل شما را تعریف می‌کنند"
      },
      {
        "row": 23,
        "rowsha": "Bit9udkCe0pa2QERa6r17MT8TZN59hctFxW7DQ/7Ka4=",
        "originContent": "- Thes files define vocab and merge rules for your model",
        "translatedContent": ""
      },
      {
        "row": 24,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "### مرحله ۳: آموزش مدل خود"
      },
      {
        "row": 25,
        "rowsha": "NMTP0o8ite3isz8AN3pbJxnD1y/ULsyqoGpOqqCM9k0=",
        "originContent": "### Step 3: Train Your Model ",
        "translatedContent": ""
      },
      {
        "row": 26,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "- برای فرآیند آموزش یا مستندات معماری انتخابی، به [nanoGPT اثر Andrej Karpathy](https://github.com/karpathy/nanoGPT) مراجعه کنید."
      },
      {
        "row": 27,
        "rowsha": "b/wzY4grG20GGDNSiBPVe1mYZhuqNzNEaHz5XhGaZ1I=",
        "originContent": "- Refer to [nanoGPT by Andrej Karpathy](https://github.com/karpathy/nanoGPT) for the training process or your chosen architecture’s docs.",
        "translatedContent": ""
      },
      {
        "row": 28,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "# پرسش‌های متداول"
      },
      {
        "row": 29,
        "rowsha": "OoCxyGfPN5TmdzAkaPphtPx303MJJ7vpfWbKrufGH5g=",
        "originContent": "# FAQ",
        "translatedContent": ""
      },
      {
        "row": 30,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## آموزش زمانی گزینشی چیست؟"
      },
      {
        "row": 31,
        "rowsha": "+5dDgPw4ILEotxso4tjjjz1cxwUei16yNQPDUKbgxoo=",
        "originContent": "## What is Selective Temporal Training ?",
        "translatedContent": ""
      },
      {
        "row": 32,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "آموزش زمانی گزینشی (STT) روشی در یادگیری ماشین است که تمام داده‌های آموزشی به طور خاص برای قرار گرفتن در یک دوره تاریخی مشخص انتخاب می‌شوند. هدف آن مدل‌سازی زبان و دانش آن دوره بدون تأثیر مفاهیم مدرن است. به عنوان مثال، مدل فعلی من (نسخه ۰.۵) فقط با داده‌های ۱۸۰۰ تا ۱۸۷۵ آموزش دیده است؛ از ابتدا آموزش دیده و نه فقط تنظیم دقیق شده، بنابراین خروجی آن بازتاب‌دهنده سبک زبانی و زمینه تاریخی همان دوره است."
      },
      {
        "row": 33,
        "rowsha": "hooEARKH4r/sDPh7JUtZAZ6TYMvBkTLZIcfw3g83xos=",
        "originContent": "Selective Temporal Training (STT) is a machine learning methodology where all training data is specifically curated to fall within a specific historical time period. It's done in order to model the language and knowledge of that era without influence from modern concepts. For example, the current model I have now (v0.5) is trained on data exclusively from 1800-1875, it's not fine tuned but trained from scratch resulting in output that reflects the linguistic style and historical context of that time period.",
        "translatedContent": ""
      },
      {
        "row": 34,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## چرا فقط از تنظیم دقیق یا LoRA استفاده نمی‌کنید؟"
      },
      {
        "row": 35,
        "rowsha": "dVMKQ2mPI1Spc6x6r/jNG0PIR5YKpalU4MXx9JmKp/I=",
        "originContent": "## Why not just use fine-tuning or LoRA?",
        "translatedContent": ""
      },
      {
        "row": 36,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "برای این پروژه، هدفم ساخت یک مدل زبانی بدون سوگیری مدرن است. اگر چیزی مثل GPT-2 را تنظیم دقیق کنم، آن قبلاً آموزش دیده و اطلاعات آن باقی خواهد ماند. اگر مدل را از ابتدا آموزش دهم، مدل زبانی وانمود به قدیمی بودن نمی‌کند، واقعاً قدیمی خواهد بود. هدف فعلی این پروژه ساخت مدلی است که بتواند منحصراً با دانش کتاب‌های لندن منتشر شده بین ۱۸۰۰ و ۱۸۷۵ استدلال کند."
      },
      {
        "row": 37,
        "rowsha": "uUg17WnHfiRHAiHJTO+5TIAJlkny2eH+Ov6Lm0GmrJQ=",
        "originContent": "For this project I'm trying to create a language model that is unclouded from modern bias. If I fine-tune something like GPT-2, it's already pre-trained and that information won't go away. If I train from scratch the language model won't pretend to be old, it just will be. The Goal for this project right now is to create something can reason exclusively using knowledge from London books published between 1800 and 1875.",
        "translatedContent": ""
      },
      {
        "row": 38,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## از چه نوع داده‌ای برای آموزش استفاده کردید؟"
      },
      {
        "row": 39,
        "rowsha": "ByP4WlNmMoG6WIiLJNd6b080/DSciCgWmj9aYSJjAF0=",
        "originContent": "## What kind of data did you use for training?",
        "translatedContent": ""
      },
      {
        "row": 40,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 4,
    "Content": "I'm using books, legal documents, newspapers, and other writings from 1800–1875 London. The list I linked (for v0) has like 200 but for the first training I just used 50 files about ~187 MB. You can view a list of the documents:\nhttps://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt\n\nDataset sizes:\nv0: ~187MB\nv0.5: ~435MB \nv1: ~6.25GB \n\n## How large are the models ?\n\nV0: 16M Parameters\n\nV0.5 123M Parameters\n\nV1: 700M Parameters\n\n# Training Specs ? \n\n# V0/V0.5\nGPU: Geforce rtx 4060\nCPU: i5-13400F \nRam: 16GB DDR5.\n\n# V1\nGPU: A100 rented\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "ContentSha": "FscW2JRCPF/8vYlOatMJlwnmRIDByIlyT+CEgRRkpw0=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "من از کتاب‌ها، اسناد حقوقی، روزنامه‌ها و سایر نوشته‌های لندن بین سال‌های ۱۸۰۰ تا ۱۸۷۵ استفاده می‌کنم. لیستی که لینک کردم (برای نسخه v0) حدود ۲۰۰ مورد دارد اما برای اولین آموزش فقط ۵۰ فایل به حجم تقریبی ۱۸۷ مگابایت را استفاده کردم. شما می‌توانید لیست اسناد را مشاهده کنید:\nhttps://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt\n\nاندازه دیتاست‌ها:\nv0: تقریباً ۱۸۷ مگابایت\nv0.5: تقریباً ۴۳۵ مگابایت \nv1: تقریباً ۶.۲۵ گیگابایت \n\n## مدل‌ها چه اندازه هستند؟\n\nV0: ۱۶ میلیون پارامتر\n\nV0.5: ۱۲۳ میلیون پارامتر\n\nV1: ۷۰۰ میلیون پارامتر\n\n# مشخصات آموزش؟\n\n# V0/V0.5\nکارت گرافیک: Geforce rtx 4060\nپردازنده: i5-13400F \nرم: ۱۶ گیگابایت DDR5.\n\n# V1\nکارت گرافیک: A100 اجاره‌ای\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "TiKmhaEUee3SRrWijVMwW3s/qbNf2ziThQdK9PtiW9M=",
        "originContent": "I'm using books, legal documents, newspapers, and other writings from 1800–1875 London. The list I linked (for v0) has like 200 but for the first training I just used 50 files about ~187 MB. You can view a list of the documents:",
        "translatedContent": "من از کتاب‌ها، اسناد حقوقی، روزنامه‌ها و سایر نوشته‌های لندن بین سال‌های ۱۸۰۰ تا ۱۸۷۵ استفاده می‌کنم. لیستی که لینک کردم (برای نسخه v0) حدود ۲۰۰ مورد دارد اما برای اولین آموزش فقط ۵۰ فایل به حجم تقریبی ۱۸۷ مگابایت را استفاده کردم. شما می‌توانید لیست اسناد را مشاهده کنید:"
      },
      {
        "row": 2,
        "rowsha": "0mxyGiLJxzp9JPCg1oA+nbIwAKJbEC4ei9kSV3Gp84Y=",
        "originContent": "https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt",
        "translatedContent": "https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt"
      },
      {
        "row": 3,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 4,
        "rowsha": "m08SxkUzMWecQCroTF6uw3RhQ50sDm4BaoM0Ql/huS0=",
        "originContent": "Dataset sizes:",
        "translatedContent": "اندازه دیتاست‌ها:"
      },
      {
        "row": 5,
        "rowsha": "uah9doEnpRGciSUsNDHMkbOxtzCOyu4QuAM07AAUp1o=",
        "originContent": "v0: ~187MB",
        "translatedContent": "v0: تقریباً ۱۸۷ مگابایت"
      },
      {
        "row": 6,
        "rowsha": "4bhyzflpyjgeXMXaoAIxXATaPQqBpKaX945kxvp1ewE=",
        "originContent": "v0.5: ~435MB ",
        "translatedContent": "v0.5: تقریباً ۴۳۵ مگابایت "
      },
      {
        "row": 7,
        "rowsha": "thwQOIv1UCJQK/UZwcFrVR5tZmLvXp7mbQYGfpiFTN8=",
        "originContent": "v1: ~6.25GB ",
        "translatedContent": "v1: تقریباً ۶.۲۵ گیگابایت "
      },
      {
        "row": 8,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 9,
        "rowsha": "rrsRMYLHKsjbkzYRKilseYJKKApHSgo9buiBkZFSvlY=",
        "originContent": "## How large are the models ?",
        "translatedContent": "## مدل‌ها چه اندازه هستند؟"
      },
      {
        "row": 10,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 11,
        "rowsha": "MVeUG+6lIlH89E699W2WfE+eQmKEul25KNMIY8AtB6M=",
        "originContent": "V0: 16M Parameters",
        "translatedContent": "V0: ۱۶ میلیون پارامتر"
      },
      {
        "row": 12,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 13,
        "rowsha": "hQIpZXf/UvJi3QFJEM6jrjG7vnb9cvTFg/cvKv0buqM=",
        "originContent": "V0.5 123M Parameters",
        "translatedContent": "V0.5: ۱۲۳ میلیون پارامتر"
      },
      {
        "row": 14,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 15,
        "rowsha": "26WDqv9Q/x8glaWZTkjUWL5VNwuuXgUuXs51BhKEskk=",
        "originContent": "V1: 700M Parameters",
        "translatedContent": "V1: ۷۰۰ میلیون پارامتر"
      },
      {
        "row": 16,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 17,
        "rowsha": "zaP9HMP5oBcc9MGTkbFGzMO2XxeWWl+QlS9Yz9nfNXc=",
        "originContent": "# Training Specs ? ",
        "translatedContent": "# مشخصات آموزش؟"
      },
      {
        "row": 18,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 19,
        "rowsha": "7TPD+AUCDeUFol0fzwax1DEgBjstPoRauQ1zQAHJdc8=",
        "originContent": "# V0/V0.5",
        "translatedContent": "# V0/V0.5"
      },
      {
        "row": 20,
        "rowsha": "EH8H1HW/C4Tb7LfJgVUnVGsk4pF9l40Rlev8tAkKhjI=",
        "originContent": "GPU: Geforce rtx 4060",
        "translatedContent": "کارت گرافیک: Geforce rtx 4060"
      },
      {
        "row": 21,
        "rowsha": "vo3FdN37kY6VUB7PruRKfBPJDgsVJyBHIUCn/g8mt68=",
        "originContent": "CPU: i5-13400F ",
        "translatedContent": "پردازنده: i5-13400F "
      },
      {
        "row": 22,
        "rowsha": "W8fXPiQKUkoNso0PPfTvjYMy0IYo85j+gNXmB0aERO4=",
        "originContent": "Ram: 16GB DDR5.",
        "translatedContent": "رم: ۱۶ گیگابایت DDR5."
      },
      {
        "row": 23,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 24,
        "rowsha": "xpPrp95oVxh/XiVWlKlS072VerOoQbJEFU46i3XD2mk=",
        "originContent": "# V1",
        "translatedContent": "# V1"
      },
      {
        "row": 25,
        "rowsha": "cBfLeLi5ORliO9eNHlkjuWbV5U2y03sn/wFBoChMpoc=",
        "originContent": "GPU: A100 rented",
        "translatedContent": "کارت گرافیک: A100 اجاره‌ای"
      },
      {
        "row": 26,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 27,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 28,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 29,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 30,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 31,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 32,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 33,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 34,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 35,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 36,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 37,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 38,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 39,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  }
]