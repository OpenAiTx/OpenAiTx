[
  {
    "Id": 1,
    "Content": "![B2DVL Header](https://raw.githubusercontent.com/Thinklab-SJTU/Bench2Drive-VL/main/./assets/Bench2Drive-VL.png)\n\nğŸš— **Bench2Drive-VL** is a closed-loop full-stack benchmark for vision-language models (VLMs) in autonomous driving. In VQA part, our rule-based expert model DriveCommenter is used for generating VQAs' ground truth in CARLA simulator (or from static datasets like Bench2Drive). Original Bench2Drive metrics are used for planning benchmarking.\n\n<h2 align=\"center\">\n  <a href=\"https://thinklab-sjtu.github.io/Bench2Drive-VL/\"> Document</a> |\n  <a href=\"https://huggingface.co/datasets/Telkwevr/Bench2Drive-VL-base\"> Dataset</a>\n</h2>\n\n![B2DVL Structure](https://raw.githubusercontent.com/Thinklab-SJTU/Bench2Drive-VL/main/./assets/struct_new.png)\n\nğŸ“š Docker support is on the way...\n\n![B2DVL Modules](https://raw.githubusercontent.com/Thinklab-SJTU/Bench2Drive-VL/main/./assets/modules_new.png)\n\n## How to use\n\n### Set up the environment\n\n1. Install CARLA:\n",
    "ContentSha": "sZWWXc32wRo7ygHm9pOPafxmpno7MxAuX9pb/gaz/XQ=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "![B2DVL Header](https://raw.githubusercontent.com/Thinklab-SJTU/Bench2Drive-VL/main/./assets/Bench2Drive-VL.png)\n\nğŸš— **Bench2Drive-VL**ì€ ììœ¨ì£¼í–‰ì—ì„œ ë¹„ì „-ì–¸ì–´ ëª¨ë¸(VLM)ì„ ìœ„í•œ í´ë¡œì¦ˆë“œ ë£¨í”„ í’€ìŠ¤íƒ ë²¤ì¹˜ë§ˆí¬ì…ë‹ˆë‹¤. VQA ë¶€ë¶„ì—ì„œëŠ” ë£° ê¸°ë°˜ ì „ë¬¸ê°€ ëª¨ë¸ DriveCommenterê°€ CARLA ì‹œë®¬ë ˆì´í„°(ë˜ëŠ” Bench2Drive ê°™ì€ ì •ì  ë°ì´í„°ì…‹)ì—ì„œ VQAì˜ ì •ë‹µ ìƒì„±ì„ ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤. ì›ë˜ Bench2Drive ì§€í‘œëŠ” ê³„íš ë²¤ì¹˜ë§ˆí‚¹ì— ì‚¬ìš©ë©ë‹ˆë‹¤.\n\n<h2 align=\"center\">\n  <a href=\"https://thinklab-sjtu.github.io/Bench2Drive-VL/\"> ë¬¸ì„œ</a> |\n  <a href=\"https://huggingface.co/datasets/Telkwevr/Bench2Drive-VL-base\"> ë°ì´í„°ì…‹</a>\n</h2>\n\n![B2DVL Structure](https://raw.githubusercontent.com/Thinklab-SJTU/Bench2Drive-VL/main/./assets/struct_new.png)\n\nğŸ“š Docker ì§€ì›ì´ ê³§ ì œê³µë©ë‹ˆë‹¤...\n\n![B2DVL Modules](https://raw.githubusercontent.com/Thinklab-SJTU/Bench2Drive-VL/main/./assets/modules_new.png)\n\n## ì‚¬ìš© ë°©ë²•\n\n### í™˜ê²½ ì„¤ì •\n\n1. CARLA ì„¤ì¹˜:\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 2,
    "Content": "    ```bash\n    mkdir carla\n    cd carla\n    wget https://carla-releases.s3.us-east-005.backblazeb2.com/Linux/CARLA_0.9.15.tar.gz\n    tar -xvf CARLA_0.9.15.tar.gz\n    cd Import && wget https://carla-releases.s3.us-east-005.backblazeb2.com/Linux/AdditionalMaps_0.9.15.tar.gz\n    cd .. && bash ImportAssets.sh\n    export CARLA_ROOT=YOUR_CARLA_PATH\n    echo \"$CARLA_ROOT/PythonAPI/carla/dist/carla-0.9.15-py3.7-linux-x86_64.egg\" >> YOUR_CONDA_PATH/envs/YOUR_CONDA_ENV_NAME/lib/python3.7/site-packages/carla.pth # python 3.8 also works well, please set YOUR_CONDA_PATH and YOUR_CONDA_ENV_NAME\n    ```",
    "ContentSha": "lntswi7b4GrUORtb/f9AbL1laKSO26+WHCYkOOjD9+4=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "    ```bash\n    mkdir carla\n    cd carla\n    wget https://carla-releases.s3.us-east-005.backblazeb2.com/Linux/CARLA_0.9.15.tar.gz\n    tar -xvf CARLA_0.9.15.tar.gz\n    cd Import && wget https://carla-releases.s3.us-east-005.backblazeb2.com/Linux/AdditionalMaps_0.9.15.tar.gz\n    cd .. && bash ImportAssets.sh\n    export CARLA_ROOT=YOUR_CARLA_PATH\n    echo \"$CARLA_ROOT/PythonAPI/carla/dist/carla-0.9.15-py3.7-linux-x86_64.egg\" >> YOUR_CONDA_PATH/envs/YOUR_CONDA_ENV_NAME/lib/python3.7/site-packages/carla.pth # python 3.8 also works well, please set YOUR_CONDA_PATH and YOUR_CONDA_ENV_NAME\n    ```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 3,
    "Content": "\n2. After installing CARLA, write an `env.sh`:\n",
    "ContentSha": "preY1tEmzN7BYUksbWjxzwRCrUFJJ08a2Kj7gJXfEag=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "2. CARLAë¥¼ ì„¤ì¹˜í•œ í›„, `env.sh`ë¥¼ ì‘ì„±í•˜ì‹­ì‹œì˜¤:\n\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 4,
    "Content": "   ```bash\n   export CARLA_ROOT=/path/to/your/carla\n\n   export CARLA_SERVER=${CARLA_ROOT}/CarlaUE4.sh\n   export PYTHONPATH=${CARLA_ROOT}/PythonAPI\n   export PYTHONPATH=$PYTHONPATH:${CARLA_ROOT}/PythonAPI/carla\n   export PYTHONPATH=$PYTHONPATH:$CARLA_ROOT/PythonAPI/carla/dist/carla-0.9.15-py3.7-linux-x86_64.egg\n\n   export WORK_DIR=/path/to/this/repo\n   export PYTHONPATH=$PYTHONPATH:${WORK_DIR}/scenario_runner\n   export PYTHONPATH=$PYTHONPATH:${WORK_DIR}/leaderboard\n   export PYTHONPATH=$PYTHONPATH:${WORK_DIR}/B2DVL_Adapter\n   export SCENARIO_RUNNER_ROOT=${WORK_DIR}/scenario_runner\n   export LEADERBOARD_ROOT=${WORK_DIR}/leaderboard\n\n   export VQA_GEN=1\n   export STRICT_MODE=1\n   # DriveCommenter drives back the ego vehicle after circumventing obstacles if STRICT_MODE > 0,\n   # must set to true if doing closed-loop eval.\n   ```",
    "ContentSha": "otYlO/ApR6znthPVFmDJ02+C4RQuBVNu9mS139Zt9po=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "   ```bash\n   export CARLA_ROOT=/path/to/your/carla\n\n   export CARLA_SERVER=${CARLA_ROOT}/CarlaUE4.sh\n   export PYTHONPATH=${CARLA_ROOT}/PythonAPI\n   export PYTHONPATH=$PYTHONPATH:${CARLA_ROOT}/PythonAPI/carla\n   export PYTHONPATH=$PYTHONPATH:$CARLA_ROOT/PythonAPI/carla/dist/carla-0.9.15-py3.7-linux-x86_64.egg\n\n   export WORK_DIR=/path/to/this/repo\n   export PYTHONPATH=$PYTHONPATH:${WORK_DIR}/scenario_runner\n   export PYTHONPATH=$PYTHONPATH:${WORK_DIR}/leaderboard\n   export PYTHONPATH=$PYTHONPATH:${WORK_DIR}/B2DVL_Adapter\n   export SCENARIO_RUNNER_ROOT=${WORK_DIR}/scenario_runner\n   export LEADERBOARD_ROOT=${WORK_DIR}/leaderboard\n\n   export VQA_GEN=1\n   export STRICT_MODE=1\n   # DriveCommenter drives back the ego vehicle after circumventing obstacles if STRICT_MODE > 0,\n   # must set to true if doing closed-loop eval.\n   ```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 5,
    "Content": "\n3. Make sure you have the correct environment variables:\n",
    "ContentSha": "fh8Ht87tDF6pBnSv+fEaSH7eEAoMWdR0Xrh0FCxWvzo=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "3. ì˜¬ë°”ë¥¸ í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì‹­ì‹œì˜¤:\n\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 6,
    "Content": "   ```shell\n   source ./env.sh\n   ```",
    "ContentSha": "SU+Xw54HULVLps6Zggj79dmE6P9PjTJ0KpdtcmtTFM4=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "   ```shell\n   source ./env.sh\n   ```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 7,
    "Content": "\n### Closed-Loop Inference\n\n1. Write a vlm config file (examples can be found under `./vlm_config`):\n   \n   *This is a JSON file, so don't forget to delete all comments!*\n   \n   *Question id please refer to [document](https://raw.githubusercontent.com/Thinklab-SJTU/Bench2Drive-VL/main/./docs/qids.md).*\n\n   *Make sure you include question 50 because action module requires its answer.*\n",
    "ContentSha": "HRDNEYdq9IMnV3uuDpNbytTNCiumAM2xIhdBg+YOe6g=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### íì‡„ ë£¨í”„ ì¶”ë¡ \n\n1. vlm êµ¬ì„± íŒŒì¼ì„ ì‘ì„±í•˜ì„¸ìš” (ì˜ˆì œëŠ” `./vlm_config`ì—ì„œ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤):\n   \n   *ì´ê²ƒì€ JSON íŒŒì¼ì´ë¯€ë¡œ, ëª¨ë“  ì£¼ì„ì„ ì‚­ì œí•˜ëŠ” ê²ƒì„ ìŠì§€ ë§ˆì„¸ìš”!*\n   \n   *ì§ˆë¬¸ IDëŠ” [ë¬¸ì„œ](https://raw.githubusercontent.com/Thinklab-SJTU/Bench2Drive-VL/main/./docs/qids.md)ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.*\n\n   *ì•¡ì…˜ ëª¨ë“ˆì´ ë‹µë³€ì„ í•„ìš”ë¡œ í•˜ë¯€ë¡œ ì§ˆë¬¸ 50ì„ ë°˜ë“œì‹œ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.*\n\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 8,
    "Content": "    ```java\n    {\n        \"TASK_CONFIGS\": {\n            \"FRAME_PER_SEC\": 10 // sensor saving frequency\n        },\n        \"INFERENCE_BASICS\": {\n            \"INPUT_WINDOW\": 1, // frame count of given image input\n            \"CONVERSATION_WINDOW\": 1, // not used anymore, to be removed\n            \"USE_ALL_CAMERAS\": false, // true if use all cameras as input\n            \"USE_BEV\": false, // true if use bev as input\n            \"NO_HISTORY_MODE\": false // do not inherit context of previous VQAs\n        },\n        \"CHAIN\": { // for inference\n            \"NODE\": [19, 15, 7, 24, 13, 47, 8, 43, 50],\n            \"EDGE\": { // \"pred\": succ\n                \"19\": [24, 13, 8],\n                \"15\": [7, 8],\n                \"7\": [8],\n                \"24\": [13, 47],\n                \"13\": [47, 8, 43],\n                \"47\": [8],\n                \"8\": [43],\n                \"43\": [50],\n                \"50\": []\n            },\n            \"INHERIT\": { // inherit context from last frame\n                \"19\": [43, 7],\n                \"15\": [7]\n            },\n            \"USE_GT\": [24] // questions which use ground truth as answer\n        },\n        \"CONTROL_RATE\": 2.0, // intervene freq of vlm\n        \"MODEL_NAME\": \"api\", // model name\n        \"MODEL_PATH\": \"../model_zoo/your_model\", // model path\n        \"GPU_ID\": 0, // the gpu model runs on\n        \"PORT\": 7023, // web port\n        \"IN_CARLA\": true,\n        \"USE_BASE64\": true, // if false, local path is used for transmitting images\n        \"NO_PERC_INFO\": false // do not pass extra perception info to vlm via prompt\n    }\n    ```",
    "ContentSha": "+7/+jzWgutLUPHwNsJFluDKDC8gLzDlBHgN0nwn9xTs=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "    ```java\n    {\n        \"TASK_CONFIGS\": {\n            \"FRAME_PER_SEC\": 10 // sensor saving frequency\n        },\n        \"INFERENCE_BASICS\": {\n            \"INPUT_WINDOW\": 1, // frame count of given image input\n            \"CONVERSATION_WINDOW\": 1, // not used anymore, to be removed\n            \"USE_ALL_CAMERAS\": false, // true if use all cameras as input\n            \"USE_BEV\": false, // true if use bev as input\n            \"NO_HISTORY_MODE\": false // do not inherit context of previous VQAs\n        },\n        \"CHAIN\": { // for inference\n            \"NODE\": [19, 15, 7, 24, 13, 47, 8, 43, 50],\n            \"EDGE\": { // \"pred\": succ\n                \"19\": [24, 13, 8],\n                \"15\": [7, 8],\n                \"7\": [8],\n                \"24\": [13, 47],\n                \"13\": [47, 8, 43],\n                \"47\": [8],\n                \"8\": [43],\n                \"43\": [50],\n                \"50\": []\n            },\n            \"INHERIT\": { // inherit context from last frame\n                \"19\": [43, 7],\n                \"15\": [7]\n            },\n            \"USE_GT\": [24] // questions which use ground truth as answer\n        },\n        \"CONTROL_RATE\": 2.0, // intervene freq of vlm\n        \"MODEL_NAME\": \"api\", // model name\n        \"MODEL_PATH\": \"../model_zoo/your_model\", // model path\n        \"GPU_ID\": 0, // the gpu model runs on\n        \"PORT\": 7023, // web port\n        \"IN_CARLA\": true,\n        \"USE_BASE64\": true, // if false, local path is used for transmitting images\n        \"NO_PERC_INFO\": false // do not pass extra perception info to vlm via prompt\n    }\n    ```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 9,
    "Content": "\n2. Write start up script:\n   \n    *If you want a quickstart, you can set MINIMAL=1 to run Bench2Drive-VL without VLM.*\n",
    "ContentSha": "ZznsG46hRUZm2JKjGBa+Y0pZSNK1TV9FEZw1gNqEQ4k=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "2. ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±:\n   \n    *ë¹ ë¥¸ ì‹œì‘ì„ ì›í•œë‹¤ë©´ MINIMAL=1ë¡œ ì„¤ì •í•˜ì—¬ VLM ì—†ì´ Bench2Drive-VLì„ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.*\n\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 10,
    "Content": "    ```bash\n    #!/bin/bash\n    BASE_PORT=20082 # CARLA port\n    BASE_TM_PORT=50000\n    BASE_ROUTES=./leaderboard/data/bench2drive220\n    TEAM_AGENT=leaderboard/team_code/data_agent.py\n    BASE_CHECKPOINT_ENDPOINT=./my_checkpoint\n    SAVE_PATH=./eval_v1/\n    GPU_RANK=0 # the gpu carla runs on\n    VLM_CONFIG=/path/to/your_vlm_config.json\n    PORT=$BASE_PORT\n    TM_PORT=$BASE_TM_PORT\n    ROUTES=\"${BASE_ROUTES}.xml\"\n    CHECKPOINT_ENDPOINT=\"${BASE_CHECKPOINT_ENDPOINT}.json\"\n    export MINIMAL=0 # if MINIMAL > 0, DriveCommenter takes control of the ego vehicle,\n    # and vlm server is not needed\n    bash leaderboard/scripts/run_evaluation.sh $PORT $TM_PORT 1 $ROUTES $TEAM_AGENT \".\" $CHECKPOINT_ENDPOINT $SAVE_PATH \"null\" $GPU_RANK $VLM_CONFIG\n    ```",
    "ContentSha": "Hh7OWTMbeIm2IIhp/LgyV1QJoecRRZeUhothj6gXzyM=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "    ```bash\n    #!/bin/bash\n    BASE_PORT=20082 # CARLA port\n    BASE_TM_PORT=50000\n    BASE_ROUTES=./leaderboard/data/bench2drive220\n    TEAM_AGENT=leaderboard/team_code/data_agent.py\n    BASE_CHECKPOINT_ENDPOINT=./my_checkpoint\n    SAVE_PATH=./eval_v1/\n    GPU_RANK=0 # the gpu carla runs on\n    VLM_CONFIG=/path/to/your_vlm_config.json\n    PORT=$BASE_PORT\n    TM_PORT=$BASE_TM_PORT\n    ROUTES=\"${BASE_ROUTES}.xml\"\n    CHECKPOINT_ENDPOINT=\"${BASE_CHECKPOINT_ENDPOINT}.json\"\n    export MINIMAL=0 # if MINIMAL > 0, DriveCommenter takes control of the ego vehicle,\n    # and vlm server is not needed\n    bash leaderboard/scripts/run_evaluation.sh $PORT $TM_PORT 1 $ROUTES $TEAM_AGENT \".\" $CHECKPOINT_ENDPOINT $SAVE_PATH \"null\" $GPU_RANK $VLM_CONFIG\n    ```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 11,
    "Content": "\n3. Start VLM Server (not needed if `MINIMAL`)\n",
    "ContentSha": "F9trEMGMH5mZDZamx9CkCzAVGyHBsCX1aPbNeFrEwHI=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "3. VLM ì„œë²„ ì‹œì‘ (`MINIMAL`ì¸ ê²½ìš° í•„ìš” ì—†ìŒ)\n\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 12,
    "Content": "    ```shell\n    python ./B2DVL_Adapter/web_interact_app.py --config /path/to/your/vlm_config.json\n    ```",
    "ContentSha": "sVl5FxswCdqDwC+NOqVs3OdvP4xsjQ/92Nb3Bwe9wK8=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "    ```shell\n    python ./B2DVL_Adapter/web_interact_app.py --config /path/to/your/vlm_config.json\n    ```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 13,
    "Content": "\n4. Start Main module\n",
    "ContentSha": "bwnBqb0hoXMWxWrggCkciZdgkZXjCJh0L9ZaX7CrBUs=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "4. ë©”ì¸ ëª¨ë“ˆ ì‹œì‘í•˜ê¸°\n\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 14,
    "Content": "    ```shell\n    bash ./startup.sh\n    ```",
    "ContentSha": "OyfRV5o9BAdYC88FKPQnUu7Rbe3V1uxIRGROXAmTSQo=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "    ```shell\n    bash ./startup.sh\n    ```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 15,
    "Content": "\n### Generate VQAs from static dataset using DriveCommenter\n\n1. Write a startup script under `./B2DVL-Adapter`\n",
    "ContentSha": "83Hsi06TPyDp0g6Gf7a4kmkm633SPSJJvpQeN+VNBT0=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### DriveCommenterë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ì  ë°ì´í„°ì…‹ì—ì„œ VQA ìƒì„±í•˜ê¸°\n\n1. `./B2DVL-Adapter` í•˜ìœ„ì— ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‘ì„±í•˜ì„¸ìš”\n\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 16,
    "Content": "    ```bash\n    #!/bin/bash\n    export SUBSET=0 # generate from a subset of given dataset\n    export STRICT_MODE=1\n    # DriveCommenter drives back the ego vehicle after circumventing obstacles if STRICT_MODE > 0\n    export SUBSET_PATH=./subset_0.txt # subset file\n    export PROCESSED_PATH=./processed_paths_0.txt # checkpoint file\n    export CACHE_PATH=./.worker_0_cache\n    # DriveCommenter supports dataset in .tar.gz\n    # it will unzip some of the dataset temporarily in cache dir\n    python ./drive_commenter_main.py --data-directory=/path/to/Bench2Drive/dataset --output-graph-directory=./outgraph     --path-maps=${CARLA_ROOT}/CarlaUE4/Content/Carla/Maps     --worker-count=1\n    # We do not recommend using multiple worker here since multi-thread in python is not very good.\n    # You can run multiple DriveCommenter at the same time with different subset and checkpoint files to do the same.\n    ```",
    "ContentSha": "IA3ThUNS8fnxwG4mg0tbTylcE5Ln8TifUHS1AczaM98=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "    ```bash\n    #!/bin/bash\n    export SUBSET=0 # generate from a subset of given dataset\n    export STRICT_MODE=1\n    # DriveCommenter drives back the ego vehicle after circumventing obstacles if STRICT_MODE > 0\n    export SUBSET_PATH=./subset_0.txt # subset file\n    export PROCESSED_PATH=./processed_paths_0.txt # checkpoint file\n    export CACHE_PATH=./.worker_0_cache\n    # DriveCommenter supports dataset in .tar.gz\n    # it will unzip some of the dataset temporarily in cache dir\n    python ./drive_commenter_main.py --data-directory=/path/to/Bench2Drive/dataset --output-graph-directory=./outgraph     --path-maps=${CARLA_ROOT}/CarlaUE4/Content/Carla/Maps     --worker-count=1\n    # We do not recommend using multiple worker here since multi-thread in python is not very good.\n    # You can run multiple DriveCommenter at the same time with different subset and checkpoint files to do the same.\n    ```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 17,
    "Content": "\n2. Run it.\n",
    "ContentSha": "FJZ7LBdjDkDeV+4bIdLNyFwABNzOdT2iALEpST5ZWyU=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "",
    "Status": "error",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 18,
    "Content": "    ```shell\n    cd ./B2DVL-Adapter\n    bash ./your_startup_script.sh\n    ```",
    "ContentSha": "I56EVdL95mA77Tv1WMNN+jFS/zg5aps7aRzfqqHU5zE=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "    ```shell\n    cd ./B2DVL-Adapter\n    bash ./your_startup_script.sh\n    ```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 19,
    "Content": "\n### Open-Loop Inference\n\n1. Write a config file.\n",
    "ContentSha": "642Vk68gNoiB8VGufWMnuWv2nlZLmi4sBgiSPb/hL/4=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### ì˜¤í”ˆ ë£¨í”„ ì¶”ë¡ \n\n1. ì„¤ì • íŒŒì¼ì„ ì‘ì„±í•©ë‹ˆë‹¤.\n\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 20,
    "Content": "    ```java\n    {\n        \"TASK_CONFIGS\": {\n            \"INFER_SUBSET\": false, // inference a subset of given dataset\n            \"USE_CHECKPOINT\": true, // record the process of inference\n            \"SUBSET_FILE\": \"./infer_configs/subset.txt\", // subset file, leave blank if not used\n            \"CHECKPOINT_FILE\": \"./infer_configs/finished_scenarios.txt\", // checkpoint file, leave blank if not used\n            \"ENTRY_EXIT_FILE\": \"./infer_configs/entry_exits.json\", // the file which specifies entry and exit point of certain scenario, \n            // you can create a file with \"{}\" as content if do not specify\n            \"FRAME_PER_SEC\": 10 // sensor frame\n        },\n        \"INFERENCE_BASICS\": {\n            \"INPUT_WINDOW\": 1,\n            \"CONVERSATION_WINDOW\": 2,\n            \"USE_ALL_CAMERAS\": true,\n            \"NO_HISTORY_MODE\": false,\n            \"APPEND_QUESTION\": true,\n            \"APPENDIX_FILE\": \"./infer_configs/append_questions.json\" // not used now, to be removed\n        },\n        \"CHAIN\": {\n            \"NODE\": [43, 50],\n            \"EDGE\": {\n                \"43\": [50],\n                \"50\": []\n            },\n            \"INHERIT\": {\n                \"19\": [43, 7],\n                \"15\": [7]\n            },\n            \"USE_GT\": []\n        }\n    }\n    ```",
    "ContentSha": "j/xZUEp0ZNlNG20j1LEXtb79A7WyjDgWPepkcuRpW/M=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "    ```java\n    {\n        \"TASK_CONFIGS\": {\n            \"INFER_SUBSET\": false, // inference a subset of given dataset\n            \"USE_CHECKPOINT\": true, // record the process of inference\n            \"SUBSET_FILE\": \"./infer_configs/subset.txt\", // subset file, leave blank if not used\n            \"CHECKPOINT_FILE\": \"./infer_configs/finished_scenarios.txt\", // checkpoint file, leave blank if not used\n            \"ENTRY_EXIT_FILE\": \"./infer_configs/entry_exits.json\", // the file which specifies entry and exit point of certain scenario, \n            // you can create a file with \"{}\" as content if do not specify\n            \"FRAME_PER_SEC\": 10 // sensor frame\n        },\n        \"INFERENCE_BASICS\": {\n            \"INPUT_WINDOW\": 1,\n            \"CONVERSATION_WINDOW\": 2,\n            \"USE_ALL_CAMERAS\": true,\n            \"NO_HISTORY_MODE\": false,\n            \"APPEND_QUESTION\": true,\n            \"APPENDIX_FILE\": \"./infer_configs/append_questions.json\" // not used now, to be removed\n        },\n        \"CHAIN\": {\n            \"NODE\": [43, 50],\n            \"EDGE\": {\n                \"43\": [50],\n                \"50\": []\n            },\n            \"INHERIT\": {\n                \"19\": [43, 7],\n                \"15\": [7]\n            },\n            \"USE_GT\": []\n        }\n    }\n    ```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 21,
    "Content": "\n2. Start up inference script:\n",
    "ContentSha": "+3pLIwo6LRPgukbsRfC3qQ3fMyzFQbtItEFpHQolUdw=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "2. ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘:\n\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 22,
    "Content": "    ```shell\n    cd ./B2DVL_Adapter\n    python inference.py --model Qwen2.5VL --model_path /path/to/Qwen2.5VL-3B-Instruct --config_dir /path/to/your_infer_config.json --image_dir /path/to/Bench2Drive/dataset --vqa_dir /path/to/vqa/dataset --num_workers 4 --out_dir ./infer_outputs\n    ```",
    "ContentSha": "9wUV1Q7741RCN/PkQymnb7rrPUiga1P9HquDT1qwBUA=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "    ```shell\n    cd ./B2DVL_Adapter\n    python inference.py --model Qwen2.5VL --model_path /path/to/Qwen2.5VL-3B-Instruct --config_dir /path/to/your_infer_config.json --image_dir /path/to/Bench2Drive/dataset --vqa_dir /path/to/vqa/dataset --num_workers 4 --out_dir ./infer_outputs\n    ```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 23,
    "Content": "\n### Evaluation\n\n1. To use your llm api for evaluation, create a `mytoken.py` under `./B2DVL-Adapter`. Take deepseek as an example:\n",
    "ContentSha": "1/kgFA2sMMF8VdtONOQ8Wk2fN+/1nS5U12KNCBcEr1I=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### í‰ê°€\n\n1. í‰ê°€ë¥¼ ìœ„í•´ llm APIë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ `./B2DVL-Adapter` ì•„ë˜ì— `mytoken.py`ë¥¼ ìƒì„±í•˜ì„¸ìš”. deepseekë¥¼ ì˜ˆë¡œ ë“¤ë©´:\n\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 24,
    "Content": "    ```python\n    DEEPSEEK_TOKEN = [\n        \"your-token-1\", # you can set multiple tokens, and they will be used in a round-robin way\n        \"your-token-2\"...\n    ]\n    DEEPSEEK_URL = \"https://api.deepseek.com/v1\"\n    ```",
    "ContentSha": "HYgpL+TSqg6Un3pfNByMbjnQIFvOSr0VzuTc3AZsi+o=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "    ```python\n    DEEPSEEK_TOKEN = [\n        \"your-token-1\", # you can set multiple tokens, and they will be used in a round-robin way\n        \"your-token-2\"...\n    ]\n    DEEPSEEK_URL = \"https://api.deepseek.com/v1\"\n    ```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 25,
    "Content": "\n    Then our script will call this api using `openai` templates.\n\n\n\n2. Write a config file:\n",
    "ContentSha": "BmkTtvsov7Wm4V89omlPyik+k85/OPw/J22yQDN+tQg=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "    ê·¸ëŸ¬ë©´ ìš°ë¦¬ ìŠ¤í¬ë¦½íŠ¸ëŠ” `openai` í…œí”Œë¦¿ì„ ì‚¬ìš©í•˜ì—¬ ì´ APIë¥¼ í˜¸ì¶œí•  ê²ƒì…ë‹ˆë‹¤.\n\n\n\n2. êµ¬ì„± íŒŒì¼ ì‘ì„±:\n\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 26,
    "Content": "    ```java\n    {\n        \"EVAL_SUBSET\": true, // eval a subset of given infer result folder\n        \"USE_CHECKPOINT\": false, // use a file to record evaluation process\n        \"SUBSET_FILE\": \"./eval_configs/subset.txt\", // subset file\n        \"CHECKPOINT_FILE\": \"./eval_configs/finished_scenarios.txt\", // checkpoint file\n        \"INFERENCE_RESULT_DIR\": \"./infer_results\", // path to inference results\n        // when doing closed-loop inference, this dir is ./output/infer_results/model_name+input_mode\n        \"B2D_DIR\": \"/path/to/Bench2Drive/dataset\", // evaluation script uses annotations in b2d,\n        // when doing closed-loop inference, this dir is ./eval_v1(SAVE_PATH you specified)/model_name+input_mode\n        \"ORIGINAL_VQA_DIR\": \"../Carla_Chain_QA/carla_vqa_gen/vqa_dataset/outgraph\",\n        // when doing closed-loop inference, this dir is ./output/vqagen/model_name+input_mode\n        \"FRAME_PER_SEC\": 10, // sensor fps\n        \"LOOK_FUTURE\": false // not used now, to be removed\n    }\n    ```",
    "ContentSha": "iMBseHvYUuIdfpxyFPHtp+8xQpl//WbdT/vKTyQZk3A=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "    ```java\n    {\n        \"EVAL_SUBSET\": true, // eval a subset of given infer result folder\n        \"USE_CHECKPOINT\": false, // use a file to record evaluation process\n        \"SUBSET_FILE\": \"./eval_configs/subset.txt\", // subset file\n        \"CHECKPOINT_FILE\": \"./eval_configs/finished_scenarios.txt\", // checkpoint file\n        \"INFERENCE_RESULT_DIR\": \"./infer_results\", // path to inference results\n        // when doing closed-loop inference, this dir is ./output/infer_results/model_name+input_mode\n        \"B2D_DIR\": \"/path/to/Bench2Drive/dataset\", // evaluation script uses annotations in b2d,\n        // when doing closed-loop inference, this dir is ./eval_v1(SAVE_PATH you specified)/model_name+input_mode\n        \"ORIGINAL_VQA_DIR\": \"../Carla_Chain_QA/carla_vqa_gen/vqa_dataset/outgraph\",\n        // when doing closed-loop inference, this dir is ./output/vqagen/model_name+input_mode\n        \"FRAME_PER_SEC\": 10, // sensor fps\n        \"LOOK_FUTURE\": false // not used now, to be removed\n    }\n    ```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 27,
    "Content": "\n2. Run evaluation script:\n",
    "ContentSha": "T1POERZX5SypMBgWoMDRguFUJHkVnCL5lqTrySCO32c=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "2. í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰:\n\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 28,
    "Content": "    ```shell\n    python eval.py --config_dir ./path/to/eval_config.json --num_workers 4 --out_dir ./eval_outputs\n    ```",
    "ContentSha": "GOO++pasmwmBY5OSSHnasXShTxmnmZ+gySgDkmgQhMk=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "    ```shell\n    python eval.py --config_dir ./path/to/eval_config.json --num_workers 4 --out_dir ./eval_outputs\n    ```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 29,
    "Content": "\n## License\nAll assets and code are under the CC-BY-NC-ND unless specified otherwise.",
    "ContentSha": "987ni+TRN5ZJhYrE4/xZJbh9IL2fYM5BhfoK7BEgbdc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## ë¼ì´ì„ ìŠ¤\nëª¨ë“  ìì‚°ê³¼ ì½”ë“œëŠ” ë³„ë„ë¡œ ëª…ì‹œë˜ì§€ ì•ŠëŠ” í•œ CC-BY-NC-ND ë¼ì´ì„ ìŠ¤ í•˜ì— ìˆìŠµë‹ˆë‹¤.\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  }
]