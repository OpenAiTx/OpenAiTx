[
  {
    "Id": 1,
    "Content": "# ProxyAsLocalModel\n\nProxy remote LLM API as Local model. Especially works for using custom LLM in JetBrains AI Assistant.\n\nPowered by Ktor and kotlinx.serialization. Thanks to their no-reflex features.\n\n## Story of this project\n\nCurrently, JetBrains AI Assistant provides a free plan with very limited quotes. I tried out and my quote ran out quickly.\n\nI already bought other LLM API tokens, such like Gemini and Qwen. So I started to think of using them in AI Assistant. Unfortunately, only local models from LM Studio and Ollama are supported. So I started to work on this proxy application that proxy third party LLM API as LM Studio and Ollama API so that I can use them in my JetBrains IDEs.\n\nThis is Just a simple task, so I started to use the official SDKs as clients and write a simple Ktor server that provides endpoints as LM Studio and Ollama. The problem appears when I try to distribute it as a GraalVM native image. The official Java SDKS uses too many dynamic features, making it hard to compile into a native image, even with a tracing agent. So I decided to implement a simple client of streaming chat completion API by myself with Ktor and kotlinx.serialization which are both no-reflex, functional and DSL styled.\n\nAs you can see, this application is distributed as a fat runnable jar and a GraalVM native image, which makes it cross-platform and fast to start.\n\nThe development of this application gives me confidence in Kotlin/Ktor/kotlinx.serialization. The Kotlin world uses more functional programming and less reflexion, which makes it more suitable for GraalVM native image, with faster startup and less memory usage.\n\n## Currently supported\n\nProxy from: OpenAI, Claude, DashScope(Alibaba Qwen), Gemini, Deepseek, Mistral, SiliconFlow.\n\nProxy as: LM Studio, Ollama.\n\nStreaming chat completion API only.\n",
    "ContentSha": "RqoxS65Q2kmC/NFTNk+2QDY1yLetU7emS9EGMv38jJo=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "# ProxyAsLocalModel\n\nProxy zdalnego API LLM jako model lokalny. Szczególnie przydatny do używania niestandardowego LLM w JetBrains AI Assistant.\n\nWspierany przez Ktor i kotlinx.serialization. Dzięki ich funkcjom bez refleksji.\n\n## Historia tego projektu\n\nObecnie JetBrains AI Assistant oferuje darmowy plan z bardzo ograniczonym limitem zapytań. Przetestowałem go i mój limit szybko się wyczerpał.\n\nKupiłem już tokeny do innych API LLM, takich jak Gemini i Qwen. Zacząłem więc zastanawiać się, jak ich użyć w AI Assistant. Niestety, obsługiwane są tylko modele lokalne z LM Studio i Ollama. Zacząłem więc pracować nad tą aplikacją proxy, która pośredniczy pomiędzy zewnętrznymi API LLM a LM Studio i Ollama, tak abym mógł ich używać w moich IDE JetBrains.\n\nTo po prostu proste zadanie, więc zacząłem korzystać z oficjalnych SDK jako klientów i napisałem prosty serwer Ktor, który udostępnia endpointy zgodne z LM Studio i Ollama. Problem pojawił się, gdy próbowałem rozprowadzać to jako natywny obraz GraalVM. Oficjalne SDK Java używają zbyt wielu dynamicznych funkcji, co utrudnia kompilację do natywnego obrazu, nawet z agentem śledzącym. Zdecydowałem się więc samodzielnie zaimplementować prostego klienta API do strumieniowania czatu z użyciem Ktor i kotlinx.serialization, które są pozbawione refleksji, funkcjonalne i mają styl DSL.\n\nJak widać, aplikacja jest dystrybuowana jako duży, uruchamialny plik jar oraz jako natywny obraz GraalVM, co czyni ją wieloplatformową i szybką w uruchamianiu.\n\nTworzenie tej aplikacji przekonało mnie do Kotlin/Ktor/kotlinx.serialization. Świat Kotlina używa więcej programowania funkcyjnego i mniej refleksji, co sprawia, że lepiej nadaje się do natywnych obrazów GraalVM, zapewniając szybszy start i mniejsze zużycie pamięci.\n\n## Aktualnie obsługiwane\n\nProxy z: OpenAI, Claude, DashScope (Alibaba Qwen), Gemini, Deepseek, Mistral, SiliconFlow.\n\nProxy jako: LM Studio, Ollama.\n\nTylko API do strumieniowego uzupełniania czatu.",
    "Status": "ok"
  },
  {
    "Id": 2,
    "Content": "## How to use\n\nThis application is a proxy server, distributed as a fat runnable jar and a GraalVM native image (Windows x64).\n\nRun the application, and you will see a help message:\n\n```\n2025-05-02 10:43:53 INFO  Help - It looks that you are starting the program for the first time here.\n2025-05-02 10:43:53 INFO  Help - A default config file is created at your_path\\config.yml with schema annotation.\n2025-05-02 10:43:53 INFO  Config - Config file watcher started at your_path\\config.yml\n2025-05-02 10:43:53 INFO  LM Studio Server - LM Studio Server started at 1234\n2025-05-02 10:43:53 INFO  Ollama Server - Ollama Server started at 11434\n2025-05-02 10:43:53 INFO  Model List - Model list loaded with: []\n```\n\nThen you can edit the config file to set up your proxy server.\n\n## Config file\n\nThis config file is automatically hot-reloaded when you change it. Only the influenced parts of the server will be updated.\n\nWhen first generating the config file, it will be created with schema annotations. This will bring completion and check in your editor.\n",
    "ContentSha": "jSta+CN7gfDD+W3lmWAocZcdmJOQZkgX0LDlOBWiy1Y=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## Jak używać\n\nTa aplikacja to serwer proxy, dystrybuowany jako samodzielny plik jar oraz natywny obraz GraalVM (Windows x64).\n\nUruchom aplikację, a zobaczysz wiadomość pomocy:\n\n```\n2025-05-02 10:43:53 INFO  Help - Wygląda na to, że uruchamiasz program po raz pierwszy tutaj.\n2025-05-02 10:43:53 INFO  Help - Domyślny plik konfiguracyjny został utworzony w twoja_ścieżka\\config.yml z adnotacją schematu.\n2025-05-02 10:43:53 INFO  Config - Obserwator pliku konfiguracyjnego uruchomiony na twoja_ścieżka\\config.yml\n2025-05-02 10:43:53 INFO  LM Studio Server - Serwer LM Studio uruchomiony na porcie 1234\n2025-05-02 10:43:53 INFO  Ollama Server - Serwer Ollama uruchomiony na porcie 11434\n2025-05-02 10:43:53 INFO  Model List - Lista modeli załadowana: []\n```\n\nNastępnie możesz edytować plik konfiguracyjny, aby skonfigurować swój serwer proxy.\n\n## Plik konfiguracyjny\n\nTen plik konfiguracyjny jest automatycznie przeładowywany na gorąco po każdej zmianie. Zaktualizowane zostaną tylko te części serwera, które zostały zmienione.\n\nPodczas pierwszego generowania pliku konfiguracyjnego zostanie on utworzony z adnotacjami schematu. Zapewni to podpowiedzi i sprawdzanie w twoim edytorze.",
    "Status": "ok"
  },
  {
    "Id": 3,
    "Content": "## Example config file\n\n```yaml\n# $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json\nlmStudio:\n  port: 1234 # This is default value\n  enabled: true # This is default value\n  host: 0.0.0.0 # This is default value\n  path: /your/path # Will be add before the original endpoints, default value is empty\nollama:\n  port: 11434 # This is default value\n  enabled: true # This is default value\n  host: 0.0.0.0 # This is default value\n  path: /your/path # Will be add before the original endpoints, default value is empty\nclient:\n  socketTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  connectionTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  requestTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  retry: 3 # This is default value\n  delayBeforeRetry: 1000 # This is default value, in milliseconds\n\napiProviders:\n  OpenAI:\n    type: OpenAi\n    baseUrl: https://api.openai.com/v1\n    apiKey: <your_api_key>\n    modelList:\n      - gpt-4o\n  Claude:\n    type: Claude\n    apiKey: <your_api_key>\n    modelList:\n      - claude-3-7-sonnet\n  Qwen:\n    type: DashScope\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - qwen-max\n      - qwen-plus\n      - qwen-turbo\n      - qwen-long\n  DeepSeek:\n    type: DeepSeek\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - deepseek-chat\n      - deepseek-reasoner\n  Mistral:\n    type: Mistral\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - codestral-latest\n      - mistral-large\n  SiliconFlow:\n    type: SiliconFlow\n    apiKey: <your_api_key>\n    modelList:\n      - Qwen/Qwen3-235B-A22B\n      - Pro/deepseek-ai/DeepSeek-V3\n      - THUDM/GLM-4-32B-0414\n  OpenRouter:\n    type: OpenRouter\n    apiKey: <your_api_key>\n    modelList:\n      - openai/gpt-4o\n  Gemini:\n    type: Gemini\n    apiKey: <your_api_key>\n    modelList:\n      - gemini-2.5-flash-preview-04-17\n```",
    "ContentSha": "70Ur2u1pGxwgBRWrkWendSuM8NT3qhqj6cqjS9POUPs=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## Przykładowy plik konfiguracyjny\n\n```yaml\n# $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json\nlmStudio:\n  port: 1234 # To jest wartość domyślna\n  enabled: true # To jest wartość domyślna\n  host: 0.0.0.0 # To jest wartość domyślna\n  path: /your/path # Zostanie dodana przed oryginalnymi endpointami, domyślna wartość to puste\nollama:\n  port: 11434 # To jest wartość domyślna\n  enabled: true # To jest wartość domyślna\n  host: 0.0.0.0 # To jest wartość domyślna\n  path: /your/path # Zostanie dodana przed oryginalnymi endpointami, domyślna wartość to puste\nclient:\n  socketTimeout: 1919810 # Long.MAX_VALUE to wartość domyślna, w milisekundach\n  connectionTimeout: 1919810 # Long.MAX_VALUE to wartość domyślna, w milisekundach\n  requestTimeout: 1919810 # Long.MAX_VALUE to wartość domyślna, w milisekundach\n  retry: 3 # To jest wartość domyślna\n  delayBeforeRetry: 1000 # To jest wartość domyślna, w milisekundach\n\napiProviders:\n  OpenAI:\n    type: OpenAi\n    baseUrl: https://api.openai.com/v1\n    apiKey: <your_api_key>\n    modelList:\n      - gpt-4o\n  Claude:\n    type: Claude\n    apiKey: <your_api_key>\n    modelList:\n      - claude-3-7-sonnet\n  Qwen:\n    type: DashScope\n    apiKey: <your_api_key>\n    modelList: # To jest wartość domyślna\n      - qwen-max\n      - qwen-plus\n      - qwen-turbo\n      - qwen-long\n  DeepSeek:\n    type: DeepSeek\n    apiKey: <your_api_key>\n    modelList: # To jest wartość domyślna\n      - deepseek-chat\n      - deepseek-reasoner\n  Mistral:\n    type: Mistral\n    apiKey: <your_api_key>\n    modelList: # To jest wartość domyślna\n      - codestral-latest\n      - mistral-large\n  SiliconFlow:\n    type: SiliconFlow\n    apiKey: <your_api_key>\n    modelList:\n      - Qwen/Qwen3-235B-A22B\n      - Pro/deepseek-ai/DeepSeek-V3\n      - THUDM/GLM-4-32B-0414\n  OpenRouter:\n    type: OpenRouter\n    apiKey: <your_api_key>\n    modelList:\n      - openai/gpt-4o\n  Gemini:\n    type: Gemini\n    apiKey: <your_api_key>\n    modelList:\n      - gemini-2.5-flash-preview-04-17\n```",
    "Status": "ok"
  }
]