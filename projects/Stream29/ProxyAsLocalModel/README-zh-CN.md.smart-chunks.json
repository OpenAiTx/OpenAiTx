[
  {
    "Id": 1,
    "Content": "# ProxyAsLocalModel\n\nProxy remote LLM API as Local model. Especially works for using custom LLM in JetBrains AI Assistant.\n\nPowered by Ktor and kotlinx.serialization. Thanks to their no-reflex features.\n\n## Story of this project\n\nCurrently, JetBrains AI Assistant provides a free plan with very limited quotes. I tried out and my quote ran out quickly.\n\nI already bought other LLM API tokens, such like Gemini and Qwen. So I started to think of using them in AI Assistant. Unfortunately, only local models from LM Studio and Ollama are supported. So I started to work on this proxy application that proxy third party LLM API as LM Studio and Ollama API so that I can use them in my JetBrains IDEs.\n\nThis is Just a simple task, so I started to use the official SDKs as clients and write a simple Ktor server that provides endpoints as LM Studio and Ollama. The problem appears when I try to distribute it as a GraalVM native image. The official Java SDKS uses too many dynamic features, making it hard to compile into a native image, even with a tracing agent. So I decided to implement a simple client of streaming chat completion API by myself with Ktor and kotlinx.serialization which are both no-reflex, functional and DSL styled.\n\nAs you can see, this application is distributed as a fat runnable jar and a GraalVM native image, which makes it cross-platform and fast to start.\n\nThe development of this application gives me confidence in Kotlin/Ktor/kotlinx.serialization. The Kotlin world uses more functional programming and less reflexion, which makes it more suitable for GraalVM native image, with faster startup and less memory usage.\n\n## Currently supported\n\nProxy from: OpenAI, Claude, DashScope(Alibaba Qwen), Gemini, Deepseek, Mistral, SiliconFlow.\n\nProxy as: LM Studio, Ollama.\n\nStreaming chat completion API only.\n",
    "ContentSha": "RqoxS65Q2kmC/NFTNk+2QDY1yLetU7emS9EGMv38jJo=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "# ProxyAsLocalModel\n\n将远程 LLM API 代理为本地模型。特别适用于在 JetBrains AI Assistant 中使用自定义 LLM。\n\n由 Ktor 和 kotlinx.serialization 提供支持。感谢它们的无反射特性。\n\n## 项目故事\n\n目前，JetBrains AI Assistant 提供的免费套餐配额非常有限。我试用了一下，很快就用完了配额。\n\n我已经购买了其他 LLM API 令牌，例如 Gemini 和 Qwen。所以我开始考虑在 AI Assistant 中使用它们。不幸的是，目前只支持来自 LM Studio 和 Ollama 的本地模型。因此，我开始开发这个代理应用，将第三方 LLM API 代理为 LM Studio 和 Ollama API，这样我就可以在 JetBrains IDE 中使用它们。\n\n这只是一个简单的任务，所以我开始使用官方 SDK 作为客户端，并编写了一个简单的 Ktor 服务器，提供与 LM Studio 和 Ollama 相同的端点。问题出现在我尝试将其分发为 GraalVM 原生镜像时。官方的 Java SDK 使用了太多动态特性，即使使用跟踪代理，也很难编译成原生镜像。所以我决定用 Ktor 和 kotlinx.serialization 自己实现一个简单的流式对话补全 API 客户端，这两者都是无反射、函数式且 DSL 风格的。\n\n如你所见，该应用以 fat 可运行 jar 和 GraalVM 原生镜像的形式分发，使其跨平台且启动迅速。\n\n这个应用的开发让我对 Kotlin/Ktor/kotlinx.serialization 更有信心。Kotlin 生态更多采用函数式编程，较少使用反射，这使得它更适合构建 GraalVM 原生镜像，启动更快，内存占用更少。\n\n## 当前支持\n\n代理来源：OpenAI、Claude、DashScope（阿里巴巴 Qwen）、Gemini、Deepseek、Mistral、SiliconFlow。\n\n代理为：LM Studio、Ollama。\n\n仅支持流式对话补全 API。",
    "Status": "ok"
  },
  {
    "Id": 2,
    "Content": "## How to use\n\nThis application is a proxy server, distributed as a fat runnable jar and a GraalVM native image (Windows x64).\n\nRun the application, and you will see a help message:\n\n```\n2025-05-02 10:43:53 INFO  Help - It looks that you are starting the program for the first time here.\n2025-05-02 10:43:53 INFO  Help - A default config file is created at your_path\\config.yml with schema annotation.\n2025-05-02 10:43:53 INFO  Config - Config file watcher started at your_path\\config.yml\n2025-05-02 10:43:53 INFO  LM Studio Server - LM Studio Server started at 1234\n2025-05-02 10:43:53 INFO  Ollama Server - Ollama Server started at 11434\n2025-05-02 10:43:53 INFO  Model List - Model list loaded with: []\n```\n\nThen you can edit the config file to set up your proxy server.\n\n## Config file\n\nThis config file is automatically hot-reloaded when you change it. Only the influenced parts of the server will be updated.\n\nWhen first generating the config file, it will be created with schema annotations. This will bring completion and check in your editor.\n",
    "ContentSha": "jSta+CN7gfDD+W3lmWAocZcdmJOQZkgX0LDlOBWiy1Y=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## 使用方法\n\n此应用程序是一个代理服务器，以可运行的 fat jar 和 GraalVM 原生镜像（Windows x64）形式分发。\n\n运行应用程序后，您将看到一条帮助信息：\n\n```\n2025-05-02 10:43:53 INFO  Help - 看起来您是第一次在此处启动程序。\n2025-05-02 10:43:53 INFO  Help - 已在 your_path\\config.yml 处创建了带有模式注释的默认配置文件。\n2025-05-02 10:43:53 INFO  Config - 配置文件监听器已启动，路径为 your_path\\config.yml\n2025-05-02 10:43:53 INFO  LM Studio Server - LM Studio 服务器已启动，端口为 1234\n2025-05-02 10:43:53 INFO  Ollama Server - Ollama 服务器已启动，端口为 11434\n2025-05-02 10:43:53 INFO  Model List - 已加载模型列表：[]\n```\n\n然后您可以编辑配置文件来设置您的代理服务器。\n\n## 配置文件\n\n当您更改此配置文件时，系统会自动热重载。只有受影响的服务器部分会被更新。\n\n首次生成配置文件时，将会带有模式注释创建。这将在您的编辑器中提供补全和检查功能。",
    "Status": "ok"
  },
  {
    "Id": 3,
    "Content": "## Example config file\n\n```yaml\n# $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json\nlmStudio:\n  port: 1234 # This is default value\n  enabled: true # This is default value\n  host: 0.0.0.0 # This is default value\n  path: /your/path # Will be add before the original endpoints, default value is empty\nollama:\n  port: 11434 # This is default value\n  enabled: true # This is default value\n  host: 0.0.0.0 # This is default value\n  path: /your/path # Will be add before the original endpoints, default value is empty\nclient:\n  socketTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  connectionTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  requestTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  retry: 3 # This is default value\n  delayBeforeRetry: 1000 # This is default value, in milliseconds\n\napiProviders:\n  OpenAI:\n    type: OpenAi\n    baseUrl: https://api.openai.com/v1\n    apiKey: <your_api_key>\n    modelList:\n      - gpt-4o\n  Claude:\n    type: Claude\n    apiKey: <your_api_key>\n    modelList:\n      - claude-3-7-sonnet\n  Qwen:\n    type: DashScope\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - qwen-max\n      - qwen-plus\n      - qwen-turbo\n      - qwen-long\n  DeepSeek:\n    type: DeepSeek\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - deepseek-chat\n      - deepseek-reasoner\n  Mistral:\n    type: Mistral\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - codestral-latest\n      - mistral-large\n  SiliconFlow:\n    type: SiliconFlow\n    apiKey: <your_api_key>\n    modelList:\n      - Qwen/Qwen3-235B-A22B\n      - Pro/deepseek-ai/DeepSeek-V3\n      - THUDM/GLM-4-32B-0414\n  OpenRouter:\n    type: OpenRouter\n    apiKey: <your_api_key>\n    modelList:\n      - openai/gpt-4o\n  Gemini:\n    type: Gemini\n    apiKey: <your_api_key>\n    modelList:\n      - gemini-2.5-flash-preview-04-17\n```",
    "ContentSha": "70Ur2u1pGxwgBRWrkWendSuM8NT3qhqj6cqjS9POUPs=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## 示例配置文件\n\n```yaml\n# $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json\nlmStudio:\n  port: 1234 # 这是默认值\n  enabled: true # 这是默认值\n  host: 0.0.0.0 # 这是默认值\n  path: /your/path # 将会添加在原始端点前，默认值为空\nollama:\n  port: 11434 # 这是默认值\n  enabled: true # 这是默认值\n  host: 0.0.0.0 # 这是默认值\n  path: /your/path # 将会添加在原始端点前，默认值为空\nclient:\n  socketTimeout: 1919810 # Long.MAX_VALUE 为默认值，单位为毫秒\n  connectionTimeout: 1919810 # Long.MAX_VALUE 为默认值，单位为毫秒\n  requestTimeout: 1919810 # Long.MAX_VALUE 为默认值，单位为毫秒\n  retry: 3 # 这是默认值\n  delayBeforeRetry: 1000 # 这是默认值，单位为毫秒\n\napiProviders:\n  OpenAI:\n    type: OpenAi\n    baseUrl: https://api.openai.com/v1\n    apiKey: <your_api_key>\n    modelList:\n      - gpt-4o\n  Claude:\n    type: Claude\n    apiKey: <your_api_key>\n    modelList:\n      - claude-3-7-sonnet\n  Qwen:\n    type: DashScope\n    apiKey: <your_api_key>\n    modelList: # 这是默认值\n      - qwen-max\n      - qwen-plus\n      - qwen-turbo\n      - qwen-long\n  DeepSeek:\n    type: DeepSeek\n    apiKey: <your_api_key>\n    modelList: # 这是默认值\n      - deepseek-chat\n      - deepseek-reasoner\n  Mistral:\n    type: Mistral\n    apiKey: <your_api_key>\n    modelList: # 这是默认值\n      - codestral-latest\n      - mistral-large\n  SiliconFlow:\n    type: SiliconFlow\n    apiKey: <your_api_key>\n    modelList:\n      - Qwen/Qwen3-235B-A22B\n      - Pro/deepseek-ai/DeepSeek-V3\n      - THUDM/GLM-4-32B-0414\n  OpenRouter:\n    type: OpenRouter\n    apiKey: <your_api_key>\n    modelList:\n      - openai/gpt-4o\n  Gemini:\n    type: Gemini\n    apiKey: <your_api_key>\n    modelList:\n      - gemini-2.5-flash-preview-04-17\n```",
    "Status": "ok"
  }
]