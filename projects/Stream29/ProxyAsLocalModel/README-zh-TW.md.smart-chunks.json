[
  {
    "Id": 1,
    "Content": "# ProxyAsLocalModel\n\nProxy remote LLM API as Local model. Especially works for using custom LLM in JetBrains AI Assistant.\n\nPowered by Ktor and kotlinx.serialization. Thanks to their no-reflex features.\n\n## Story of this project\n\nCurrently, JetBrains AI Assistant provides a free plan with very limited quotes. I tried out and my quote ran out quickly.\n\nI already bought other LLM API tokens, such like Gemini and Qwen. So I started to think of using them in AI Assistant. Unfortunately, only local models from LM Studio and Ollama are supported. So I started to work on this proxy application that proxy third party LLM API as LM Studio and Ollama API so that I can use them in my JetBrains IDEs.\n\nThis is Just a simple task, so I started to use the official SDKs as clients and write a simple Ktor server that provides endpoints as LM Studio and Ollama. The problem appears when I try to distribute it as a GraalVM native image. The official Java SDKS uses too many dynamic features, making it hard to compile into a native image, even with a tracing agent. So I decided to implement a simple client of streaming chat completion API by myself with Ktor and kotlinx.serialization which are both no-reflex, functional and DSL styled.\n\nAs you can see, this application is distributed as a fat runnable jar and a GraalVM native image, which makes it cross-platform and fast to start.\n\nThe development of this application gives me confidence in Kotlin/Ktor/kotlinx.serialization. The Kotlin world uses more functional programming and less reflexion, which makes it more suitable for GraalVM native image, with faster startup and less memory usage.\n\n## Currently supported\n\nProxy from: OpenAI, Claude, DashScope(Alibaba Qwen), Gemini, Deepseek, Mistral, SiliconFlow.\n\nProxy as: LM Studio, Ollama.\n\nStreaming chat completion API only.\n",
    "ContentSha": "RqoxS65Q2kmC/NFTNk+2QDY1yLetU7emS9EGMv38jJo=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "# ProxyAsLocalModel\n\n將遠端 LLM API 代理為本地模型。特別適用於在 JetBrains AI Assistant 中使用自訂 LLM。\n\n由 Ktor 和 kotlinx.serialization 提供技術支持。感謝它們無需反射的特性。\n\n## 本專案的故事\n\n目前，JetBrains AI Assistant 提供的免費方案配額非常有限。我試用了一下，很快就用完了配額。\n\n我已經購買了其他 LLM API 令牌，例如 Gemini 和 Qwen。因此我開始思考如何在 AI Assistant 中使用它們。不幸的是，目前僅支援來自 LM Studio 和 Ollama 的本地模型。因此我開始著手開發這個代理應用程式，將第三方 LLM API 代理為 LM Studio 和 Ollama API，這樣我就能在 JetBrains IDEs 中使用它們。\n\n這本來只是個簡單的任務，所以我開始使用官方 SDK 作為客戶端，並編寫一個簡單的 Ktor 伺服器，提供與 LM Studio 和 Ollama 相同的端點。問題出現在我嘗試將其發佈為 GraalVM 原生映像時。官方 Java SDK 使用了太多動態特性，即使使用追蹤代理，也很難編譯成原生映像。因此，我決定自己用 Ktor 和 kotlinx.serialization（兩者都無需反射、具備函式化和 DSL 風格）實現一個簡單的串流聊天補全 API 客戶端。\n\n如你所見，這個應用程式以可執行的 fat jar 和 GraalVM 原生映像發佈，實現跨平台和快速啟動。\n\n這個應用程式的開發讓我對 Kotlin/Ktor/kotlinx.serialization 更有信心。Kotlin 世界更多採用函式編程，較少使用反射，這讓它更適合用於 GraalVM 原生映像，啟動更快且記憶體佔用更低。\n\n## 目前支援\n\n代理來源：OpenAI、Claude、DashScope（阿里巴巴 Qwen）、Gemini、Deepseek、Mistral、SiliconFlow。\n\n代理為：LM Studio、Ollama。\n\n僅支援串流聊天補全 API。",
    "Status": "ok"
  },
  {
    "Id": 2,
    "Content": "## How to use\n\nThis application is a proxy server, distributed as a fat runnable jar and a GraalVM native image (Windows x64).\n\nRun the application, and you will see a help message:\n\n```\n2025-05-02 10:43:53 INFO  Help - It looks that you are starting the program for the first time here.\n2025-05-02 10:43:53 INFO  Help - A default config file is created at your_path\\config.yml with schema annotation.\n2025-05-02 10:43:53 INFO  Config - Config file watcher started at your_path\\config.yml\n2025-05-02 10:43:53 INFO  LM Studio Server - LM Studio Server started at 1234\n2025-05-02 10:43:53 INFO  Ollama Server - Ollama Server started at 11434\n2025-05-02 10:43:53 INFO  Model List - Model list loaded with: []\n```\n\nThen you can edit the config file to set up your proxy server.\n\n## Config file\n\nThis config file is automatically hot-reloaded when you change it. Only the influenced parts of the server will be updated.\n\nWhen first generating the config file, it will be created with schema annotations. This will bring completion and check in your editor.\n",
    "ContentSha": "jSta+CN7gfDD+W3lmWAocZcdmJOQZkgX0LDlOBWiy1Y=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## 如何使用\n\n此應用程式是一個代理伺服器，以可執行的 fat jar 及 GraalVM 原生映像檔（Windows x64）發佈。\n\n執行應用程式後，您會看到一則說明訊息：\n\n```\n2025-05-02 10:43:53 INFO  Help - 看起來您是第一次在這裡啟動程式。\n2025-05-02 10:43:53 INFO  Help - 已在 your_path\\config.yml 建立了預設設定檔，並附有結構註解。\n2025-05-02 10:43:53 INFO  Config - 設定檔監控已啟動於 your_path\\config.yml\n2025-05-02 10:43:53 INFO  LM Studio Server - LM Studio 伺服器已啟動於 1234\n2025-05-02 10:43:53 INFO  Ollama Server - Ollama 伺服器已啟動於 11434\n2025-05-02 10:43:53 INFO  Model List - 已載入模型清單：[]\n```\n\n然後，您可以編輯設定檔來設置您的代理伺服器。\n\n## 設定檔\n\n當您更改此設定檔時，系統會自動熱重載。只有受影響的伺服器部分會被更新。\n\n首次生成設定檔時，會自動帶有結構註解。這將在您的編輯器中帶來自動補全與檢查功能。",
    "Status": "ok"
  },
  {
    "Id": 3,
    "Content": "## Example config file\n\n```yaml\n# $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json\nlmStudio:\n  port: 1234 # This is default value\n  enabled: true # This is default value\n  host: 0.0.0.0 # This is default value\n  path: /your/path # Will be add before the original endpoints, default value is empty\nollama:\n  port: 11434 # This is default value\n  enabled: true # This is default value\n  host: 0.0.0.0 # This is default value\n  path: /your/path # Will be add before the original endpoints, default value is empty\nclient:\n  socketTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  connectionTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  requestTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  retry: 3 # This is default value\n  delayBeforeRetry: 1000 # This is default value, in milliseconds\n\napiProviders:\n  OpenAI:\n    type: OpenAi\n    baseUrl: https://api.openai.com/v1\n    apiKey: <your_api_key>\n    modelList:\n      - gpt-4o\n  Claude:\n    type: Claude\n    apiKey: <your_api_key>\n    modelList:\n      - claude-3-7-sonnet\n  Qwen:\n    type: DashScope\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - qwen-max\n      - qwen-plus\n      - qwen-turbo\n      - qwen-long\n  DeepSeek:\n    type: DeepSeek\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - deepseek-chat\n      - deepseek-reasoner\n  Mistral:\n    type: Mistral\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - codestral-latest\n      - mistral-large\n  SiliconFlow:\n    type: SiliconFlow\n    apiKey: <your_api_key>\n    modelList:\n      - Qwen/Qwen3-235B-A22B\n      - Pro/deepseek-ai/DeepSeek-V3\n      - THUDM/GLM-4-32B-0414\n  OpenRouter:\n    type: OpenRouter\n    apiKey: <your_api_key>\n    modelList:\n      - openai/gpt-4o\n  Gemini:\n    type: Gemini\n    apiKey: <your_api_key>\n    modelList:\n      - gemini-2.5-flash-preview-04-17\n```",
    "ContentSha": "70Ur2u1pGxwgBRWrkWendSuM8NT3qhqj6cqjS9POUPs=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## 範例設定檔\n\n```yaml\n# $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json\nlmStudio:\n  port: 1234 # 這是預設值\n  enabled: true # 這是預設值\n  host: 0.0.0.0 # 這是預設值\n  path: /your/path # 將會加在原始端點之前，預設值為空\nollama:\n  port: 11434 # 這是預設值\n  enabled: true # 這是預設值\n  host: 0.0.0.0 # 這是預設值\n  path: /your/path # 將會加在原始端點之前，預設值為空\nclient:\n  socketTimeout: 1919810 # 預設值為 Long.MAX_VALUE，單位為毫秒\n  connectionTimeout: 1919810 # 預設值為 Long.MAX_VALUE，單位為毫秒\n  requestTimeout: 1919810 # 預設值為 Long.MAX_VALUE，單位為毫秒\n  retry: 3 # 這是預設值\n  delayBeforeRetry: 1000 # 這是預設值，單位為毫秒\n\napiProviders:\n  OpenAI:\n    type: OpenAi\n    baseUrl: https://api.openai.com/v1\n    apiKey: <your_api_key>\n    modelList:\n      - gpt-4o\n  Claude:\n    type: Claude\n    apiKey: <your_api_key>\n    modelList:\n      - claude-3-7-sonnet\n  Qwen:\n    type: DashScope\n    apiKey: <your_api_key>\n    modelList: # 這是預設值\n      - qwen-max\n      - qwen-plus\n      - qwen-turbo\n      - qwen-long\n  DeepSeek:\n    type: DeepSeek\n    apiKey: <your_api_key>\n    modelList: # 這是預設值\n      - deepseek-chat\n      - deepseek-reasoner\n  Mistral:\n    type: Mistral\n    apiKey: <your_api_key>\n    modelList: # 這是預設值\n      - codestral-latest\n      - mistral-large\n  SiliconFlow:\n    type: SiliconFlow\n    apiKey: <your_api_key>\n    modelList:\n      - Qwen/Qwen3-235B-A22B\n      - Pro/deepseek-ai/DeepSeek-V3\n      - THUDM/GLM-4-32B-0414\n  OpenRouter:\n    type: OpenRouter\n    apiKey: <your_api_key>\n    modelList:\n      - openai/gpt-4o\n  Gemini:\n    type: Gemini\n    apiKey: <your_api_key>\n    modelList:\n      - gemini-2.5-flash-preview-04-17\n```",
    "Status": "ok"
  }
]