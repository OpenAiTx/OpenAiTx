[
  {
    "Id": 1,
    "Content": "# ProxyAsLocalModel\n\nProxy remote LLM API as Local model. Especially works for using custom LLM in JetBrains AI Assistant.\n\nPowered by Ktor and kotlinx.serialization. Thanks to their no-reflex features.\n\n## Story of this project\n\nCurrently, JetBrains AI Assistant provides a free plan with very limited quotes. I tried out and my quote ran out quickly.\n\nI already bought other LLM API tokens, such like Gemini and Qwen. So I started to think of using them in AI Assistant. Unfortunately, only local models from LM Studio and Ollama are supported. So I started to work on this proxy application that proxy third party LLM API as LM Studio and Ollama API so that I can use them in my JetBrains IDEs.\n\nThis is Just a simple task, so I started to use the official SDKs as clients and write a simple Ktor server that provides endpoints as LM Studio and Ollama. The problem appears when I try to distribute it as a GraalVM native image. The official Java SDKS uses too many dynamic features, making it hard to compile into a native image, even with a tracing agent. So I decided to implement a simple client of streaming chat completion API by myself with Ktor and kotlinx.serialization which are both no-reflex, functional and DSL styled.\n\nAs you can see, this application is distributed as a fat runnable jar and a GraalVM native image, which makes it cross-platform and fast to start.\n\nThe development of this application gives me confidence in Kotlin/Ktor/kotlinx.serialization. The Kotlin world uses more functional programming and less reflexion, which makes it more suitable for GraalVM native image, with faster startup and less memory usage.\n\n## Currently supported\n\nProxy from: OpenAI, Claude, DashScope(Alibaba Qwen), Gemini, Deepseek, Mistral, SiliconFlow.\n\nProxy as: LM Studio, Ollama.\n\nStreaming chat completion API only.\n",
    "ContentSha": "RqoxS65Q2kmC/NFTNk+2QDY1yLetU7emS9EGMv38jJo=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "# ProxyAsLocalModel\n\nProxy API LLM distant en tant que modèle local. Fonctionne notamment pour utiliser un LLM personnalisé dans JetBrains AI Assistant.\n\nPropulsé par Ktor et kotlinx.serialization. Merci à leurs fonctionnalités sans réflexion.\n\n## Histoire de ce projet\n\nActuellement, JetBrains AI Assistant propose un plan gratuit avec des quotas très limités. J'ai essayé et mon quota s'est rapidement épuisé.\n\nJ'avais déjà acheté d'autres jetons d'API LLM, tels que Gemini et Qwen. J'ai donc commencé à réfléchir à la façon de les utiliser dans AI Assistant. Malheureusement, seuls les modèles locaux de LM Studio et Ollama sont pris en charge. J'ai donc commencé à travailler sur cette application proxy qui fait l’intermédiaire entre des API LLM tierces et les API de LM Studio et Ollama afin de pouvoir les utiliser dans mes IDE JetBrains.\n\nC'est une tâche assez simple, j'ai donc commencé à utiliser les SDK officiels comme clients et à écrire un serveur Ktor simple qui fournit les endpoints de LM Studio et Ollama. Le problème est apparu lorsque j'ai essayé de le distribuer en tant qu'image native GraalVM. Les SDK Java officiels utilisent trop de fonctionnalités dynamiques, ce qui rend la compilation en image native difficile, même avec un agent de traçage. J'ai donc décidé d’implémenter moi-même un client simple pour l’API de complétion de chat en streaming avec Ktor et kotlinx.serialization, qui sont tous deux sans réflexion, fonctionnels et orientés DSL.\n\nComme vous pouvez le constater, cette application est distribuée sous forme de jar exécutable autonome et d’image native GraalVM, ce qui la rend multiplateforme et rapide au démarrage.\n\nLe développement de cette application m'a donné confiance dans Kotlin/Ktor/kotlinx.serialization. Le monde Kotlin utilise plus de programmation fonctionnelle et moins de réflexion, ce qui le rend plus adapté à l'image native GraalVM, avec un démarrage plus rapide et une consommation mémoire réduite.\n\n## Actuellement pris en charge\n\nProxy depuis : OpenAI, Claude, DashScope (Alibaba Qwen), Gemini, Deepseek, Mistral, SiliconFlow.\n\nProxy en tant que : LM Studio, Ollama.\n\nAPI de complétion de chat en streaming uniquement.",
    "Status": "ok"
  },
  {
    "Id": 2,
    "Content": "## How to use\n\nThis application is a proxy server, distributed as a fat runnable jar and a GraalVM native image (Windows x64).\n\nRun the application, and you will see a help message:\n\n```\n2025-05-02 10:43:53 INFO  Help - It looks that you are starting the program for the first time here.\n2025-05-02 10:43:53 INFO  Help - A default config file is created at your_path\\config.yml with schema annotation.\n2025-05-02 10:43:53 INFO  Config - Config file watcher started at your_path\\config.yml\n2025-05-02 10:43:53 INFO  LM Studio Server - LM Studio Server started at 1234\n2025-05-02 10:43:53 INFO  Ollama Server - Ollama Server started at 11434\n2025-05-02 10:43:53 INFO  Model List - Model list loaded with: []\n```\n\nThen you can edit the config file to set up your proxy server.\n\n## Config file\n\nThis config file is automatically hot-reloaded when you change it. Only the influenced parts of the server will be updated.\n\nWhen first generating the config file, it will be created with schema annotations. This will bring completion and check in your editor.\n",
    "ContentSha": "jSta+CN7gfDD+W3lmWAocZcdmJOQZkgX0LDlOBWiy1Y=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## Comment utiliser\n\nCette application est un serveur proxy, distribuée sous forme de jar exécutable complet et d’une image native GraalVM (Windows x64).\n\nExécutez l’application, et vous verrez un message d’aide :\n\n```\n2025-05-02 10:43:53 INFO  Help - Il semble que vous démarrez le programme pour la première fois ici.\n2025-05-02 10:43:53 INFO  Help - Un fichier de configuration par défaut a été créé à votre_chemin\\config.yml avec annotation de schéma.\n2025-05-02 10:43:53 INFO  Config - Surveillance du fichier de configuration démarrée à votre_chemin\\config.yml\n2025-05-02 10:43:53 INFO  LM Studio Server - Serveur LM Studio démarré sur 1234\n2025-05-02 10:43:53 INFO  Ollama Server - Serveur Ollama démarré sur 11434\n2025-05-02 10:43:53 INFO  Model List - Liste des modèles chargée avec : []\n```\n\nVous pouvez ensuite éditer le fichier de configuration pour configurer votre serveur proxy.\n\n## Fichier de configuration\n\nCe fichier de configuration est automatiquement rechargé à chaud lorsque vous le modifiez. Seules les parties concernées du serveur seront mises à jour.\n\nLors de la première génération du fichier de configuration, il sera créé avec des annotations de schéma. Cela apportera la complétion et la vérification dans votre éditeur.",
    "Status": "ok"
  },
  {
    "Id": 3,
    "Content": "## Example config file\n\n```yaml\n# $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json\nlmStudio:\n  port: 1234 # This is default value\n  enabled: true # This is default value\n  host: 0.0.0.0 # This is default value\n  path: /your/path # Will be add before the original endpoints, default value is empty\nollama:\n  port: 11434 # This is default value\n  enabled: true # This is default value\n  host: 0.0.0.0 # This is default value\n  path: /your/path # Will be add before the original endpoints, default value is empty\nclient:\n  socketTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  connectionTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  requestTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  retry: 3 # This is default value\n  delayBeforeRetry: 1000 # This is default value, in milliseconds\n\napiProviders:\n  OpenAI:\n    type: OpenAi\n    baseUrl: https://api.openai.com/v1\n    apiKey: <your_api_key>\n    modelList:\n      - gpt-4o\n  Claude:\n    type: Claude\n    apiKey: <your_api_key>\n    modelList:\n      - claude-3-7-sonnet\n  Qwen:\n    type: DashScope\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - qwen-max\n      - qwen-plus\n      - qwen-turbo\n      - qwen-long\n  DeepSeek:\n    type: DeepSeek\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - deepseek-chat\n      - deepseek-reasoner\n  Mistral:\n    type: Mistral\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - codestral-latest\n      - mistral-large\n  SiliconFlow:\n    type: SiliconFlow\n    apiKey: <your_api_key>\n    modelList:\n      - Qwen/Qwen3-235B-A22B\n      - Pro/deepseek-ai/DeepSeek-V3\n      - THUDM/GLM-4-32B-0414\n  OpenRouter:\n    type: OpenRouter\n    apiKey: <your_api_key>\n    modelList:\n      - openai/gpt-4o\n  Gemini:\n    type: Gemini\n    apiKey: <your_api_key>\n    modelList:\n      - gemini-2.5-flash-preview-04-17\n```",
    "ContentSha": "70Ur2u1pGxwgBRWrkWendSuM8NT3qhqj6cqjS9POUPs=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## Exemple de fichier de configuration\n\n```yaml\n# $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json\nlmStudio:\n  port: 1234 # Ceci est la valeur par défaut\n  enabled: true # Ceci est la valeur par défaut\n  host: 0.0.0.0 # Ceci est la valeur par défaut\n  path: /your/path # Sera ajouté avant les points de terminaison d'origine, la valeur par défaut est vide\nollama:\n  port: 11434 # Ceci est la valeur par défaut\n  enabled: true # Ceci est la valeur par défaut\n  host: 0.0.0.0 # Ceci est la valeur par défaut\n  path: /your/path # Sera ajouté avant les points de terminaison d'origine, la valeur par défaut est vide\nclient:\n  socketTimeout: 1919810 # Long.MAX_VALUE est la valeur par défaut, en millisecondes\n  connectionTimeout: 1919810 # Long.MAX_VALUE est la valeur par défaut, en millisecondes\n  requestTimeout: 1919810 # Long.MAX_VALUE est la valeur par défaut, en millisecondes\n  retry: 3 # Ceci est la valeur par défaut\n  delayBeforeRetry: 1000 # Ceci est la valeur par défaut, en millisecondes\n\napiProviders:\n  OpenAI:\n    type: OpenAi\n    baseUrl: https://api.openai.com/v1\n    apiKey: <votre_clé_api>\n    modelList:\n      - gpt-4o\n  Claude:\n    type: Claude\n    apiKey: <votre_clé_api>\n    modelList:\n      - claude-3-7-sonnet\n  Qwen:\n    type: DashScope\n    apiKey: <votre_clé_api>\n    modelList: # Ceci est la valeur par défaut\n      - qwen-max\n      - qwen-plus\n      - qwen-turbo\n      - qwen-long\n  DeepSeek:\n    type: DeepSeek\n    apiKey: <votre_clé_api>\n    modelList: # Ceci est la valeur par défaut\n      - deepseek-chat\n      - deepseek-reasoner\n  Mistral:\n    type: Mistral\n    apiKey: <votre_clé_api>\n    modelList: # Ceci est la valeur par défaut\n      - codestral-latest\n      - mistral-large\n  SiliconFlow:\n    type: SiliconFlow\n    apiKey: <votre_clé_api>\n    modelList:\n      - Qwen/Qwen3-235B-A22B\n      - Pro/deepseek-ai/DeepSeek-V3\n      - THUDM/GLM-4-32B-0414\n  OpenRouter:\n    type: OpenRouter\n    apiKey: <votre_clé_api>\n    modelList:\n      - openai/gpt-4o\n  Gemini:\n    type: Gemini\n    apiKey: <votre_clé_api>\n    modelList:\n      - gemini-2.5-flash-preview-04-17\n```",
    "Status": "ok"
  }
]