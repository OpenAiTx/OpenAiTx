[
  {
    "Id": 1,
    "Content": "# ProxyAsLocalModel\n\nProxy remote LLM API as Local model. Especially works for using custom LLM in JetBrains AI Assistant.\n\nPowered by Ktor and kotlinx.serialization. Thanks to their no-reflex features.\n\n## Story of this project\n\nCurrently, JetBrains AI Assistant provides a free plan with very limited quotes. I tried out and my quote ran out quickly.\n\nI already bought other LLM API tokens, such like Gemini and Qwen. So I started to think of using them in AI Assistant. Unfortunately, only local models from LM Studio and Ollama are supported. So I started to work on this proxy application that proxy third party LLM API as LM Studio and Ollama API so that I can use them in my JetBrains IDEs.\n\nThis is Just a simple task, so I started to use the official SDKs as clients and write a simple Ktor server that provides endpoints as LM Studio and Ollama. The problem appears when I try to distribute it as a GraalVM native image. The official Java SDKS uses too many dynamic features, making it hard to compile into a native image, even with a tracing agent. So I decided to implement a simple client of streaming chat completion API by myself with Ktor and kotlinx.serialization which are both no-reflex, functional and DSL styled.\n\nAs you can see, this application is distributed as a fat runnable jar and a GraalVM native image, which makes it cross-platform and fast to start.\n\nThe development of this application gives me confidence in Kotlin/Ktor/kotlinx.serialization. The Kotlin world uses more functional programming and less reflexion, which makes it more suitable for GraalVM native image, with faster startup and less memory usage.\n\n## Currently supported\n\nProxy from: OpenAI, Claude, DashScope(Alibaba Qwen), Gemini, Deepseek, Mistral, SiliconFlow.\n\nProxy as: LM Studio, Ollama.\n\nStreaming chat completion API only.\n",
    "ContentSha": "RqoxS65Q2kmC/NFTNk+2QDY1yLetU7emS9EGMv38jJo=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "# ProxyAsLocalModel\n\nپراکسی API مدل زبان بزرگ (LLM) راه دور به عنوان مدل محلی. به ویژه برای استفاده از LLM سفارشی در JetBrains AI Assistant مناسب است.\n\nبا قدرت گرفته از Ktor و kotlinx.serialization. با تشکر از ویژگی‌های بدون بازتاب آن‌ها.\n\n## داستان این پروژه\n\nدر حال حاضر، JetBrains AI Assistant یک پلن رایگان با سهمیه بسیار محدود ارائه می‌دهد. من امتحان کردم و سهمیه من خیلی زود تمام شد.\n\nمن قبلاً توکن‌های API مدل‌های LLM دیگر، مانند Gemini و Qwen را خریده‌ام. بنابراین شروع به فکر کردن درباره استفاده از آن‌ها در AI Assistant کردم. متاسفانه، فقط مدل‌های محلی از LM Studio و Ollama پشتیبانی می‌شوند. بنابراین شروع به کار روی این برنامه پراکسی کردم که API مدل‌های LLM شخص ثالث را به عنوان APIهای LM Studio و Ollama پراکسی می‌کند تا بتوانم از آن‌ها در IDEهای JetBrains خود استفاده کنم.\n\nاین یک کار ساده بود، بنابراین شروع به استفاده از SDKهای رسمی به عنوان کلاینت و نوشتن یک سرور ساده Ktor کردم که اندپوینت‌هایی مانند LM Studio و Ollama ارائه می‌دهد. مشکل زمانی ظاهر شد که سعی کردم آن را به عنوان یک تصویر بومی GraalVM توزیع کنم. SDKهای رسمی جاوا از ویژگی‌های پویا زیادی استفاده می‌کنند که کامپایل آن به یک تصویر بومی را حتی با عامل رهگیری دشوار می‌کند. بنابراین تصمیم گرفتم یک کلاینت ساده برای API تکمیل چت استریمینگ با Ktor و kotlinx.serialization که هر دو بدون بازتاب، فانکشنال و با سبک DSL هستند، پیاده‌سازی کنم.\n\nهمانطور که می‌بینید، این برنامه به صورت یک jar اجرایی fat و یک تصویر بومی GraalVM توزیع می‌شود که آن را چندسکویی و سریع‌الاجرا می‌کند.\n\nتوسعه این برنامه به من اعتماد به نفس در Kotlin/Ktor/kotlinx.serialization داده است. دنیای Kotlin بیشتر از برنامه‌نویسی تابعی و کمتر از بازتاب استفاده می‌کند، که آن را برای تصویر بومی GraalVM مناسب‌تر می‌کند، با راه‌اندازی سریع‌تر و مصرف حافظه کمتر.\n\n## پشتیبانی فعلی\n\nپراکسی از: OpenAI، Claude، DashScope (Alibaba Qwen)، Gemini، Deepseek، Mistral، SiliconFlow.\n\nپراکسی به عنوان: LM Studio، Ollama.\n\nفقط API تکمیل چت به صورت استریمینگ.",
    "Status": "ok"
  },
  {
    "Id": 2,
    "Content": "## How to use\n\nThis application is a proxy server, distributed as a fat runnable jar and a GraalVM native image (Windows x64).\n\nRun the application, and you will see a help message:\n\n```\n2025-05-02 10:43:53 INFO  Help - It looks that you are starting the program for the first time here.\n2025-05-02 10:43:53 INFO  Help - A default config file is created at your_path\\config.yml with schema annotation.\n2025-05-02 10:43:53 INFO  Config - Config file watcher started at your_path\\config.yml\n2025-05-02 10:43:53 INFO  LM Studio Server - LM Studio Server started at 1234\n2025-05-02 10:43:53 INFO  Ollama Server - Ollama Server started at 11434\n2025-05-02 10:43:53 INFO  Model List - Model list loaded with: []\n```\n\nThen you can edit the config file to set up your proxy server.\n\n## Config file\n\nThis config file is automatically hot-reloaded when you change it. Only the influenced parts of the server will be updated.\n\nWhen first generating the config file, it will be created with schema annotations. This will bring completion and check in your editor.\n",
    "ContentSha": "jSta+CN7gfDD+W3lmWAocZcdmJOQZkgX0LDlOBWiy1Y=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## نحوه استفاده\n\nاین برنامه یک سرور پروکسی است که به صورت یک فایل jar اجرایی (fat runnable jar) و یک ایمیج بومی GraalVM (ویندوز x64) توزیع شده است.\n\nبرنامه را اجرا کنید، و یک پیام راهنما مشاهده خواهید کرد:\n\n```\n2025-05-02 10:43:53 INFO  Help - به نظر می‌رسد که برای اولین بار برنامه را در اینجا اجرا می‌کنید.\n2025-05-02 10:43:53 INFO  Help - یک فایل پیکربندی پیش‌فرض در your_path\\config.yml با توضیحات طرح‌واره ایجاد شد.\n2025-05-02 10:43:53 INFO  Config - پایشگر فایل پیکربندی در your_path\\config.yml آغاز شد\n2025-05-02 10:43:53 INFO  LM Studio Server - سرور LM Studio در 1234 راه‌اندازی شد\n2025-05-02 10:43:53 INFO  Ollama Server - سرور Ollama در 11434 راه‌اندازی شد\n2025-05-02 10:43:53 INFO  Model List - لیست مدل با: [] بارگذاری شد\n```\n\nسپس می‌توانید فایل پیکربندی را برای راه‌اندازی سرور پروکسی خود ویرایش کنید.\n\n## فایل پیکربندی\n\nاین فایل پیکربندی به طور خودکار هنگام تغییر، به صورت داغ (hot-reload) بارگذاری مجدد می‌شود. فقط بخش‌های تأثیرپذیر سرور به‌روزرسانی خواهند شد.\n\nهنگام تولید اولیه فایل پیکربندی، این فایل با توضیحات طرح‌واره ایجاد خواهد شد. این کار تکمیل خودکار و بررسی در ویرایشگر شما را به همراه دارد.",
    "Status": "ok"
  },
  {
    "Id": 3,
    "Content": "## Example config file\n\n```yaml\n# $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json\nlmStudio:\n  port: 1234 # This is default value\n  enabled: true # This is default value\n  host: 0.0.0.0 # This is default value\n  path: /your/path # Will be add before the original endpoints, default value is empty\nollama:\n  port: 11434 # This is default value\n  enabled: true # This is default value\n  host: 0.0.0.0 # This is default value\n  path: /your/path # Will be add before the original endpoints, default value is empty\nclient:\n  socketTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  connectionTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  requestTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  retry: 3 # This is default value\n  delayBeforeRetry: 1000 # This is default value, in milliseconds\n\napiProviders:\n  OpenAI:\n    type: OpenAi\n    baseUrl: https://api.openai.com/v1\n    apiKey: <your_api_key>\n    modelList:\n      - gpt-4o\n  Claude:\n    type: Claude\n    apiKey: <your_api_key>\n    modelList:\n      - claude-3-7-sonnet\n  Qwen:\n    type: DashScope\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - qwen-max\n      - qwen-plus\n      - qwen-turbo\n      - qwen-long\n  DeepSeek:\n    type: DeepSeek\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - deepseek-chat\n      - deepseek-reasoner\n  Mistral:\n    type: Mistral\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - codestral-latest\n      - mistral-large\n  SiliconFlow:\n    type: SiliconFlow\n    apiKey: <your_api_key>\n    modelList:\n      - Qwen/Qwen3-235B-A22B\n      - Pro/deepseek-ai/DeepSeek-V3\n      - THUDM/GLM-4-32B-0414\n  OpenRouter:\n    type: OpenRouter\n    apiKey: <your_api_key>\n    modelList:\n      - openai/gpt-4o\n  Gemini:\n    type: Gemini\n    apiKey: <your_api_key>\n    modelList:\n      - gemini-2.5-flash-preview-04-17\n```",
    "ContentSha": "70Ur2u1pGxwgBRWrkWendSuM8NT3qhqj6cqjS9POUPs=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## نمونه فایل پیکربندی\n\n```yaml\n# $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json\nlmStudio:\n  port: 1234 # این مقدار پیش‌فرض است\n  enabled: true # این مقدار پیش‌فرض است\n  host: 0.0.0.0 # این مقدار پیش‌فرض است\n  path: /your/path # قبل از نقاط پایانی اصلی اضافه می‌شود، مقدار پیش‌فرض خالی است\nollama:\n  port: 11434 # این مقدار پیش‌فرض است\n  enabled: true # این مقدار پیش‌فرض است\n  host: 0.0.0.0 # این مقدار پیش‌فرض است\n  path: /your/path # قبل از نقاط پایانی اصلی اضافه می‌شود، مقدار پیش‌فرض خالی است\nclient:\n  socketTimeout: 1919810 # Long.MAX_VALUE مقدار پیش‌فرض است، بر حسب میلی‌ثانیه\n  connectionTimeout: 1919810 # Long.MAX_VALUE مقدار پیش‌فرض است، بر حسب میلی‌ثانیه\n  requestTimeout: 1919810 # Long.MAX_VALUE مقدار پیش‌فرض است، بر حسب میلی‌ثانیه\n  retry: 3 # این مقدار پیش‌فرض است\n  delayBeforeRetry: 1000 # این مقدار پیش‌فرض است، بر حسب میلی‌ثانیه\n\napiProviders:\n  OpenAI:\n    type: OpenAi\n    baseUrl: https://api.openai.com/v1\n    apiKey: <your_api_key>\n    modelList:\n      - gpt-4o\n  Claude:\n    type: Claude\n    apiKey: <your_api_key>\n    modelList:\n      - claude-3-7-sonnet\n  Qwen:\n    type: DashScope\n    apiKey: <your_api_key>\n    modelList: # این مقدار پیش‌فرض است\n      - qwen-max\n      - qwen-plus\n      - qwen-turbo\n      - qwen-long\n  DeepSeek:\n    type: DeepSeek\n    apiKey: <your_api_key>\n    modelList: # این مقدار پیش‌فرض است\n      - deepseek-chat\n      - deepseek-reasoner\n  Mistral:\n    type: Mistral\n    apiKey: <your_api_key>\n    modelList: # این مقدار پیش‌فرض است\n      - codestral-latest\n      - mistral-large\n  SiliconFlow:\n    type: SiliconFlow\n    apiKey: <your_api_key>\n    modelList:\n      - Qwen/Qwen3-235B-A22B\n      - Pro/deepseek-ai/DeepSeek-V3\n      - THUDM/GLM-4-32B-0414\n  OpenRouter:\n    type: OpenRouter\n    apiKey: <your_api_key>\n    modelList:\n      - openai/gpt-4o\n  Gemini:\n    type: Gemini\n    apiKey: <your_api_key>\n    modelList:\n      - gemini-2.5-flash-preview-04-17\n```",
    "Status": "ok"
  }
]