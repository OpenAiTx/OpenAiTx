[
  {
    "Id": 1,
    "Content": "# ProxyAsLocalModel\n\nProxy remote LLM API as Local model. Especially works for using custom LLM in JetBrains AI Assistant.\n\nPowered by Ktor and kotlinx.serialization. Thanks to their no-reflex features.\n\n## Story of this project\n\nCurrently, JetBrains AI Assistant provides a free plan with very limited quotes. I tried out and my quote ran out quickly.\n\nI already bought other LLM API tokens, such like Gemini and Qwen. So I started to think of using them in AI Assistant. Unfortunately, only local models from LM Studio and Ollama are supported. So I started to work on this proxy application that proxy third party LLM API as LM Studio and Ollama API so that I can use them in my JetBrains IDEs.\n\nThis is Just a simple task, so I started to use the official SDKs as clients and write a simple Ktor server that provides endpoints as LM Studio and Ollama. The problem appears when I try to distribute it as a GraalVM native image. The official Java SDKS uses too many dynamic features, making it hard to compile into a native image, even with a tracing agent. So I decided to implement a simple client of streaming chat completion API by myself with Ktor and kotlinx.serialization which are both no-reflex, functional and DSL styled.\n\nAs you can see, this application is distributed as a fat runnable jar and a GraalVM native image, which makes it cross-platform and fast to start.\n\nThe development of this application gives me confidence in Kotlin/Ktor/kotlinx.serialization. The Kotlin world uses more functional programming and less reflexion, which makes it more suitable for GraalVM native image, with faster startup and less memory usage.\n\n## Currently supported\n\nProxy from: OpenAI, Claude, DashScope(Alibaba Qwen), Gemini, Deepseek, Mistral, SiliconFlow.\n\nProxy as: LM Studio, Ollama.\n\nStreaming chat completion API only.\n",
    "ContentSha": "RqoxS65Q2kmC/NFTNk+2QDY1yLetU7emS9EGMv38jJo=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "# ProxyAsLocalModel\n\nProxy API LLM từ xa như một mô hình cục bộ. Đặc biệt hoạt động hiệu quả khi sử dụng LLM tùy chỉnh trong JetBrains AI Assistant.\n\nĐược phát triển bởi Ktor và kotlinx.serialization. Cảm ơn các tính năng không sử dụng phản xạ của chúng.\n\n## Câu chuyện về dự án này\n\nHiện tại, JetBrains AI Assistant cung cấp một gói miễn phí với hạn mức rất hạn chế. Tôi đã thử sử dụng và hạn mức của tôi nhanh chóng hết.\n\nTôi đã mua các token API LLM khác, như Gemini và Qwen. Vì vậy, tôi bắt đầu nghĩ đến việc sử dụng chúng trong AI Assistant. Thật không may, chỉ có các mô hình cục bộ từ LM Studio và Ollama được hỗ trợ. Vì vậy, tôi bắt đầu phát triển ứng dụng proxy này, cho phép proxy API LLM bên thứ ba thành API của LM Studio và Ollama để tôi có thể sử dụng chúng trong các IDE JetBrains của mình.\n\nĐây chỉ là một tác vụ đơn giản, vì vậy tôi bắt đầu sử dụng các SDK chính thức như các client và viết một server Ktor đơn giản cung cấp các endpoint như LM Studio và Ollama. Vấn đề xuất hiện khi tôi cố gắng phân phối nó dưới dạng native image của GraalVM. Các SDK Java chính thức sử dụng quá nhiều tính năng động, khiến cho việc biên dịch thành native image trở nên khó khăn, ngay cả khi sử dụng tracing agent. Vì vậy, tôi quyết định tự mình triển khai một client đơn giản cho API streaming chat completion bằng Ktor và kotlinx.serialization, cả hai đều không sử dụng phản xạ, có tính chức năng và kiểu DSL.\n\nNhư bạn có thể thấy, ứng dụng này được phân phối dưới dạng một fat runnable jar và native image GraalVM, giúp nó đa nền tảng và khởi động nhanh.\n\nViệc phát triển ứng dụng này giúp tôi tự tin hơn với Kotlin/Ktor/kotlinx.serialization. Thế giới Kotlin sử dụng nhiều lập trình hàm hơn và ít phản xạ hơn, điều này khiến nó phù hợp hơn với native image của GraalVM, với thời gian khởi động nhanh hơn và sử dụng bộ nhớ ít hơn.\n\n## Hiện tại hỗ trợ\n\nProxy từ: OpenAI, Claude, DashScope (Alibaba Qwen), Gemini, Deepseek, Mistral, SiliconFlow.\n\nProxy thành: LM Studio, Ollama.\n\nChỉ hỗ trợ API hoàn thành chat dạng streaming.",
    "Status": "ok"
  },
  {
    "Id": 2,
    "Content": "## How to use\n\nThis application is a proxy server, distributed as a fat runnable jar and a GraalVM native image (Windows x64).\n\nRun the application, and you will see a help message:\n\n```\n2025-05-02 10:43:53 INFO  Help - It looks that you are starting the program for the first time here.\n2025-05-02 10:43:53 INFO  Help - A default config file is created at your_path\\config.yml with schema annotation.\n2025-05-02 10:43:53 INFO  Config - Config file watcher started at your_path\\config.yml\n2025-05-02 10:43:53 INFO  LM Studio Server - LM Studio Server started at 1234\n2025-05-02 10:43:53 INFO  Ollama Server - Ollama Server started at 11434\n2025-05-02 10:43:53 INFO  Model List - Model list loaded with: []\n```\n\nThen you can edit the config file to set up your proxy server.\n\n## Config file\n\nThis config file is automatically hot-reloaded when you change it. Only the influenced parts of the server will be updated.\n\nWhen first generating the config file, it will be created with schema annotations. This will bring completion and check in your editor.\n",
    "ContentSha": "jSta+CN7gfDD+W3lmWAocZcdmJOQZkgX0LDlOBWiy1Y=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## Cách sử dụng\n\nỨng dụng này là một máy chủ proxy, được phân phối dưới dạng một file jar chạy độc lập và một image native GraalVM (Windows x64).\n\nChạy ứng dụng, bạn sẽ thấy một thông báo trợ giúp:\n\n```\n2025-05-02 10:43:53 INFO  Help - Có vẻ như bạn đang khởi động chương trình lần đầu tiên tại đây.\n2025-05-02 10:43:53 INFO  Help - Một file cấu hình mặc định đã được tạo tại your_path\\config.yml với chú thích schema.\n2025-05-02 10:43:53 INFO  Config - Bộ theo dõi file cấu hình đã bắt đầu tại your_path\\config.yml\n2025-05-02 10:43:53 INFO  LM Studio Server - Máy chủ LM Studio đã khởi động tại 1234\n2025-05-02 10:43:53 INFO  Ollama Server - Máy chủ Ollama đã khởi động tại 11434\n2025-05-02 10:43:53 INFO  Model List - Danh sách mô hình đã được tải với: []\n```\n\nSau đó, bạn có thể chỉnh sửa file cấu hình để thiết lập máy chủ proxy của mình.\n\n## File cấu hình\n\nFile cấu hình này sẽ tự động được tải lại khi bạn thay đổi nó. Chỉ những phần bị ảnh hưởng của máy chủ mới được cập nhật.\n\nKhi lần đầu tạo file cấu hình, nó sẽ được tạo với chú thích schema. Điều này sẽ giúp bạn hoàn thành và kiểm tra trong trình soạn thảo của mình.",
    "Status": "ok"
  },
  {
    "Id": 3,
    "Content": "## Example config file\n\n```yaml\n# $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json\nlmStudio:\n  port: 1234 # This is default value\n  enabled: true # This is default value\n  host: 0.0.0.0 # This is default value\n  path: /your/path # Will be add before the original endpoints, default value is empty\nollama:\n  port: 11434 # This is default value\n  enabled: true # This is default value\n  host: 0.0.0.0 # This is default value\n  path: /your/path # Will be add before the original endpoints, default value is empty\nclient:\n  socketTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  connectionTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  requestTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  retry: 3 # This is default value\n  delayBeforeRetry: 1000 # This is default value, in milliseconds\n\napiProviders:\n  OpenAI:\n    type: OpenAi\n    baseUrl: https://api.openai.com/v1\n    apiKey: <your_api_key>\n    modelList:\n      - gpt-4o\n  Claude:\n    type: Claude\n    apiKey: <your_api_key>\n    modelList:\n      - claude-3-7-sonnet\n  Qwen:\n    type: DashScope\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - qwen-max\n      - qwen-plus\n      - qwen-turbo\n      - qwen-long\n  DeepSeek:\n    type: DeepSeek\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - deepseek-chat\n      - deepseek-reasoner\n  Mistral:\n    type: Mistral\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - codestral-latest\n      - mistral-large\n  SiliconFlow:\n    type: SiliconFlow\n    apiKey: <your_api_key>\n    modelList:\n      - Qwen/Qwen3-235B-A22B\n      - Pro/deepseek-ai/DeepSeek-V3\n      - THUDM/GLM-4-32B-0414\n  OpenRouter:\n    type: OpenRouter\n    apiKey: <your_api_key>\n    modelList:\n      - openai/gpt-4o\n  Gemini:\n    type: Gemini\n    apiKey: <your_api_key>\n    modelList:\n      - gemini-2.5-flash-preview-04-17\n```",
    "ContentSha": "70Ur2u1pGxwgBRWrkWendSuM8NT3qhqj6cqjS9POUPs=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## Ví dụ tập tin cấu hình\n\n```yaml\n# $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json\nlmStudio:\n  port: 1234 # Đây là giá trị mặc định\n  enabled: true # Đây là giá trị mặc định\n  host: 0.0.0.0 # Đây là giá trị mặc định\n  path: /your/path # Sẽ được thêm trước các endpoint gốc, giá trị mặc định là rỗng\nollama:\n  port: 11434 # Đây là giá trị mặc định\n  enabled: true # Đây là giá trị mặc định\n  host: 0.0.0.0 # Đây là giá trị mặc định\n  path: /your/path # Sẽ được thêm trước các endpoint gốc, giá trị mặc định là rỗng\nclient:\n  socketTimeout: 1919810 # Long.MAX_VALUE là giá trị mặc định, tính bằng mili giây\n  connectionTimeout: 1919810 # Long.MAX_VALUE là giá trị mặc định, tính bằng mili giây\n  requestTimeout: 1919810 # Long.MAX_VALUE là giá trị mặc định, tính bằng mili giây\n  retry: 3 # Đây là giá trị mặc định\n  delayBeforeRetry: 1000 # Đây là giá trị mặc định, tính bằng mili giây\n\napiProviders:\n  OpenAI:\n    type: OpenAi\n    baseUrl: https://api.openai.com/v1\n    apiKey: <your_api_key>\n    modelList:\n      - gpt-4o\n  Claude:\n    type: Claude\n    apiKey: <your_api_key>\n    modelList:\n      - claude-3-7-sonnet\n  Qwen:\n    type: DashScope\n    apiKey: <your_api_key>\n    modelList: # Đây là giá trị mặc định\n      - qwen-max\n      - qwen-plus\n      - qwen-turbo\n      - qwen-long\n  DeepSeek:\n    type: DeepSeek\n    apiKey: <your_api_key>\n    modelList: # Đây là giá trị mặc định\n      - deepseek-chat\n      - deepseek-reasoner\n  Mistral:\n    type: Mistral\n    apiKey: <your_api_key>\n    modelList: # Đây là giá trị mặc định\n      - codestral-latest\n      - mistral-large\n  SiliconFlow:\n    type: SiliconFlow\n    apiKey: <your_api_key>\n    modelList:\n      - Qwen/Qwen3-235B-A22B\n      - Pro/deepseek-ai/DeepSeek-V3\n      - THUDM/GLM-4-32B-0414\n  OpenRouter:\n    type: OpenRouter\n    apiKey: <your_api_key>\n    modelList:\n      - openai/gpt-4o\n  Gemini:\n    type: Gemini\n    apiKey: <your_api_key>\n    modelList:\n      - gemini-2.5-flash-preview-04-17\n```",
    "Status": "ok"
  }
]