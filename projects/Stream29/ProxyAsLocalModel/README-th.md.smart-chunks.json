[
  {
    "Id": 1,
    "Content": "# ProxyAsLocalModel\n\nProxy remote LLM API as Local model. Especially works for using custom LLM in JetBrains AI Assistant.\n\nPowered by Ktor and kotlinx.serialization. Thanks to their no-reflex features.\n\n## Story of this project\n\nCurrently, JetBrains AI Assistant provides a free plan with very limited quotes. I tried out and my quote ran out quickly.\n\nI already bought other LLM API tokens, such like Gemini and Qwen. So I started to think of using them in AI Assistant. Unfortunately, only local models from LM Studio and Ollama are supported. So I started to work on this proxy application that proxy third party LLM API as LM Studio and Ollama API so that I can use them in my JetBrains IDEs.\n\nThis is Just a simple task, so I started to use the official SDKs as clients and write a simple Ktor server that provides endpoints as LM Studio and Ollama. The problem appears when I try to distribute it as a GraalVM native image. The official Java SDKS uses too many dynamic features, making it hard to compile into a native image, even with a tracing agent. So I decided to implement a simple client of streaming chat completion API by myself with Ktor and kotlinx.serialization which are both no-reflex, functional and DSL styled.\n\nAs you can see, this application is distributed as a fat runnable jar and a GraalVM native image, which makes it cross-platform and fast to start.\n\nThe development of this application gives me confidence in Kotlin/Ktor/kotlinx.serialization. The Kotlin world uses more functional programming and less reflexion, which makes it more suitable for GraalVM native image, with faster startup and less memory usage.\n\n## Currently supported\n\nProxy from: OpenAI, Claude, DashScope(Alibaba Qwen), Gemini, Deepseek, Mistral, SiliconFlow.\n\nProxy as: LM Studio, Ollama.\n\nStreaming chat completion API only.\n",
    "ContentSha": "RqoxS65Q2kmC/NFTNk+2QDY1yLetU7emS9EGMv38jJo=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "# ProxyAsLocalModel\n\nพร็อกซี่ LLM API ระยะไกลเป็นโมเดลโลคัล ใช้งานได้ดีโดยเฉพาะสำหรับการใช้ LLM แบบกำหนดเองใน JetBrains AI Assistant\n\nขับเคลื่อนด้วย Ktor และ kotlinx.serialization ขอบคุณฟีเจอร์ที่ไม่ใช้รีเฟล็กซ์ของพวกเขา\n\n## เรื่องราวของโปรเจกต์นี้\n\nปัจจุบัน JetBrains AI Assistant ให้บริการแผนฟรีแต่มีโควตาจำกัดมาก ฉันลองใช้งานและโควตาของฉันหมดอย่างรวดเร็ว\n\nฉันซื้อโทเค็น LLM API อื่นๆ ไว้อยู่แล้ว เช่น Gemini และ Qwen ดังนั้นฉันจึงเริ่มคิดที่จะใช้มันใน AI Assistant น่าเสียดายที่รองรับเฉพาะโมเดลโลคัลจาก LM Studio และ Ollama เท่านั้น ฉันจึงเริ่มพัฒนาแอปพลิเคชันพร็อกซี่นี้เพื่อทำหน้าที่เป็นพร็อกซี่ให้กับ LLM API ของบุคคลที่สามให้เหมือนกับ LM Studio และ Ollama API เพื่อให้สามารถใช้งานใน JetBrains IDEs ของฉันได้\n\nนี่เป็นงานที่ง่ายมาก ฉันจึงเริ่มต้นด้วยการใช้ SDK อย่างเป็นทางการเป็นไคลเอนต์และเขียน Ktor เซิร์ฟเวอร์ง่ายๆ ที่ให้บริการ endpoint เหมือนกับ LM Studio และ Ollama ปัญหาเกิดขึ้นเมื่อฉันพยายามแจกจ่ายแอปนี้เป็น GraalVM native image SDK Java อย่างเป็นทางการใช้ฟีเจอร์ไดนามิกมากเกินไป ทำให้คอมไพล์เป็น native image ได้ยาก แม้จะใช้ tracing agent ก็ตาม ดังนั้นฉันจึงตัดสินใจพัฒนาไคลเอนต์แบบง่ายสำหรับ streaming chat completion API ด้วยตัวเองโดยใช้ Ktor และ kotlinx.serialization ซึ่งทั้งสองอย่างนี้ไม่ใช้รีเฟล็กซ์ ใช้งานแบบ functional และ DSL\n\nดังที่เห็น แอปพลิเคชันนี้แจกจ่ายเป็น fat runnable jar และ GraalVM native image ซึ่งทำให้ข้ามแพลตฟอร์มและเริ่มต้นได้รวดเร็ว\n\nการพัฒนาแอปพลิเคชันนี้ทำให้ฉันมั่นใจใน Kotlin/Ktor/kotlinx.serialization โลกของ Kotlin ใช้ functional programming มากกว่า และใช้รีเฟล็กชันน้อยกว่า ซึ่งทำให้เหมาะกับ GraalVM native image มากกว่า เริ่มต้นได้เร็วและใช้หน่วยความจำน้อยกว่า\n\n## รองรับในปัจจุบัน\n\nพร็อกซี่จาก: OpenAI, Claude, DashScope(Alibaba Qwen), Gemini, Deepseek, Mistral, SiliconFlow\n\nพร็อกซี่เป็น: LM Studio, Ollama\n\nรองรับเฉพาะ Streaming chat completion API",
    "Status": "ok"
  },
  {
    "Id": 2,
    "Content": "## How to use\n\nThis application is a proxy server, distributed as a fat runnable jar and a GraalVM native image (Windows x64).\n\nRun the application, and you will see a help message:\n\n```\n2025-05-02 10:43:53 INFO  Help - It looks that you are starting the program for the first time here.\n2025-05-02 10:43:53 INFO  Help - A default config file is created at your_path\\config.yml with schema annotation.\n2025-05-02 10:43:53 INFO  Config - Config file watcher started at your_path\\config.yml\n2025-05-02 10:43:53 INFO  LM Studio Server - LM Studio Server started at 1234\n2025-05-02 10:43:53 INFO  Ollama Server - Ollama Server started at 11434\n2025-05-02 10:43:53 INFO  Model List - Model list loaded with: []\n```\n\nThen you can edit the config file to set up your proxy server.\n\n## Config file\n\nThis config file is automatically hot-reloaded when you change it. Only the influenced parts of the server will be updated.\n\nWhen first generating the config file, it will be created with schema annotations. This will bring completion and check in your editor.\n",
    "ContentSha": "jSta+CN7gfDD+W3lmWAocZcdmJOQZkgX0LDlOBWiy1Y=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## วิธีการใช้งาน\n\nแอปพลิเคชันนี้เป็น proxy server ซึ่งแจกจ่ายในรูปแบบ fat runnable jar และ GraalVM native image (Windows x64)\n\nเมื่อรันแอปพลิเคชัน คุณจะเห็นข้อความช่วยเหลือดังนี้:\n\n```\n2025-05-02 10:43:53 INFO  Help - ดูเหมือนว่าคุณกำลังเริ่มโปรแกรมนี้เป็นครั้งแรกที่นี่\n2025-05-02 10:43:53 INFO  Help - ไฟล์ config เริ่มต้นถูกสร้างขึ้นที่ your_path\\config.yml พร้อมคำอธิบาย schema\n2025-05-02 10:43:53 INFO  Config - ตัวเฝ้าดูไฟล์ config เริ่มทำงานที่ your_path\\config.yml\n2025-05-02 10:43:53 INFO  LM Studio Server - LM Studio Server เริ่มทำงานที่ 1234\n2025-05-02 10:43:53 INFO  Ollama Server - Ollama Server เริ่มทำงานที่ 11434\n2025-05-02 10:43:53 INFO  Model List - รายการโมเดลถูกโหลดด้วย: []\n```\n\nจากนั้นคุณสามารถแก้ไขไฟล์ config เพื่อกำหนดค่า proxy server ของคุณ\n\n## ไฟล์ Config\n\nไฟล์ config นี้จะถูก hot-reload โดยอัตโนมัติเมื่อคุณมีการเปลี่ยนแปลง เฉพาะส่วนที่ได้รับผลกระทบของเซิร์ฟเวอร์เท่านั้นที่จะถูกอัปเดต\n\nเมื่อไฟล์ config ถูกสร้างขึ้นครั้งแรก จะมีคำอธิบาย schema อยู่ด้วย ซึ่งจะช่วยให้มีการเติมคำและตรวจสอบใน editor ของคุณ",
    "Status": "ok"
  },
  {
    "Id": 3,
    "Content": "## Example config file\n\n```yaml\n# $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json\nlmStudio:\n  port: 1234 # This is default value\n  enabled: true # This is default value\n  host: 0.0.0.0 # This is default value\n  path: /your/path # Will be add before the original endpoints, default value is empty\nollama:\n  port: 11434 # This is default value\n  enabled: true # This is default value\n  host: 0.0.0.0 # This is default value\n  path: /your/path # Will be add before the original endpoints, default value is empty\nclient:\n  socketTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  connectionTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  requestTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  retry: 3 # This is default value\n  delayBeforeRetry: 1000 # This is default value, in milliseconds\n\napiProviders:\n  OpenAI:\n    type: OpenAi\n    baseUrl: https://api.openai.com/v1\n    apiKey: <your_api_key>\n    modelList:\n      - gpt-4o\n  Claude:\n    type: Claude\n    apiKey: <your_api_key>\n    modelList:\n      - claude-3-7-sonnet\n  Qwen:\n    type: DashScope\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - qwen-max\n      - qwen-plus\n      - qwen-turbo\n      - qwen-long\n  DeepSeek:\n    type: DeepSeek\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - deepseek-chat\n      - deepseek-reasoner\n  Mistral:\n    type: Mistral\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - codestral-latest\n      - mistral-large\n  SiliconFlow:\n    type: SiliconFlow\n    apiKey: <your_api_key>\n    modelList:\n      - Qwen/Qwen3-235B-A22B\n      - Pro/deepseek-ai/DeepSeek-V3\n      - THUDM/GLM-4-32B-0414\n  OpenRouter:\n    type: OpenRouter\n    apiKey: <your_api_key>\n    modelList:\n      - openai/gpt-4o\n  Gemini:\n    type: Gemini\n    apiKey: <your_api_key>\n    modelList:\n      - gemini-2.5-flash-preview-04-17\n```",
    "ContentSha": "70Ur2u1pGxwgBRWrkWendSuM8NT3qhqj6cqjS9POUPs=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## ตัวอย่างไฟล์ config\n\n```yaml\n# $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json\nlmStudio:\n  port: 1234 # ค่านี้เป็นค่าเริ่มต้น\n  enabled: true # ค่านี้เป็นค่าเริ่มต้น\n  host: 0.0.0.0 # ค่านี้เป็นค่าเริ่มต้น\n  path: /your/path # จะถูกเพิ่มก่อน endpoint เดิม, ค่าเริ่มต้นคือค่าว่าง\nollama:\n  port: 11434 # ค่านี้เป็นค่าเริ่มต้น\n  enabled: true # ค่านี้เป็นค่าเริ่มต้น\n  host: 0.0.0.0 # ค่านี้เป็นค่าเริ่มต้น\n  path: /your/path # จะถูกเพิ่มก่อน endpoint เดิม, ค่าเริ่มต้นคือค่าว่าง\nclient:\n  socketTimeout: 1919810 # Long.MAX_VALUE คือค่าเริ่มต้น, หน่วยเป็นมิลลิวินาที\n  connectionTimeout: 1919810 # Long.MAX_VALUE คือค่าเริ่มต้น, หน่วยเป็นมิลลิวินาที\n  requestTimeout: 1919810 # Long.MAX_VALUE คือค่าเริ่มต้น, หน่วยเป็นมิลลิวินาที\n  retry: 3 # ค่านี้เป็นค่าเริ่มต้น\n  delayBeforeRetry: 1000 # ค่านี้เป็นค่าเริ่มต้น, หน่วยเป็นมิลลิวินาที\n\napiProviders:\n  OpenAI:\n    type: OpenAi\n    baseUrl: https://api.openai.com/v1\n    apiKey: <your_api_key>\n    modelList:\n      - gpt-4o\n  Claude:\n    type: Claude\n    apiKey: <your_api_key>\n    modelList:\n      - claude-3-7-sonnet\n  Qwen:\n    type: DashScope\n    apiKey: <your_api_key>\n    modelList: # ค่านี้เป็นค่าเริ่มต้น\n      - qwen-max\n      - qwen-plus\n      - qwen-turbo\n      - qwen-long\n  DeepSeek:\n    type: DeepSeek\n    apiKey: <your_api_key>\n    modelList: # ค่านี้เป็นค่าเริ่มต้น\n      - deepseek-chat\n      - deepseek-reasoner\n  Mistral:\n    type: Mistral\n    apiKey: <your_api_key>\n    modelList: # ค่านี้เป็นค่าเริ่มต้น\n      - codestral-latest\n      - mistral-large\n  SiliconFlow:\n    type: SiliconFlow\n    apiKey: <your_api_key>\n    modelList:\n      - Qwen/Qwen3-235B-A22B\n      - Pro/deepseek-ai/DeepSeek-V3\n      - THUDM/GLM-4-32B-0414\n  OpenRouter:\n    type: OpenRouter\n    apiKey: <your_api_key>\n    modelList:\n      - openai/gpt-4o\n  Gemini:\n    type: Gemini\n    apiKey: <your_api_key>\n    modelList:\n      - gemini-2.5-flash-preview-04-17\n```",
    "Status": "ok"
  }
]