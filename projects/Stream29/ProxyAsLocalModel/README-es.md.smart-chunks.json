[
  {
    "Id": 1,
    "Content": "# ProxyAsLocalModel\n\nProxy remote LLM API as Local model. Especially works for using custom LLM in JetBrains AI Assistant.\n\nPowered by Ktor and kotlinx.serialization. Thanks to their no-reflex features.\n\n## Story of this project\n\nCurrently, JetBrains AI Assistant provides a free plan with very limited quotes. I tried out and my quote ran out quickly.\n\nI already bought other LLM API tokens, such like Gemini and Qwen. So I started to think of using them in AI Assistant. Unfortunately, only local models from LM Studio and Ollama are supported. So I started to work on this proxy application that proxy third party LLM API as LM Studio and Ollama API so that I can use them in my JetBrains IDEs.\n\nThis is Just a simple task, so I started to use the official SDKs as clients and write a simple Ktor server that provides endpoints as LM Studio and Ollama. The problem appears when I try to distribute it as a GraalVM native image. The official Java SDKS uses too many dynamic features, making it hard to compile into a native image, even with a tracing agent. So I decided to implement a simple client of streaming chat completion API by myself with Ktor and kotlinx.serialization which are both no-reflex, functional and DSL styled.\n\nAs you can see, this application is distributed as a fat runnable jar and a GraalVM native image, which makes it cross-platform and fast to start.\n\nThe development of this application gives me confidence in Kotlin/Ktor/kotlinx.serialization. The Kotlin world uses more functional programming and less reflexion, which makes it more suitable for GraalVM native image, with faster startup and less memory usage.\n\n## Currently supported\n\nProxy from: OpenAI, Claude, DashScope(Alibaba Qwen), Gemini, Deepseek, Mistral, SiliconFlow.\n\nProxy as: LM Studio, Ollama.\n\nStreaming chat completion API only.\n",
    "ContentSha": "RqoxS65Q2kmC/NFTNk+2QDY1yLetU7emS9EGMv38jJo=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "# ProxyAsLocalModel\n\nProxy API de LLM remotos como modelo local. Especialmente útil para usar LLM personalizados en JetBrains AI Assistant.\n\nImpulsado por Ktor y kotlinx.serialization. Gracias a sus características sin reflexión.\n\n## Historia de este proyecto\n\nActualmente, JetBrains AI Assistant ofrece un plan gratuito con cuotas muy limitadas. Lo probé y mi cuota se agotó rápidamente.\n\nYa había comprado otros tokens de API de LLM, como Gemini y Qwen. Así que empecé a pensar en usarlos en AI Assistant. Desafortunadamente, solo se admiten modelos locales de LM Studio y Ollama. Así que comencé a trabajar en esta aplicación proxy que actúa como intermediario de API de LLM de terceros como LM Studio y Ollama API para poder usarlos en mis IDEs de JetBrains.\n\nEsta es solo una tarea sencilla, así que empecé a usar los SDKs oficiales como clientes y a escribir un servidor Ktor simple que proporcione endpoints como LM Studio y Ollama. El problema apareció cuando intenté distribuirlo como una imagen nativa de GraalVM. Los SDKs oficiales de Java usan demasiadas funciones dinámicas, lo que dificulta la compilación en una imagen nativa, incluso con un agente de rastreo. Así que decidí implementar yo mismo un cliente simple de la API de chat de completado por streaming utilizando Ktor y kotlinx.serialization, que son ambos sin reflexión, funcionales y con estilo DSL.\n\nComo puedes ver, esta aplicación se distribuye como un jar ejecutable \"fat\" y una imagen nativa de GraalVM, lo que la hace multiplataforma y rápida de iniciar.\n\nEl desarrollo de esta aplicación me ha dado confianza en Kotlin/Ktor/kotlinx.serialization. El mundo Kotlin usa más programación funcional y menos reflexión, lo que lo hace más adecuado para imágenes nativas de GraalVM, con un arranque más rápido y menor uso de memoria.\n\n## Actualmente soportado\n\nProxy desde: OpenAI, Claude, DashScope (Alibaba Qwen), Gemini, Deepseek, Mistral, SiliconFlow.\n\nProxy como: LM Studio, Ollama.\n\nSolo API de chat de completado por streaming.",
    "Status": "ok"
  },
  {
    "Id": 2,
    "Content": "## How to use\n\nThis application is a proxy server, distributed as a fat runnable jar and a GraalVM native image (Windows x64).\n\nRun the application, and you will see a help message:\n\n```\n2025-05-02 10:43:53 INFO  Help - It looks that you are starting the program for the first time here.\n2025-05-02 10:43:53 INFO  Help - A default config file is created at your_path\\config.yml with schema annotation.\n2025-05-02 10:43:53 INFO  Config - Config file watcher started at your_path\\config.yml\n2025-05-02 10:43:53 INFO  LM Studio Server - LM Studio Server started at 1234\n2025-05-02 10:43:53 INFO  Ollama Server - Ollama Server started at 11434\n2025-05-02 10:43:53 INFO  Model List - Model list loaded with: []\n```\n\nThen you can edit the config file to set up your proxy server.\n\n## Config file\n\nThis config file is automatically hot-reloaded when you change it. Only the influenced parts of the server will be updated.\n\nWhen first generating the config file, it will be created with schema annotations. This will bring completion and check in your editor.\n",
    "ContentSha": "jSta+CN7gfDD+W3lmWAocZcdmJOQZkgX0LDlOBWiy1Y=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## Cómo usar\n\nEsta aplicación es un servidor proxy, distribuido como un archivo jar ejecutable autónomo y una imagen nativa de GraalVM (Windows x64).\n\nEjecute la aplicación y verá un mensaje de ayuda:\n\n```\n2025-05-02 10:43:53 INFO  Help - Parece que está iniciando el programa por primera vez aquí.\n2025-05-02 10:43:53 INFO  Help - Se ha creado un archivo de configuración predeterminado en your_path\\config.yml con anotaciones de esquema.\n2025-05-02 10:43:53 INFO  Config - El observador del archivo de configuración se inició en your_path\\config.yml\n2025-05-02 10:43:53 INFO  LM Studio Server - Servidor LM Studio iniciado en 1234\n2025-05-02 10:43:53 INFO  Ollama Server - Servidor Ollama iniciado en 11434\n2025-05-02 10:43:53 INFO  Model List - Lista de modelos cargada con: []\n```\n\nLuego puede editar el archivo de configuración para configurar su servidor proxy.\n\n## Archivo de configuración\n\nEste archivo de configuración se recarga automáticamente en caliente cuando lo modifica. Solo se actualizarán las partes del servidor que se vean afectadas.\n\nCuando se genera por primera vez el archivo de configuración, se creará con anotaciones de esquema. Esto proporcionará autocompletado y verificación en su editor.",
    "Status": "ok"
  },
  {
    "Id": 3,
    "Content": "## Example config file\n\n```yaml\n# $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json\nlmStudio:\n  port: 1234 # This is default value\n  enabled: true # This is default value\n  host: 0.0.0.0 # This is default value\n  path: /your/path # Will be add before the original endpoints, default value is empty\nollama:\n  port: 11434 # This is default value\n  enabled: true # This is default value\n  host: 0.0.0.0 # This is default value\n  path: /your/path # Will be add before the original endpoints, default value is empty\nclient:\n  socketTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  connectionTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  requestTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  retry: 3 # This is default value\n  delayBeforeRetry: 1000 # This is default value, in milliseconds\n\napiProviders:\n  OpenAI:\n    type: OpenAi\n    baseUrl: https://api.openai.com/v1\n    apiKey: <your_api_key>\n    modelList:\n      - gpt-4o\n  Claude:\n    type: Claude\n    apiKey: <your_api_key>\n    modelList:\n      - claude-3-7-sonnet\n  Qwen:\n    type: DashScope\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - qwen-max\n      - qwen-plus\n      - qwen-turbo\n      - qwen-long\n  DeepSeek:\n    type: DeepSeek\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - deepseek-chat\n      - deepseek-reasoner\n  Mistral:\n    type: Mistral\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - codestral-latest\n      - mistral-large\n  SiliconFlow:\n    type: SiliconFlow\n    apiKey: <your_api_key>\n    modelList:\n      - Qwen/Qwen3-235B-A22B\n      - Pro/deepseek-ai/DeepSeek-V3\n      - THUDM/GLM-4-32B-0414\n  OpenRouter:\n    type: OpenRouter\n    apiKey: <your_api_key>\n    modelList:\n      - openai/gpt-4o\n  Gemini:\n    type: Gemini\n    apiKey: <your_api_key>\n    modelList:\n      - gemini-2.5-flash-preview-04-17\n```",
    "ContentSha": "70Ur2u1pGxwgBRWrkWendSuM8NT3qhqj6cqjS9POUPs=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## Ejemplo de archivo de configuración\n\n```yaml\n# $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json\nlmStudio:\n  port: 1234 # Este es el valor por defecto\n  enabled: true # Este es el valor por defecto\n  host: 0.0.0.0 # Este es el valor por defecto\n  path: /your/path # Se añadirá antes de los endpoints originales, el valor por defecto es vacío\nollama:\n  port: 11434 # Este es el valor por defecto\n  enabled: true # Este es el valor por defecto\n  host: 0.0.0.0 # Este es el valor por defecto\n  path: /your/path # Se añadirá antes de los endpoints originales, el valor por defecto es vacío\nclient:\n  socketTimeout: 1919810 # Long.MAX_VALUE es el valor por defecto, en milisegundos\n  connectionTimeout: 1919810 # Long.MAX_VALUE es el valor por defecto, en milisegundos\n  requestTimeout: 1919810 # Long.MAX_VALUE es el valor por defecto, en milisegundos\n  retry: 3 # Este es el valor por defecto\n  delayBeforeRetry: 1000 # Este es el valor por defecto, en milisegundos\n\napiProviders:\n  OpenAI:\n    type: OpenAi\n    baseUrl: https://api.openai.com/v1\n    apiKey: <your_api_key>\n    modelList:\n      - gpt-4o\n  Claude:\n    type: Claude\n    apiKey: <your_api_key>\n    modelList:\n      - claude-3-7-sonnet\n  Qwen:\n    type: DashScope\n    apiKey: <your_api_key>\n    modelList: # Este es el valor por defecto\n      - qwen-max\n      - qwen-plus\n      - qwen-turbo\n      - qwen-long\n  DeepSeek:\n    type: DeepSeek\n    apiKey: <your_api_key>\n    modelList: # Este es el valor por defecto\n      - deepseek-chat\n      - deepseek-reasoner\n  Mistral:\n    type: Mistral\n    apiKey: <your_api_key>\n    modelList: # Este es el valor por defecto\n      - codestral-latest\n      - mistral-large\n  SiliconFlow:\n    type: SiliconFlow\n    apiKey: <your_api_key>\n    modelList:\n      - Qwen/Qwen3-235B-A22B\n      - Pro/deepseek-ai/DeepSeek-V3\n      - THUDM/GLM-4-32B-0414\n  OpenRouter:\n    type: OpenRouter\n    apiKey: <your_api_key>\n    modelList:\n      - openai/gpt-4o\n  Gemini:\n    type: Gemini\n    apiKey: <your_api_key>\n    modelList:\n      - gemini-2.5-flash-preview-04-17\n```",
    "Status": "ok"
  }
]