[
  {
    "Id": 1,
    "Content": "# ProxyAsLocalModel\n\nProxy remote LLM API as Local model. Especially works for using custom LLM in JetBrains AI Assistant.\n\nPowered by Ktor and kotlinx.serialization. Thanks to their no-reflex features.\n\n## Story of this project\n\nCurrently, JetBrains AI Assistant provides a free plan with very limited quotes. I tried out and my quote ran out quickly.\n\nI already bought other LLM API tokens, such like Gemini and Qwen. So I started to think of using them in AI Assistant. Unfortunately, only local models from LM Studio and Ollama are supported. So I started to work on this proxy application that proxy third party LLM API as LM Studio and Ollama API so that I can use them in my JetBrains IDEs.\n\nThis is Just a simple task, so I started to use the official SDKs as clients and write a simple Ktor server that provides endpoints as LM Studio and Ollama. The problem appears when I try to distribute it as a GraalVM native image. The official Java SDKS uses too many dynamic features, making it hard to compile into a native image, even with a tracing agent. So I decided to implement a simple client of streaming chat completion API by myself with Ktor and kotlinx.serialization which are both no-reflex, functional and DSL styled.\n\nAs you can see, this application is distributed as a fat runnable jar and a GraalVM native image, which makes it cross-platform and fast to start.\n\nThe development of this application gives me confidence in Kotlin/Ktor/kotlinx.serialization. The Kotlin world uses more functional programming and less reflexion, which makes it more suitable for GraalVM native image, with faster startup and less memory usage.\n\n## Currently supported\n\nProxy from: OpenAI, Claude, DashScope(Alibaba Qwen), Gemini, Deepseek, Mistral, SiliconFlow.\n\nProxy as: LM Studio, Ollama.\n\nStreaming chat completion API only.\n",
    "ContentSha": "RqoxS65Q2kmC/NFTNk+2QDY1yLetU7emS9EGMv38jJo=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "# ProxyAsLocalModel\n\nUzaktaki LLM API'sini yerel model olarak proxy'le. Özellikle JetBrains AI Assistant'ta özel LLM kullanımı için çalışır.\n\nKtor ve kotlinx.serialization tarafından desteklenmektedir. No-reflex özellikleri sayesinde.\n\n## Bu projenin hikayesi\n\nŞu anda JetBrains AI Assistant, çok sınırlı kotalı ücretsiz bir plan sunuyor. Denedim ve kotam hızla tükendi.\n\nZaten Gemini ve Qwen gibi başka LLM API tokenları satın almıştım. Bu yüzden bunları AI Assistant'ta kullanmayı düşünmeye başladım. Ne yazık ki, yalnızca LM Studio ve Ollama'nın yerel modelleri destekleniyor. Bu nedenle, üçüncü parti LLM API'sini LM Studio ve Ollama API'si olarak proxy'leyen ve böylece JetBrains IDE'lerimde kullanabilmemi sağlayan bu proxy uygulaması üzerinde çalışmaya başladım.\n\nBu sadece basit bir iş, bu yüzden istemci olarak resmi SDK'ları kullanıp LM Studio ve Ollama olarak endpoint sağlayan basit bir Ktor sunucusu yazmaya başladım. Sorun, bunu GraalVM native image olarak dağıtmaya çalıştığımda ortaya çıktı. Resmi Java SDK'ları çok fazla dinamik özellik kullanıyor, bu da tracing agent ile bile native image olarak derlemeyi zorlaştırıyor. Bu yüzden, hem no-reflex, fonksiyonel ve DSL tarzında olan Ktor ve kotlinx.serialization ile streaming chat completion API için basit bir istemciyi kendim yazmaya karar verdim.\n\nGörüldüğü üzere, bu uygulama platformlar arası ve hızlı başlatılabilen bir fat runnable jar ve GraalVM native image olarak dağıtılmaktadır.\n\nBu uygulamanın geliştirilmesi bana Kotlin/Ktor/kotlinx.serialization konusunda güven verdi. Kotlin dünyası daha fazla fonksiyonel programlama ve daha az refleksiyon kullanıyor, bu da onu GraalVM native image için daha uygun, daha hızlı başlatılan ve daha az bellek kullanan bir hale getiriyor.\n\n## Şu anda desteklenenler\n\nProxy kaynakları: OpenAI, Claude, DashScope(Alibaba Qwen), Gemini, Deepseek, Mistral, SiliconFlow.\n\nProxy olarak: LM Studio, Ollama.\n\nSadece streaming chat completion API.",
    "Status": "ok"
  },
  {
    "Id": 2,
    "Content": "## How to use\n\nThis application is a proxy server, distributed as a fat runnable jar and a GraalVM native image (Windows x64).\n\nRun the application, and you will see a help message:\n\n```\n2025-05-02 10:43:53 INFO  Help - It looks that you are starting the program for the first time here.\n2025-05-02 10:43:53 INFO  Help - A default config file is created at your_path\\config.yml with schema annotation.\n2025-05-02 10:43:53 INFO  Config - Config file watcher started at your_path\\config.yml\n2025-05-02 10:43:53 INFO  LM Studio Server - LM Studio Server started at 1234\n2025-05-02 10:43:53 INFO  Ollama Server - Ollama Server started at 11434\n2025-05-02 10:43:53 INFO  Model List - Model list loaded with: []\n```\n\nThen you can edit the config file to set up your proxy server.\n\n## Config file\n\nThis config file is automatically hot-reloaded when you change it. Only the influenced parts of the server will be updated.\n\nWhen first generating the config file, it will be created with schema annotations. This will bring completion and check in your editor.\n",
    "ContentSha": "jSta+CN7gfDD+W3lmWAocZcdmJOQZkgX0LDlOBWiy1Y=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## Nasıl Kullanılır\n\nBu uygulama bir proxy sunucusudur ve çalıştırılabilir fat jar ile GraalVM yerel görüntüsü (Windows x64) olarak dağıtılır.\n\nUygulamayı çalıştırın, ardından bir yardım mesajı göreceksiniz:\n\n```\n2025-05-02 10:43:53 INFO  Help - Görünüşe göre programı burada ilk kez başlatıyorsunuz.\n2025-05-02 10:43:53 INFO  Help - Varsayılan bir yapılandırma dosyası your_path\\config.yml konumunda şema açıklamalarıyla oluşturuldu.\n2025-05-02 10:43:53 INFO  Config - Yapılandırma dosyası izleyicisi your_path\\config.yml konumunda başlatıldı\n2025-05-02 10:43:53 INFO  LM Studio Server - LM Studio Sunucusu 1234 üzerinde başlatıldı\n2025-05-02 10:43:53 INFO  Ollama Server - Ollama Sunucusu 11434 üzerinde başlatıldı\n2025-05-02 10:43:53 INFO  Model List - Model listesi şu şekilde yüklendi: []\n```\n\nDaha sonra proxy sunucunuzu kurmak için yapılandırma dosyasını düzenleyebilirsiniz.\n\n## Yapılandırma Dosyası\n\nBu yapılandırma dosyası, siz değiştirdiğinizde otomatik olarak sıcak yeniden yüklenir. Sunucunun yalnızca etkilenen bölümleri güncellenecektir.\n\nYapılandırma dosyası ilk oluşturulduğunda, şema açıklamaları ile birlikte gelir. Bu, editörünüzde tamamlama ve kontrol imkanı sağlar.",
    "Status": "ok"
  },
  {
    "Id": 3,
    "Content": "## Example config file\n\n```yaml\n# $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json\nlmStudio:\n  port: 1234 # This is default value\n  enabled: true # This is default value\n  host: 0.0.0.0 # This is default value\n  path: /your/path # Will be add before the original endpoints, default value is empty\nollama:\n  port: 11434 # This is default value\n  enabled: true # This is default value\n  host: 0.0.0.0 # This is default value\n  path: /your/path # Will be add before the original endpoints, default value is empty\nclient:\n  socketTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  connectionTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  requestTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  retry: 3 # This is default value\n  delayBeforeRetry: 1000 # This is default value, in milliseconds\n\napiProviders:\n  OpenAI:\n    type: OpenAi\n    baseUrl: https://api.openai.com/v1\n    apiKey: <your_api_key>\n    modelList:\n      - gpt-4o\n  Claude:\n    type: Claude\n    apiKey: <your_api_key>\n    modelList:\n      - claude-3-7-sonnet\n  Qwen:\n    type: DashScope\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - qwen-max\n      - qwen-plus\n      - qwen-turbo\n      - qwen-long\n  DeepSeek:\n    type: DeepSeek\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - deepseek-chat\n      - deepseek-reasoner\n  Mistral:\n    type: Mistral\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - codestral-latest\n      - mistral-large\n  SiliconFlow:\n    type: SiliconFlow\n    apiKey: <your_api_key>\n    modelList:\n      - Qwen/Qwen3-235B-A22B\n      - Pro/deepseek-ai/DeepSeek-V3\n      - THUDM/GLM-4-32B-0414\n  OpenRouter:\n    type: OpenRouter\n    apiKey: <your_api_key>\n    modelList:\n      - openai/gpt-4o\n  Gemini:\n    type: Gemini\n    apiKey: <your_api_key>\n    modelList:\n      - gemini-2.5-flash-preview-04-17\n```",
    "ContentSha": "70Ur2u1pGxwgBRWrkWendSuM8NT3qhqj6cqjS9POUPs=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## Örnek yapılandırma dosyası\n\n```yaml\n# $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json\nlmStudio:\n  port: 1234 # Bu varsayılan değerdir\n  enabled: true # Bu varsayılan değerdir\n  host: 0.0.0.0 # Bu varsayılan değerdir\n  path: /your/path # Orijinal uç noktalardan önce eklenecek, varsayılan değeri boştur\nollama:\n  port: 11434 # Bu varsayılan değerdir\n  enabled: true # Bu varsayılan değerdir\n  host: 0.0.0.0 # Bu varsayılan değerdir\n  path: /your/path # Orijinal uç noktalardan önce eklenecek, varsayılan değeri boştur\nclient:\n  socketTimeout: 1919810 # Varsayılan değer Long.MAX_VALUE'dur, milisaniye cinsindendir\n  connectionTimeout: 1919810 # Varsayılan değer Long.MAX_VALUE'dur, milisaniye cinsindendir\n  requestTimeout: 1919810 # Varsayılan değer Long.MAX_VALUE'dur, milisaniye cinsindendir\n  retry: 3 # Bu varsayılan değerdir\n  delayBeforeRetry: 1000 # Bu varsayılan değerdir, milisaniye cinsindendir\n\napiProviders:\n  OpenAI:\n    type: OpenAi\n    baseUrl: https://api.openai.com/v1\n    apiKey: <your_api_key>\n    modelList:\n      - gpt-4o\n  Claude:\n    type: Claude\n    apiKey: <your_api_key>\n    modelList:\n      - claude-3-7-sonnet\n  Qwen:\n    type: DashScope\n    apiKey: <your_api_key>\n    modelList: # Bu varsayılan değerdir\n      - qwen-max\n      - qwen-plus\n      - qwen-turbo\n      - qwen-long\n  DeepSeek:\n    type: DeepSeek\n    apiKey: <your_api_key>\n    modelList: # Bu varsayılan değerdir\n      - deepseek-chat\n      - deepseek-reasoner\n  Mistral:\n    type: Mistral\n    apiKey: <your_api_key>\n    modelList: # Bu varsayılan değerdir\n      - codestral-latest\n      - mistral-large\n  SiliconFlow:\n    type: SiliconFlow\n    apiKey: <your_api_key>\n    modelList:\n      - Qwen/Qwen3-235B-A22B\n      - Pro/deepseek-ai/DeepSeek-V3\n      - THUDM/GLM-4-32B-0414\n  OpenRouter:\n    type: OpenRouter\n    apiKey: <your_api_key>\n    modelList:\n      - openai/gpt-4o\n  Gemini:\n    type: Gemini\n    apiKey: <your_api_key>\n    modelList:\n      - gemini-2.5-flash-preview-04-17\n```",
    "Status": "ok"
  }
]