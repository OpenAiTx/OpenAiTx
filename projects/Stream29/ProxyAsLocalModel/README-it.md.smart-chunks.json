[
  {
    "Id": 1,
    "Content": "# ProxyAsLocalModel\n\nProxy remote LLM API as Local model. Especially works for using custom LLM in JetBrains AI Assistant.\n\nPowered by Ktor and kotlinx.serialization. Thanks to their no-reflex features.\n\n## Story of this project\n\nCurrently, JetBrains AI Assistant provides a free plan with very limited quotes. I tried out and my quote ran out quickly.\n\nI already bought other LLM API tokens, such like Gemini and Qwen. So I started to think of using them in AI Assistant. Unfortunately, only local models from LM Studio and Ollama are supported. So I started to work on this proxy application that proxy third party LLM API as LM Studio and Ollama API so that I can use them in my JetBrains IDEs.\n\nThis is Just a simple task, so I started to use the official SDKs as clients and write a simple Ktor server that provides endpoints as LM Studio and Ollama. The problem appears when I try to distribute it as a GraalVM native image. The official Java SDKS uses too many dynamic features, making it hard to compile into a native image, even with a tracing agent. So I decided to implement a simple client of streaming chat completion API by myself with Ktor and kotlinx.serialization which are both no-reflex, functional and DSL styled.\n\nAs you can see, this application is distributed as a fat runnable jar and a GraalVM native image, which makes it cross-platform and fast to start.\n\nThe development of this application gives me confidence in Kotlin/Ktor/kotlinx.serialization. The Kotlin world uses more functional programming and less reflexion, which makes it more suitable for GraalVM native image, with faster startup and less memory usage.\n\n## Currently supported\n\nProxy from: OpenAI, Claude, DashScope(Alibaba Qwen), Gemini, Deepseek, Mistral, SiliconFlow.\n\nProxy as: LM Studio, Ollama.\n\nStreaming chat completion API only.\n",
    "ContentSha": "RqoxS65Q2kmC/NFTNk+2QDY1yLetU7emS9EGMv38jJo=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "# ProxyAsLocalModel\n\nProxy API LLM remoti come modelli locali. Funziona in particolare per l'utilizzo di LLM personalizzati in JetBrains AI Assistant.\n\nBasato su Ktor e kotlinx.serialization. Grazie alle loro caratteristiche senza riflessione.\n\n## Storia di questo progetto\n\nAttualmente, JetBrains AI Assistant offre un piano gratuito con quote molto limitate. L'ho provato e la mia quota si è esaurita rapidamente.\n\nAvevo già acquistato altri token API LLM, come Gemini e Qwen. Quindi ho iniziato a pensare di usarli in AI Assistant. Sfortunatamente, sono supportati solo i modelli locali da LM Studio e Ollama. Così ho iniziato a lavorare su questa applicazione proxy che fa da proxy alle API LLM di terze parti come se fossero API di LM Studio e Ollama, così da poterle utilizzare nei miei IDE JetBrains.\n\nQuesto è solo un compito semplice, quindi ho iniziato a utilizzare gli SDK ufficiali come client e a scrivere un semplice server Ktor che fornisce endpoint come LM Studio e Ollama. Il problema è apparso quando ho cercato di distribuirlo come immagine nativa GraalVM. Gli SDK Java ufficiali usano troppe funzionalità dinamiche, rendendo difficile la compilazione in un'immagine nativa, anche con un tracing agent. Quindi ho deciso di implementare io stesso un semplice client per l'API di completamento chat in streaming con Ktor e kotlinx.serialization, che sono entrambi senza riflessione, funzionali e in stile DSL.\n\nCome puoi vedere, questa applicazione è distribuita come jar eseguibile fat e come immagine nativa GraalVM, il che la rende multipiattaforma e veloce all'avvio.\n\nLo sviluppo di questa applicazione mi ha dato fiducia in Kotlin/Ktor/kotlinx.serialization. Il mondo Kotlin utilizza più programmazione funzionale e meno riflessione, il che lo rende più adatto per immagini native GraalVM, con avvio più rapido e minore utilizzo di memoria.\n\n## Attualmente supportato\n\nProxy da: OpenAI, Claude, DashScope (Alibaba Qwen), Gemini, Deepseek, Mistral, SiliconFlow.\n\nProxy come: LM Studio, Ollama.\n\nSolo API di completamento chat in streaming.",
    "Status": "ok"
  },
  {
    "Id": 2,
    "Content": "## How to use\n\nThis application is a proxy server, distributed as a fat runnable jar and a GraalVM native image (Windows x64).\n\nRun the application, and you will see a help message:\n\n```\n2025-05-02 10:43:53 INFO  Help - It looks that you are starting the program for the first time here.\n2025-05-02 10:43:53 INFO  Help - A default config file is created at your_path\\config.yml with schema annotation.\n2025-05-02 10:43:53 INFO  Config - Config file watcher started at your_path\\config.yml\n2025-05-02 10:43:53 INFO  LM Studio Server - LM Studio Server started at 1234\n2025-05-02 10:43:53 INFO  Ollama Server - Ollama Server started at 11434\n2025-05-02 10:43:53 INFO  Model List - Model list loaded with: []\n```\n\nThen you can edit the config file to set up your proxy server.\n\n## Config file\n\nThis config file is automatically hot-reloaded when you change it. Only the influenced parts of the server will be updated.\n\nWhen first generating the config file, it will be created with schema annotations. This will bring completion and check in your editor.\n",
    "ContentSha": "jSta+CN7gfDD+W3lmWAocZcdmJOQZkgX0LDlOBWiy1Y=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## Come usare\n\nQuesta applicazione è un server proxy, distribuito come un jar eseguibile standalone e come un'immagine nativa GraalVM (Windows x64).\n\nEsegui l'applicazione e vedrai un messaggio di aiuto:\n\n```\n2025-05-02 10:43:53 INFO  Help - Sembra che tu stia avviando il programma per la prima volta qui.\n2025-05-02 10:43:53 INFO  Help - Un file di configurazione predefinito è stato creato in your_path\\config.yml con annotazioni di schema.\n2025-05-02 10:43:53 INFO  Config - Il watcher del file di configurazione è stato avviato in your_path\\config.yml\n2025-05-02 10:43:53 INFO  LM Studio Server - LM Studio Server avviato su 1234\n2025-05-02 10:43:53 INFO  Ollama Server - Ollama Server avviato su 11434\n2025-05-02 10:43:53 INFO  Model List - Elenco modelli caricato con: []\n```\n\nSuccessivamente puoi modificare il file di configurazione per impostare il tuo server proxy.\n\n## File di configurazione\n\nQuesto file di configurazione viene automaticamente ricaricato a caldo quando lo modifichi. Solo le parti interessate del server verranno aggiornate.\n\nQuando il file di configurazione viene generato per la prima volta, sarà creato con annotazioni di schema. Questo fornirà completamento e verifica nel tuo editor.",
    "Status": "ok"
  },
  {
    "Id": 3,
    "Content": "## Example config file\n\n```yaml\n# $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json\nlmStudio:\n  port: 1234 # This is default value\n  enabled: true # This is default value\n  host: 0.0.0.0 # This is default value\n  path: /your/path # Will be add before the original endpoints, default value is empty\nollama:\n  port: 11434 # This is default value\n  enabled: true # This is default value\n  host: 0.0.0.0 # This is default value\n  path: /your/path # Will be add before the original endpoints, default value is empty\nclient:\n  socketTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  connectionTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  requestTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  retry: 3 # This is default value\n  delayBeforeRetry: 1000 # This is default value, in milliseconds\n\napiProviders:\n  OpenAI:\n    type: OpenAi\n    baseUrl: https://api.openai.com/v1\n    apiKey: <your_api_key>\n    modelList:\n      - gpt-4o\n  Claude:\n    type: Claude\n    apiKey: <your_api_key>\n    modelList:\n      - claude-3-7-sonnet\n  Qwen:\n    type: DashScope\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - qwen-max\n      - qwen-plus\n      - qwen-turbo\n      - qwen-long\n  DeepSeek:\n    type: DeepSeek\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - deepseek-chat\n      - deepseek-reasoner\n  Mistral:\n    type: Mistral\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - codestral-latest\n      - mistral-large\n  SiliconFlow:\n    type: SiliconFlow\n    apiKey: <your_api_key>\n    modelList:\n      - Qwen/Qwen3-235B-A22B\n      - Pro/deepseek-ai/DeepSeek-V3\n      - THUDM/GLM-4-32B-0414\n  OpenRouter:\n    type: OpenRouter\n    apiKey: <your_api_key>\n    modelList:\n      - openai/gpt-4o\n  Gemini:\n    type: Gemini\n    apiKey: <your_api_key>\n    modelList:\n      - gemini-2.5-flash-preview-04-17\n```",
    "ContentSha": "70Ur2u1pGxwgBRWrkWendSuM8NT3qhqj6cqjS9POUPs=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## Esempio di file di configurazione\n\n```yaml\n# $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json\nlmStudio:\n  port: 1234 # Questo è il valore predefinito\n  enabled: true # Questo è il valore predefinito\n  host: 0.0.0.0 # Questo è il valore predefinito\n  path: /your/path # Verrà aggiunto prima degli endpoint originali, il valore predefinito è vuoto\nollama:\n  port: 11434 # Questo è il valore predefinito\n  enabled: true # Questo è il valore predefinito\n  host: 0.0.0.0 # Questo è il valore predefinito\n  path: /your/path # Verrà aggiunto prima degli endpoint originali, il valore predefinito è vuoto\nclient:\n  socketTimeout: 1919810 # Long.MAX_VALUE è il valore predefinito, in millisecondi\n  connectionTimeout: 1919810 # Long.MAX_VALUE è il valore predefinito, in millisecondi\n  requestTimeout: 1919810 # Long.MAX_VALUE è il valore predefinito, in millisecondi\n  retry: 3 # Questo è il valore predefinito\n  delayBeforeRetry: 1000 # Questo è il valore predefinito, in millisecondi\n\napiProviders:\n  OpenAI:\n    type: OpenAi\n    baseUrl: https://api.openai.com/v1\n    apiKey: <your_api_key>\n    modelList:\n      - gpt-4o\n  Claude:\n    type: Claude\n    apiKey: <your_api_key>\n    modelList:\n      - claude-3-7-sonnet\n  Qwen:\n    type: DashScope\n    apiKey: <your_api_key>\n    modelList: # Questo è il valore predefinito\n      - qwen-max\n      - qwen-plus\n      - qwen-turbo\n      - qwen-long\n  DeepSeek:\n    type: DeepSeek\n    apiKey: <your_api_key>\n    modelList: # Questo è il valore predefinito\n      - deepseek-chat\n      - deepseek-reasoner\n  Mistral:\n    type: Mistral\n    apiKey: <your_api_key>\n    modelList: # Questo è il valore predefinito\n      - codestral-latest\n      - mistral-large\n  SiliconFlow:\n    type: SiliconFlow\n    apiKey: <your_api_key>\n    modelList:\n      - Qwen/Qwen3-235B-A22B\n      - Pro/deepseek-ai/DeepSeek-V3\n      - THUDM/GLM-4-32B-0414\n  OpenRouter:\n    type: OpenRouter\n    apiKey: <your_api_key>\n    modelList:\n      - openai/gpt-4o\n  Gemini:\n    type: Gemini\n    apiKey: <your_api_key>\n    modelList:\n      - gemini-2.5-flash-preview-04-17\n```",
    "Status": "ok"
  }
]