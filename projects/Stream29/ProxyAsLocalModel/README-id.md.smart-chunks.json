[
  {
    "Id": 1,
    "Content": "# ProxyAsLocalModel\n\nProxy remote LLM API as Local model. Especially works for using custom LLM in JetBrains AI Assistant.\n\nPowered by Ktor and kotlinx.serialization. Thanks to their no-reflex features.\n\n## Story of this project\n\nCurrently, JetBrains AI Assistant provides a free plan with very limited quotes. I tried out and my quote ran out quickly.\n\nI already bought other LLM API tokens, such like Gemini and Qwen. So I started to think of using them in AI Assistant. Unfortunately, only local models from LM Studio and Ollama are supported. So I started to work on this proxy application that proxy third party LLM API as LM Studio and Ollama API so that I can use them in my JetBrains IDEs.\n\nThis is Just a simple task, so I started to use the official SDKs as clients and write a simple Ktor server that provides endpoints as LM Studio and Ollama. The problem appears when I try to distribute it as a GraalVM native image. The official Java SDKS uses too many dynamic features, making it hard to compile into a native image, even with a tracing agent. So I decided to implement a simple client of streaming chat completion API by myself with Ktor and kotlinx.serialization which are both no-reflex, functional and DSL styled.\n\nAs you can see, this application is distributed as a fat runnable jar and a GraalVM native image, which makes it cross-platform and fast to start.\n\nThe development of this application gives me confidence in Kotlin/Ktor/kotlinx.serialization. The Kotlin world uses more functional programming and less reflexion, which makes it more suitable for GraalVM native image, with faster startup and less memory usage.\n\n## Currently supported\n\nProxy from: OpenAI, Claude, DashScope(Alibaba Qwen), Gemini, Deepseek, Mistral, SiliconFlow.\n\nProxy as: LM Studio, Ollama.\n\nStreaming chat completion API only.\n",
    "ContentSha": "RqoxS65Q2kmC/NFTNk+2QDY1yLetU7emS9EGMv38jJo=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "# ProxyAsLocalModel\n\nProxy API LLM jarak jauh sebagai model Lokal. Khususnya berfungsi untuk menggunakan LLM kustom di JetBrains AI Assistant.\n\nDidukung oleh Ktor dan kotlinx.serialization. Terima kasih atas fitur tanpa refleksi mereka.\n\n## Cerita dari proyek ini\n\nSaat ini, JetBrains AI Assistant menyediakan paket gratis dengan kuota yang sangat terbatas. Saya mencobanya dan kuota saya cepat habis.\n\nSaya sudah membeli token API LLM lain, seperti Gemini dan Qwen. Jadi saya mulai berpikir untuk menggunakannya di AI Assistant. Sayangnya, hanya model lokal dari LM Studio dan Ollama yang didukung. Maka saya mulai mengerjakan aplikasi proxy ini yang memproxy API LLM pihak ketiga sebagai API LM Studio dan Ollama agar saya bisa menggunakannya di IDE JetBrains saya.\n\nIni hanyalah tugas sederhana, jadi saya mulai menggunakan SDK resmi sebagai klien dan menulis server Ktor sederhana yang menyediakan endpoint seperti LM Studio dan Ollama. Masalah muncul ketika saya mencoba mendistribusikannya sebagai image native GraalVM. SDK Java resmi menggunakan terlalu banyak fitur dinamis, sehingga sulit untuk dikompilasi menjadi image native, bahkan dengan tracing agent. Jadi saya memutuskan untuk mengimplementasikan klien sederhana untuk API chat completion streaming sendiri menggunakan Ktor dan kotlinx.serialization yang keduanya tanpa refleksi, fungsional, dan bergaya DSL.\n\nSeperti yang Anda lihat, aplikasi ini didistribusikan sebagai fat runnable jar dan image native GraalVM, yang membuatnya lintas platform dan cepat untuk dijalankan.\n\nPengembangan aplikasi ini memberi saya kepercayaan diri pada Kotlin/Ktor/kotlinx.serialization. Dunia Kotlin lebih banyak menggunakan pemrograman fungsional dan lebih sedikit refleksi, sehingga lebih cocok untuk image native GraalVM, dengan waktu startup yang lebih cepat dan penggunaan memori yang lebih sedikit.\n\n## Saat ini didukung\n\nProxy dari: OpenAI, Claude, DashScope (Alibaba Qwen), Gemini, Deepseek, Mistral, SiliconFlow.\n\nProxy sebagai: LM Studio, Ollama.\n\nHanya API chat completion streaming.",
    "Status": "ok"
  },
  {
    "Id": 2,
    "Content": "## How to use\n\nThis application is a proxy server, distributed as a fat runnable jar and a GraalVM native image (Windows x64).\n\nRun the application, and you will see a help message:\n\n```\n2025-05-02 10:43:53 INFO  Help - It looks that you are starting the program for the first time here.\n2025-05-02 10:43:53 INFO  Help - A default config file is created at your_path\\config.yml with schema annotation.\n2025-05-02 10:43:53 INFO  Config - Config file watcher started at your_path\\config.yml\n2025-05-02 10:43:53 INFO  LM Studio Server - LM Studio Server started at 1234\n2025-05-02 10:43:53 INFO  Ollama Server - Ollama Server started at 11434\n2025-05-02 10:43:53 INFO  Model List - Model list loaded with: []\n```\n\nThen you can edit the config file to set up your proxy server.\n\n## Config file\n\nThis config file is automatically hot-reloaded when you change it. Only the influenced parts of the server will be updated.\n\nWhen first generating the config file, it will be created with schema annotations. This will bring completion and check in your editor.\n",
    "ContentSha": "jSta+CN7gfDD+W3lmWAocZcdmJOQZkgX0LDlOBWiy1Y=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## Cara Penggunaan\n\nAplikasi ini adalah server proxy, didistribusikan sebagai fat runnable jar dan GraalVM native image (Windows x64).\n\nJalankan aplikasi, dan Anda akan melihat pesan bantuan:\n\n```\n2025-05-02 10:43:53 INFO  Help - Sepertinya Anda menjalankan program untuk pertama kalinya di sini.\n2025-05-02 10:43:53 INFO  Help - File konfigurasi default telah dibuat di your_path\\config.yml dengan anotasi skema.\n2025-05-02 10:43:53 INFO  Config - Pemantau file konfigurasi dimulai di your_path\\config.yml\n2025-05-02 10:43:53 INFO  LM Studio Server - LM Studio Server dimulai pada 1234\n2025-05-02 10:43:53 INFO  Ollama Server - Ollama Server dimulai pada 11434\n2025-05-02 10:43:53 INFO  Model List - Daftar model dimuat dengan: []\n```\n\nKemudian Anda dapat mengedit file konfigurasi untuk mengatur server proxy Anda.\n\n## File Konfigurasi\n\nFile konfigurasi ini secara otomatis dimuat ulang (hot-reload) saat Anda mengubahnya. Hanya bagian server yang terpengaruh yang akan diperbarui.\n\nSaat pertama kali menghasilkan file konfigurasi, file tersebut akan dibuat dengan anotasi skema. Ini akan memberikan fitur pelengkapan dan pemeriksaan di editor Anda.",
    "Status": "ok"
  },
  {
    "Id": 3,
    "Content": "## Example config file\n\n```yaml\n# $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json\nlmStudio:\n  port: 1234 # This is default value\n  enabled: true # This is default value\n  host: 0.0.0.0 # This is default value\n  path: /your/path # Will be add before the original endpoints, default value is empty\nollama:\n  port: 11434 # This is default value\n  enabled: true # This is default value\n  host: 0.0.0.0 # This is default value\n  path: /your/path # Will be add before the original endpoints, default value is empty\nclient:\n  socketTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  connectionTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  requestTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  retry: 3 # This is default value\n  delayBeforeRetry: 1000 # This is default value, in milliseconds\n\napiProviders:\n  OpenAI:\n    type: OpenAi\n    baseUrl: https://api.openai.com/v1\n    apiKey: <your_api_key>\n    modelList:\n      - gpt-4o\n  Claude:\n    type: Claude\n    apiKey: <your_api_key>\n    modelList:\n      - claude-3-7-sonnet\n  Qwen:\n    type: DashScope\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - qwen-max\n      - qwen-plus\n      - qwen-turbo\n      - qwen-long\n  DeepSeek:\n    type: DeepSeek\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - deepseek-chat\n      - deepseek-reasoner\n  Mistral:\n    type: Mistral\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - codestral-latest\n      - mistral-large\n  SiliconFlow:\n    type: SiliconFlow\n    apiKey: <your_api_key>\n    modelList:\n      - Qwen/Qwen3-235B-A22B\n      - Pro/deepseek-ai/DeepSeek-V3\n      - THUDM/GLM-4-32B-0414\n  OpenRouter:\n    type: OpenRouter\n    apiKey: <your_api_key>\n    modelList:\n      - openai/gpt-4o\n  Gemini:\n    type: Gemini\n    apiKey: <your_api_key>\n    modelList:\n      - gemini-2.5-flash-preview-04-17\n```",
    "ContentSha": "70Ur2u1pGxwgBRWrkWendSuM8NT3qhqj6cqjS9POUPs=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## Contoh file konfigurasi\n\n```yaml\n# $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json\nlmStudio:\n  port: 1234 # Ini adalah nilai default\n  enabled: true # Ini adalah nilai default\n  host: 0.0.0.0 # Ini adalah nilai default\n  path: /your/path # Akan ditambahkan sebelum endpoint asli, nilai default adalah kosong\nollama:\n  port: 11434 # Ini adalah nilai default\n  enabled: true # Ini adalah nilai default\n  host: 0.0.0.0 # Ini adalah nilai default\n  path: /your/path # Akan ditambahkan sebelum endpoint asli, nilai default adalah kosong\nclient:\n  socketTimeout: 1919810 # Long.MAX_VALUE adalah nilai default, dalam milidetik\n  connectionTimeout: 1919810 # Long.MAX_VALUE adalah nilai default, dalam milidetik\n  requestTimeout: 1919810 # Long.MAX_VALUE adalah nilai default, dalam milidetik\n  retry: 3 # Ini adalah nilai default\n  delayBeforeRetry: 1000 # Ini adalah nilai default, dalam milidetik\n\napiProviders:\n  OpenAI:\n    type: OpenAi\n    baseUrl: https://api.openai.com/v1\n    apiKey: <your_api_key>\n    modelList:\n      - gpt-4o\n  Claude:\n    type: Claude\n    apiKey: <your_api_key>\n    modelList:\n      - claude-3-7-sonnet\n  Qwen:\n    type: DashScope\n    apiKey: <your_api_key>\n    modelList: # Ini adalah nilai default\n      - qwen-max\n      - qwen-plus\n      - qwen-turbo\n      - qwen-long\n  DeepSeek:\n    type: DeepSeek\n    apiKey: <your_api_key>\n    modelList: # Ini adalah nilai default\n      - deepseek-chat\n      - deepseek-reasoner\n  Mistral:\n    type: Mistral\n    apiKey: <your_api_key>\n    modelList: # Ini adalah nilai default\n      - codestral-latest\n      - mistral-large\n  SiliconFlow:\n    type: SiliconFlow\n    apiKey: <your_api_key>\n    modelList:\n      - Qwen/Qwen3-235B-A22B\n      - Pro/deepseek-ai/DeepSeek-V3\n      - THUDM/GLM-4-32B-0414\n  OpenRouter:\n    type: OpenRouter\n    apiKey: <your_api_key>\n    modelList:\n      - openai/gpt-4o\n  Gemini:\n    type: Gemini\n    apiKey: <your_api_key>\n    modelList:\n      - gemini-2.5-flash-preview-04-17\n```",
    "Status": "ok"
  }
]