[
  {
    "Id": 1,
    "Content": "# ProxyAsLocalModel\n\nProxy remote LLM API as Local model. Especially works for using custom LLM in JetBrains AI Assistant.\n\nPowered by Ktor and kotlinx.serialization. Thanks to their no-reflex features.\n\n## Story of this project\n\nCurrently, JetBrains AI Assistant provides a free plan with very limited quotes. I tried out and my quote ran out quickly.\n\nI already bought other LLM API tokens, such like Gemini and Qwen. So I started to think of using them in AI Assistant. Unfortunately, only local models from LM Studio and Ollama are supported. So I started to work on this proxy application that proxy third party LLM API as LM Studio and Ollama API so that I can use them in my JetBrains IDEs.\n\nThis is Just a simple task, so I started to use the official SDKs as clients and write a simple Ktor server that provides endpoints as LM Studio and Ollama. The problem appears when I try to distribute it as a GraalVM native image. The official Java SDKS uses too many dynamic features, making it hard to compile into a native image, even with a tracing agent. So I decided to implement a simple client of streaming chat completion API by myself with Ktor and kotlinx.serialization which are both no-reflex, functional and DSL styled.\n\nAs you can see, this application is distributed as a fat runnable jar and a GraalVM native image, which makes it cross-platform and fast to start.\n\nThe development of this application gives me confidence in Kotlin/Ktor/kotlinx.serialization. The Kotlin world uses more functional programming and less reflexion, which makes it more suitable for GraalVM native image, with faster startup and less memory usage.\n\n## Currently supported\n\nProxy from: OpenAI, Claude, DashScope(Alibaba Qwen), Gemini, Deepseek, Mistral, SiliconFlow.\n\nProxy as: LM Studio, Ollama.\n\nStreaming chat completion API only.\n",
    "ContentSha": "RqoxS65Q2kmC/NFTNk+2QDY1yLetU7emS9EGMv38jJo=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "# ProxyAsLocalModel\n\nProxy de API LLM remota como modelo local. Funciona especialmente para utilizar LLM personalizada no JetBrains AI Assistant.\n\nDesenvolvido com Ktor e kotlinx.serialization. Graças às suas funcionalidades sem reflexão.\n\n## História deste projeto\n\nAtualmente, o JetBrains AI Assistant oferece um plano gratuito com cotas muito limitadas. Experimentei e minha cota acabou rapidamente.\n\nJá adquiri tokens de API de outros LLM, como Gemini e Qwen. Então comecei a pensar em usá-los no AI Assistant. Infelizmente, apenas modelos locais do LM Studio e Ollama são suportados. Por isso, comecei a trabalhar neste aplicativo proxy que faz a ponte entre APIs LLM de terceiros e as APIs do LM Studio e Ollama, para que eu possa usá-los nos meus IDEs da JetBrains.\n\nEsta é uma tarefa simples, então comecei usando os SDKs oficiais como clientes e escrevi um servidor Ktor simples que fornece endpoints como LM Studio e Ollama. O problema surgiu quando tentei distribuir como uma imagem nativa do GraalVM. Os SDKs Java oficiais usam muitos recursos dinâmicos, dificultando a compilação para uma imagem nativa, mesmo usando um agente de rastreamento. Então decidi implementar um cliente simples da API de conclusão de chat em streaming por conta própria, usando Ktor e kotlinx.serialization, ambos sem reflexão, funcionais e com estilo DSL.\n\nComo pode ver, este aplicativo é distribuído como um fat jar executável e uma imagem nativa GraalVM, o que o torna multiplataforma e rápido para iniciar.\n\nO desenvolvimento deste aplicativo me deu confiança no Kotlin/Ktor/kotlinx.serialization. O mundo Kotlin utiliza mais programação funcional e menos reflexão, o que o torna mais adequado para imagem nativa GraalVM, com inicialização mais rápida e menor uso de memória.\n\n## Atualmente suportado\n\nProxy de: OpenAI, Claude, DashScope(Alibaba Qwen), Gemini, Deepseek, Mistral, SiliconFlow.\n\nProxy como: LM Studio, Ollama.\n\nApenas API de conclusão de chat em streaming.",
    "Status": "ok"
  },
  {
    "Id": 2,
    "Content": "## How to use\n\nThis application is a proxy server, distributed as a fat runnable jar and a GraalVM native image (Windows x64).\n\nRun the application, and you will see a help message:\n\n```\n2025-05-02 10:43:53 INFO  Help - It looks that you are starting the program for the first time here.\n2025-05-02 10:43:53 INFO  Help - A default config file is created at your_path\\config.yml with schema annotation.\n2025-05-02 10:43:53 INFO  Config - Config file watcher started at your_path\\config.yml\n2025-05-02 10:43:53 INFO  LM Studio Server - LM Studio Server started at 1234\n2025-05-02 10:43:53 INFO  Ollama Server - Ollama Server started at 11434\n2025-05-02 10:43:53 INFO  Model List - Model list loaded with: []\n```\n\nThen you can edit the config file to set up your proxy server.\n\n## Config file\n\nThis config file is automatically hot-reloaded when you change it. Only the influenced parts of the server will be updated.\n\nWhen first generating the config file, it will be created with schema annotations. This will bring completion and check in your editor.\n",
    "ContentSha": "jSta+CN7gfDD+W3lmWAocZcdmJOQZkgX0LDlOBWiy1Y=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## Como usar\n\nEste aplicativo é um servidor proxy, distribuído como um fat runnable jar e uma imagem nativa GraalVM (Windows x64).\n\nExecute o aplicativo e você verá uma mensagem de ajuda:\n\n```\n2025-05-02 10:43:53 INFO  Help - Parece que você está iniciando o programa pela primeira vez aqui.\n2025-05-02 10:43:53 INFO  Help - Um arquivo de configuração padrão foi criado em seu_caminho\\config.yml com anotações de esquema.\n2025-05-02 10:43:53 INFO  Config - Observador do arquivo de configuração iniciado em seu_caminho\\config.yml\n2025-05-02 10:43:53 INFO  LM Studio Server - Servidor LM Studio iniciado na porta 1234\n2025-05-02 10:43:53 INFO  Ollama Server - Servidor Ollama iniciado na porta 11434\n2025-05-02 10:43:53 INFO  Model List - Lista de modelos carregada com: []\n```\n\nDepois, você pode editar o arquivo de configuração para configurar seu servidor proxy.\n\n## Arquivo de configuração\n\nEste arquivo de configuração é automaticamente recarregado a quente quando você o altera. Apenas as partes influenciadas do servidor serão atualizadas.\n\nAo gerar o arquivo de configuração pela primeira vez, ele será criado com anotações de esquema. Isso proporcionará preenchimento automático e verificação em seu editor.",
    "Status": "ok"
  },
  {
    "Id": 3,
    "Content": "## Example config file\n\n```yaml\n# $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json\nlmStudio:\n  port: 1234 # This is default value\n  enabled: true # This is default value\n  host: 0.0.0.0 # This is default value\n  path: /your/path # Will be add before the original endpoints, default value is empty\nollama:\n  port: 11434 # This is default value\n  enabled: true # This is default value\n  host: 0.0.0.0 # This is default value\n  path: /your/path # Will be add before the original endpoints, default value is empty\nclient:\n  socketTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  connectionTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  requestTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  retry: 3 # This is default value\n  delayBeforeRetry: 1000 # This is default value, in milliseconds\n\napiProviders:\n  OpenAI:\n    type: OpenAi\n    baseUrl: https://api.openai.com/v1\n    apiKey: <your_api_key>\n    modelList:\n      - gpt-4o\n  Claude:\n    type: Claude\n    apiKey: <your_api_key>\n    modelList:\n      - claude-3-7-sonnet\n  Qwen:\n    type: DashScope\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - qwen-max\n      - qwen-plus\n      - qwen-turbo\n      - qwen-long\n  DeepSeek:\n    type: DeepSeek\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - deepseek-chat\n      - deepseek-reasoner\n  Mistral:\n    type: Mistral\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - codestral-latest\n      - mistral-large\n  SiliconFlow:\n    type: SiliconFlow\n    apiKey: <your_api_key>\n    modelList:\n      - Qwen/Qwen3-235B-A22B\n      - Pro/deepseek-ai/DeepSeek-V3\n      - THUDM/GLM-4-32B-0414\n  OpenRouter:\n    type: OpenRouter\n    apiKey: <your_api_key>\n    modelList:\n      - openai/gpt-4o\n  Gemini:\n    type: Gemini\n    apiKey: <your_api_key>\n    modelList:\n      - gemini-2.5-flash-preview-04-17\n```",
    "ContentSha": "70Ur2u1pGxwgBRWrkWendSuM8NT3qhqj6cqjS9POUPs=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## Exemplo de arquivo de configuração\n\n```yaml\n# $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json\nlmStudio:\n  port: 1234 # Este é o valor padrão\n  enabled: true # Este é o valor padrão\n  host: 0.0.0.0 # Este é o valor padrão\n  path: /seu/caminho # Será adicionado antes dos endpoints originais, valor padrão é vazio\nollama:\n  port: 11434 # Este é o valor padrão\n  enabled: true # Este é o valor padrão\n  host: 0.0.0.0 # Este é o valor padrão\n  path: /seu/caminho # Será adicionado antes dos endpoints originais, valor padrão é vazio\nclient:\n  socketTimeout: 1919810 # Long.MAX_VALUE é o valor padrão, em milissegundos\n  connectionTimeout: 1919810 # Long.MAX_VALUE é o valor padrão, em milissegundos\n  requestTimeout: 1919810 # Long.MAX_VALUE é o valor padrão, em milissegundos\n  retry: 3 # Este é o valor padrão\n  delayBeforeRetry: 1000 # Este é o valor padrão, em milissegundos\n\napiProviders:\n  OpenAI:\n    type: OpenAi\n    baseUrl: https://api.openai.com/v1\n    apiKey: <sua_api_key>\n    modelList:\n      - gpt-4o\n  Claude:\n    type: Claude\n    apiKey: <sua_api_key>\n    modelList:\n      - claude-3-7-sonnet\n  Qwen:\n    type: DashScope\n    apiKey: <sua_api_key>\n    modelList: # Este é o valor padrão\n      - qwen-max\n      - qwen-plus\n      - qwen-turbo\n      - qwen-long\n  DeepSeek:\n    type: DeepSeek\n    apiKey: <sua_api_key>\n    modelList: # Este é o valor padrão\n      - deepseek-chat\n      - deepseek-reasoner\n  Mistral:\n    type: Mistral\n    apiKey: <sua_api_key>\n    modelList: # Este é o valor padrão\n      - codestral-latest\n      - mistral-large\n  SiliconFlow:\n    type: SiliconFlow\n    apiKey: <sua_api_key>\n    modelList:\n      - Qwen/Qwen3-235B-A22B\n      - Pro/deepseek-ai/DeepSeek-V3\n      - THUDM/GLM-4-32B-0414\n  OpenRouter:\n    type: OpenRouter\n    apiKey: <sua_api_key>\n    modelList:\n      - openai/gpt-4o\n  Gemini:\n    type: Gemini\n    apiKey: <sua_api_key>\n    modelList:\n      - gemini-2.5-flash-preview-04-17\n```",
    "Status": "ok"
  }
]