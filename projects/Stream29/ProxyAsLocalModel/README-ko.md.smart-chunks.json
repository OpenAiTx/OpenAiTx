[
  {
    "Id": 1,
    "Content": "# ProxyAsLocalModel\n\nProxy remote LLM API as Local model. Especially works for using custom LLM in JetBrains AI Assistant.\n\nPowered by Ktor and kotlinx.serialization. Thanks to their no-reflex features.\n\n## Story of this project\n\nCurrently, JetBrains AI Assistant provides a free plan with very limited quotes. I tried out and my quote ran out quickly.\n\nI already bought other LLM API tokens, such like Gemini and Qwen. So I started to think of using them in AI Assistant. Unfortunately, only local models from LM Studio and Ollama are supported. So I started to work on this proxy application that proxy third party LLM API as LM Studio and Ollama API so that I can use them in my JetBrains IDEs.\n\nThis is Just a simple task, so I started to use the official SDKs as clients and write a simple Ktor server that provides endpoints as LM Studio and Ollama. The problem appears when I try to distribute it as a GraalVM native image. The official Java SDKS uses too many dynamic features, making it hard to compile into a native image, even with a tracing agent. So I decided to implement a simple client of streaming chat completion API by myself with Ktor and kotlinx.serialization which are both no-reflex, functional and DSL styled.\n\nAs you can see, this application is distributed as a fat runnable jar and a GraalVM native image, which makes it cross-platform and fast to start.\n\nThe development of this application gives me confidence in Kotlin/Ktor/kotlinx.serialization. The Kotlin world uses more functional programming and less reflexion, which makes it more suitable for GraalVM native image, with faster startup and less memory usage.\n\n## Currently supported\n\nProxy from: OpenAI, Claude, DashScope(Alibaba Qwen), Gemini, Deepseek, Mistral, SiliconFlow.\n\nProxy as: LM Studio, Ollama.\n\nStreaming chat completion API only.\n",
    "ContentSha": "RqoxS65Q2kmC/NFTNk+2QDY1yLetU7emS9EGMv38jJo=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "# ProxyAsLocalModel\n\n원격 LLM API를 로컬 모델로 프록시합니다. 특히 JetBrains AI Assistant에서 커스텀 LLM을 사용할 때 유용합니다.\n\nKtor와 kotlinx.serialization 기반입니다. 이들의 비-리플렉스 기능에 감사드립니다.\n\n## 이 프로젝트의 이야기\n\n현재 JetBrains AI Assistant는 매우 제한된 쿼터가 있는 무료 플랜을 제공합니다. 직접 사용해보니 쿼터가 금방 소진되었습니다.\n\n저는 이미 Gemini, Qwen 등 다른 LLM API 토큰을 구매한 상태였습니다. 그래서 AI Assistant에서 이들을 사용하고 싶다는 생각이 들었습니다. 불행히도 LM Studio와 Ollama의 로컬 모델만 지원합니다. 그래서 타사 LLM API를 LM Studio와 Ollama API로 프록시하여 JetBrains IDE에서 사용할 수 있도록 이 프록시 애플리케이션 작업을 시작했습니다.\n\n이것은 단순한 작업이기에, 공식 SDK를 클라이언트로 사용하고 LM Studio와 Ollama와 동일한 엔드포인트를 제공하는 간단한 Ktor 서버를 작성하기 시작했습니다. 그런데 이를 GraalVM 네이티브 이미지로 배포하려고 하니 문제가 발생했습니다. 공식 Java SDK는 동적 기능을 너무 많이 사용하여, 트레이싱 에이전트를 사용해도 네이티브 이미지로 컴파일하기가 어렵습니다. 그래서 직접 Ktor와 kotlinx.serialization을 사용하여 스트리밍 채팅 완성 API의 간단한 클라이언트를 구현하기로 했습니다. 이들은 모두 비-리플렉스, 함수형, 그리고 DSL 스타일입니다.\n\n보시다시피, 이 애플리케이션은 fat runnable jar와 GraalVM 네이티브 이미지로 배포되어 크로스 플랫폼이면서 빠른 시작을 자랑합니다.\n\n이 애플리케이션 개발을 통해 Kotlin/Ktor/kotlinx.serialization에 대한 신뢰가 생겼습니다. Kotlin 세계는 더 함수형 프로그래밍을 지향하고 리플렉션 사용이 적어, GraalVM 네이티브 이미지에 더 적합하며, 빠른 시작과 적은 메모리 사용이 가능합니다.\n\n## 현재 지원 현황\n\n프록시 소스: OpenAI, Claude, DashScope(Alibaba Qwen), Gemini, Deepseek, Mistral, SiliconFlow.\n\n프록시 대상: LM Studio, Ollama.\n\n스트리밍 채팅 완성 API만 지원합니다.",
    "Status": "ok"
  },
  {
    "Id": 2,
    "Content": "## How to use\n\nThis application is a proxy server, distributed as a fat runnable jar and a GraalVM native image (Windows x64).\n\nRun the application, and you will see a help message:\n\n```\n2025-05-02 10:43:53 INFO  Help - It looks that you are starting the program for the first time here.\n2025-05-02 10:43:53 INFO  Help - A default config file is created at your_path\\config.yml with schema annotation.\n2025-05-02 10:43:53 INFO  Config - Config file watcher started at your_path\\config.yml\n2025-05-02 10:43:53 INFO  LM Studio Server - LM Studio Server started at 1234\n2025-05-02 10:43:53 INFO  Ollama Server - Ollama Server started at 11434\n2025-05-02 10:43:53 INFO  Model List - Model list loaded with: []\n```\n\nThen you can edit the config file to set up your proxy server.\n\n## Config file\n\nThis config file is automatically hot-reloaded when you change it. Only the influenced parts of the server will be updated.\n\nWhen first generating the config file, it will be created with schema annotations. This will bring completion and check in your editor.\n",
    "ContentSha": "jSta+CN7gfDD+W3lmWAocZcdmJOQZkgX0LDlOBWiy1Y=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## 사용 방법\n\n이 애플리케이션은 프록시 서버로, fat runnable jar와 GraalVM 네이티브 이미지(Windows x64)로 배포됩니다.\n\n애플리케이션을 실행하면, 다음과 같은 도움말 메시지가 표시됩니다:\n\n```\n2025-05-02 10:43:53 INFO  Help - 프로그램을 이 위치에서 처음 시작하는 것 같습니다.\n2025-05-02 10:43:53 INFO  Help - 기본 설정 파일이 your_path\\config.yml에 스키마 주석과 함께 생성되었습니다.\n2025-05-02 10:43:53 INFO  Config - 설정 파일 감시자가 your_path\\config.yml에서 시작되었습니다.\n2025-05-02 10:43:53 INFO  LM Studio Server - LM Studio 서버가 1234 포트에서 시작되었습니다.\n2025-05-02 10:43:53 INFO  Ollama Server - Ollama 서버가 11434 포트에서 시작되었습니다.\n2025-05-02 10:43:53 INFO  Model List - 모델 목록이 로드되었습니다: []\n```\n\n이제 설정 파일을 편집하여 프록시 서버를 구성할 수 있습니다.\n\n## 설정 파일\n\n이 설정 파일은 변경 시 자동으로 핫 리로드됩니다. 서버의 영향받는 부분만 업데이트됩니다.\n\n처음 설정 파일이 생성될 때, 스키마 주석과 함께 생성됩니다. 이를 통해 에디터에서 자동 완성과 검증 기능을 사용할 수 있습니다.",
    "Status": "ok"
  },
  {
    "Id": 3,
    "Content": "## Example config file\n\n```yaml\n# $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json\nlmStudio:\n  port: 1234 # This is default value\n  enabled: true # This is default value\n  host: 0.0.0.0 # This is default value\n  path: /your/path # Will be add before the original endpoints, default value is empty\nollama:\n  port: 11434 # This is default value\n  enabled: true # This is default value\n  host: 0.0.0.0 # This is default value\n  path: /your/path # Will be add before the original endpoints, default value is empty\nclient:\n  socketTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  connectionTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  requestTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  retry: 3 # This is default value\n  delayBeforeRetry: 1000 # This is default value, in milliseconds\n\napiProviders:\n  OpenAI:\n    type: OpenAi\n    baseUrl: https://api.openai.com/v1\n    apiKey: <your_api_key>\n    modelList:\n      - gpt-4o\n  Claude:\n    type: Claude\n    apiKey: <your_api_key>\n    modelList:\n      - claude-3-7-sonnet\n  Qwen:\n    type: DashScope\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - qwen-max\n      - qwen-plus\n      - qwen-turbo\n      - qwen-long\n  DeepSeek:\n    type: DeepSeek\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - deepseek-chat\n      - deepseek-reasoner\n  Mistral:\n    type: Mistral\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - codestral-latest\n      - mistral-large\n  SiliconFlow:\n    type: SiliconFlow\n    apiKey: <your_api_key>\n    modelList:\n      - Qwen/Qwen3-235B-A22B\n      - Pro/deepseek-ai/DeepSeek-V3\n      - THUDM/GLM-4-32B-0414\n  OpenRouter:\n    type: OpenRouter\n    apiKey: <your_api_key>\n    modelList:\n      - openai/gpt-4o\n  Gemini:\n    type: Gemini\n    apiKey: <your_api_key>\n    modelList:\n      - gemini-2.5-flash-preview-04-17\n```",
    "ContentSha": "70Ur2u1pGxwgBRWrkWendSuM8NT3qhqj6cqjS9POUPs=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## 예시 설정 파일\n\n```yaml\n# $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json\nlmStudio:\n  port: 1234 # 기본값입니다\n  enabled: true # 기본값입니다\n  host: 0.0.0.0 # 기본값입니다\n  path: /your/path # 원래 엔드포인트 앞에 추가됩니다, 기본값은 비어 있음\nollama:\n  port: 11434 # 기본값입니다\n  enabled: true # 기본값입니다\n  host: 0.0.0.0 # 기본값입니다\n  path: /your/path # 원래 엔드포인트 앞에 추가됩니다, 기본값은 비어 있음\nclient:\n  socketTimeout: 1919810 # Long.MAX_VALUE가 기본값, 밀리초 단위\n  connectionTimeout: 1919810 # Long.MAX_VALUE가 기본값, 밀리초 단위\n  requestTimeout: 1919810 # Long.MAX_VALUE가 기본값, 밀리초 단위\n  retry: 3 # 기본값입니다\n  delayBeforeRetry: 1000 # 기본값입니다, 밀리초 단위\n\napiProviders:\n  OpenAI:\n    type: OpenAi\n    baseUrl: https://api.openai.com/v1\n    apiKey: <your_api_key>\n    modelList:\n      - gpt-4o\n  Claude:\n    type: Claude\n    apiKey: <your_api_key>\n    modelList:\n      - claude-3-7-sonnet\n  Qwen:\n    type: DashScope\n    apiKey: <your_api_key>\n    modelList: # 기본값입니다\n      - qwen-max\n      - qwen-plus\n      - qwen-turbo\n      - qwen-long\n  DeepSeek:\n    type: DeepSeek\n    apiKey: <your_api_key>\n    modelList: # 기본값입니다\n      - deepseek-chat\n      - deepseek-reasoner\n  Mistral:\n    type: Mistral\n    apiKey: <your_api_key>\n    modelList: # 기본값입니다\n      - codestral-latest\n      - mistral-large\n  SiliconFlow:\n    type: SiliconFlow\n    apiKey: <your_api_key>\n    modelList:\n      - Qwen/Qwen3-235B-A22B\n      - Pro/deepseek-ai/DeepSeek-V3\n      - THUDM/GLM-4-32B-0414\n  OpenRouter:\n    type: OpenRouter\n    apiKey: <your_api_key>\n    modelList:\n      - openai/gpt-4o\n  Gemini:\n    type: Gemini\n    apiKey: <your_api_key>\n    modelList:\n      - gemini-2.5-flash-preview-04-17\n```",
    "Status": "ok"
  }
]