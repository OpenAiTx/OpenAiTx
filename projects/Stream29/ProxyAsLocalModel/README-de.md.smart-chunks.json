[
  {
    "Id": 1,
    "Content": "# ProxyAsLocalModel\n\nProxy remote LLM API as Local model. Especially works for using custom LLM in JetBrains AI Assistant.\n\nPowered by Ktor and kotlinx.serialization. Thanks to their no-reflex features.\n\n## Story of this project\n\nCurrently, JetBrains AI Assistant provides a free plan with very limited quotes. I tried out and my quote ran out quickly.\n\nI already bought other LLM API tokens, such like Gemini and Qwen. So I started to think of using them in AI Assistant. Unfortunately, only local models from LM Studio and Ollama are supported. So I started to work on this proxy application that proxy third party LLM API as LM Studio and Ollama API so that I can use them in my JetBrains IDEs.\n\nThis is Just a simple task, so I started to use the official SDKs as clients and write a simple Ktor server that provides endpoints as LM Studio and Ollama. The problem appears when I try to distribute it as a GraalVM native image. The official Java SDKS uses too many dynamic features, making it hard to compile into a native image, even with a tracing agent. So I decided to implement a simple client of streaming chat completion API by myself with Ktor and kotlinx.serialization which are both no-reflex, functional and DSL styled.\n\nAs you can see, this application is distributed as a fat runnable jar and a GraalVM native image, which makes it cross-platform and fast to start.\n\nThe development of this application gives me confidence in Kotlin/Ktor/kotlinx.serialization. The Kotlin world uses more functional programming and less reflexion, which makes it more suitable for GraalVM native image, with faster startup and less memory usage.\n\n## Currently supported\n\nProxy from: OpenAI, Claude, DashScope(Alibaba Qwen), Gemini, Deepseek, Mistral, SiliconFlow.\n\nProxy as: LM Studio, Ollama.\n\nStreaming chat completion API only.\n",
    "ContentSha": "RqoxS65Q2kmC/NFTNk+2QDY1yLetU7emS9EGMv38jJo=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "# ProxyAsLocalModel\n\nProxy-Remote-LLM-API als lokales Modell. Besonders geeignet für die Verwendung von benutzerdefinierten LLMs im JetBrains AI Assistant.\n\nAngetrieben von Ktor und kotlinx.serialization. Dank ihrer No-Reflection-Eigenschaften.\n\n## Geschichte dieses Projekts\n\nDerzeit bietet der JetBrains AI Assistant einen kostenlosen Plan mit sehr begrenzten Kontingenten. Ich habe ihn ausprobiert und mein Kontingent war schnell aufgebraucht.\n\nIch habe bereits andere LLM-API-Tokens gekauft, wie zum Beispiel Gemini und Qwen. Daher kam mir die Idee, diese im AI Assistant zu verwenden. Leider werden nur lokale Modelle von LM Studio und Ollama unterstützt. Also begann ich mit der Entwicklung dieser Proxy-Anwendung, die Drittanbieter-LLM-APIs als LM Studio- und Ollama-API weiterleitet, sodass ich sie in meinen JetBrains-IDEs verwenden kann.\n\nDies ist nur eine einfache Aufgabe, daher begann ich damit, die offiziellen SDKs als Clients zu verwenden und einen einfachen Ktor-Server zu schreiben, der Endpunkte wie LM Studio und Ollama bereitstellt. Das Problem trat auf, als ich versuchte, es als GraalVM-Native-Image zu verteilen. Die offiziellen Java-SDKs nutzen zu viele dynamische Features, was die Kompilierung in ein Native-Image erschwert, selbst mit einem Tracing-Agent. Also entschied ich mich, einen einfachen Client für die Streaming-Chat-Completion-API selbst zu implementieren, mit Ktor und kotlinx.serialization, die beide ohne Reflection, funktional und DSL-basiert sind.\n\nWie Sie sehen können, wird diese Anwendung als Fat-Runnable-Jar und als GraalVM-Native-Image ausgeliefert, was sie plattformübergreifend und schnell startend macht.\n\nDie Entwicklung dieser Anwendung hat mir Vertrauen in Kotlin/Ktor/kotlinx.serialization gegeben. Die Kotlin-Welt verwendet mehr funktionale Programmierung und weniger Reflection, was sie für GraalVM-Native-Images besser geeignet macht, mit schnellerem Start und geringerem Speicherverbrauch.\n\n## Derzeit unterstützt\n\nProxy von: OpenAI, Claude, DashScope (Alibaba Qwen), Gemini, Deepseek, Mistral, SiliconFlow.\n\nProxy als: LM Studio, Ollama.\n\nNur Streaming-Chat-Completion-API.",
    "Status": "ok"
  },
  {
    "Id": 2,
    "Content": "## How to use\n\nThis application is a proxy server, distributed as a fat runnable jar and a GraalVM native image (Windows x64).\n\nRun the application, and you will see a help message:\n\n```\n2025-05-02 10:43:53 INFO  Help - It looks that you are starting the program for the first time here.\n2025-05-02 10:43:53 INFO  Help - A default config file is created at your_path\\config.yml with schema annotation.\n2025-05-02 10:43:53 INFO  Config - Config file watcher started at your_path\\config.yml\n2025-05-02 10:43:53 INFO  LM Studio Server - LM Studio Server started at 1234\n2025-05-02 10:43:53 INFO  Ollama Server - Ollama Server started at 11434\n2025-05-02 10:43:53 INFO  Model List - Model list loaded with: []\n```\n\nThen you can edit the config file to set up your proxy server.\n\n## Config file\n\nThis config file is automatically hot-reloaded when you change it. Only the influenced parts of the server will be updated.\n\nWhen first generating the config file, it will be created with schema annotations. This will bring completion and check in your editor.\n",
    "ContentSha": "jSta+CN7gfDD+W3lmWAocZcdmJOQZkgX0LDlOBWiy1Y=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## Verwendung\n\nDiese Anwendung ist ein Proxy-Server, der als ausführbares Fat-Jar und als GraalVM-Native-Image (Windows x64) verteilt wird.\n\nStarten Sie die Anwendung, und Sie sehen eine Hilfemeldung:\n\n```\n2025-05-02 10:43:53 INFO  Help - Es scheint, dass Sie das Programm hier zum ersten Mal starten.\n2025-05-02 10:43:53 INFO  Help - Eine Standard-Konfigurationsdatei wurde unter your_path\\config.yml mit Schema-Anmerkungen erstellt.\n2025-05-02 10:43:53 INFO  Config - Konfigurationsdatei-Überwachung gestartet unter your_path\\config.yml\n2025-05-02 10:43:53 INFO  LM Studio Server - LM Studio Server gestartet auf 1234\n2025-05-02 10:43:53 INFO  Ollama Server - Ollama Server gestartet auf 11434\n2025-05-02 10:43:53 INFO  Model List - Modellliste geladen mit: []\n```\n\nAnschließend können Sie die Konfigurationsdatei bearbeiten, um Ihren Proxy-Server einzurichten.\n\n## Konfigurationsdatei\n\nDiese Konfigurationsdatei wird automatisch neu geladen, sobald Sie sie ändern. Nur die betroffenen Teile des Servers werden aktualisiert.\n\nBei der ersten Erstellung der Konfigurationsdatei wird diese mit Schema-Anmerkungen erstellt. Dies ermöglicht Vervollständigungen und Überprüfungen in Ihrem Editor.",
    "Status": "ok"
  },
  {
    "Id": 3,
    "Content": "## Example config file\n\n```yaml\n# $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json\nlmStudio:\n  port: 1234 # This is default value\n  enabled: true # This is default value\n  host: 0.0.0.0 # This is default value\n  path: /your/path # Will be add before the original endpoints, default value is empty\nollama:\n  port: 11434 # This is default value\n  enabled: true # This is default value\n  host: 0.0.0.0 # This is default value\n  path: /your/path # Will be add before the original endpoints, default value is empty\nclient:\n  socketTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  connectionTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  requestTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  retry: 3 # This is default value\n  delayBeforeRetry: 1000 # This is default value, in milliseconds\n\napiProviders:\n  OpenAI:\n    type: OpenAi\n    baseUrl: https://api.openai.com/v1\n    apiKey: <your_api_key>\n    modelList:\n      - gpt-4o\n  Claude:\n    type: Claude\n    apiKey: <your_api_key>\n    modelList:\n      - claude-3-7-sonnet\n  Qwen:\n    type: DashScope\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - qwen-max\n      - qwen-plus\n      - qwen-turbo\n      - qwen-long\n  DeepSeek:\n    type: DeepSeek\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - deepseek-chat\n      - deepseek-reasoner\n  Mistral:\n    type: Mistral\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - codestral-latest\n      - mistral-large\n  SiliconFlow:\n    type: SiliconFlow\n    apiKey: <your_api_key>\n    modelList:\n      - Qwen/Qwen3-235B-A22B\n      - Pro/deepseek-ai/DeepSeek-V3\n      - THUDM/GLM-4-32B-0414\n  OpenRouter:\n    type: OpenRouter\n    apiKey: <your_api_key>\n    modelList:\n      - openai/gpt-4o\n  Gemini:\n    type: Gemini\n    apiKey: <your_api_key>\n    modelList:\n      - gemini-2.5-flash-preview-04-17\n```",
    "ContentSha": "70Ur2u1pGxwgBRWrkWendSuM8NT3qhqj6cqjS9POUPs=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## Beispiel-Konfigurationsdatei\n\n```yaml\n# $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json\nlmStudio:\n  port: 1234 # Dies ist der Standardwert\n  enabled: true # Dies ist der Standardwert\n  host: 0.0.0.0 # Dies ist der Standardwert\n  path: /your/path # Wird vor den ursprünglichen Endpunkten hinzugefügt, Standardwert ist leer\nollama:\n  port: 11434 # Dies ist der Standardwert\n  enabled: true # Dies ist der Standardwert\n  host: 0.0.0.0 # Dies ist der Standardwert\n  path: /your/path # Wird vor den ursprünglichen Endpunkten hinzugefügt, Standardwert ist leer\nclient:\n  socketTimeout: 1919810 # Long.MAX_VALUE ist der Standardwert, in Millisekunden\n  connectionTimeout: 1919810 # Long.MAX_VALUE ist der Standardwert, in Millisekunden\n  requestTimeout: 1919810 # Long.MAX_VALUE ist der Standardwert, in Millisekunden\n  retry: 3 # Dies ist der Standardwert\n  delayBeforeRetry: 1000 # Dies ist der Standardwert, in Millisekunden\n\napiProviders:\n  OpenAI:\n    type: OpenAi\n    baseUrl: https://api.openai.com/v1\n    apiKey: <your_api_key>\n    modelList:\n      - gpt-4o\n  Claude:\n    type: Claude\n    apiKey: <your_api_key>\n    modelList:\n      - claude-3-7-sonnet\n  Qwen:\n    type: DashScope\n    apiKey: <your_api_key>\n    modelList: # Dies ist der Standardwert\n      - qwen-max\n      - qwen-plus\n      - qwen-turbo\n      - qwen-long\n  DeepSeek:\n    type: DeepSeek\n    apiKey: <your_api_key>\n    modelList: # Dies ist der Standardwert\n      - deepseek-chat\n      - deepseek-reasoner\n  Mistral:\n    type: Mistral\n    apiKey: <your_api_key>\n    modelList: # Dies ist der Standardwert\n      - codestral-latest\n      - mistral-large\n  SiliconFlow:\n    type: SiliconFlow\n    apiKey: <your_api_key>\n    modelList:\n      - Qwen/Qwen3-235B-A22B\n      - Pro/deepseek-ai/DeepSeek-V3\n      - THUDM/GLM-4-32B-0414\n  OpenRouter:\n    type: OpenRouter\n    apiKey: <your_api_key>\n    modelList:\n      - openai/gpt-4o\n  Gemini:\n    type: Gemini\n    apiKey: <your_api_key>\n    modelList:\n      - gemini-2.5-flash-preview-04-17\n```",
    "Status": "ok"
  }
]