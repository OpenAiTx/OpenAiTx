[
  {
    "Id": 1,
    "Content": "# ProxyAsLocalModel\n\nProxy remote LLM API as Local model. Especially works for using custom LLM in JetBrains AI Assistant.\n\nPowered by Ktor and kotlinx.serialization. Thanks to their no-reflex features.\n\n## Story of this project\n\nCurrently, JetBrains AI Assistant provides a free plan with very limited quotes. I tried out and my quote ran out quickly.\n\nI already bought other LLM API tokens, such like Gemini and Qwen. So I started to think of using them in AI Assistant. Unfortunately, only local models from LM Studio and Ollama are supported. So I started to work on this proxy application that proxy third party LLM API as LM Studio and Ollama API so that I can use them in my JetBrains IDEs.\n\nThis is Just a simple task, so I started to use the official SDKs as clients and write a simple Ktor server that provides endpoints as LM Studio and Ollama. The problem appears when I try to distribute it as a GraalVM native image. The official Java SDKS uses too many dynamic features, making it hard to compile into a native image, even with a tracing agent. So I decided to implement a simple client of streaming chat completion API by myself with Ktor and kotlinx.serialization which are both no-reflex, functional and DSL styled.\n\nAs you can see, this application is distributed as a fat runnable jar and a GraalVM native image, which makes it cross-platform and fast to start.\n\nThe development of this application gives me confidence in Kotlin/Ktor/kotlinx.serialization. The Kotlin world uses more functional programming and less reflexion, which makes it more suitable for GraalVM native image, with faster startup and less memory usage.\n\n## Currently supported\n\nProxy from: OpenAI, Claude, DashScope(Alibaba Qwen), Gemini, Deepseek, Mistral, SiliconFlow.\n\nProxy as: LM Studio, Ollama.\n\nStreaming chat completion API only.\n",
    "ContentSha": "RqoxS65Q2kmC/NFTNk+2QDY1yLetU7emS9EGMv38jJo=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "# ProxyAsLocalModel\n\nوكيل API لنموذج LLM البعيد كنموذج محلي. يعمل بشكل خاص لاستخدام نماذج LLM المخصصة في مساعد JetBrains AI.\n\nمدعوم من Ktor و kotlinx.serialization. شكرًا لميزاتهما الخالية من الانعكاس.\n\n## قصة هذا المشروع\n\nحاليًا، يوفر مساعد JetBrains AI خطة مجانية مع حصص محدودة جدًا. جربتها ونفذت حصتي بسرعة.\n\nلقد اشتريت بالفعل رموز API لنماذج LLM أخرى، مثل Gemini و Qwen. لذلك بدأت أفكر في استخدامها في المساعد الذكي. للأسف، لا يتم دعم سوى النماذج المحلية من LM Studio و Ollama. لذا بدأت العمل على هذا التطبيق الوكيل الذي يقوم بوساطة واجهة برمجة التطبيقات LLM التابعة لجهات خارجية ليتم تقديمها كـ LM Studio و Ollama API حتى أتمكن من استخدامها في بيئات JetBrains IDEs الخاصة بي.\n\nهذه مهمة بسيطة فقط، لذا بدأت باستخدام الحزم البرمجية الرسمية كعملاء وكتبت خادم Ktor بسيط يوفر نقاط نهاية مثل LM Studio و Ollama. ظهرت المشكلة عندما حاولت توزيعه كصورة أصلية لـ GraalVM. تستخدم الحزم البرمجية الرسمية لجافا العديد من الميزات الديناميكية، مما يجعل من الصعب تجميعها كصورة أصلية، حتى مع وكيل تتبع. لذلك قررت تنفيذ عميل بسيط لواجهة برمجة تطبيقات إكمال الدردشة المتدفقة بنفسي باستخدام Ktor و kotlinx.serialization، وكلاهما خالي من الانعكاس، وظيفي، وأسلوبه قائم على DSL.\n\nكما ترى، يتم توزيع هذا التطبيق كملف jar قابل للتشغيل وكصورة أصلية لـ GraalVM، مما يجعله متعدد المنصات وسريع التشغيل.\n\nمنحني تطوير هذا التطبيق ثقة في Kotlin/Ktor/kotlinx.serialization. يستخدم عالم Kotlin برمجة وظيفية أكثر وانعكاسًا أقل، مما يجعله أكثر ملاءمة لصورة GraalVM الأصلية، مع بدء تشغيل أسرع واستخدام أقل للذاكرة.\n\n## المدعوم حاليًا\n\nالوكالة من: OpenAI، Claude، DashScope (Alibaba Qwen)، Gemini، Deepseek، Mistral، SiliconFlow.\n\nالوكالة كـ: LM Studio، Ollama.\n\nواجهة برمجة تطبيقات إكمال الدردشة المتدفقة فقط.",
    "Status": "ok"
  },
  {
    "Id": 2,
    "Content": "## How to use\n\nThis application is a proxy server, distributed as a fat runnable jar and a GraalVM native image (Windows x64).\n\nRun the application, and you will see a help message:\n\n```\n2025-05-02 10:43:53 INFO  Help - It looks that you are starting the program for the first time here.\n2025-05-02 10:43:53 INFO  Help - A default config file is created at your_path\\config.yml with schema annotation.\n2025-05-02 10:43:53 INFO  Config - Config file watcher started at your_path\\config.yml\n2025-05-02 10:43:53 INFO  LM Studio Server - LM Studio Server started at 1234\n2025-05-02 10:43:53 INFO  Ollama Server - Ollama Server started at 11434\n2025-05-02 10:43:53 INFO  Model List - Model list loaded with: []\n```\n\nThen you can edit the config file to set up your proxy server.\n\n## Config file\n\nThis config file is automatically hot-reloaded when you change it. Only the influenced parts of the server will be updated.\n\nWhen first generating the config file, it will be created with schema annotations. This will bring completion and check in your editor.\n",
    "ContentSha": "jSta+CN7gfDD+W3lmWAocZcdmJOQZkgX0LDlOBWiy1Y=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## كيفية الاستخدام\n\nهذا التطبيق هو خادم وكيل (بروكسي)، يتم توزيعه كملف jar قابل للتشغيل بشكل مستقل وكتصوير أصلي لـ GraalVM (Windows x64).\n\nقم بتشغيل التطبيق، وسترى رسالة مساعدة:\n\n```\n2025-05-02 10:43:53 INFO  Help - يبدو أنك تقوم بتشغيل البرنامج لأول مرة هنا.\n2025-05-02 10:43:53 INFO  Help - تم إنشاء ملف الإعداد الافتراضي في your_path\\config.yml مع توضيحات المخطط.\n2025-05-02 10:43:53 INFO  Config - تم بدء مراقب ملف الإعدادات في your_path\\config.yml\n2025-05-02 10:43:53 INFO  LM Studio Server - تم بدء خادم LM Studio على المنفذ 1234\n2025-05-02 10:43:53 INFO  Ollama Server - تم بدء خادم Ollama على المنفذ 11434\n2025-05-02 10:43:53 INFO  Model List - تم تحميل قائمة النماذج مع: []\n```\n\nبعد ذلك يمكنك تعديل ملف الإعدادات لضبط خادم الوكيل الخاص بك.\n\n## ملف الإعدادات\n\nيتم إعادة تحميل ملف الإعدادات تلقائيًا عند تغييره. سيتم تحديث الأجزاء المتأثرة فقط من الخادم.\n\nعند إنشاء ملف الإعدادات لأول مرة، سيتم إنشاؤه مع توضيحات المخطط. هذا سيوفر الإكمال والفحص في محررك.",
    "Status": "ok"
  },
  {
    "Id": 3,
    "Content": "## Example config file\n\n```yaml\n# $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json\nlmStudio:\n  port: 1234 # This is default value\n  enabled: true # This is default value\n  host: 0.0.0.0 # This is default value\n  path: /your/path # Will be add before the original endpoints, default value is empty\nollama:\n  port: 11434 # This is default value\n  enabled: true # This is default value\n  host: 0.0.0.0 # This is default value\n  path: /your/path # Will be add before the original endpoints, default value is empty\nclient:\n  socketTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  connectionTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  requestTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  retry: 3 # This is default value\n  delayBeforeRetry: 1000 # This is default value, in milliseconds\n\napiProviders:\n  OpenAI:\n    type: OpenAi\n    baseUrl: https://api.openai.com/v1\n    apiKey: <your_api_key>\n    modelList:\n      - gpt-4o\n  Claude:\n    type: Claude\n    apiKey: <your_api_key>\n    modelList:\n      - claude-3-7-sonnet\n  Qwen:\n    type: DashScope\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - qwen-max\n      - qwen-plus\n      - qwen-turbo\n      - qwen-long\n  DeepSeek:\n    type: DeepSeek\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - deepseek-chat\n      - deepseek-reasoner\n  Mistral:\n    type: Mistral\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - codestral-latest\n      - mistral-large\n  SiliconFlow:\n    type: SiliconFlow\n    apiKey: <your_api_key>\n    modelList:\n      - Qwen/Qwen3-235B-A22B\n      - Pro/deepseek-ai/DeepSeek-V3\n      - THUDM/GLM-4-32B-0414\n  OpenRouter:\n    type: OpenRouter\n    apiKey: <your_api_key>\n    modelList:\n      - openai/gpt-4o\n  Gemini:\n    type: Gemini\n    apiKey: <your_api_key>\n    modelList:\n      - gemini-2.5-flash-preview-04-17\n```",
    "ContentSha": "70Ur2u1pGxwgBRWrkWendSuM8NT3qhqj6cqjS9POUPs=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## مثال على ملف الإعدادات\n\n```yaml\n# $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json\nlmStudio:\n  port: 1234 # هذه هي القيمة الافتراضية\n  enabled: true # هذه هي القيمة الافتراضية\n  host: 0.0.0.0 # هذه هي القيمة الافتراضية\n  path: /your/path # سيتم إضافته قبل نقاط النهاية الأصلية، القيمة الافتراضية فارغة\nollama:\n  port: 11434 # هذه هي القيمة الافتراضية\n  enabled: true # هذه هي القيمة الافتراضية\n  host: 0.0.0.0 # هذه هي القيمة الافتراضية\n  path: /your/path # سيتم إضافته قبل نقاط النهاية الأصلية، القيمة الافتراضية فارغة\nclient:\n  socketTimeout: 1919810 # القيمة الافتراضية هي Long.MAX_VALUE، بالمللي ثانية\n  connectionTimeout: 1919810 # القيمة الافتراضية هي Long.MAX_VALUE، بالمللي ثانية\n  requestTimeout: 1919810 # القيمة الافتراضية هي Long.MAX_VALUE، بالمللي ثانية\n  retry: 3 # هذه هي القيمة الافتراضية\n  delayBeforeRetry: 1000 # هذه هي القيمة الافتراضية، بالمللي ثانية\n\napiProviders:\n  OpenAI:\n    type: OpenAi\n    baseUrl: https://api.openai.com/v1\n    apiKey: <your_api_key>\n    modelList:\n      - gpt-4o\n  Claude:\n    type: Claude\n    apiKey: <your_api_key>\n    modelList:\n      - claude-3-7-sonnet\n  Qwen:\n    type: DashScope\n    apiKey: <your_api_key>\n    modelList: # هذه هي القيمة الافتراضية\n      - qwen-max\n      - qwen-plus\n      - qwen-turbo\n      - qwen-long\n  DeepSeek:\n    type: DeepSeek\n    apiKey: <your_api_key>\n    modelList: # هذه هي القيمة الافتراضية\n      - deepseek-chat\n      - deepseek-reasoner\n  Mistral:\n    type: Mistral\n    apiKey: <your_api_key>\n    modelList: # هذه هي القيمة الافتراضية\n      - codestral-latest\n      - mistral-large\n  SiliconFlow:\n    type: SiliconFlow\n    apiKey: <your_api_key>\n    modelList:\n      - Qwen/Qwen3-235B-A22B\n      - Pro/deepseek-ai/DeepSeek-V3\n      - THUDM/GLM-4-32B-0414\n  OpenRouter:\n    type: OpenRouter\n    apiKey: <your_api_key>\n    modelList:\n      - openai/gpt-4o\n  Gemini:\n    type: Gemini\n    apiKey: <your_api_key>\n    modelList:\n      - gemini-2.5-flash-preview-04-17\n```",
    "Status": "ok"
  }
]