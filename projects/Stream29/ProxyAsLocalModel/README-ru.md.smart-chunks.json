[
  {
    "Id": 1,
    "Content": "# ProxyAsLocalModel\n\nProxy remote LLM API as Local model. Especially works for using custom LLM in JetBrains AI Assistant.\n\nPowered by Ktor and kotlinx.serialization. Thanks to their no-reflex features.\n\n## Story of this project\n\nCurrently, JetBrains AI Assistant provides a free plan with very limited quotes. I tried out and my quote ran out quickly.\n\nI already bought other LLM API tokens, such like Gemini and Qwen. So I started to think of using them in AI Assistant. Unfortunately, only local models from LM Studio and Ollama are supported. So I started to work on this proxy application that proxy third party LLM API as LM Studio and Ollama API so that I can use them in my JetBrains IDEs.\n\nThis is Just a simple task, so I started to use the official SDKs as clients and write a simple Ktor server that provides endpoints as LM Studio and Ollama. The problem appears when I try to distribute it as a GraalVM native image. The official Java SDKS uses too many dynamic features, making it hard to compile into a native image, even with a tracing agent. So I decided to implement a simple client of streaming chat completion API by myself with Ktor and kotlinx.serialization which are both no-reflex, functional and DSL styled.\n\nAs you can see, this application is distributed as a fat runnable jar and a GraalVM native image, which makes it cross-platform and fast to start.\n\nThe development of this application gives me confidence in Kotlin/Ktor/kotlinx.serialization. The Kotlin world uses more functional programming and less reflexion, which makes it more suitable for GraalVM native image, with faster startup and less memory usage.\n\n## Currently supported\n\nProxy from: OpenAI, Claude, DashScope(Alibaba Qwen), Gemini, Deepseek, Mistral, SiliconFlow.\n\nProxy as: LM Studio, Ollama.\n\nStreaming chat completion API only.\n",
    "ContentSha": "RqoxS65Q2kmC/NFTNk+2QDY1yLetU7emS9EGMv38jJo=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "# ProxyAsLocalModel\n\nПроксирование удалённого LLM API как локальной модели. Особенно полезно для использования пользовательских LLM в JetBrains AI Assistant.\n\nРаботает на базе Ktor и kotlinx.serialization. Спасибо их безрефлексивным возможностям.\n\n## История этого проекта\n\nВ настоящее время JetBrains AI Assistant предоставляет бесплатный тариф с очень ограниченными квотами. Я попробовал, и моя квота быстро закончилась.\n\nЯ уже приобрёл токены других LLM API, таких как Gemini и Qwen. Поэтому я задумался об их использовании в AI Assistant. К сожалению, поддерживаются только локальные модели из LM Studio и Ollama. Поэтому я начал работать над этим прокси-приложением, которое проксирует сторонние LLM API как API LM Studio и Ollama, чтобы я мог использовать их в своих IDE от JetBrains.\n\nЭто довольно простая задача, поэтому я начал использовать официальные SDK в качестве клиентов и написал простой сервер на Ktor, предоставляющий эндпоинты, как у LM Studio и Ollama. Проблема возникла, когда я попытался распространить это приложение в виде нативного образа GraalVM. Официальные Java SDK используют слишком много динамических функций, что затрудняет компиляцию в нативный образ, даже с использованием tracing agent. Поэтому я решил самостоятельно реализовать простой клиент для streaming chat completion API на Ktor и kotlinx.serialization, которые оба не используют рефлексию, функциональны и поддерживают стиль DSL.\n\nКак видите, это приложение распространяется как fat runnable jar и как нативный образ GraalVM, что делает его кроссплатформенным и быстрым при запуске.\n\nРазработка этого приложения укрепила мою уверенность в Kotlin/Ktor/kotlinx.serialization. В мире Kotlin больше функционального программирования и меньше рефлексии, что делает его более подходящим для GraalVM native image, с более быстрым запуском и меньшим потреблением памяти.\n\n## В настоящее время поддерживается\n\nПроксирование из: OpenAI, Claude, DashScope(Alibaba Qwen), Gemini, Deepseek, Mistral, SiliconFlow.\n\nПроксирование как: LM Studio, Ollama.\n\nТолько streaming chat completion API.",
    "Status": "ok"
  },
  {
    "Id": 2,
    "Content": "## How to use\n\nThis application is a proxy server, distributed as a fat runnable jar and a GraalVM native image (Windows x64).\n\nRun the application, and you will see a help message:\n\n```\n2025-05-02 10:43:53 INFO  Help - It looks that you are starting the program for the first time here.\n2025-05-02 10:43:53 INFO  Help - A default config file is created at your_path\\config.yml with schema annotation.\n2025-05-02 10:43:53 INFO  Config - Config file watcher started at your_path\\config.yml\n2025-05-02 10:43:53 INFO  LM Studio Server - LM Studio Server started at 1234\n2025-05-02 10:43:53 INFO  Ollama Server - Ollama Server started at 11434\n2025-05-02 10:43:53 INFO  Model List - Model list loaded with: []\n```\n\nThen you can edit the config file to set up your proxy server.\n\n## Config file\n\nThis config file is automatically hot-reloaded when you change it. Only the influenced parts of the server will be updated.\n\nWhen first generating the config file, it will be created with schema annotations. This will bring completion and check in your editor.\n",
    "ContentSha": "jSta+CN7gfDD+W3lmWAocZcdmJOQZkgX0LDlOBWiy1Y=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## Как использовать\n\nЭто приложение является прокси-сервером, распространяется в виде fat-jar, готового к запуску, и нативного образа GraalVM (Windows x64).\n\nЗапустите приложение, и вы увидите сообщение справки:\n\n```\n2025-05-02 10:43:53 INFO  Help - Похоже, вы запускаете программу здесь впервые.\n2025-05-02 10:43:53 INFO  Help - Файл конфигурации по умолчанию создан по адресу your_path\\config.yml с аннотациями схемы.\n2025-05-02 10:43:53 INFO  Config - Наблюдение за файлом конфигурации начато по адресу your_path\\config.yml\n2025-05-02 10:43:53 INFO  LM Studio Server - LM Studio Server запущен на порту 1234\n2025-05-02 10:43:53 INFO  Ollama Server - Ollama Server запущен на порту 11434\n2025-05-02 10:43:53 INFO  Model List - Список моделей загружен: []\n```\n\nЗатем вы можете отредактировать файл конфигурации для настройки вашего прокси-сервера.\n\n## Файл конфигурации\n\nЭтот файл конфигурации автоматически перезагружается при изменении. Будут обновлены только затронутые части сервера.\n\nПри первом создании файла конфигурации он будет сгенерирован с аннотациями схемы. Это обеспечит автодополнение и проверку в вашем редакторе.",
    "Status": "ok"
  },
  {
    "Id": 3,
    "Content": "## Example config file\n\n```yaml\n# $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json\nlmStudio:\n  port: 1234 # This is default value\n  enabled: true # This is default value\n  host: 0.0.0.0 # This is default value\n  path: /your/path # Will be add before the original endpoints, default value is empty\nollama:\n  port: 11434 # This is default value\n  enabled: true # This is default value\n  host: 0.0.0.0 # This is default value\n  path: /your/path # Will be add before the original endpoints, default value is empty\nclient:\n  socketTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  connectionTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  requestTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  retry: 3 # This is default value\n  delayBeforeRetry: 1000 # This is default value, in milliseconds\n\napiProviders:\n  OpenAI:\n    type: OpenAi\n    baseUrl: https://api.openai.com/v1\n    apiKey: <your_api_key>\n    modelList:\n      - gpt-4o\n  Claude:\n    type: Claude\n    apiKey: <your_api_key>\n    modelList:\n      - claude-3-7-sonnet\n  Qwen:\n    type: DashScope\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - qwen-max\n      - qwen-plus\n      - qwen-turbo\n      - qwen-long\n  DeepSeek:\n    type: DeepSeek\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - deepseek-chat\n      - deepseek-reasoner\n  Mistral:\n    type: Mistral\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - codestral-latest\n      - mistral-large\n  SiliconFlow:\n    type: SiliconFlow\n    apiKey: <your_api_key>\n    modelList:\n      - Qwen/Qwen3-235B-A22B\n      - Pro/deepseek-ai/DeepSeek-V3\n      - THUDM/GLM-4-32B-0414\n  OpenRouter:\n    type: OpenRouter\n    apiKey: <your_api_key>\n    modelList:\n      - openai/gpt-4o\n  Gemini:\n    type: Gemini\n    apiKey: <your_api_key>\n    modelList:\n      - gemini-2.5-flash-preview-04-17\n```",
    "ContentSha": "70Ur2u1pGxwgBRWrkWendSuM8NT3qhqj6cqjS9POUPs=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## Пример файла конфигурации\n\n```yaml\n# $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json\nlmStudio:\n  port: 1234 # Это значение по умолчанию\n  enabled: true # Это значение по умолчанию\n  host: 0.0.0.0 # Это значение по умолчанию\n  path: /your/path # Будет добавлен перед исходными конечными точками, значение по умолчанию — пустое\nollama:\n  port: 11434 # Это значение по умолчанию\n  enabled: true # Это значение по умолчанию\n  host: 0.0.0.0 # Это значение по умолчанию\n  path: /your/path # Будет добавлен перед исходными конечными точками, значение по умолчанию — пустое\nclient:\n  socketTimeout: 1919810 # Long.MAX_VALUE по умолчанию, в миллисекундах\n  connectionTimeout: 1919810 # Long.MAX_VALUE по умолчанию, в миллисекундах\n  requestTimeout: 1919810 # Long.MAX_VALUE по умолчанию, в миллисекундах\n  retry: 3 # Это значение по умолчанию\n  delayBeforeRetry: 1000 # Это значение по умолчанию, в миллисекундах\n\napiProviders:\n  OpenAI:\n    type: OpenAi\n    baseUrl: https://api.openai.com/v1\n    apiKey: <your_api_key>\n    modelList:\n      - gpt-4o\n  Claude:\n    type: Claude\n    apiKey: <your_api_key>\n    modelList:\n      - claude-3-7-sonnet\n  Qwen:\n    type: DashScope\n    apiKey: <your_api_key>\n    modelList: # Это значение по умолчанию\n      - qwen-max\n      - qwen-plus\n      - qwen-turbo\n      - qwen-long\n  DeepSeek:\n    type: DeepSeek\n    apiKey: <your_api_key>\n    modelList: # Это значение по умолчанию\n      - deepseek-chat\n      - deepseek-reasoner\n  Mistral:\n    type: Mistral\n    apiKey: <your_api_key>\n    modelList: # Это значение по умолчанию\n      - codestral-latest\n      - mistral-large\n  SiliconFlow:\n    type: SiliconFlow\n    apiKey: <your_api_key>\n    modelList:\n      - Qwen/Qwen3-235B-A22B\n      - Pro/deepseek-ai/DeepSeek-V3\n      - THUDM/GLM-4-32B-0414\n  OpenRouter:\n    type: OpenRouter\n    apiKey: <your_api_key>\n    modelList:\n      - openai/gpt-4o\n  Gemini:\n    type: Gemini\n    apiKey: <your_api_key>\n    modelList:\n      - gemini-2.5-flash-preview-04-17\n```",
    "Status": "ok"
  }
]