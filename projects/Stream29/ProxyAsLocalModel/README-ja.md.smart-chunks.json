[
  {
    "Id": 1,
    "Content": "# ProxyAsLocalModel\n\nProxy remote LLM API as Local model. Especially works for using custom LLM in JetBrains AI Assistant.\n\nPowered by Ktor and kotlinx.serialization. Thanks to their no-reflex features.\n\n## Story of this project\n\nCurrently, JetBrains AI Assistant provides a free plan with very limited quotes. I tried out and my quote ran out quickly.\n\nI already bought other LLM API tokens, such like Gemini and Qwen. So I started to think of using them in AI Assistant. Unfortunately, only local models from LM Studio and Ollama are supported. So I started to work on this proxy application that proxy third party LLM API as LM Studio and Ollama API so that I can use them in my JetBrains IDEs.\n\nThis is Just a simple task, so I started to use the official SDKs as clients and write a simple Ktor server that provides endpoints as LM Studio and Ollama. The problem appears when I try to distribute it as a GraalVM native image. The official Java SDKS uses too many dynamic features, making it hard to compile into a native image, even with a tracing agent. So I decided to implement a simple client of streaming chat completion API by myself with Ktor and kotlinx.serialization which are both no-reflex, functional and DSL styled.\n\nAs you can see, this application is distributed as a fat runnable jar and a GraalVM native image, which makes it cross-platform and fast to start.\n\nThe development of this application gives me confidence in Kotlin/Ktor/kotlinx.serialization. The Kotlin world uses more functional programming and less reflexion, which makes it more suitable for GraalVM native image, with faster startup and less memory usage.\n\n## Currently supported\n\nProxy from: OpenAI, Claude, DashScope(Alibaba Qwen), Gemini, Deepseek, Mistral, SiliconFlow.\n\nProxy as: LM Studio, Ollama.\n\nStreaming chat completion API only.\n",
    "ContentSha": "RqoxS65Q2kmC/NFTNk+2QDY1yLetU7emS9EGMv38jJo=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "# ProxyAsLocalModel\n\nリモートLLM APIをローカルモデルとしてプロキシします。特にJetBrains AI AssistantでカスタムLLMを利用する際に有効です。\n\nKtorとkotlinx.serializationによって実現されています。これらのリフレクション非依存の特徴に感謝します。\n\n## このプロジェクトの経緯\n\n現在、JetBrains AI Assistantは非常に制限された無料プランを提供しています。試してみたところ、すぐに上限に達してしまいました。\n\n私はすでにGeminiやQwenなど他のLLM APIトークンを購入済みでした。そのため、これらをAI Assistantで利用できないかと考え始めました。しかし、LM StudioとOllamaのローカルモデルのみがサポートされていました。そこで、サードパーティ製LLM APIをLM StudioやOllama APIとしてプロキシするこのアプリケーションの開発に着手しました。これにより、JetBrains IDE上でこれらを利用できるようになります。\n\nこれはシンプルなタスクだったため、公式SDKをクライアントとして利用し、LM StudioやOllamaのエンドポイントを提供するシンプルなKtorサーバーを書き始めました。しかし、GraalVMネイティブイメージとして配布しようとした際に問題が発生しました。公式のJava SDKは動的機能を多用しており、トレースエージェントを用いてもネイティブイメージへのコンパイルが困難でした。そこで、Ktorとkotlinx.serialization（どちらもリフレクション非依存、関数型、DSLスタイル）を使って、ストリーミングチャット補完APIのシンプルなクライアントを自作することにしました。\n\nご覧の通り、このアプリケーションはファットランナブルjarおよびGraalVMネイティブイメージとして配布されており、クロスプラットフォームかつ高速起動を実現しています。\n\nこのアプリケーションの開発により、Kotlin/Ktor/kotlinx.serializationへの信頼が深まりました。Kotlinの世界はより関数型でリフレクションが少ないため、GraalVMネイティブイメージにより適しており、高速な起動と低メモリ使用を実現します。\n\n## 現在サポートしているもの\n\nプロキシ元: OpenAI, Claude, DashScope(Alibaba Qwen), Gemini, Deepseek, Mistral, SiliconFlow.\n\nプロキシ先: LM Studio, Ollama.\n\nストリーミングチャット補完APIのみ対応。",
    "Status": "ok"
  },
  {
    "Id": 2,
    "Content": "## How to use\n\nThis application is a proxy server, distributed as a fat runnable jar and a GraalVM native image (Windows x64).\n\nRun the application, and you will see a help message:\n\n```\n2025-05-02 10:43:53 INFO  Help - It looks that you are starting the program for the first time here.\n2025-05-02 10:43:53 INFO  Help - A default config file is created at your_path\\config.yml with schema annotation.\n2025-05-02 10:43:53 INFO  Config - Config file watcher started at your_path\\config.yml\n2025-05-02 10:43:53 INFO  LM Studio Server - LM Studio Server started at 1234\n2025-05-02 10:43:53 INFO  Ollama Server - Ollama Server started at 11434\n2025-05-02 10:43:53 INFO  Model List - Model list loaded with: []\n```\n\nThen you can edit the config file to set up your proxy server.\n\n## Config file\n\nThis config file is automatically hot-reloaded when you change it. Only the influenced parts of the server will be updated.\n\nWhen first generating the config file, it will be created with schema annotations. This will bring completion and check in your editor.\n",
    "ContentSha": "jSta+CN7gfDD+W3lmWAocZcdmJOQZkgX0LDlOBWiy1Y=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## 使い方\n\nこのアプリケーションはプロキシサーバーであり、ファット実行可能JARとGraalVMネイティブイメージ（Windows x64）として配布されています。\n\nアプリケーションを実行すると、ヘルプメッセージが表示されます。\n\n```\n2025-05-02 10:43:53 INFO  Help - 初めてこのプログラムを起動したようです。\n2025-05-02 10:43:53 INFO  Help - デフォルトの設定ファイルが your_path\\config.yml にスキーマ注釈付きで作成されました。\n2025-05-02 10:43:53 INFO  Config - 設定ファイルウォッチャーが your_path\\config.yml で開始されました\n2025-05-02 10:43:53 INFO  LM Studio Server - LM Studio Server が 1234 で開始されました\n2025-05-02 10:43:53 INFO  Ollama Server - Ollama Server が 11434 で開始されました\n2025-05-02 10:43:53 INFO  Model List - モデルリストが読み込まれました: []\n```\n\nその後、設定ファイルを編集してプロキシサーバーをセットアップできます。\n\n## 設定ファイル\n\nこの設定ファイルは、変更時に自動的にホットリロードされます。影響を受けるサーバー部分のみが更新されます。\n\n初めて設定ファイルを生成する際、スキーマ注釈付きで作成されます。これにより、エディタで補完やチェックが利用できます。",
    "Status": "ok"
  },
  {
    "Id": 3,
    "Content": "## Example config file\n\n```yaml\n# $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json\nlmStudio:\n  port: 1234 # This is default value\n  enabled: true # This is default value\n  host: 0.0.0.0 # This is default value\n  path: /your/path # Will be add before the original endpoints, default value is empty\nollama:\n  port: 11434 # This is default value\n  enabled: true # This is default value\n  host: 0.0.0.0 # This is default value\n  path: /your/path # Will be add before the original endpoints, default value is empty\nclient:\n  socketTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  connectionTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  requestTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  retry: 3 # This is default value\n  delayBeforeRetry: 1000 # This is default value, in milliseconds\n\napiProviders:\n  OpenAI:\n    type: OpenAi\n    baseUrl: https://api.openai.com/v1\n    apiKey: <your_api_key>\n    modelList:\n      - gpt-4o\n  Claude:\n    type: Claude\n    apiKey: <your_api_key>\n    modelList:\n      - claude-3-7-sonnet\n  Qwen:\n    type: DashScope\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - qwen-max\n      - qwen-plus\n      - qwen-turbo\n      - qwen-long\n  DeepSeek:\n    type: DeepSeek\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - deepseek-chat\n      - deepseek-reasoner\n  Mistral:\n    type: Mistral\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - codestral-latest\n      - mistral-large\n  SiliconFlow:\n    type: SiliconFlow\n    apiKey: <your_api_key>\n    modelList:\n      - Qwen/Qwen3-235B-A22B\n      - Pro/deepseek-ai/DeepSeek-V3\n      - THUDM/GLM-4-32B-0414\n  OpenRouter:\n    type: OpenRouter\n    apiKey: <your_api_key>\n    modelList:\n      - openai/gpt-4o\n  Gemini:\n    type: Gemini\n    apiKey: <your_api_key>\n    modelList:\n      - gemini-2.5-flash-preview-04-17\n```",
    "ContentSha": "70Ur2u1pGxwgBRWrkWendSuM8NT3qhqj6cqjS9POUPs=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## サンプル設定ファイル\n\n```yaml\n# $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json\nlmStudio:\n  port: 1234 # これはデフォルト値です\n  enabled: true # これはデフォルト値です\n  host: 0.0.0.0 # これはデフォルト値です\n  path: /your/path # 元のエンドポイントの前に追加されます。デフォルト値は空です\nollama:\n  port: 11434 # これはデフォルト値です\n  enabled: true # これはデフォルト値です\n  host: 0.0.0.0 # これはデフォルト値です\n  path: /your/path # 元のエンドポイントの前に追加されます。デフォルト値は空です\nclient:\n  socketTimeout: 1919810 # デフォルト値は Long.MAX_VALUE、ミリ秒単位\n  connectionTimeout: 1919810 # デフォルト値は Long.MAX_VALUE、ミリ秒単位\n  requestTimeout: 1919810 # デフォルト値は Long.MAX_VALUE、ミリ秒単位\n  retry: 3 # これはデフォルト値です\n  delayBeforeRetry: 1000 # これはデフォルト値です、ミリ秒単位\n\napiProviders:\n  OpenAI:\n    type: OpenAi\n    baseUrl: https://api.openai.com/v1\n    apiKey: <your_api_key>\n    modelList:\n      - gpt-4o\n  Claude:\n    type: Claude\n    apiKey: <your_api_key>\n    modelList:\n      - claude-3-7-sonnet\n  Qwen:\n    type: DashScope\n    apiKey: <your_api_key>\n    modelList: # これはデフォルト値です\n      - qwen-max\n      - qwen-plus\n      - qwen-turbo\n      - qwen-long\n  DeepSeek:\n    type: DeepSeek\n    apiKey: <your_api_key>\n    modelList: # これはデフォルト値です\n      - deepseek-chat\n      - deepseek-reasoner\n  Mistral:\n    type: Mistral\n    apiKey: <your_api_key>\n    modelList: # これはデフォルト値です\n      - codestral-latest\n      - mistral-large\n  SiliconFlow:\n    type: SiliconFlow\n    apiKey: <your_api_key>\n    modelList:\n      - Qwen/Qwen3-235B-A22B\n      - Pro/deepseek-ai/DeepSeek-V3\n      - THUDM/GLM-4-32B-0414\n  OpenRouter:\n    type: OpenRouter\n    apiKey: <your_api_key>\n    modelList:\n      - openai/gpt-4o\n  Gemini:\n    type: Gemini\n    apiKey: <your_api_key>\n    modelList:\n      - gemini-2.5-flash-preview-04-17\n```",
    "Status": "ok"
  }
]