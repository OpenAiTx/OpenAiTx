[
  {
    "Id": 1,
    "Content": "# ProxyAsLocalModel\n\nProxy remote LLM API as Local model. Especially works for using custom LLM in JetBrains AI Assistant.\n\nPowered by Ktor and kotlinx.serialization. Thanks to their no-reflex features.\n\n## Story of this project\n\nCurrently, JetBrains AI Assistant provides a free plan with very limited quotes. I tried out and my quote ran out quickly.\n\nI already bought other LLM API tokens, such like Gemini and Qwen. So I started to think of using them in AI Assistant. Unfortunately, only local models from LM Studio and Ollama are supported. So I started to work on this proxy application that proxy third party LLM API as LM Studio and Ollama API so that I can use them in my JetBrains IDEs.\n\nThis is Just a simple task, so I started to use the official SDKs as clients and write a simple Ktor server that provides endpoints as LM Studio and Ollama. The problem appears when I try to distribute it as a GraalVM native image. The official Java SDKS uses too many dynamic features, making it hard to compile into a native image, even with a tracing agent. So I decided to implement a simple client of streaming chat completion API by myself with Ktor and kotlinx.serialization which are both no-reflex, functional and DSL styled.\n\nAs you can see, this application is distributed as a fat runnable jar and a GraalVM native image, which makes it cross-platform and fast to start.\n\nThe development of this application gives me confidence in Kotlin/Ktor/kotlinx.serialization. The Kotlin world uses more functional programming and less reflexion, which makes it more suitable for GraalVM native image, with faster startup and less memory usage.\n\n## Currently supported\n\nProxy from: OpenAI, Claude, DashScope(Alibaba Qwen), Gemini, Deepseek, Mistral, SiliconFlow.\n\nProxy as: LM Studio, Ollama.\n\nStreaming chat completion API only.\n",
    "ContentSha": "RqoxS65Q2kmC/NFTNk+2QDY1yLetU7emS9EGMv38jJo=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "# ProxyAsLocalModel\n\nProxy externe LLM API als lokaal model. Werkt vooral voor het gebruik van aangepaste LLM in JetBrains AI Assistant.\n\nAangedreven door Ktor en kotlinx.serialization. Dankzij hun no-reflex-functionaliteiten.\n\n## Verhaal van dit project\n\nMomenteel biedt JetBrains AI Assistant een gratis plan met zeer beperkte quota. Ik heb het uitgeprobeerd en mijn quota was snel op.\n\nIk heb al tokens gekocht voor andere LLM API’s, zoals Gemini en Qwen. Dus begon ik na te denken over hoe ik deze in AI Assistant kon gebruiken. Helaas worden alleen lokale modellen van LM Studio en Ollama ondersteund. Daarom begon ik te werken aan deze proxy-applicatie die externe LLM API’s proxiet als LM Studio- en Ollama-API, zodat ik ze kan gebruiken in mijn JetBrains IDE’s.\n\nDit is slechts een eenvoudige taak, dus begon ik de officiële SDK’s als clients te gebruiken en schreef ik een eenvoudige Ktor-server die endpoints aanbiedt als LM Studio en Ollama. Het probleem ontstond toen ik het wilde distribueren als een GraalVM native image. De officiële Java SDK’s gebruiken te veel dynamische functionaliteiten, waardoor het moeilijk is om te compileren naar een native image, zelfs met een tracing agent. Daarom besloot ik zelf een eenvoudige client te implementeren voor de streaming chat completion API met Ktor en kotlinx.serialization, die beide no-reflex, functioneel en DSL-georiënteerd zijn.\n\nZoals je kunt zien, wordt deze applicatie gedistribueerd als een fat runnable jar en een GraalVM native image, waardoor het cross-platform is en snel opstart.\n\nDe ontwikkeling van deze applicatie heeft mij vertrouwen gegeven in Kotlin/Ktor/kotlinx.serialization. De Kotlin-wereld gebruikt meer functioneel programmeren en minder reflectie, wat het geschikter maakt voor GraalVM native image, met snellere opstart en minder geheugengebruik.\n\n## Momenteel ondersteund\n\nProxy van: OpenAI, Claude, DashScope (Alibaba Qwen), Gemini, Deepseek, Mistral, SiliconFlow.\n\nProxy als: LM Studio, Ollama.\n\nAlleen streaming chat completion API.",
    "Status": "ok"
  },
  {
    "Id": 2,
    "Content": "## How to use\n\nThis application is a proxy server, distributed as a fat runnable jar and a GraalVM native image (Windows x64).\n\nRun the application, and you will see a help message:\n\n```\n2025-05-02 10:43:53 INFO  Help - It looks that you are starting the program for the first time here.\n2025-05-02 10:43:53 INFO  Help - A default config file is created at your_path\\config.yml with schema annotation.\n2025-05-02 10:43:53 INFO  Config - Config file watcher started at your_path\\config.yml\n2025-05-02 10:43:53 INFO  LM Studio Server - LM Studio Server started at 1234\n2025-05-02 10:43:53 INFO  Ollama Server - Ollama Server started at 11434\n2025-05-02 10:43:53 INFO  Model List - Model list loaded with: []\n```\n\nThen you can edit the config file to set up your proxy server.\n\n## Config file\n\nThis config file is automatically hot-reloaded when you change it. Only the influenced parts of the server will be updated.\n\nWhen first generating the config file, it will be created with schema annotations. This will bring completion and check in your editor.\n",
    "ContentSha": "jSta+CN7gfDD+W3lmWAocZcdmJOQZkgX0LDlOBWiy1Y=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## Hoe te gebruiken\n\nDeze applicatie is een proxyserver, verspreid als een fat runnable jar en een GraalVM native image (Windows x64).\n\nStart de applicatie en je ziet een helpbericht:\n\n```\n2025-05-02 10:43:53 INFO  Help - Het lijkt erop dat je het programma hier voor de eerste keer start.\n2025-05-02 10:43:53 INFO  Help - Een standaard configuratiebestand is aangemaakt op your_path\\config.yml met schema-annotatie.\n2025-05-02 10:43:53 INFO  Config - Configuratiebestand-watcher gestart op your_path\\config.yml\n2025-05-02 10:43:53 INFO  LM Studio Server - LM Studio Server gestart op 1234\n2025-05-02 10:43:53 INFO  Ollama Server - Ollama Server gestart op 11434\n2025-05-02 10:43:53 INFO  Model List - Modellenlijst geladen met: []\n```\n\nDaarna kun je het configuratiebestand bewerken om je proxyserver in te stellen.\n\n## Configuratiebestand\n\nDit configuratiebestand wordt automatisch hot-geregeld wanneer je het wijzigt. Alleen de getroffen onderdelen van de server worden bijgewerkt.\n\nBij het eerste aanmaken van het configuratiebestand, wordt het aangemaakt met schema-annotaties. Dit zorgt voor aanvulling en controle in je editor.",
    "Status": "ok"
  },
  {
    "Id": 3,
    "Content": "## Example config file\n\n```yaml\n# $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json\nlmStudio:\n  port: 1234 # This is default value\n  enabled: true # This is default value\n  host: 0.0.0.0 # This is default value\n  path: /your/path # Will be add before the original endpoints, default value is empty\nollama:\n  port: 11434 # This is default value\n  enabled: true # This is default value\n  host: 0.0.0.0 # This is default value\n  path: /your/path # Will be add before the original endpoints, default value is empty\nclient:\n  socketTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  connectionTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  requestTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds\n  retry: 3 # This is default value\n  delayBeforeRetry: 1000 # This is default value, in milliseconds\n\napiProviders:\n  OpenAI:\n    type: OpenAi\n    baseUrl: https://api.openai.com/v1\n    apiKey: <your_api_key>\n    modelList:\n      - gpt-4o\n  Claude:\n    type: Claude\n    apiKey: <your_api_key>\n    modelList:\n      - claude-3-7-sonnet\n  Qwen:\n    type: DashScope\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - qwen-max\n      - qwen-plus\n      - qwen-turbo\n      - qwen-long\n  DeepSeek:\n    type: DeepSeek\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - deepseek-chat\n      - deepseek-reasoner\n  Mistral:\n    type: Mistral\n    apiKey: <your_api_key>\n    modelList: # This is default value\n      - codestral-latest\n      - mistral-large\n  SiliconFlow:\n    type: SiliconFlow\n    apiKey: <your_api_key>\n    modelList:\n      - Qwen/Qwen3-235B-A22B\n      - Pro/deepseek-ai/DeepSeek-V3\n      - THUDM/GLM-4-32B-0414\n  OpenRouter:\n    type: OpenRouter\n    apiKey: <your_api_key>\n    modelList:\n      - openai/gpt-4o\n  Gemini:\n    type: Gemini\n    apiKey: <your_api_key>\n    modelList:\n      - gemini-2.5-flash-preview-04-17\n```",
    "ContentSha": "70Ur2u1pGxwgBRWrkWendSuM8NT3qhqj6cqjS9POUPs=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## Voorbeeld configuratiebestand\n\n```yaml\n# $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json\nlmStudio:\n  port: 1234 # Dit is de standaardwaarde\n  enabled: true # Dit is de standaardwaarde\n  host: 0.0.0.0 # Dit is de standaardwaarde\n  path: /your/path # Wordt toegevoegd vóór de originele eindpunten, standaardwaarde is leeg\nollama:\n  port: 11434 # Dit is de standaardwaarde\n  enabled: true # Dit is de standaardwaarde\n  host: 0.0.0.0 # Dit is de standaardwaarde\n  path: /your/path # Wordt toegevoegd vóór de originele eindpunten, standaardwaarde is leeg\nclient:\n  socketTimeout: 1919810 # Long.MAX_VALUE is de standaardwaarde, in milliseconden\n  connectionTimeout: 1919810 # Long.MAX_VALUE is de standaardwaarde, in milliseconden\n  requestTimeout: 1919810 # Long.MAX_VALUE is de standaardwaarde, in milliseconden\n  retry: 3 # Dit is de standaardwaarde\n  delayBeforeRetry: 1000 # Dit is de standaardwaarde, in milliseconden\n\napiProviders:\n  OpenAI:\n    type: OpenAi\n    baseUrl: https://api.openai.com/v1\n    apiKey: <your_api_key>\n    modelList:\n      - gpt-4o\n  Claude:\n    type: Claude\n    apiKey: <your_api_key>\n    modelList:\n      - claude-3-7-sonnet\n  Qwen:\n    type: DashScope\n    apiKey: <your_api_key>\n    modelList: # Dit is de standaardwaarde\n      - qwen-max\n      - qwen-plus\n      - qwen-turbo\n      - qwen-long\n  DeepSeek:\n    type: DeepSeek\n    apiKey: <your_api_key>\n    modelList: # Dit is de standaardwaarde\n      - deepseek-chat\n      - deepseek-reasoner\n  Mistral:\n    type: Mistral\n    apiKey: <your_api_key>\n    modelList: # Dit is de standaardwaarde\n      - codestral-latest\n      - mistral-large\n  SiliconFlow:\n    type: SiliconFlow\n    apiKey: <your_api_key>\n    modelList:\n      - Qwen/Qwen3-235B-A22B\n      - Pro/deepseek-ai/DeepSeek-V3\n      - THUDM/GLM-4-32B-0414\n  OpenRouter:\n    type: OpenRouter\n    apiKey: <your_api_key>\n    modelList:\n      - openai/gpt-4o\n  Gemini:\n    type: Gemini\n    apiKey: <your_api_key>\n    modelList:\n      - gemini-2.5-flash-preview-04-17\n```",
    "Status": "ok"
  }
]