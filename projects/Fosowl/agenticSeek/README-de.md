# AgenticSeek: Private, lokale Manus-Alternative.

<p align="center">
<img align="center" src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/agentic_seek_logo.png" width="300" height="300" alt="Agentic Seek Logo">
<p>

  English | [‰∏≠Êñá](https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHS.md) | [ÁπÅÈ´î‰∏≠Êñá](https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHT.md) | [Fran√ßais](https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_FR.md) | [Êó•Êú¨Ë™û](https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_JP.md) | [Portugu√™s (Brasil)](https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_PTBR.md) | [Espa√±ol](https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_ES.md)

*Eine **100% lokale Alternative zu Manus AI**, dieser sprachgesteuerte KI-Assistent durchsucht autonom das Web, schreibt Code und plant Aufgaben, w√§hrend alle Daten auf Ihrem Ger√§t bleiben. Entwickelt f√ºr lokale Reasoning-Modelle l√§uft er vollst√§ndig auf Ihrer eigenen Hardware, gew√§hrleistet vollst√§ndige Privatsph√§re und keinerlei Cloud-Abh√§ngigkeit.*

[![Visit AgenticSeek](https://img.shields.io/static/v1?label=Website&message=AgenticSeek&color=blue&style=flat-square)](https://fosowl.github.io/agenticSeek.html) ![License](https://img.shields.io/badge/license-GPL--3.0-green) [![Discord](https://img.shields.io/badge/Discord-Join%20Us-7289DA?logo=discord&logoColor=white)](https://discord.gg/8hGDaME3TC) [![Twitter](https://img.shields.io/twitter/url/https/twitter.com/fosowl.svg?style=social&label=Update%20%40Fosowl)](https://x.com/Martin993886460) [![GitHub stars](https://img.shields.io/github/stars/Fosowl/agenticSeek?style=social)](https://github.com/Fosowl/agenticSeek/stargazers)

### Warum AgenticSeek?

* üîí Vollst√§ndig lokal & privat ‚Äì Alles l√§uft auf Ihrem Computer ‚Äî keine Cloud, keine Datenweitergabe. Ihre Dateien, Unterhaltungen und Suchanfragen bleiben privat.

* üåê Intelligentes Web-Browsing ‚Äì AgenticSeek kann das Internet selbstst√§ndig durchsuchen ‚Äî suchen, lesen, Informationen extrahieren, Webformulare ausf√ºllen ‚Äî komplett freih√§ndig.

* üíª Autonomer Coding-Assistent ‚Äì Brauchen Sie Code? Er kann Programme in Python, C, Go, Java und mehr schreiben, debuggen und ausf√ºhren ‚Äî ganz ohne Aufsicht.

* üß† Intelligente Agenten-Auswahl ‚Äì Sie fragen, er findet automatisch den besten Agenten f√ºr die Aufgabe. Wie ein Expertenteam, das bereit ist zu helfen.

* üìã Plant & f√ºhrt komplexe Aufgaben aus ‚Äì Von Reiseplanung bis zu komplexen Projekten ‚Äî er kann gro√üe Aufgaben in Schritte unterteilen und mit mehreren KI-Agenten erledigen.

* üéôÔ∏è Sprachsteuerung ‚Äì Klare, schnelle und futuristische Sprach-zu-Text- und Text-zu-Sprache-Funktionen, die es erm√∂glichen, mit ihm zu sprechen, als w√§re er Ihr pers√∂nlicher KI-Assistent aus einem Sci-Fi-Film. (In Entwicklung)

### **Demo**

> *Kannst du nach dem agenticSeek-Projekt suchen, herausfinden, welche F√§higkeiten ben√∂tigt werden, dann die Datei CV_candidates.zip √∂ffnen und mir anschlie√üend mitteilen, welche am besten zum Projekt passen?*

https://github.com/user-attachments/assets/b8ca60e9-7b3b-4533-840e-08f9ac426316

Hinweis: Diese Demo sowie alle darin erscheinenden Dateien (z. B. CV_candidates.zip) sind vollst√§ndig fiktiv. Wir sind kein Unternehmen, sondern suchen Open-Source-Mitwirkende, keine Bewerber.

> üõ†‚ö†Ô∏èÔ∏è **Aktive Entwicklung**

> üôè Dieses Projekt begann als Nebenprojekt und hat weder eine Roadmap noch Finanzierung. Es ist viel gr√∂√üer geworden als erwartet und landete in GitHub Trending. Beitr√§ge, Feedback und Geduld sind sehr willkommen.

## Voraussetzungen

Bevor Sie beginnen, stellen Sie sicher, dass folgende Software installiert ist:

*   **Git:** Zum Klonen des Repositories. [Git herunterladen](https://git-scm.com/downloads)
*   **Python 3.10.x:** Wir empfehlen nachdr√ºcklich die Python-Version 3.10.x zu verwenden. Andere Versionen k√∂nnen zu Abh√§ngigkeitsfehlern f√ºhren. [Python 3.10 herunterladen](https://www.python.org/downloads/release/python-3100/) (W√§hlen Sie eine 3.10.x-Version).
*   **Docker Engine & Docker Compose:** Zum Ausf√ºhren geb√ºndelter Dienste wie SearxNG.
    *   Installieren Sie Docker Desktop (enth√§lt Docker Compose V2): [Windows](https://docs.docker.com/desktop/install/windows-install/) | [Mac](https://docs.docker.com/desktop/install/mac-install/) | [Linux](https://docs.docker.com/desktop/install/linux-install/)
    *   Alternativ k√∂nnen Sie Docker Engine und Docker Compose unter Linux separat installieren: [Docker Engine](https://docs.docker.com/engine/install/) | [Docker Compose](https://docs.docker.com/compose/install/) (Stellen Sie sicher, dass Compose V2 installiert ist, z. B. `sudo apt-get install docker-compose-plugin`).

### 1. **Repository klonen und Setup**

```sh
git clone https://github.com/Fosowl/agenticSeek.git
cd agenticSeek
mv .env.example .env
```

### 2. Inhalt der .env-Datei anpassen

```sh
SEARXNG_BASE_URL="http://127.0.0.1:8080"
REDIS_BASE_URL="redis://redis:6379/0"
WORK_DIR="/Users/mlg/Documents/workspace_for_ai"
OLLAMA_PORT="11434"
LM_STUDIO_PORT="1234"
CUSTOM_ADDITIONAL_LLM_PORT="11435"
OPENAI_API_KEY='optional'
DEEPSEEK_API_KEY='optional'
OPENROUTER_API_KEY='optional'
TOGETHER_API_KEY='optional'
GOOGLE_API_KEY='optional'
ANTHROPIC_API_KEY='optional'
```

Aktualisieren Sie die `.env`-Datei nach Bedarf mit Ihren eigenen Werten:

- **SEARXNG_BASE_URL**: Unver√§ndert lassen
- **REDIS_BASE_URL**: Unver√§ndert lassen
- **WORK_DIR**: Pfad zu Ihrem Arbeitsverzeichnis auf Ihrem lokalen Rechner. AgenticSeek kann diese Dateien lesen und mit ihnen interagieren.
- **OLLAMA_PORT**: Portnummer f√ºr den Ollama-Dienst.
- **LM_STUDIO_PORT**: Portnummer f√ºr den LM Studio-Dienst.
- **CUSTOM_ADDITIONAL_LLM_PORT**: Port f√ºr einen zus√§tzlichen benutzerdefinierten LLM-Dienst.

**API-Keys sind v√∂llig optional f√ºr Nutzer, die LLM lokal ausf√ºhren. Das ist der Hauptzweck dieses Projekts. Lassen Sie die Felder leer, wenn Sie √ºber ausreichend Hardware verf√ºgen.**

### 3. **Docker starten**

Stellen Sie sicher, dass Docker auf Ihrem System installiert und gestartet ist. Sie k√∂nnen Docker mit folgenden Befehlen starten:

- **Unter Linux/macOS:**  
    √ñffnen Sie ein Terminal und f√ºhren Sie aus:
    ```sh
    sudo systemctl start docker
    ```
    Oder starten Sie Docker Desktop √ºber das Anwendungsmen√º, falls installiert.

- **Unter Windows:**  
    Starten Sie Docker Desktop √ºber das Startmen√º.

Sie k√∂nnen √ºberpr√ºfen, ob Docker l√§uft, indem Sie folgenden Befehl ausf√ºhren:
```sh
docker info
```
Wenn Sie Informationen zu Ihrer Docker-Installation sehen, l√§uft Docker korrekt.

Eine Zusammenfassung finden Sie in der Tabelle der [lokalen Anbieter](#list-of-local-providers) unten.

N√§chster Schritt: [AgenticSeek lokal ausf√ºhren](#start-services-and-run)

*Siehe den Abschnitt [Fehlerbehebung](#troubleshooting), falls Sie Probleme haben.*
*Wenn Ihre Hardware keine lokalen LLMs ausf√ºhren kann, siehe [Setup zur Nutzung mit einer API](#setup-to-run-with-an-api).*
*F√ºr detaillierte Erkl√§rungen zu `config.ini`, siehe [Config-Abschnitt](#config).*

---

## Setup zum lokalen Ausf√ºhren von LLM auf Ihrem Rechner

**Hardware-Anforderungen:**

Um LLMs lokal auszuf√ºhren, ben√∂tigen Sie entsprechende Hardware. Mindestens eine GPU, die Magistral, Qwen oder Deepseek 14B ausf√ºhren kann, ist erforderlich. Siehe FAQ f√ºr detaillierte Modell-/Performance-Empfehlungen.

**Lokalen Anbieter einrichten**  

Starten Sie Ihren lokalen Anbieter, zum Beispiel mit ollama:

```sh
ollama serve
```

Eine Liste der unterst√ºtzten lokalen Anbieter finden Sie unten.

**config.ini aktualisieren**

Passen Sie die config.ini-Datei an, um provider_name auf einen unterst√ºtzten Anbieter und provider_model auf ein von Ihrem Anbieter unterst√ºtztes LLM zu setzen. Wir empfehlen Reasoning-Modelle wie *Magistral* oder *Deepseek*.

Siehe **FAQ** am Ende des README f√ºr ben√∂tigte Hardware.

```sh
[MAIN]
is_local = True # Ob Sie lokal oder mit Remote-Anbieter arbeiten.
provider_name = ollama # oder lm-studio, openai, etc..
provider_model = deepseek-r1:14b # W√§hlen Sie ein Modell, das zu Ihrer Hardware passt
provider_server_address = 127.0.0.1:11434
agent_name = Jarvis # Name Ihrer KI
recover_last_session = True # Ob die letzte Sitzung wiederhergestellt werden soll
save_session = True # Ob die aktuelle Sitzung gespeichert werden soll
speak = False # Text zu Sprache
listen = False # Sprache zu Text, nur f√ºr CLI, experimentell
jarvis_personality = False # Ob eine ‚ÄûJarvis‚Äú-√§hnliche Pers√∂nlichkeit genutzt werden soll (experimentell)
languages = en zh # Liste der Sprachen, Text zu Sprache nutzt standardm√§√üig die erste auf der Liste
[BROWSER]
headless_browser = True # Unver√§ndert lassen, au√üer bei Nutzung von CLI auf dem Host.
stealth_mode = True # Nutze undetected selenium zur Reduzierung der Browser-Erkennung
```

**Warnung**:

- Das Format der `config.ini`-Datei unterst√ºtzt keine Kommentare. 
Kopieren Sie NICHT die Beispiel-Konfiguration direkt, da Kommentare zu Fehlern f√ºhren. Passen Sie die `config.ini` manuell mit Ihren gew√ºnschten Einstellungen an ‚Äì ohne Kommentare.

- Setzen Sie *NICHT* provider_name auf `openai`, wenn Sie LM-studio zum Ausf√ºhren von LLMs verwenden. Setzen Sie es stattdessen auf `lm-studio`.

- Einige Anbieter (z. B. lm-studio) erfordern, dass Sie `http://` vor die IP-Adresse setzen. Zum Beispiel `http://127.0.0.1:1234`

**Liste lokaler Anbieter**

| Anbieter   | Lokal? | Beschreibung                                               |
|------------|--------|------------------------------------------------------------|
| ollama     | Ja     | F√ºhren Sie LLMs lokal einfach mit ollama als LLM-Anbieter aus |
| lm-studio  | Ja     | F√ºhren Sie LLM lokal mit LM Studio aus (setze `provider_name` auf `lm-studio`)|
| openai     | Ja     |  Nutzung einer OpenAI-kompatiblen API (z. B. llama.cpp server)  |

N√§chster Schritt: [Dienste starten und AgenticSeek ausf√ºhren](#Start-services-and-Run)  

*Siehe den Abschnitt [Fehlerbehebung](#troubleshooting), falls Sie Probleme haben.*
*Wenn Ihre Hardware keine lokalen LLMs ausf√ºhren kann, siehe [Setup zur Nutzung mit einer API](#setup-to-run-with-an-api).*
*F√ºr detaillierte Erkl√§rungen zu `config.ini`, siehe [Config-Abschnitt](#config).*

## Setup zur Nutzung mit einer API

Dieses Setup verwendet externe, cloudbasierte LLM-Anbieter. Sie ben√∂tigen einen API-Key von Ihrem gew√§hlten Dienst.

**1. W√§hlen Sie einen API-Anbieter und erhalten Sie einen API-Key:**

Siehe [Liste der API-Anbieter](#list-of-api-providers) unten. Besuchen Sie deren Websites, um sich zu registrieren und einen API-Key zu erhalten.

**2. Setzen Sie Ihren API-Key als Umgebungsvariable:**


*   **Linux/macOS:**
    √ñffnen Sie Ihr Terminal und verwenden Sie den Befehl `export`. Am besten f√ºgen Sie diesen Befehl in die Profil-Datei Ihrer Shell ein (z. B. `~/.bashrc`, `~/.zshrc`) f√ºr dauerhafte Speicherung.
    ```sh
    export PROVIDER_API_KEY="your_api_key_here" 
    # Ersetzen Sie PROVIDER_API_KEY durch den spezifischen Variablennamen, z. B. OPENAI_API_KEY, GOOGLE_API_KEY
    ```
    Beispiel f√ºr TogetherAI:
    ```sh
    export TOGETHER_API_KEY="xxxxxxxxxxxxxxxxxxxxxx"
    ```
*   **Windows:**
*   **Eingabeaufforderung (tempor√§r f√ºr aktuelle Sitzung):**
    ```cmd
    set PROVIDER_API_KEY=dein_api_schl√ºssel_hier
    ```
*   **PowerShell (tempor√§r f√ºr aktuelle Sitzung):**
    ```powershell
    $env:PROVIDER_API_KEY="dein_api_schl√ºssel_hier"
    ```
*   **Permanent:** Suche nach ‚ÄûUmgebungsvariablen‚Äú in der Windows-Suchleiste, klicke auf ‚ÄûSystemumgebungsvariablen bearbeiten‚Äú und dann auf die Schaltfl√§che ‚ÄûUmgebungsvariablen...‚Äú. F√ºge eine neue Benutzervariable mit dem passenden Namen (z. B. `OPENAI_API_KEY`) und deinem Schl√ºssel als Wert hinzu.

*(Siehe FAQ: [Wie setze ich API-Schl√ºssel?](#how-do-i-set-api-keys) f√ºr weitere Details).*


**3. Aktualisiere `config.ini`:**
```ini
[MAIN]
is_local = False
provider_name = openai # Oder google, deepseek, togetherAI, huggingface
provider_model = gpt-3.5-turbo # Oder gemini-1.5-flash, deepseek-chat, mistralai/Mixtral-8x7B-Instruct-v0.1 etc.
provider_server_address = # Wird typischerweise ignoriert oder kann leer bleiben, wenn is_local = False f√ºr die meisten APIs
# ... andere Einstellungen ...
```
*Warnung:* Stelle sicher, dass keine Leerzeichen am Ende der Werte in der `config.ini` stehen.

**Liste der API-Anbieter**

| Anbieter     | `provider_name` | Lokal? | Beschreibung                                      | API-Schl√ºssel-Link (Beispiele)                       |
|--------------|-----------------|--------|---------------------------------------------------|------------------------------------------------------|
| OpenAI       | `openai`        | Nein   | Nutzung der ChatGPT-Modelle √ºber die OpenAI-API.  | [platform.openai.com/signup](https://platform.openai.com/signup) |
| Google Gemini| `google`        | Nein   | Nutzung der Google Gemini-Modelle via Google AI Studio. | [aistudio.google.com/keys](https://aistudio.google.com/keys) |
| Deepseek     | `deepseek`      | Nein   | Nutzung von Deepseek-Modellen √ºber deren API.      | [platform.deepseek.com](https://platform.deepseek.com) |
| Hugging Face | `huggingface`   | Nein   | Nutzung von Modellen der Hugging Face Inference API. | [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens) |
| TogetherAI   | `togetherAI`    | Nein   | Nutzung verschiedener Open-Source-Modelle √ºber TogetherAI API. | [api.together.ai/settings/api-keys](https://api.together.ai/settings/api-keys) |

*Hinweis:*
*   Wir raten davon ab, `gpt-4o` oder andere OpenAI-Modelle f√ºr komplexes Web-Browsing und Aufgabenplanung zu verwenden, da die aktuellen Prompt-Optimierungen auf Modelle wie Deepseek ausgerichtet sind.
*   Coding-/Bash-Aufgaben k√∂nnten mit Gemini auf Probleme sto√üen, da das Modell eventuell Formatierungsanweisungen, die f√ºr Deepseek optimiert sind, nicht strikt befolgt.
*   Die `provider_server_address` in `config.ini` wird generell nicht verwendet, wenn `is_local = False`, da der API-Endpunkt in der Regel in der jeweiligen Provider-Bibliothek fest hinterlegt ist.

N√§chster Schritt: [Dienste starten und AgenticSeek ausf√ºhren](#Start-services-and-Run)

*Siehe den Abschnitt **Bekannte Probleme**, falls du auf Schwierigkeiten st√∂√üt.*

*Siehe den Abschnitt **Config** f√ºr eine ausf√ºhrliche Erkl√§rung der Konfigurationsdatei.*

---

## Dienste starten und AgenticSeek ausf√ºhren

Standardm√§√üig l√§uft AgenticSeek vollst√§ndig in Docker.

Starte die ben√∂tigten Dienste. Dies startet alle Dienste aus der docker-compose.yml, einschlie√ülich:
    - searxng
    - redis (ben√∂tigt von searxng)
    - frontend
    - backend (bei Nutzung von `full`)

```sh
./start_services.sh full # MacOS
start ./start_services.cmd full # Windows
```

**Warnung:** Dieser Schritt l√§dt und startet alle Docker-Images, was bis zu 30 Minuten dauern kann. Warte nach dem Starten der Dienste, bis der Backend-Dienst vollst√§ndig l√§uft (du solltest **backend: "GET /health HTTP/1.1" 200 OK** im Log sehen), bevor du Nachrichten sendest. Das Backend kann beim ersten Start bis zu 5 Minuten ben√∂tigen.

Gehe zu `http://localhost:3000/` ‚Äì dort solltest du die Weboberfl√§che sehen.

*Fehlerbehebung beim Dienststart:* Wenn diese Skripte fehlschlagen, stelle sicher, dass die Docker Engine l√§uft und Docker Compose (V2, `docker compose`) korrekt installiert ist. √úberpr√ºfe die Ausgaben im Terminal auf Fehlermeldungen. Siehe [FAQ: Hilfe! Ich erhalte einen Fehler beim Ausf√ºhren von AgenticSeek oder dessen Skripten.](#faq-troubleshooting)

**Optional:** Auf dem Host ausf√ºhren (CLI-Modus):

Um die CLI-Oberfl√§che zu nutzen, musst du das Paket auf dem Host installieren:

```sh
./install.sh
./install.bat # Windows
```

Dienste starten:

```sh
./start_services.sh # MacOS
start ./start_services.cmd # Windows
```

Nutze die CLI: `python3 cli.py`


---

## Nutzung

Stelle sicher, dass die Dienste mit `./start_services.sh full` laufen und √∂ffne `localhost:3000` f√ºr die Weboberfl√§che.

Du kannst auch Sprache-zu-Text nutzen, indem du `listen = True` in der config aktivierst. Nur im CLI-Modus verf√ºgbar.

Um zu beenden, sage oder tippe einfach `goodbye`.

Hier einige Anwendungsbeispiele:

> *Erstelle ein Snake-Spiel in Python!*

> *Durchsuche das Web nach den besten Caf√©s in Rennes, Frankreich, und speichere drei mit Adresse in rennes_cafes.txt.*

> *Schreibe ein Go-Programm, das die Fakult√§t einer Zahl berechnet, und speichere es als factorial.go in deinem Arbeitsbereich.*

> *Durchsuche meinen summer_pictures-Ordner nach allen JPG-Dateien, benenne sie mit dem heutigen Datum um und speichere eine Liste der umbenannten Dateien in photos_list.txt.*

> *Suche online nach beliebten Sci-Fi-Filmen aus 2024 und w√§hle drei f√ºr heute Abend aus. Speichere die Liste in movie_night.txt.*

> *Durchsuche das Web nach den neuesten KI-Nachrichtenartikeln aus 2025, w√§hle drei aus und schreibe ein Python-Skript, das deren Titel und Zusammenfassungen extrahiert. Speichere das Skript als news_scraper.py und die Zusammenfassungen in ai_news.txt im Verzeichnis /home/projects.*

> *Suche am Freitag im Web nach einer kostenlosen Aktienkurs-API, registriere dich mit supersuper7434567@gmail.com und schreibe dann ein Python-Skript, das t√§glich die Preise von Tesla abruft und die Ergebnisse in stock_prices.csv speichert.*

*Beachte, dass das Ausf√ºllen von Formularen noch experimentell ist und fehlschlagen kann.*



Nachdem du deine Anfrage eingegeben hast, weist AgenticSeek den am besten geeigneten Agenten f√ºr die Aufgabe zu.

Da dies ein fr√ºhes Prototypstadium ist, kann es sein, dass das Agenten-Routing-System nicht immer den passenden Agenten basierend auf deiner Anfrage ausw√§hlt.

Daher solltest du sehr explizit formulieren, was du m√∂chtest und wie die KI vorgehen soll. Wenn du beispielsweise eine Websuche m√∂chtest, sage nicht:

`Kennst du gute L√§nder f√ºr Solo-Reisen?`

Sondern frage:

`F√ºhre eine Websuche durch und finde heraus, welche L√§nder sich am besten f√ºr Solo-Reisen eignen`

---

## **Setup zur Ausf√ºhrung des LLM auf deinem eigenen Server**  

Wenn du einen leistungsf√§higen Computer oder Server hast, den du nutzen m√∂chtest, aber von deinem Laptop aus darauf zugreifen willst, kannst du das LLM auf einem entfernten Server mit unserem eigenen LLM-Server ausf√ºhren.

Auf deinem ‚ÄûServer‚Äú, der das KI-Modell ausf√ºhren soll, ermittle die IP-Adresse:

```sh
ip a | grep "inet " | grep -v 127.0.0.1 | awk '{print $2}' | cut -d/ -f1 # lokale IP
curl https://ipinfo.io/ip # √∂ffentliche IP
```

Hinweis: F√ºr Windows oder macOS nutze jeweils ipconfig oder ifconfig, um die IP-Adresse zu ermitteln.

Klonen das Repository und wechsle in den Ordner `server/`.


```sh
git clone --depth 1 https://github.com/Fosowl/agenticSeek.git
cd agenticSeek/llm_server/
```

Installiere die spezifischen Anforderungen f√ºr den Server:

```sh
pip3 install -r requirements.txt
```

Starte das Server-Skript.

```sh
python3 app.py --provider ollama --port 3333
```

Du hast die Wahl zwischen der Nutzung von `ollama` und `llamacpp` als LLM-Service.


Nun auf deinem pers√∂nlichen Computer:

√Ñndere die Datei `config.ini`, setze `provider_name` auf `server` und `provider_model` auf `deepseek-r1:xxb`.
Setze `provider_server_address` auf die IP-Adresse des Rechners, der das Modell ausf√ºhrt.

```sh
[MAIN]
is_local = False
provider_name = server
provider_model = deepseek-r1:70b
provider_server_address = x.x.x.x:3333
```


N√§chster Schritt: [Dienste starten und AgenticSeek ausf√ºhren](#Start-services-and-Run)  

---

## Sprache-zu-Text

Achtung: Sprache-zu-Text funktioniert derzeit nur im CLI-Modus.

Beachte, dass Sprache-zu-Text aktuell nur auf Englisch funktioniert.

Die Sprache-zu-Text-Funktion ist standardm√§√üig deaktiviert. Um sie zu aktivieren, setze die Option listen in der config.ini-Datei auf True:

```
listen = True
```

Wenn aktiviert, wartet die Sprache-zu-Text-Funktion auf ein Ausl√∂se-Schl√ºsselwort, das der Name des Agenten ist, bevor deine Eingabe verarbeitet wird. Du kannst den Namen des Agenten anpassen, indem du den Wert `agent_name` in der *config.ini* aktualisierst:

```
agent_name = Friday
```

F√ºr eine optimale Erkennung empfehlen wir, einen gel√§ufigen englischen Namen wie ‚ÄûJohn‚Äú oder ‚ÄûEmma‚Äú als Agentennamen zu verwenden.

Sobald das Transkript angezeigt wird, sagen Sie den Namen des Agenten laut, um ihn zu aktivieren (z.B. ‚ÄûFriday‚Äú).

Sprechen Sie Ihre Anfrage deutlich aus.

Beenden Sie Ihre Anfrage mit einer Best√§tigungsphrase, um das System zum Fortfahren zu veranlassen. Beispiele f√ºr Best√§tigungsphrasen sind:
```
"do it", "go ahead", "execute", "run", "start", "thanks", "would ya", "please", "okay?", "proceed", "continue", "go on", "do that", "go it", "do you understand?"
```

## Config

Beispiel-Konfiguration:
```
[MAIN]
is_local = True
provider_name = ollama
provider_model = deepseek-r1:32b
provider_server_address = http://127.0.0.1:11434 # Beispiel f√ºr Ollama; benutze http://127.0.0.1:1234 f√ºr LM-Studio
agent_name = Friday
recover_last_session = False
save_session = False
speak = False
listen = False

jarvis_personality = False
languages = en zh # Liste der Sprachen f√ºr TTS und ggf. Routing.
[BROWSER]
headless_browser = False
stealth_mode = False
```

**Erkl√§rung der `config.ini`-Einstellungen**:

*   **`[MAIN]` Abschnitt:**
    *   `is_local`: `True`, wenn ein lokaler LLM-Provider verwendet wird (Ollama, LM-Studio, lokaler OpenAI-kompatibler Server) oder die selbst gehostete Server-Option. `False`, wenn eine Cloud-basierte API (OpenAI, Google, etc.) verwendet wird.
    *   `provider_name`: Gibt den LLM-Provider an.
        *   Lokale Optionen: `ollama`, `lm-studio`, `openai` (f√ºr lokale OpenAI-kompatible Server), `server` (f√ºr das selbst gehostete Server-Setup).
        *   API-Optionen: `openai`, `google`, `deepseek`, `huggingface`, `togetherAI`.
    *   `provider_model`: Der spezifische Modellname oder die ID f√ºr den gew√§hlten Provider (z.B. `deepseekcoder:6.7b` f√ºr Ollama, `gpt-3.5-turbo` f√ºr die OpenAI-API, `mistralai/Mixtral-8x7B-Instruct-v0.1` f√ºr TogetherAI).
    *   `provider_server_address`: Die Adresse Ihres LLM-Providers.
        *   F√ºr lokale Provider: z.B. `http://127.0.0.1:11434` f√ºr Ollama, `http://127.0.0.1:1234` f√ºr LM-Studio.
        *   F√ºr den `server`-Provider-Typ: Die Adresse Ihres selbst gehosteten LLM-Servers (z.B. `http://your_server_ip:3333`).
        *   F√ºr Cloud-APIs (`is_local = False`): Wird meist ignoriert oder kann leer gelassen werden, da der API-Endpunkt i.d.R. durch die Client-Bibliothek verarbeitet wird.
    *   `agent_name`: Name des KI-Assistenten (z.B. Friday). Wird als Triggerwort f√ºr Sprache-zu-Text verwendet, falls aktiviert.
    *   `recover_last_session`: `True`, um den Zustand der vorherigen Sitzung wiederherzustellen, `False` f√ºr einen Neustart.
    *   `save_session`: `True`, um den aktuellen Sitzungsstatus f√ºr eine m√∂gliche Wiederherstellung zu speichern, sonst `False`.
    *   `speak`: `True`, um Text-zu-Sprache-Sprachausgabe zu aktivieren, `False` zum Deaktivieren.
    *   `listen`: `True`, um Sprache-zu-Text-Eingabe zu aktivieren (nur CLI-Modus), `False` zum Deaktivieren.
    *   `work_dir`: **Wichtig:** Das Verzeichnis, in dem AgenticSeek Dateien liest/schreibt. **Stellen Sie sicher, dass dieser Pfad auf Ihrem System g√ºltig und zug√§nglich ist.**
    *   `jarvis_personality`: `True` f√ºr einen experimentellen, ‚ÄûJarvis-√§hnlichen‚Äú Systemprompt, `False` f√ºr den Standardprompt.
    *   `languages`: Kommagetrennte Liste von Sprachen (z.B. `en, zh, fr`). Wird f√ºr die Auswahl der TTS-Stimme verwendet (Standard ist die erste) und kann dem LLM-Router helfen. Vermeiden Sie zu viele oder sehr √§hnliche Sprachen f√ºr die Effizienz des Routers.
*   **`[BROWSER]` Abschnitt:**
    *   `headless_browser`: `True`, um den automatisierten Browser ohne sichtbares Fenster auszuf√ºhren (empfohlen f√ºr Web-Oberfl√§che oder nicht-interaktive Nutzung). `False`, um das Browserfenster anzuzeigen (n√ºtzlich f√ºr CLI-Modus oder Debugging).
    *   `stealth_mode`: `True`, um Ma√ünahmen zu aktivieren, die die Erkennung von Browserautomatisierung erschweren. Dies kann die manuelle Installation von Browser-Erweiterungen wie anticaptcha erfordern.

Dieser Abschnitt fasst die unterst√ºtzten LLM-Provider-Typen zusammen. Konfigurieren Sie diese in der `config.ini`.

**Lokale Provider (laufen auf Ihrer eigenen Hardware):**

| Provider-Name in `config.ini` | `is_local` | Beschreibung                                                                 | Setup-Abschnitt                                                    |
|-------------------------------|------------|-----------------------------------------------------------------------------|--------------------------------------------------------------------|
| `ollama`                      | `True`     | Nutzt Ollama zum Bereitstellen lokaler LLMs.                                | [Setup f√ºr lokalen LLM-Betrieb](#setup-for-running-llm-locally-on-your-machine) |
| `lm-studio`                   | `True`     | Nutzt LM-Studio zum Bereitstellen lokaler LLMs.                             | [Setup f√ºr lokalen LLM-Betrieb](#setup-for-running-llm-locally-on-your-machine) |
| `openai` (f√ºr lokalen Server) | `True`     | Verbindung zu lokalem Server mit OpenAI-kompatibler API (z.B. llama.cpp).   | [Setup f√ºr lokalen LLM-Betrieb](#setup-for-running-llm-locally-on-your-machine) |
| `server`                      | `False`    | Verbindung zum AgenticSeek-LLM-Server auf einem anderen Rechner.             | [Setup f√ºr eigenen LLM-Server](#setup-to-run-the-llm-on-your-own-server) |

**API-Provider (Cloud-basiert):**

| Provider-Name in `config.ini` | `is_local` | Beschreibung                                      | Setup-Abschnitt                                      |
|-------------------------------|------------|--------------------------------------------------|------------------------------------------------------|
| `openai`                      | `False`    | Nutzung der offiziellen OpenAI-API (z.B. GPT-3.5, GPT-4). | [Setup f√ºr API-Betrieb](#setup-to-run-with-an-api)         |
| `google`                      | `False`    | Nutzung der Gemini-Modelle von Google via API.    | [Setup f√ºr API-Betrieb](#setup-to-run-with-an-api)         |
| `deepseek`                    | `False`    | Nutzung der offiziellen Deepseek-API.             | [Setup f√ºr API-Betrieb](#setup-to-run-with-an-api)         |
| `huggingface`                 | `False`    | Nutzung der Hugging Face Inference API.           | [Setup f√ºr API-Betrieb](#setup-to-run-with-an-api)         |
| `togetherAI`                  | `False`    | Nutzung der TogetherAI-API f√ºr verschiedene Open-Modelle. | [Setup f√ºr API-Betrieb](#setup-to-run-with-an-api)         |

---
## Fehlerbehebung

Wenn Sie auf Probleme sto√üen, finden Sie in diesem Abschnitt Hilfestellungen.

# Bekannte Probleme

## ChromeDriver-Probleme

**Fehlerbeispiel:** `SessionNotCreatedException: Message: session not created: This version of ChromeDriver only supports Chrome version XXX`

*   **Ursache:** Die installierte ChromeDriver-Version ist nicht kompatibel mit Ihrer Google Chrome-Version.
*   **L√∂sung:**
    1.  **Chrome-Version pr√ºfen:** √ñffnen Sie Google Chrome, gehen Sie zu `Einstellungen > √úber Chrome`, um Ihre Version zu finden (z.B. ‚ÄûVersion 120.0.6099.110‚Äú).
    2.  **Passenden ChromeDriver herunterladen:**
        *   F√ºr Chrome-Versionen 115 und neuer: Gehen Sie zu den [Chrome for Testing (CfT) JSON Endpoints](https://googlechromelabs.github.io/chrome-for-testing/). Finden Sie den "stable"-Kanal und laden Sie den ChromeDriver f√ºr Ihr Betriebssystem herunter, der zu Ihrer Chrome-Hauptversion passt.
        *   F√ºr √§ltere Versionen (seltener): Sie finden diese ggf. auf der Seite [ChromeDriver - WebDriver for Chrome](https://chromedriver.chromium.org/downloads).
        *   Das untenstehende Bild zeigt ein Beispiel von der CfT-Seite:
            ![Download Chromedriver specific version from Chrome for Testing page](https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/chromedriver_readme.png)
    3.  **ChromeDriver installieren:**
        *   Stellen Sie sicher, dass der heruntergeladene `chromedriver` (bzw. `chromedriver.exe` unter Windows) in einem Verzeichnis liegt, das in Ihrer PATH-Umgebungsvariablen aufgef√ºhrt ist (z.B. `/usr/local/bin` unter Linux/macOS oder einen eigenen Scripts-Ordner unter Windows).
        *   Alternativ k√∂nnen Sie ihn im Hauptverzeichnis des `agenticSeek`-Projekts ablegen.
        *   Machen Sie den Treiber ausf√ºhrbar (z.B. `chmod +x chromedriver` unter Linux/macOS).
    4.  Siehe auch den Abschnitt [ChromeDriver-Installation](#chromedriver-installation) im Haupt-Installationsleitfaden f√ºr weitere Details.

Wenn dieser Abschnitt unvollst√§ndig ist oder Sie auf weitere ChromeDriver-Probleme sto√üen, durchsuchen Sie bitte die bestehenden [GitHub Issues](https://github.com/Fosowl/agenticSeek/issues) oder erstellen Sie ein neues.

`Exception: Failed to initialize browser: Message: session not created: This version of ChromeDriver only supports Chrome version 113
Current browser version is 134.0.6998.89 with binary path`

Dies passiert, wenn die Versionen Ihres Browsers und des Chromedrivers nicht √ºbereinstimmen.

Sie m√ºssen die neueste Version herunterladen:

https://developer.chrome.com/docs/chromedriver/downloads

Wenn Sie Chrome Version 115 oder neuer verwenden, gehen Sie zu:

https://googlechromelabs.github.io/chrome-for-testing/

Und laden Sie die zum Betriebssystem passende Chromedriver-Version herunter.

![alt text](https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/chromedriver_readme.png)

Wenn dieser Abschnitt unvollst√§ndig ist, erstellen Sie bitte ein Issue.

##  Probleme mit Connection Adapters

```
Exception: Provider lm-studio failed: HTTP request failed: No connection adapters were found for '127.0.0.1:1234/v1/chat/completions'` (Hinweis: Port kann variieren)
```

*   **Ursache:** Die `provider_server_address` in der `config.ini` f√ºr `lm-studio` (oder andere lokale OpenAI-kompatible Server) fehlt das Pr√§fix `http://` oder sie verweist auf den falschen Port.
*   **L√∂sung:**
    *   Stellen Sie sicher, dass die Adresse mit `http://` beginnt. LM-Studio verwendet typischerweise `http://127.0.0.1:1234`.
    *   Korrigierte `config.ini`: `provider_server_address = http://127.0.0.1:1234` (oder Ihr tats√§chlicher LM-Studio-Server-Port).

## SearxNG-Basis-URL nicht angegeben

```
raise ValueError("SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.")
ValueError: SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.`
```

## FAQ

**F: Welche Hardware ben√∂tige ich?**  

| Modellgr√∂√üe | GPU      | Kommentar                                                                                         |
|-------------|----------|--------------------------------------------------------------------------------------------------|
| 7B          | 8 GB VRAM| ‚ö†Ô∏è Nicht empfohlen. Schlechte Performance, h√§ufige Halluzinationen, und Planungsagenten schlagen fehl. |
| 14B         | 12 GB VRAM (z.B. RTX 3060) | ‚úÖ F√ºr einfache Aufgaben nutzbar. Kann bei Web-Browsing und Planung ins Straucheln geraten. |
| 32B         | 24+ GB VRAM (z.B. RTX 4090) | üöÄ Erfolgreich bei den meisten Aufgaben, k√∂nnte bei komplexer Planung an Grenzen sto√üen.   |
| 70B+        | 48+ GB VRAM| üí™ Exzellent. Empfohlen f√ºr fortgeschrittene Anwendungsf√§lle.                                 |

**F: Ich erhalte einen Fehler ‚Äì was tun?**  

Stellen Sie sicher, dass lokal ausgef√ºhrt wird (`ollama serve`), Ihre `config.ini` zum Provider passt und alle Abh√§ngigkeiten installiert sind. Wenn nichts hilft, gerne ein Issue erstellen.

**F: Kann es wirklich 100% lokal laufen?**  

Ja, mit Ollama, lm-studio oder server-Providern laufen alle Sprachmodelle und TTS/STT-Modelle lokal. Nicht-lokale Optionen (OpenAI oder andere APIs) sind optional.

**F: Warum AgenticSeek nutzen, wenn ich Manus habe?**

Im Gegensatz zu Manus setzt AgenticSeek auf Unabh√§ngigkeit von externen Systemen, bietet mehr Kontrolle, Privatsph√§re und vermeidet API-Kosten.

**F: Wer steckt hinter dem Projekt?**

Das Projekt wurde von mir ins Leben gerufen, zusammen mit zwei Freunden, die als Maintainer und Mitwirkende aus der Open-Source-Community auf GitHub agieren. Wir sind einfach eine Gruppe engagierter Leute, kein Startup oder Unternehmen.

Jedes AgenticSeek-Konto auf X au√üer meinem pers√∂nlichen (https://x.com/Martin993886460) ist eine Nachahmung.

## Mitwirken

Wir suchen Entwickler, die AgenticSeek verbessern! Schau dir offene Issues oder Diskussionen an.

[Beitragsleitfaden](https://raw.githubusercontent.com/Fosowl/agenticSeek/main/docs/CONTRIBUTING.md)

[![Star History Chart](https://api.star-history.com/svg?repos=Fosowl/agenticSeek&type=Date)](https://www.star-history.com/#Fosowl/agenticSeek&Date)

## Maintainer:

 > [Fosowl](https://github.com/Fosowl) | Pariser Zeit 

 > [antoineVIVIES](https://github.com/antoineVIVIES) | Taipeh Zeit 

 > [steveh8758](https://github.com/steveh8758) | Taipeh Zeit 

## Besonderer Dank:

 > [tcsenpai](https://github.com/tcsenpai) und [plitc](https://github.com/plitc) f√ºr die Unterst√ºtzung bei der Backend-Dockerisierung

## Sponsoren:

5$ oder mehr monatliche Sponsoren werden hier genannt:
- **tatra-labs**
Certainly! Please provide the content for Part 4 of 4 that you wish to have translated.

---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-06-16

---