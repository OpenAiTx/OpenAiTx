# AgenticSeek : Alternative priv√©e et locale √† Manus.

<p align="center">
<img align="center" src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/agentic_seek_logo.png" width="300" height="300" alt="Agentic Seek Logo">
<p>

  English | [‰∏≠Êñá](https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHS.md) | [ÁπÅÈ´î‰∏≠Êñá](https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHT.md) | [Fran√ßais](https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_FR.md) | [Êó•Êú¨Ë™û](https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_JP.md) | [Portugu√™s (Brasil)](https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_PTBR.md) | [Espa√±ol](https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_ES.md)

*Une **alternative 100% locale √† Manus AI**, cet assistant IA activ√© par la voix navigue de fa√ßon autonome sur le web, √©crit du code et planifie des t√¢ches tout en gardant toutes les donn√©es sur votre appareil. Con√ßu pour les mod√®les de raisonnement locaux, il fonctionne enti√®rement sur votre mat√©riel, garantissant une confidentialit√© totale et aucune d√©pendance au cloud.*

[![Visiter AgenticSeek](https://img.shields.io/static/v1?label=Website&message=AgenticSeek&color=blue&style=flat-square)](https://fosowl.github.io/agenticSeek.html) ![License](https://img.shields.io/badge/license-GPL--3.0-green) [![Discord](https://img.shields.io/badge/Discord-Join%20Us-7289DA?logo=discord&logoColor=white)](https://discord.gg/8hGDaME3TC) [![Twitter](https://img.shields.io/twitter/url/https/twitter.com/fosowl.svg?style=social&label=Update%20%40Fosowl)](https://x.com/Martin993886460) [![GitHub stars](https://img.shields.io/github/stars/Fosowl/agenticSeek?style=social)](https://github.com/Fosowl/agenticSeek/stargazers)

### Pourquoi AgenticSeek ?

* üîí Enti√®rement local & priv√© - Tout s'ex√©cute sur votre machine ‚Äî pas de cloud, pas de partage de donn√©es. Vos fichiers, conversations et recherches restent priv√©s.

* üåê Navigation web intelligente - AgenticSeek peut naviguer sur Internet de fa√ßon autonome ‚Äî rechercher, lire, extraire des informations, remplir des formulaires web ‚Äî tout cela en mode mains libres.

* üíª Assistant de codage autonome - Besoin de code ? Il peut √©crire, d√©boguer et ex√©cuter des programmes en Python, C, Go, Java, et plus encore ‚Äî sans supervision.

* üß† S√©lection intelligente d‚Äôagents - Vous demandez, il choisit automatiquement le meilleur agent pour la t√¢che. Comme si vous aviez une √©quipe d‚Äôexperts √† disposition.

* üìã Planifie & ex√©cute des t√¢ches complexes - De la planification de voyage √† la gestion de projets complexes ‚Äî il peut d√©couper de grandes t√¢ches en √©tapes et les r√©aliser en utilisant plusieurs agents IA.

* üéôÔ∏è Activation vocale - Voix propre, rapide et futuriste, et transcription vocale permettant de lui parler comme √† votre IA personnelle d‚Äôun film de science-fiction. (En cours de d√©veloppement)

### **D√©mo**

> *Peux-tu rechercher le projet agenticSeek, apprendre quelles comp√©tences sont requises, puis ouvrir le CV_candidates.zip et ensuite me dire lesquelles correspondent le mieux au projet*

https://github.com/user-attachments/assets/b8ca60e9-7b3b-4533-840e-08f9ac426316

Avertissement : Cette d√©mo, y compris tous les fichiers qui apparaissent (ex : CV_candidates.zip), est enti√®rement fictive. Nous ne sommes pas une entreprise, nous recherchons des contributeurs open-source, pas des candidats.

> üõ†‚ö†Ô∏èÔ∏è **Travail en cours actif**

> üôè Ce projet a commenc√© comme un projet annexe, sans feuille de route ni financement. Il a largement d√©pass√© mes attentes en finissant dans GitHub Trending. Les contributions, retours et votre patience sont grandement appr√©ci√©s.

## Pr√©requis

Avant de commencer, assurez-vous d‚Äôavoir les logiciels suivants install√©s :

*   **Git :** Pour cloner le d√©p√¥t. [T√©l√©charger Git](https://git-scm.com/downloads)
*   **Python 3.10.x :** Nous recommandons fortement d‚Äôutiliser Python version 3.10.x. L‚Äôutilisation d‚Äôautres versions peut entra√Æner des erreurs de d√©pendances. [T√©l√©charger Python 3.10](https://www.python.org/downloads/release/python-3100/) (choisissez une version 3.10.x).
*   **Docker Engine & Docker Compose :** Pour ex√©cuter les services group√©s comme SearxNG.
    *   Installer Docker Desktop (qui inclut Docker Compose V2) : [Windows](https://docs.docker.com/desktop/install/windows-install/) | [Mac](https://docs.docker.com/desktop/install/mac-install/) | [Linux](https://docs.docker.com/desktop/install/linux-install/)
    *   Alternativement, installez Docker Engine et Docker Compose s√©par√©ment sur Linux : [Docker Engine](https://docs.docker.com/engine/install/) | [Docker Compose](https://docs.docker.com/compose/install/) (assurez-vous d‚Äôinstaller Compose V2, ex : `sudo apt-get install docker-compose-plugin`).

### 1. **Cloner le d√©p√¥t et configurer**

```sh
git clone https://github.com/Fosowl/agenticSeek.git
cd agenticSeek
mv .env.example .env
```

### 2. Modifier le contenu du fichier .env

```sh
SEARXNG_BASE_URL="http://127.0.0.1:8080"
REDIS_BASE_URL="redis://redis:6379/0"
WORK_DIR="/Users/mlg/Documents/workspace_for_ai"
OLLAMA_PORT="11434"
LM_STUDIO_PORT="1234"
CUSTOM_ADDITIONAL_LLM_PORT="11435"
OPENAI_API_KEY='optional'
DEEPSEEK_API_KEY='optional'
OPENROUTER_API_KEY='optional'
TOGETHER_API_KEY='optional'
GOOGLE_API_KEY='optional'
ANTHROPIC_API_KEY='optional'
```

Mettez √† jour le fichier `.env` avec vos propres valeurs si besoin :

- **SEARXNG_BASE_URL** : Laisser inchang√© 
- **REDIS_BASE_URL** : Laisser inchang√© 
- **WORK_DIR** : Chemin vers votre dossier de travail local. AgenticSeek pourra lire et interagir avec ces fichiers.
- **OLLAMA_PORT** : Num√©ro de port pour le service Ollama.
- **LM_STUDIO_PORT** : Num√©ro de port pour le service LM Studio.
- **CUSTOM_ADDITIONAL_LLM_PORT** : Port pour tout service LLM personnalis√© suppl√©mentaire.

**Les cl√©s API sont totalement optionnelles pour les utilisateurs qui choisissent de faire tourner les LLM localement. C‚Äôest le but principal de ce projet. Laissez vide si vous avez un mat√©riel suffisant**

### 3. **D√©marrer Docker**

Assurez-vous que Docker est install√© et en cours d‚Äôex√©cution sur votre syst√®me. Vous pouvez d√©marrer Docker avec les commandes suivantes :

- **Sur Linux/macOS :**  
    Ouvrez un terminal et lancez :
    ```sh
    sudo systemctl start docker
    ```
    Ou lancez Docker Desktop depuis votre menu d‚Äôapplications si install√©.

- **Sur Windows :**  
    Lancez Docker Desktop depuis le menu D√©marrer.

Vous pouvez v√©rifier que Docker fonctionne en ex√©cutant :
```sh
docker info
```
Si vous voyez des informations sur votre installation Docker, cela fonctionne correctement.

Voir le tableau des [Fournisseurs locaux](#list-of-local-providers) ci-dessous pour un r√©sum√©.

Prochaine √©tape : [Lancer AgenticSeek en local](#start-services-and-run)

*Voir la section [D√©pannage](#troubleshooting) si vous rencontrez des probl√®mes.*
*Si votre mat√©riel ne peut pas ex√©cuter les LLM localement, voir [Configuration pour utilisation via API](#setup-to-run-with-an-api).*
*Pour une explication d√©taill√©e de `config.ini`, voir la [Section Config](#config).*

---

## Configuration pour faire tourner un LLM localement sur votre machine

**Configuration mat√©rielle requise :**

Pour faire tourner les LLM localement, vous aurez besoin d‚Äôun mat√©riel suffisant. Au minimum, un GPU capable d‚Äôex√©cuter Magistral, Qwen ou Deepseek 14B est requis. Voir la FAQ pour des recommandations d√©taill√©es sur les mod√®les/performances.

**D√©marrez votre fournisseur local**  

Lancez votre fournisseur local, par exemple avec ollama :

```sh
ollama serve
```

Voir ci-dessous la liste des fournisseurs locaux pris en charge.

**Mettre √† jour le config.ini**

Modifiez le fichier config.ini pour d√©finir provider_name sur un fournisseur pris en charge et provider_model sur un LLM support√© par votre fournisseur. Nous recommandons un mod√®le de raisonnement tel que *Magistral* ou *Deepseek*.

Voir la **FAQ** √† la fin du README pour le mat√©riel requis.

```sh
[MAIN]
is_local = True # Si vous ex√©cutez localement ou avec un fournisseur distant.
provider_name = ollama # ou lm-studio, openai, etc..
provider_model = deepseek-r1:14b # choisissez un mod√®le adapt√© √† votre mat√©riel
provider_server_address = 127.0.0.1:11434
agent_name = Jarvis # nom de votre IA
recover_last_session = True # pour r√©cup√©rer la session pr√©c√©dente
save_session = True # pour m√©moriser la session courante
speak = False # texte vers parole
listen = False # reconnaissance vocale, uniquement pour CLI, exp√©rimental
jarvis_personality = False # Pour un comportement plus "Jarvis" (exp√©rimental)
languages = en zh # Liste des langues, la synth√®se vocale utilisera la premi√®re langue de la liste
[BROWSER]
headless_browser = True # laisser inchang√© sauf utilisation CLI sur h√¥te.
stealth_mode = True # Utilise selenium ind√©tectable pour r√©duire la d√©tection par les sites
```

**Attention** :

- Le format du fichier `config.ini` ne supporte pas les commentaires. 
Ne copiez/collez pas la configuration d‚Äôexemple directement, car les commentaires provoqueront des erreurs. Modifiez plut√¥t manuellement le fichier `config.ini` selon vos besoins, sans commentaires.

- Ne mettez *PAS* provider_name √† `openai` si vous utilisez LM-studio pour ex√©cuter les LLM. Utilisez `lm-studio`.

- Certains fournisseurs (ex : lm-studio) exigent `http://` devant l‚ÄôIP. Par exemple `http://127.0.0.1:1234`

**Liste des fournisseurs locaux**

| Fournisseur  | Local ? | Description                                               |
|--------------|---------|-----------------------------------------------------------|
| ollama       | Oui     | Ex√©cutez des LLM localement facilement avec ollama        |
| lm-studio    | Oui     | Ex√©cutez un LLM localement avec LM studio (utiliser `provider_name` √† `lm-studio`)|
| openai       | Oui     | Utilisez une API compatible openai (ex : serveur llama.cpp)  |

Prochaine √©tape : [D√©marrer les services et lancer AgenticSeek](#Start-services-and-Run)  

*Voir la section [D√©pannage](#troubleshooting) si vous rencontrez des probl√®mes.*
*Si votre mat√©riel ne peut pas ex√©cuter les LLM localement, voir [Configuration pour utilisation via API](#setup-to-run-with-an-api).*
*Pour une explication d√©taill√©e de `config.ini`, voir la [Section Config](#config).*

## Configuration pour utilisation via API

Cette configuration utilise des fournisseurs LLM externes, bas√©s sur le cloud. Vous aurez besoin d‚Äôune cl√© API provenant du service choisi.

**1. Choisissez un fournisseur API et obtenez une cl√© API :**

Consultez la [Liste des fournisseurs API](#list-of-api-providers) ci-dessous. Rendez-vous sur leur site web pour vous inscrire et obtenir une cl√© API.

**2. D√©finissez votre cl√© API comme variable d‚Äôenvironnement :**


*   **Linux/macOS :**
    Ouvrez votre terminal et utilisez la commande `export`. Il est conseill√© de l‚Äôajouter √† votre fichier de profil de shell (ex : `~/.bashrc`, `~/.zshrc`) pour la persistance.
    ```sh
    export PROVIDER_API_KEY="votre_cle_api_ici" 
    # Remplacez PROVIDER_API_KEY par le nom de variable sp√©cifique, ex : OPENAI_API_KEY, GOOGLE_API_KEY
    ```
    Exemple pour TogetherAI :
    ```sh
    export TOGETHER_API_KEY="xxxxxxxxxxxxxxxxxxxxxx"
    ```
*   **Windows :**
*   **Invite de commandes (Temporaire pour la session en cours) :**
    ```cmd
    set PROVIDER_API_KEY=your_api_key_here
    ```
*   **PowerShell (Temporaire pour la session en cours) :**
    ```powershell
    $env:PROVIDER_API_KEY="your_api_key_here"
    ```
*   **De fa√ßon permanente :** Recherchez "variables d'environnement" dans la barre de recherche Windows, cliquez sur "Modifier les variables d'environnement syst√®me", puis cliquez sur le bouton "Variables d'environnement...". Ajoutez une nouvelle variable utilisateur avec le nom appropri√© (par exemple, `OPENAI_API_KEY`) et votre cl√© comme valeur.

*(Voir la FAQ : [Comment d√©finir les cl√©s API ?](#how-do-i-set-api-keys) pour plus de d√©tails).*

**3. Mettre √† jour `config.ini` :**
```ini
[MAIN]
is_local = False
provider_name = openai # Ou google, deepseek, togetherAI, huggingface
provider_model = gpt-3.5-turbo # Ou gemini-1.5-flash, deepseek-chat, mistralai/Mixtral-8x7B-Instruct-v0.1 etc.
provider_server_address = # G√©n√©ralement ignor√© ou peut rester vide lorsque is_local = False pour la plupart des APIs
# ... autres param√®tres ...
```
*Attention :* Assurez-vous qu'il n'y a pas d'espaces √† la fin des valeurs dans le fichier `config.ini`.

**Liste des fournisseurs d'API**

| Fournisseur   | `provider_name` | Local ? | Description                                           | Lien de cl√© API (Exemples)                     |
|---------------|-----------------|---------|------------------------------------------------------|-----------------------------------------------|
| OpenAI        | `openai`        | Non     | Utilise les mod√®les ChatGPT via l'API d'OpenAI.      | [platform.openai.com/signup](https://platform.openai.com/signup) |
| Google Gemini | `google`        | Non     | Utilise les mod√®les Google Gemini via Google AI Studio.| [aistudio.google.com/keys](https://aistudio.google.com/keys) |
| Deepseek      | `deepseek`      | Non     | Utilise les mod√®les Deepseek via leur API.            | [platform.deepseek.com](https://platform.deepseek.com) |
| Hugging Face  | `huggingface`   | Non     | Utilise les mod√®les de l'API Hugging Face Inference.  | [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens) |
| TogetherAI    | `togetherAI`    | Non     | Utilise divers mod√®les open source via l'API TogetherAI.| [api.together.ai/settings/api-keys](https://api.together.ai/settings/api-keys) |

*Remarque :*
*   Nous d√©conseillons l'utilisation de `gpt-4o` ou d'autres mod√®les OpenAI pour la navigation web complexe et la planification de t√¢ches, car les optimisations actuelles des prompts sont adapt√©es √† des mod√®les comme Deepseek.
*   Les t√¢ches de codage/bash peuvent rencontrer des probl√®mes avec Gemini, car il ne suit pas toujours strictement le formatage des prompts optimis√©s pour Deepseek.
*   Le champ `provider_server_address` dans `config.ini` n'est g√©n√©ralement pas utilis√© lorsque `is_local = False` car le point d'acc√®s API est g√©n√©ralement cod√© en dur dans la biblioth√®que du fournisseur concern√©.

√âtape suivante : [D√©marrer les services et lancer AgenticSeek](#Start-services-and-Run)

*Voir la section **Probl√®mes connus** si vous rencontrez des difficult√©s*

*Voir la section **Config** pour une explication d√©taill√©e du fichier de configuration.*

---

## D√©marrer les services et lancer AgenticSeek

Par d√©faut, AgenticSeek fonctionne enti√®rement dans Docker.

D√©marrez les services requis. Cela d√©marrera tous les services du fichier docker-compose.yml, y compris :
    - searxng
    - redis (requis par searxng)
    - frontend
    - backend (si vous utilisez `full`)

```sh
./start_services.sh full # MacOS
start ./start_services.cmd full # Windows
```

**Attention :** Cette √©tape t√©l√©chargera et chargera toutes les images Docker, ce qui peut prendre jusqu'√† 30 minutes. Apr√®s le d√©marrage des services, veuillez attendre que le service backend soit compl√®tement op√©rationnel (vous devriez voir **backend: "GET /health HTTP/1.1" 200 OK** dans le journal) avant d'envoyer des messages. Les services backend peuvent prendre 5 minutes √† d√©marrer lors du premier lancement.

Acc√©dez √† `http://localhost:3000/` et vous devriez voir l'interface web.

*D√©pannage du d√©marrage des services :* Si ces scripts √©chouent, assurez-vous que Docker Engine est en cours d'ex√©cution et que Docker Compose (V2, `docker compose`) est correctement install√©. V√©rifiez les messages d'erreur dans le terminal. Voir [FAQ : Aide ! J'obtiens une erreur lors de l'ex√©cution d'AgenticSeek ou de ses scripts.](#faq-troubleshooting)

**Optionnel :** Ex√©cuter sur l'h√¥te (mode CLI) :

Pour utiliser l'interface CLI, vous devez installer le paquet sur l'h√¥te :

```sh
./install.sh
./install.bat # windows
```

D√©marrez les services :

```sh
./start_services.sh # MacOS
start ./start_services.cmd # Windows
```

Utilisez la CLI : `python3 cli.py`

---

## Utilisation

Assurez-vous que les services sont actifs avec `./start_services.sh full` et rendez-vous sur `localhost:3000` pour l'interface web.

Vous pouvez √©galement utiliser la reconnaissance vocale en d√©finissant `listen = True` dans la configuration. Uniquement en mode CLI.

Pour quitter, dites/√©crivez simplement `goodbye`.

Voici quelques exemples d'utilisation :

> *Cr√©er un jeu du serpent en python !*

> *Cherche sur le web les meilleurs caf√©s √† Rennes, France, et sauvegarde une liste de trois avec leurs adresses dans rennes_cafes.txt.*

> *√âcris un programme Go pour calculer la factorielle d'un nombre, sauvegarde-le sous factorial.go dans ton espace de travail*

> *Recherche dans mon dossier summer_pictures tous les fichiers JPG, renomme-les avec la date d‚Äôaujourd‚Äôhui, et sauvegarde la liste des fichiers renomm√©s dans photos_list.txt*

> *Recherche en ligne les films de science-fiction populaires de 2024 et choisis-en trois √† regarder ce soir. Sauvegarde la liste dans movie_night.txt.*

> *Cherche sur le web les derniers articles d‚Äôactualit√© IA de 2025, s√©lectionne-en trois, et √©cris un script Python pour extraire leurs titres et r√©sum√©s. Sauvegarde le script sous news_scraper.py et les r√©sum√©s dans ai_news.txt dans /home/projects*

> *Vendredi, cherche sur le web une API gratuite de prix d‚Äôactions, inscris-toi avec supersuper7434567@gmail.com puis √©cris un script Python pour r√©cup√©rer les prix quotidiens de Tesla via l‚ÄôAPI et sauvegarde les r√©sultats dans stock_prices.csv*

*Notez que les capacit√©s de remplissage de formulaire sont encore exp√©rimentales et peuvent √©chouer.*

Apr√®s avoir saisi votre requ√™te, AgenticSeek allouera le meilleur agent pour la t√¢che.

Comme il s'agit d'un prototype pr√©coce, le syst√®me de routage des agents peut ne pas toujours choisir le bon agent en fonction de votre requ√™te.

Par cons√©quent, vous devez √™tre tr√®s explicite sur ce que vous voulez et comment l‚ÄôIA doit proc√©der ; par exemple, si vous souhaitez qu‚Äôelle effectue une recherche web, ne dites pas :

`Connais-tu de bons pays pour voyager en solo ?`

Demandez plut√¥t :

`Fais une recherche web et trouve quels sont les meilleurs pays pour voyager en solo`

---

## **Configuration pour ex√©cuter le LLM sur votre propre serveur**  

Si vous disposez d'un ordinateur puissant ou d'un serveur que vous pouvez utiliser, mais que vous souhaitez l'utiliser √† distance depuis votre ordinateur portable, vous avez la possibilit√© d'ex√©cuter le LLM sur un serveur distant en utilisant notre serveur llm personnalis√©.

Sur votre "serveur" qui ex√©cutera le mod√®le IA, r√©cup√©rez l'adresse IP

```sh
ip a | grep "inet " | grep -v 127.0.0.1 | awk '{print $2}' | cut -d/ -f1 # ip locale
curl https://ipinfo.io/ip # ip publique
```

Remarque : Pour Windows ou macOS, utilisez respectivement ipconfig ou ifconfig pour trouver l'adresse IP.

Clonez le d√©p√¥t et entrez dans le dossier `server/`.

```sh
git clone --depth 1 https://github.com/Fosowl/agenticSeek.git
cd agenticSeek/llm_server/
```

Installez les d√©pendances sp√©cifiques au serveur :

```sh
pip3 install -r requirements.txt
```

Ex√©cutez le script serveur.

```sh
python3 app.py --provider ollama --port 3333
```

Vous avez le choix entre utiliser `ollama` ou `llamacpp` comme service LLM.

Maintenant sur votre ordinateur personnel :

Modifiez le fichier `config.ini` pour d√©finir `provider_name` sur `server` et `provider_model` sur `deepseek-r1:xxb`.
D√©finissez `provider_server_address` sur l'adresse IP de la machine qui ex√©cutera le mod√®le.

```sh
[MAIN]
is_local = False
provider_name = server
provider_model = deepseek-r1:70b
provider_server_address = x.x.x.x:3333
```

√âtape suivante : [D√©marrer les services et lancer AgenticSeek](#Start-services-and-Run)  

---

## Reconnaissance vocale (Speech to Text)

Attention : la reconnaissance vocale ne fonctionne actuellement qu‚Äôen mode CLI.

Veuillez noter qu‚Äôactuellement la reconnaissance vocale ne fonctionne qu‚Äôen anglais.

La fonctionnalit√© de reconnaissance vocale est d√©sactiv√©e par d√©faut. Pour l‚Äôactiver, d√©finissez l‚Äôoption listen sur True dans le fichier config.ini :

```
listen = True
```

Lorsqu‚Äôelle est activ√©e, la reconnaissance vocale attend un mot-cl√© d√©clencheur, qui est le nom de l‚Äôagent, avant de commencer √† traiter votre entr√©e. Vous pouvez personnaliser le nom de l‚Äôagent en modifiant la valeur `agent_name` dans le fichier *config.ini* :

```
agent_name = Friday
```

Pour une reconnaissance optimale, nous recommandons d'utiliser un pr√©nom anglais courant comme "John" ou "Emma" comme nom d'agent.

Une fois que vous voyez la transcription appara√Ætre, dites le nom de l'agent √† voix haute pour le r√©veiller (ex. : "Friday").

√ânoncez clairement votre requ√™te.

Terminez votre demande par une phrase de confirmation pour indiquer au syst√®me de proc√©der. Exemples de phrases de confirmation :
```
"do it", "go ahead", "execute", "run", "start", "thanks", "would ya", "please", "okay?", "proceed", "continue", "go on", "do that", "go it", "do you understand?"
```

## Config

Exemple de configuration :
```
[MAIN]
is_local = True
provider_name = ollama
provider_model = deepseek-r1:32b
provider_server_address = http://127.0.0.1:11434 # Exemple pour Ollama ; utilisez http://127.0.0.1:1234 pour LM-Studio
agent_name = Friday
recover_last_session = False
save_session = False
speak = False
listen = False

jarvis_personality = False
languages = en zh # Liste des langues pour la synth√®se vocale et potentiellement le routage.
[BROWSER]
headless_browser = False
stealth_mode = False
```

**Explication des param√®tres `config.ini`** :

*   **Section **`[MAIN]`** :**
    *   `is_local` : `True` si vous utilisez un fournisseur LLM local (Ollama, LM-Studio, serveur compatible OpenAI local) ou l'option serveur auto-h√©berg√©. `False` si vous utilisez une API cloud (OpenAI, Google, etc.).
    *   `provider_name` : Sp√©cifie le fournisseur LLM.
        *   Options locales : `ollama`, `lm-studio`, `openai` (pour les serveurs compatibles OpenAI locaux), `server` (pour l'installation en auto-h√©bergement).
        *   Options API : `openai`, `google`, `deepseek`, `huggingface`, `togetherAI`.
    *   `provider_model` : Le nom ou ID du mod√®le sp√©cifique pour le fournisseur choisi (ex. : `deepseekcoder:6.7b` pour Ollama, `gpt-3.5-turbo` pour l'API OpenAI, `mistralai/Mixtral-8x7B-Instruct-v0.1` pour TogetherAI).
    *   `provider_server_address` : L'adresse de votre fournisseur LLM.
        *   Pour les fournisseurs locaux : ex. `http://127.0.0.1:11434` pour Ollama, `http://127.0.0.1:1234` pour LM-Studio.
        *   Pour le type de fournisseur `server` : l'adresse de votre serveur LLM auto-h√©berg√© (ex. : `http://votre_ip_serveur:3333`).
        *   Pour les API cloud (`is_local = False`) : cela est souvent ignor√© ou peut rester vide, car le point de terminaison de l'API est g√©n√©ralement g√©r√© par la biblioth√®que cliente.
    *   `agent_name` : Nom de l'assistant IA (ex. : Friday). Utilis√© comme mot d√©clencheur pour la reconnaissance vocale si activ√©e.
    *   `recover_last_session` : `True` pour tenter de restaurer l'√©tat de la session pr√©c√©dente, `False` pour recommencer √† z√©ro.
    *   `save_session` : `True` pour sauvegarder l'√©tat actuel de la session pour une √©ventuelle r√©cup√©ration, `False` sinon.
    *   `speak` : `True` pour activer la synth√®se vocale, `False` pour d√©sactiver.
    *   `listen` : `True` pour activer la reconnaissance vocale (mode CLI uniquement), `False` pour d√©sactiver.
    *   `work_dir` : **Crucial :** Le r√©pertoire dans lequel AgenticSeek lira/√©crira les fichiers. **Assurez-vous que ce chemin est valide et accessible sur votre syst√®me.**
    *   `jarvis_personality` : `True` pour utiliser un prompt syst√®me de type "Jarvis" (exp√©rimental), `False` pour le prompt standard.
    *   `languages` : Liste de langues s√©par√©es par des virgules (ex. : `en, zh, fr`). Utilis√© pour la s√©lection de la voix TTS (par d√©faut la premi√®re) et peut aider le routeur LLM. √âvitez d‚Äôindiquer trop de langues ou des langues tr√®s similaires pour l‚Äôefficacit√© du routeur.
*   **Section **`[BROWSER]`** :**
    *   `headless_browser` : `True` pour ex√©cuter le navigateur automatis√© sans fen√™tre visible (recommand√© pour l‚Äôinterface web ou une utilisation non interactive). `False` pour afficher la fen√™tre du navigateur (utile pour le mode CLI ou le d√©bogage).
    *   `stealth_mode` : `True` pour activer des mesures rendant l‚Äôautomatisation du navigateur plus difficile √† d√©tecter. Peut n√©cessiter l‚Äôinstallation manuelle d‚Äôextensions comme anticaptcha.


Cette section r√©sume les types de fournisseurs LLM support√©s. Configurez-les dans `config.ini`.

**Fournisseurs locaux (ex√©cut√©s sur votre mat√©riel) :**

| Nom du fournisseur dans `config.ini` | `is_local` | Description                                                                 | Section de configuration                                                    |
|-------------------------------|------------|-----------------------------------------------------------------------------|-----------------------------------------------------------------------------|
| `ollama`                      | `True`     | Utilise Ollama pour servir des LLM locaux.                                  | [Configuration pour ex√©cuter un LLM localement](#setup-for-running-llm-locally-on-your-machine) |
| `lm-studio`                   | `True`     | Utilise LM-Studio pour servir des LLM locaux.                               | [Configuration pour ex√©cuter un LLM localement](#setup-for-running-llm-locally-on-your-machine) |
| `openai` (pour serveur local) | `True`     | Connexion √† un serveur local exposant une API compatible OpenAI (ex : llama.cpp). | [Configuration pour ex√©cuter un LLM localement](#setup-for-running-llm-locally-on-your-machine) |
| `server`                      | `False`    | Connexion au serveur LLM AgenticSeek auto-h√©berg√© sur une autre machine.    | [Configuration pour ex√©cuter le LLM sur votre propre serveur](#setup-to-run-the-llm-on-your-own-server) |

**Fournisseurs API (bas√©s sur le cloud) :**

| Nom du fournisseur dans `config.ini` | `is_local` | Description                                        | Section de configuration                                 |
|-------------------------------|------------|----------------------------------------------------|----------------------------------------------------------|
| `openai`                      | `False`    | Utilise l‚ÄôAPI officielle d‚ÄôOpenAI (ex. GPT-3.5, GPT-4). | [Configuration pour utiliser une API](#setup-to-run-with-an-api) |
| `google`                      | `False`    | Utilise les mod√®les Gemini de Google via API.           | [Configuration pour utiliser une API](#setup-to-run-with-an-api) |
| `deepseek`                    | `False`    | Utilise l‚ÄôAPI officielle de Deepseek.                  | [Configuration pour utiliser une API](#setup-to-run-with-an-api) |
| `huggingface`                 | `False`    | Utilise l‚ÄôAPI d‚Äôinf√©rence de Hugging Face.             | [Configuration pour utiliser une API](#setup-to-run-with-an-api) |
| `togetherAI`                  | `False`    | Utilise l‚ÄôAPI TogetherAI pour divers mod√®les open source. | [Configuration pour utiliser une API](#setup-to-run-with-an-api) |

---
## D√©pannage

Si vous rencontrez des probl√®mes, cette section vous guide.

# Probl√®mes connus

## Probl√®mes de ChromeDriver

**Exemple d‚Äôerreur :** `SessionNotCreatedException: Message: session not created: This version of ChromeDriver only supports Chrome version XXX`

*   **Cause :** La version de ChromeDriver install√©e est incompatible avec la version de votre navigateur Google Chrome.
*   **Solution :**
    1.  **V√©rifiez la version de Chrome :** Ouvrez Google Chrome, allez dans `Param√®tres > √Ä propos de Chrome` pour trouver votre version (ex. : "Version 120.0.6099.110").
    2.  **T√©l√©chargez le ChromeDriver correspondant :**
        *   Pour Chrome version 115 et sup√©rieure : Rendez-vous sur les [Chrome for Testing (CfT) JSON Endpoints](https://googlechromelabs.github.io/chrome-for-testing/). Trouvez le canal "stable" et t√©l√©chargez le ChromeDriver pour votre OS correspondant √† la version majeure de votre Chrome.
        *   Pour les versions plus anciennes (plus rare) : Vous pouvez les trouver sur la page [ChromeDriver - WebDriver for Chrome](https://chromedriver.chromium.org/downloads).
        *   L'image ci-dessous montre un exemple depuis la page CfT :
            ![T√©l√©charger une version sp√©cifique de Chromedriver sur la page Chrome for Testing](https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/chromedriver_readme.png)
    3.  **Installez ChromeDriver :**
        *   Assurez-vous que le `chromedriver` t√©l√©charg√© (ou `chromedriver.exe` sur Windows) est plac√© dans un dossier r√©f√©renc√© par la variable d‚Äôenvironnement PATH de votre syst√®me (ex. : `/usr/local/bin` sous Linux/macOS, ou un dossier de scripts ajout√© au PATH sous Windows).
        *   Alternativement, placez-le dans le r√©pertoire racine du projet `agenticSeek`.
        *   Assurez-vous que le pilote est ex√©cutable (ex. : `chmod +x chromedriver` sous Linux/macOS).
    4.  Consultez la section [Installation de ChromeDriver](#chromedriver-installation) dans le guide principal d'installation pour plus de d√©tails.

Si cette section est incompl√®te ou si vous rencontrez d‚Äôautres probl√®mes avec ChromeDriver, veuillez consulter les [Issues GitHub existants](https://github.com/Fosowl/agenticSeek/issues) ou en ouvrir un nouveau.

`Exception: Failed to initialize browser: Message: session not created: This version of ChromeDriver only supports Chrome version 113
Current browser version is 134.0.6998.89 with binary path`

Cela arrive s'il y a un d√©calage entre la version de votre navigateur et celle de chromedriver.

Vous devez t√©l√©charger la derni√®re version :

https://developer.chrome.com/docs/chromedriver/downloads

Si vous utilisez Chrome version 115 ou sup√©rieure, allez sur :

https://googlechromelabs.github.io/chrome-for-testing/

Et t√©l√©chargez la version de chromedriver correspondant √† votre syst√®me d‚Äôexploitation.

![alt text](https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/chromedriver_readme.png)

Si cette section est incompl√®te, veuillez ouvrir une issue.

##  Probl√®mes d‚Äôadaptateurs de connexion

```
Exception: Provider lm-studio failed: HTTP request failed: No connection adapters were found for '127.0.0.1:1234/v1/chat/completions'` (Note : le port peut varier)
```

*   **Cause :** Le param√®tre `provider_server_address` dans `config.ini` pour `lm-studio` (ou tout autre serveur local compatible OpenAI) ne comporte pas le pr√©fixe `http://` ou pointe vers le mauvais port.
*   **Solution :**
    *   Assurez-vous que l‚Äôadresse commence par `http://`. LM-Studio utilise g√©n√©ralement `http://127.0.0.1:1234` par d√©faut.
    *   Correction dans `config.ini` : `provider_server_address = http://127.0.0.1:1234` (ou le port r√©el de votre serveur LM-Studio).

## URL de base SearxNG non fournie

```
raise ValueError("SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.")
ValueError: SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.`
```

## FAQ

**Q : De quel mat√©riel ai-je besoin ?**  

| Taille du mod√®le  | GPU  | Remarques                                               |
|-------------------|------|--------------------------------------------------------|
| 7B                | 8GB Vram | ‚ö†Ô∏è Non recommand√©. Performances faibles, hallucinations fr√©quentes, les agents planificateurs √©choueront probablement. |
| 14B               | 12 GB VRAM (ex. RTX 3060) | ‚úÖ Utilisable pour des t√¢ches simples. Peut peiner sur la navigation web et la planification. |
| 32B               | 24+ GB VRAM (ex. RTX 4090) | üöÄ R√©ussite sur la plupart des t√¢ches, peut encore avoir des difficult√©s pour la planification. |
| 70B+              | 48+ GB Vram | üí™ Excellent. Recommand√© pour des usages avanc√©s. |

**Q : J‚Äôai une erreur, que faire ?**  

Assurez-vous que le local est lanc√© (`ollama serve`), que votre `config.ini` correspond √† votre fournisseur, et que les d√©pendances sont install√©es. Si rien ne fonctionne, n‚Äôh√©sitez pas √† ouvrir une issue.

**Q : Peut-on vraiment tout faire tourner en local √† 100% ?**  

Oui, avec Ollama, lm-studio ou les fournisseurs `server`, toute la reconnaissance vocale, LLM et la synth√®se vocale tournent en local. Les options non-locales (OpenAI ou autres API) sont facultatives.

**Q : Pourquoi utiliser AgenticSeek alors que j‚Äôai Manus ?**

Contrairement √† Manus, AgenticSeek privil√©gie l‚Äôind√©pendance vis-√†-vis des syst√®mes externes, vous donnant plus de contr√¥le, de confidentialit√© et √©vitant les co√ªts d‚ÄôAPI.

**Q : Qui est √† l‚Äôorigine du projet ?**

Le projet a √©t√© cr√©√© par moi-m√™me, avec deux amis qui sont mainteneurs et contributeurs issus de la communaut√© open source sur GitHub. Nous ne sommes qu‚Äôun groupe de passionn√©s, pas une startup ni affili√©s √† une organisation.

Tout compte AgenticSeek sur X autre que mon compte personnel (https://x.com/Martin993886460) est une usurpation.

## Contribuer

Nous recherchons des d√©veloppeurs pour am√©liorer AgenticSeek ! Consultez les issues ouvertes ou les discussions.

[Guide de contribution](https://raw.githubusercontent.com/Fosowl/agenticSeek/main/docs/CONTRIBUTING.md)

[![Star History Chart](https://api.star-history.com/svg?repos=Fosowl/agenticSeek&type=Date)](https://www.star-history.com/#Fosowl/agenticSeek&Date)

## Mainteneurs :

 > [Fosowl](https://github.com/Fosowl) | Fuseau horaire de Paris 

 > [antoineVIVIES](https://github.com/antoineVIVIES) | Fuseau horaire de Taipei 

 > [steveh8758](https://github.com/steveh8758) | Fuseau horaire de Taipei 

## Remerciements particuliers :

 > [tcsenpai](https://github.com/tcsenpai) et [plitc](https://github.com/plitc) pour leur aide √† la dockerisation du backend

## Sponsors :

Les sponsors mensuels de 5 $ ou plus apparaissent ici :
- **tatra-labs**

Certainly! However, you have not provided the content of "Part 4 of 4" to be translated. Please provide the text of the technical document you want translated into French, and I will proceed accordingly.

---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-06-16

---