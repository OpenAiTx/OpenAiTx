# AgenticSeek: Alternativa privada e local ao Manus.

<p align="center">
<img align="center" src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/agentic_seek_logo.png" width="300" height="300" alt="Agentic Seek Logo">
<p>

  English | [‰∏≠Êñá](https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHS.md) | [ÁπÅÈ´î‰∏≠Êñá](https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHT.md) | [Fran√ßais](https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_FR.md) | [Êó•Êú¨Ë™û](https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_JP.md) | [Portugu√™s (Brasil)](https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_PTBR.md) | [Espa√±ol](https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_ES.md)

*Uma **alternativa 100% local ao Manus AI**, este assistente de IA habilitado por voz navega autonomamente na web, escreve c√≥digo e planeja tarefas enquanto mant√©m todos os dados em seu dispositivo. Projetado para modelos de racioc√≠nio locais, ele roda inteiramente no seu hardware, garantindo total privacidade e nenhuma depend√™ncia da nuvem.*

[![Visite AgenticSeek](https://img.shields.io/static/v1?label=Website&message=AgenticSeek&color=blue&style=flat-square)](https://fosowl.github.io/agenticSeek.html) ![Licen√ßa](https://img.shields.io/badge/license-GPL--3.0-green) [![Discord](https://img.shields.io/badge/Discord-Join%20Us-7289DA?logo=discord&logoColor=white)](https://discord.gg/8hGDaME3TC) [![Twitter](https://img.shields.io/twitter/url/https/twitter.com/fosowl.svg?style=social&label=Update%20%40Fosowl)](https://x.com/Martin993886460) [![Estrelas no GitHub](https://img.shields.io/github/stars/Fosowl/agenticSeek?style=social)](https://github.com/Fosowl/agenticSeek/stargazers)

### Por que AgenticSeek?

* üîí Totalmente Local & Privado - Tudo roda em sua m√°quina ‚Äî sem nuvem, sem compartilhamento de dados. Seus arquivos, conversas e pesquisas permanecem privados.

* üåê Navega√ß√£o Inteligente na Web - O AgenticSeek pode navegar na internet sozinho ‚Äî pesquisar, ler, extrair informa√ß√µes, preencher formul√°rios ‚Äî tudo sem usar as m√£os.

* üíª Assistente Aut√¥nomo de Programa√ß√£o - Precisa de c√≥digo? Ele pode escrever, depurar e executar programas em Python, C, Go, Java e mais ‚Äî tudo sem supervis√£o.

* üß† Sele√ß√£o Inteligente de Agentes - Voc√™ pede, ele descobre automaticamente o melhor agente para a tarefa. Como ter uma equipe de especialistas pronta para ajudar.

* üìã Planeja & Executa Tarefas Complexas - De planejamento de viagens a projetos complexos ‚Äî ele pode dividir grandes tarefas em etapas e realizar usando m√∫ltiplos agentes de IA.

* üéôÔ∏è Habilitado por Voz - Voz limpa, r√°pida e futurista e convers√£o de fala para texto, permitindo que voc√™ converse como se fosse sua IA pessoal de um filme de fic√ß√£o cient√≠fica. (Em desenvolvimento)

### **Demo**

> *Voc√™ pode pesquisar pelo projeto agenticSeek, aprender quais habilidades s√£o necess√°rias, depois abrir o CV_candidates.zip e ent√£o me dizer quais candidatos combinam melhor com o projeto*

https://github.com/user-attachments/assets/b8ca60e9-7b3b-4533-840e-08f9ac426316

Aviso: Esta demonstra√ß√£o, incluindo todos os arquivos que aparecem (ex: CV_candidates.zip), √© inteiramente fict√≠cia. N√£o somos uma corpora√ß√£o, buscamos colaboradores open-source, n√£o candidatos.

> üõ†‚ö†Ô∏èÔ∏è **Trabalho Ativo em Andamento**

> üôè Este projeto come√ßou como um projeto paralelo e n√£o tem roteiro nem financiamento. Cresceu muito al√©m do esperado, chegando ao GitHub Trending. Contribui√ß√µes, feedback e paci√™ncia s√£o profundamente apreciados.

## Pr√©-requisitos

Antes de come√ßar, certifique-se de ter o seguinte software instalado:

*   **Git:** Para clonar o reposit√≥rio. [Baixe o Git](https://git-scm.com/downloads)
*   **Python 3.10.x:** Recomendamos fortemente o uso da vers√£o Python 3.10.x. Usar outras vers√µes pode causar erros de depend√™ncia. [Baixe o Python 3.10](https://www.python.org/downloads/release/python-3100/) (escolha uma vers√£o 3.10.x).
*   **Docker Engine & Docker Compose:** Para rodar servi√ßos agrupados como o SearxNG.
    *   Instale o Docker Desktop (que inclui Docker Compose V2): [Windows](https://docs.docker.com/desktop/install/windows-install/) | [Mac](https://docs.docker.com/desktop/install/mac-install/) | [Linux](https://docs.docker.com/desktop/install/linux-install/)
    *   Alternativamente, instale o Docker Engine e o Docker Compose separadamente no Linux: [Docker Engine](https://docs.docker.com/engine/install/) | [Docker Compose](https://docs.docker.com/compose/install/) (certifique-se de instalar o Compose V2, ex: `sudo apt-get install docker-compose-plugin`).

### 1. **Clone o reposit√≥rio e configure**

```sh
git clone https://github.com/Fosowl/agenticSeek.git
cd agenticSeek
mv .env.example .env
```

### 2. Altere o conte√∫do do arquivo .env

```sh
SEARXNG_BASE_URL="http://127.0.0.1:8080"
REDIS_BASE_URL="redis://redis:6379/0"
WORK_DIR="/Users/mlg/Documents/workspace_for_ai"
OLLAMA_PORT="11434"
LM_STUDIO_PORT="1234"
CUSTOM_ADDITIONAL_LLM_PORT="11435"
OPENAI_API_KEY='opcional'
DEEPSEEK_API_KEY='opcional'
OPENROUTER_API_KEY='opcional'
TOGETHER_API_KEY='opcional'
GOOGLE_API_KEY='opcional'
ANTHROPIC_API_KEY='opcional'
```

Atualize o arquivo `.env` com seus pr√≥prios valores conforme necess√°rio:

- **SEARXNG_BASE_URL**: Deixe inalterado
- **REDIS_BASE_URL**: Deixe inalterado
- **WORK_DIR**: Caminho para seu diret√≥rio de trabalho em sua m√°quina local. O AgenticSeek poder√° ler e interagir com estes arquivos.
- **OLLAMA_PORT**: N√∫mero da porta para o servi√ßo Ollama.
- **LM_STUDIO_PORT**: N√∫mero da porta para o servi√ßo LM Studio.
- **CUSTOM_ADDITIONAL_LLM_PORT**: Porta para qualquer servi√ßo LLM personalizado adicional.

**As chaves de API s√£o totalmente opcionais para usu√°rios que optam por rodar LLM localmente. Esse √© o objetivo principal deste projeto. Deixe em branco se voc√™ possuir hardware suficiente**

### 3. **Inicie o Docker**

Certifique-se de que o Docker est√° instalado e em execu√ß√£o em seu sistema. Voc√™ pode iniciar o Docker usando os seguintes comandos:

- **No Linux/macOS:**  
    Abra um terminal e execute:
    ```sh
    sudo systemctl start docker
    ```
    Ou inicie o Docker Desktop a partir do menu de aplicativos, se instalado.

- **No Windows:**  
    Inicie o Docker Desktop pelo menu Iniciar.

Voc√™ pode verificar se o Docker est√° rodando executando:
```sh
docker info
```
Se voc√™ visualizar informa√ß√µes sobre sua instala√ß√£o do Docker, ele est√° funcionando corretamente.

Veja a tabela de [Provedores Locais](#list-of-local-providers) abaixo para um resumo.

Pr√≥ximo passo: [Execute o AgenticSeek localmente](#start-services-and-run)

*Veja a se√ß√£o [Solu√ß√£o de Problemas](#troubleshooting) se estiver tendo problemas.*
*Se seu hardware n√£o conseguir rodar LLMs localmente, veja [Configura√ß√£o para rodar com uma API](#setup-to-run-with-an-api).*
*Para explica√ß√µes detalhadas do `config.ini`, veja a [Se√ß√£o de Configura√ß√£o](#config).*

---

## Configura√ß√£o para rodar LLM localmente em sua m√°quina

**Requisitos de Hardware:**

Para rodar LLMs localmente, voc√™ precisar√° de hardware suficiente. No m√≠nimo, √© necess√°rio uma GPU capaz de rodar Magistral, Qwen ou Deepseek 14B. Veja o FAQ para recomenda√ß√µes detalhadas de modelo/desempenho.

**Configure seu provedor local**

Inicie seu provedor local, por exemplo, com o ollama:

```sh
ollama serve
```

Veja abaixo uma lista de provedores locais suportados.

**Atualize o config.ini**

Altere o arquivo config.ini para definir o provider_name para um provedor suportado e provider_model para um LLM suportado por seu provedor. Recomendamos modelos de racioc√≠nio como *Magistral* ou *Deepseek*.

Veja o **FAQ** no final do README para hardware necess√°rio.

```sh
[MAIN]
is_local = True # Sempre que estiver rodando localmente ou com provedor remoto.
provider_name = ollama # ou lm-studio, openai, etc..
provider_model = deepseek-r1:14b # escolha um modelo que se encaixe em seu hardware
provider_server_address = 127.0.0.1:11434
agent_name = Jarvis # nome da sua IA
recover_last_session = True # se deseja recuperar a sess√£o anterior
save_session = True # se deseja lembrar a sess√£o atual
speak = False # texto para fala
listen = False # fala para texto, apenas para CLI, experimental
jarvis_personality = False # Se deseja usar uma personalidade mais "Jarvis" (experimental)
languages = en zh # Lista de idiomas, Texto para fala usar√° o primeiro idioma da lista como padr√£o
[BROWSER]
headless_browser = True # deixe inalterado, a menos que use CLI no host.
stealth_mode = True # Use selenium n√£o detectado para reduzir detec√ß√£o do navegador
```

**Aviso**:

- O formato do arquivo `config.ini` n√£o suporta coment√°rios. 
N√£o copie e cole a configura√ß√£o de exemplo diretamente, pois os coment√°rios causar√£o erros. Em vez disso, modifique manualmente o arquivo `config.ini` com suas configura√ß√µes desejadas, excluindo quaisquer coment√°rios.

- *N√ÉO* defina provider_name como `openai` se estiver usando LM-studio para rodar LLMs. Defina como `lm-studio`.

- Alguns provedores (ex: lm-studio) exigem que voc√™ tenha `http://` na frente do IP. Por exemplo, `http://127.0.0.1:1234`

**Lista de provedores locais**

| Provedor   | Local? | Descri√ß√£o                                                        |
|------------|--------|------------------------------------------------------------------|
| ollama     | Sim    | Rode LLMs localmente com facilidade usando ollama como provedor   |
| lm-studio  | Sim    | Rode LLM localmente com o LM studio (defina `provider_name` como `lm-studio`)|
| openai     | Sim    |  Use API compat√≠vel com openai (ex: servidor llama.cpp)           |

Pr√≥ximo passo: [Inicie os servi√ßos e rode o AgenticSeek](#Start-services-and-Run)  

*Veja a se√ß√£o [Solu√ß√£o de Problemas](#troubleshooting) se estiver tendo problemas.*
*Se seu hardware n√£o conseguir rodar LLMs localmente, veja [Configura√ß√£o para rodar com uma API](#setup-to-run-with-an-api).*
*Para explica√ß√µes detalhadas do `config.ini`, veja a [Se√ß√£o de Configura√ß√£o](#config).*

## Configura√ß√£o para rodar com uma API

Esta configura√ß√£o usa provedores de LLM externos, baseados em nuvem. Voc√™ precisar√° de uma chave de API do servi√ßo escolhido.

**1. Escolha um Provedor de API e obtenha uma chave de API:**

Consulte a [Lista de Provedores de API](#list-of-api-providers) abaixo. Visite os sites deles para se cadastrar e obter uma chave de API.

**2. Defina sua chave de API como uma vari√°vel de ambiente:**


*   **Linux/macOS:**
    Abra seu terminal e use o comando `export`. √â melhor adicionar isso ao arquivo de perfil do seu shell (ex: `~/.bashrc`, `~/.zshrc`) para persist√™ncia.
    ```sh
    export PROVIDER_API_KEY="sua_chave_api_aqui"
    # Substitua PROVIDER_API_KEY pelo nome da vari√°vel espec√≠fica, ex: OPENAI_API_KEY, GOOGLE_API_KEY
    ```
    Exemplo para TogetherAI:
    ```sh
    export TOGETHER_API_KEY="xxxxxxxxxxxxxxxxxxxxxx"
    ```
*   **Windows:**
*   **Prompt de Comando (Tempor√°rio para a sess√£o atual):**
    ```cmd
    set PROVIDER_API_KEY=your_api_key_here
    ```
*   **PowerShell (Tempor√°rio para a sess√£o atual):**
    ```powershell
    $env:PROVIDER_API_KEY="your_api_key_here"
    ```
*   **Permanentemente:** Pesquise por "vari√°veis de ambiente" na barra de pesquisa do Windows, clique em "Editar as vari√°veis de ambiente do sistema" e, em seguida, clique no bot√£o "Vari√°veis de Ambiente...". Adicione uma nova vari√°vel de Usu√°rio com o nome apropriado (por exemplo, `OPENAI_API_KEY`) e sua chave como valor.

*(Veja o FAQ: [Como configuro chaves de API?](#how-do-i-set-api-keys) para mais detalhes).*


**3. Atualize o `config.ini`:**
```ini
[MAIN]
is_local = False
provider_name = openai # Ou google, deepseek, togetherAI, huggingface
provider_model = gpt-3.5-turbo # Ou gemini-1.5-flash, deepseek-chat, mistralai/Mixtral-8x7B-Instruct-v0.1 etc.
provider_server_address = # Normalmente ignorado ou pode ser deixado em branco quando is_local = False para a maioria das APIs
# ... outras configura√ß√µes ...
```
*Aten√ß√£o:* Certifique-se de que n√£o h√° espa√ßos no final dos valores no `config.ini`.

**Lista de Provedores de API**

| Provedor      | `provider_name` | Local? | Descri√ß√£o                                          | Link da chave da API (Exemplos)              |
|---------------|-----------------|--------|----------------------------------------------------|----------------------------------------------|
| OpenAI        | `openai`        | N√£o    | Usa modelos ChatGPT via API da OpenAI.             | [platform.openai.com/signup](https://platform.openai.com/signup) |
| Google Gemini | `google`        | N√£o    | Usa modelos Google Gemini via Google AI Studio.     | [aistudio.google.com/keys](https://aistudio.google.com/keys) |
| Deepseek      | `deepseek`      | N√£o    | Usa modelos Deepseek via sua API.                   | [platform.deepseek.com](https://platform.deepseek.com) |
| Hugging Face  | `huggingface`   | N√£o    | Usa modelos da Hugging Face Inference API.          | [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens) |
| TogetherAI    | `togetherAI`    | N√£o    | Usa diversos modelos open-source via TogetherAI API.| [api.together.ai/settings/api-keys](https://api.together.ai/settings/api-keys) |

*Nota:*
*   N√£o recomendamos o uso de `gpt-4o` ou outros modelos da OpenAI para navega√ß√£o web complexa e planejamento de tarefas, pois as otimiza√ß√µes atuais de prompt s√£o voltadas para modelos como Deepseek.
*   Tarefas de codifica√ß√£o/bash podem apresentar problemas com o Gemini, pois ele pode n√£o seguir rigorosamente os prompts de formata√ß√£o otimizados para o Deepseek.
*   O campo `provider_server_address` no `config.ini` geralmente n√£o √© usado quando `is_local = False`, pois o endpoint da API normalmente √© definido na biblioteca do provedor.

Pr√≥xima etapa: [Inicie os servi√ßos e rode o AgenticSeek](#Start-services-and-Run)

*Consulte a se√ß√£o **Problemas conhecidos** caso esteja enfrentando problemas*

*Consulte a se√ß√£o **Config** para uma explica√ß√£o detalhada do arquivo de configura√ß√£o.*

---

## Inicie os servi√ßos e execute

Por padr√£o, o AgenticSeek √© executado totalmente em docker.

Inicie os servi√ßos necess√°rios. Isso iniciar√° todos os servi√ßos do docker-compose.yml, incluindo:
    - searxng
    - redis (necess√°rio pelo searxng)
    - frontend
    - backend (se usar `full`)

```sh
./start_services.sh full # MacOS
start ./start_services.cmd full # Windows
```

**Aten√ß√£o:** Esta etapa ir√° baixar e carregar todas as imagens do Docker, o que pode levar at√© 30 minutos. Ap√≥s iniciar os servi√ßos, aguarde at√© que o servi√ßo backend esteja totalmente rodando (voc√™ deve ver **backend: "GET /health HTTP/1.1" 200 OK** no log) antes de enviar qualquer mensagem. Os servi√ßos backend podem levar at√© 5 minutos para iniciar na primeira execu√ß√£o.

Acesse `http://localhost:3000/` e voc√™ dever√° ver a interface web.

*Resolu√ß√£o de problemas ao iniciar o servi√ßo:* Se esses scripts falharem, certifique-se de que o Docker Engine est√° em execu√ß√£o e que o Docker Compose (V2, `docker compose`) est√° instalado corretamente. Verifique a sa√≠da no terminal para mensagens de erro. Veja [FAQ: Help! I get an error when running AgenticSeek or its scripts.](#faq-troubleshooting)

**Opcional:** Rodar na m√°quina local (modo CLI):

Para rodar com interface CLI voc√™ precisar√° instalar o pacote na m√°quina local:

```sh
./install.sh
./install.bat # windows
```

Inicie os servi√ßos:

```sh
./start_services.sh # MacOS
start ./start_services.cmd # Windows
```

Use o CLI: `python3 cli.py`


---

## Uso

Certifique-se de que os servi√ßos est√£o ativos e em execu√ß√£o com `./start_services.sh full` e acesse `localhost:3000` para a interface web.

Voc√™ tamb√©m pode usar reconhecimento de voz (speech to text) configurando `listen = True` no config. Apenas para o modo CLI.

Para sair, simplesmente diga/digite `goodbye`.

Aqui est√£o alguns exemplos de uso:

> *Crie um jogo da cobrinha em python!*

> *Pesquise na web pelos melhores caf√©s em Rennes, Fran√ßa, e salve uma lista de tr√™s com seus endere√ßos em rennes_cafes.txt.*

> *Escreva um programa em Go para calcular o fatorial de um n√∫mero, salve como factorial.go em sua √°rea de trabalho*

> *Procure na pasta summer_pictures todos os arquivos JPG, renomeie-os com a data de hoje e salve uma lista dos arquivos renomeados em photos_list.txt*

> *Pesquise online por filmes de fic√ß√£o cient√≠fica populares de 2024 e escolha tr√™s para assistir hoje √† noite. Salve a lista em movie_night.txt.*

> *Pesquise na web os artigos mais recentes sobre IA de 2025, selecione tr√™s e escreva um script Python para capturar seus t√≠tulos e resumos. Salve o script como news_scraper.py e os resumos em ai_news.txt em /home/projects*

> *Sexta-feira, pesquise na web por uma API gratuita de cota√ß√£o de a√ß√µes, registre-se com supersuper7434567@gmail.com, depois escreva um script Python para buscar, usando a API, os pre√ßos di√°rios da Tesla, e salve os resultados em stock_prices.csv*

*Observe que as capacidades de preenchimento de formul√°rios ainda s√£o experimentais e podem falhar.*



Ap√≥s digitar sua consulta, o AgenticSeek ir√° alocar o melhor agente para a tarefa.

Como este √© um prot√≥tipo inicial, o sistema de roteamento de agentes pode n√£o alocar sempre o agente correto com base na sua consulta.

Portanto, seja muito expl√≠cito no que deseja e em como a IA deve proceder. Por exemplo, se quiser que ela fa√ßa uma busca na web, n√£o diga:

`Voc√™ conhece alguns bons pa√≠ses para viajar sozinho?`

Em vez disso, pe√ßa:

`Fa√ßa uma busca na web e descubra quais s√£o os melhores pa√≠ses para viajar sozinho`

---

## **Configura√ß√£o para rodar o LLM em seu pr√≥prio servidor**  

Se voc√™ tem um computador potente ou um servidor dispon√≠vel, mas deseja us√°-lo a partir do seu laptop, voc√™ pode rodar o LLM em um servidor remoto usando nosso servidor LLM personalizado.

No seu "servidor" que ir√° rodar o modelo de IA, obtenha o endere√ßo IP

```sh
ip a | grep "inet " | grep -v 127.0.0.1 | awk '{print $2}' | cut -d/ -f1 # ip local
curl https://ipinfo.io/ip # ip p√∫blico
```

Nota: Para Windows ou macOS, use ipconfig ou ifconfig respectivamente para encontrar o endere√ßo IP.

Clone o reposit√≥rio e entre na pasta `server/`.


```sh
git clone --depth 1 https://github.com/Fosowl/agenticSeek.git
cd agenticSeek/llm_server/
```

Instale os requisitos espec√≠ficos do servidor:

```sh
pip3 install -r requirements.txt
```

Execute o script do servidor.

```sh
python3 app.py --provider ollama --port 3333
```

Voc√™ pode escolher entre usar `ollama` e `llamacpp` como servi√ßo LLM.


Agora no seu computador pessoal:

Altere o arquivo `config.ini` para definir `provider_name` como `server` e `provider_model` como `deepseek-r1:xxb`.
Defina o `provider_server_address` para o endere√ßo IP da m√°quina que ir√° rodar o modelo.

```sh
[MAIN]
is_local = False
provider_name = server
provider_model = deepseek-r1:70b
provider_server_address = x.x.x.x:3333
```


Pr√≥xima etapa: [Inicie os servi√ßos e rode o AgenticSeek](#Start-services-and-Run)  

---

## Reconhecimento de fala (Speech to Text)

Aten√ß√£o: o reconhecimento de fala s√≥ funciona no modo CLI no momento.

Por favor, note que atualmente o reconhecimento de fala s√≥ funciona em ingl√™s.

A funcionalidade de reconhecimento de fala vem desabilitada por padr√£o. Para habilit√°-la, defina a op√ß√£o listen como True no arquivo config.ini:

```
listen = True
```

Quando habilitada, a fun√ß√£o de reconhecimento de fala escuta por uma palavra-chave de ativa√ß√£o, que √© o nome do agente, antes de come√ßar a processar sua entrada. Voc√™ pode personalizar o nome do agente atualizando o valor `agent_name` no arquivo *config.ini*:

```
agent_name = Friday
```

Para reconhecimento ideal, recomendamos o uso de um nome comum em ingl√™s como "John" ou "Emma" como nome do agente

Assim que voc√™ ver a transcri√ß√£o come√ßar a aparecer, diga o nome do agente em voz alta para acord√°-lo (por exemplo, "Friday").

Fale sua pergunta claramente.

Encerre seu pedido com uma frase de confirma√ß√£o para sinalizar ao sistema que deve prosseguir. Exemplos de frases de confirma√ß√£o incluem:
```
"fa√ßa isso", "vai em frente", "execute", "rodar", "iniciar", "obrigado", "pode ser?", "por favor", "ok?", "prossiga", "continue", "v√° em frente", "fa√ßa isso", "entendeu?"
```

## Config

Exemplo de configura√ß√£o:
```
[MAIN]
is_local = True
provider_name = ollama
provider_model = deepseek-r1:32b
provider_server_address = http://127.0.0.1:11434 # Exemplo para Ollama; use http://127.0.0.1:1234 para LM-Studio
agent_name = Friday
recover_last_session = False
save_session = False
speak = False
listen = False

jarvis_personality = False
languages = en zh # Lista de idiomas para TTS e potencialmente roteamento.
[BROWSER]
headless_browser = False
stealth_mode = False
```

**Explica√ß√£o das configura√ß√µes do `config.ini`**:

*   **Se√ß√£o `[MAIN]`:**
    *   `is_local`: `True` se estiver usando um provedor LLM local (Ollama, LM-Studio, servidor local compat√≠vel com OpenAI) ou a op√ß√£o de servidor auto-hospedado. `False` se estiver usando uma API em nuvem (OpenAI, Google, etc.).
    *   `provider_name`: Especifica o provedor LLM.
        *   Op√ß√µes locais: `ollama`, `lm-studio`, `openai` (para servidores locais compat√≠veis com OpenAI), `server` (para configura√ß√£o de servidor auto-hospedado).
        *   Op√ß√µes de API: `openai`, `google`, `deepseek`, `huggingface`, `togetherAI`.
    *   `provider_model`: O nome ou ID do modelo espec√≠fico para o provedor escolhido (por exemplo, `deepseekcoder:6.7b` para Ollama, `gpt-3.5-turbo` para API OpenAI, `mistralai/Mixtral-8x7B-Instruct-v0.1` para TogetherAI).
    *   `provider_server_address`: O endere√ßo do seu provedor LLM.
        *   Para provedores locais: por exemplo, `http://127.0.0.1:11434` para Ollama, `http://127.0.0.1:1234` para LM-Studio.
        *   Para o tipo de provedor `server`: o endere√ßo do seu servidor LLM auto-hospedado (por exemplo, `http://seu_ip_servidor:3333`).
        *   Para APIs em nuvem (`is_local = False`): geralmente √© ignorado ou pode ser deixado em branco, pois o endpoint da API √© normalmente gerenciado pela biblioteca cliente.
    *   `agent_name`: Nome do assistente de IA (por exemplo, Friday). Usado como palavra de ativa√ß√£o para reconhecimento de fala, se ativado.
    *   `recover_last_session`: `True` para tentar restaurar o estado da sess√£o anterior, `False` para iniciar do zero.
    *   `save_session`: `True` para salvar o estado da sess√£o atual para poss√≠vel recupera√ß√£o, `False` caso contr√°rio.
    *   `speak`: `True` para ativar sa√≠da de voz via texto para fala, `False` para desativar.
    *   `listen`: `True` para ativar entrada de voz via reconhecimento de fala (somente modo CLI), `False` para desativar.
    *   `work_dir`: **Crucial:** O diret√≥rio onde o AgenticSeek ir√° ler/gravar arquivos. **Certifique-se de que este caminho √© v√°lido e acess√≠vel em seu sistema.**
    *   `jarvis_personality`: `True` para usar um prompt de sistema mais "estilo Jarvis" (experimental), `False` para o prompt padr√£o.
    *   `languages`: Uma lista de idiomas separada por v√≠rgulas (por exemplo, `en, zh, fr`). Usada para sele√ß√£o de voz TTS (padr√£o √© o primeiro) e pode auxiliar o roteador LLM. Evite muitos idiomas ou idiomas muito semelhantes para efici√™ncia do roteador.
*   **Se√ß√£o `[BROWSER]`:**
    *   `headless_browser`: `True` para rodar o navegador automatizado sem janela vis√≠vel (recomendado para interface web ou uso n√£o interativo). `False` para exibir a janela do navegador (√∫til para modo CLI ou depura√ß√£o).
    *   `stealth_mode`: `True` para ativar medidas que dificultam a detec√ß√£o da automa√ß√£o do navegador. Pode exigir instala√ß√£o manual de extens√µes como anticaptcha.

Esta se√ß√£o resume os tipos de provedores LLM suportados. Configure-os no `config.ini`.

**Provedores Locais (Executados no Seu Pr√≥prio Hardware):**

| Nome do Provedor no `config.ini` | `is_local` | Descri√ß√£o                                                                 | Se√ß√£o de Configura√ß√£o                                               |
|-------------------------------|------------|-------------------------------------------------------------------------|---------------------------------------------------------------------|
| `ollama`                      | `True`     | Usa o Ollama para servir LLMs locais.                                   | [Configura√ß√£o para rodar LLM localmente](#setup-for-running-llm-locally-on-your-machine) |
| `lm-studio`                   | `True`     | Usa o LM-Studio para servir LLMs locais.                                | [Configura√ß√£o para rodar LLM localmente](#setup-for-running-llm-locally-on-your-machine) |
| `openai` (para servidor local)| `True`     | Conecta a um servidor local que exp√µe uma API compat√≠vel com OpenAI (ex.: llama.cpp). | [Configura√ß√£o para rodar LLM localmente](#setup-for-running-llm-locally-on-your-machine) |
| `server`                      | `False`    | Conecta ao servidor LLM auto-hospedado do AgenticSeek rodando em outra m√°quina. | [Configura√ß√£o para rodar o LLM no seu pr√≥prio servidor](#setup-to-run-the-llm-on-your-own-server) |

**Provedores de API (Baseados em Nuvem):**

| Nome do Provedor no `config.ini` | `is_local` | Descri√ß√£o                                       | Se√ß√£o de Configura√ß√£o                               |
|-------------------------------|------------|------------------------------------------------|-----------------------------------------------------|
| `openai`                      | `False`    | Usa a API oficial da OpenAI (ex.: GPT-3.5, GPT-4). | [Configura√ß√£o para rodar com uma API](#setup-to-run-with-an-api) |
| `google`                      | `False`    | Usa modelos Gemini do Google via API.           | [Configura√ß√£o para rodar com uma API](#setup-to-run-with-an-api) |
| `deepseek`                    | `False`    | Usa a API oficial Deepseek.                     | [Configura√ß√£o para rodar com uma API](#setup-to-run-with-an-api) |
| `huggingface`                 | `False`    | Usa a API de Infer√™ncia do Hugging Face.        | [Configura√ß√£o para rodar com uma API](#setup-to-run-with-an-api) |
| `togetherAI`                  | `False`    | Usa a API TogetherAI para v√°rios modelos abertos.| [Configura√ß√£o para rodar com uma API](#setup-to-run-with-an-api) |

---
## Solu√ß√£o de Problemas

Se voc√™ encontrar problemas, esta se√ß√£o traz orienta√ß√µes.

# Problemas Conhecidos

## Problemas com o ChromeDriver

**Exemplo de Erro:** `SessionNotCreatedException: Message: session not created: This version of ChromeDriver only supports Chrome version XXX`

*   **Causa:** A vers√£o do ChromeDriver instalada √© incompat√≠vel com a vers√£o do seu navegador Google Chrome.
*   **Solu√ß√£o:**
    1.  **Verifique a Vers√£o do Chrome:** Abra o Google Chrome, v√° em `Configura√ß√µes > Sobre o Chrome` para encontrar sua vers√£o (exemplo: "Vers√£o 120.0.6099.110").
    2.  **Baixe o ChromeDriver Correspondente:**
        *   Para vers√µes do Chrome 115 ou mais recentes: Acesse os [Endpoints JSON do Chrome for Testing (CfT)](https://googlechromelabs.github.io/chrome-for-testing/). Encontre o canal "stable" e baixe o ChromeDriver para seu SO que corresponda √† vers√£o principal do seu Chrome.
        *   Para vers√µes mais antigas (menos comuns): Voc√™ pode encontr√°-las na p√°gina [ChromeDriver - WebDriver for Chrome](https://chromedriver.chromium.org/downloads).
        *   A imagem abaixo mostra um exemplo da p√°gina CfT:
            ![Baixe a vers√£o espec√≠fica do Chromedriver na p√°gina Chrome for Testing](https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/chromedriver_readme.png)
    3.  **Instale o ChromeDriver:**
        *   Certifique-se de que o `chromedriver` baixado (ou `chromedriver.exe` no Windows) esteja em um diret√≥rio listado na vari√°vel de ambiente PATH do seu sistema (ex.: `/usr/local/bin` no Linux/macOS ou uma pasta de scripts personalizada adicionada ao PATH no Windows).
        *   Alternativamente, coloque-o no diret√≥rio raiz do projeto `agenticSeek`.
        *   Certifique-se de que o driver seja execut√°vel (ex.: `chmod +x chromedriver` no Linux/macOS).
    4.  Consulte a se√ß√£o [Instala√ß√£o do ChromeDriver](#chromedriver-installation) no guia principal de Instala√ß√£o para mais detalhes.

Se esta se√ß√£o estiver incompleta ou voc√™ encontrar outros problemas com o ChromeDriver, considere pesquisar nos [Issues do GitHub](https://github.com/Fosowl/agenticSeek/issues) existentes ou abrir um novo.

`Exception: Failed to initialize browser: Message: session not created: This version of ChromeDriver only supports Chrome version 113
Current browser version is 134.0.6998.89 with binary path`

Isso ocorre se houver incompatibilidade entre a vers√£o do navegador e do chromedriver.

Voc√™ precisa acessar e baixar a vers√£o mais recente:

https://developer.chrome.com/docs/chromedriver/downloads

Se voc√™ estiver usando o Chrome vers√£o 115 ou mais recente, acesse:

https://googlechromelabs.github.io/chrome-for-testing/

E baixe a vers√£o do chromedriver correspondente ao seu SO.

![alt text](https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/chromedriver_readme.png)

Se esta se√ß√£o estiver incompleta, por favor, abra uma issue.

## Problemas com connection adapters

```
Exception: Provider lm-studio failed: HTTP request failed: No connection adapters were found for '127.0.0.1:1234/v1/chat/completions'` (Nota: a porta pode variar)
```

*   **Causa:** O `provider_server_address` no `config.ini` para o `lm-studio` (ou outros servidores locais compat√≠veis com OpenAI) est√° sem o prefixo `http://` ou est√° apontando para a porta errada.
*   **Solu√ß√£o:**
    *   Certifique-se de que o endere√ßo inclua `http://`. O LM-Studio normalmente usa `http://127.0.0.1:1234`.
    *   Corrija no `config.ini`: `provider_server_address = http://127.0.0.1:1234` (ou a porta real do seu servidor LM-Studio).

## SearxNG Base URL N√£o Fornecido

```
raise ValueError("SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.")
ValueError: SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.`
```

## Perguntas Frequentes (FAQ)

**P: Que hardware eu preciso?**  

| Tamanho do Modelo | GPU           | Coment√°rio                                                                                  |
|-------------------|---------------|--------------------------------------------------------------------------------------------|
| 7B                | 8GB Vram      | ‚ö†Ô∏è N√£o recomendado. Baixo desempenho, alucina√ß√µes frequentes, agentes planejadores devem falhar. |
| 14B               | 12 GB VRAM (ex.: RTX 3060) | ‚úÖ Utiliz√°vel para tarefas simples. Pode ter dificuldades com navega√ß√£o web e planejamento. |
| 32B               | 24+ GB VRAM (ex.: RTX 4090) | üöÄ Sucesso na maioria das tarefas, pode ainda ter dificuldades com planejamento de tarefas  |
| 70B+              | 48+ GB Vram   | üí™ Excelente. Recomendado para casos de uso avan√ßados.                                      |

**P: Recebo um erro, o que fa√ßo?**  

Certifique-se de que o local est√° rodando (`ollama serve`), que seu `config.ini` corresponde ao seu provedor e que as depend√™ncias est√£o instaladas. Se nada funcionar, sinta-se √† vontade para abrir uma issue.

**P: Realmente pode rodar 100% localmente?**  

Sim, com Ollama, lm-studio ou provedores de servidor, todo reconhecimento de fala, LLM e modelo de texto para fala rodam localmente. Op√ß√µes n√£o locais (OpenAI ou outras APIs) s√£o opcionais.

**P: Por que devo usar o AgenticSeek se j√° tenho o Manus?**

Ao contr√°rio do Manus, o AgenticSeek prioriza independ√™ncia de sistemas externos, oferecendo mais controle, privacidade e evitando custos de API.

**P: Quem est√° por tr√°s do projeto?**

O projeto foi criado por mim, junto com dois amigos que atuam como mantenedores e colaboradores da comunidade open source no GitHub. Somos apenas um grupo de pessoas apaixonadas, n√£o uma startup ou afiliados a qualquer organiza√ß√£o.

Qualquer conta do AgenticSeek no X al√©m da minha pessoal (https://x.com/Martin993886460) √© uma falsifica√ß√£o.

## Contribua

Estamos procurando desenvolvedores para melhorar o AgenticSeek! Veja as issues abertas ou discuss√µes.

[Guia de contribui√ß√£o](https://raw.githubusercontent.com/Fosowl/agenticSeek/main/docs/CONTRIBUTING.md)

[![Star History Chart](https://api.star-history.com/svg?repos=Fosowl/agenticSeek&type=Date)](https://www.star-history.com/#Fosowl/agenticSeek&Date)

## Mantenedores:

 > [Fosowl](https://github.com/Fosowl) | Hor√°rio de Paris 

 > [antoineVIVIES](https://github.com/antoineVIVIES) | Hor√°rio de Taipei

 > [steveh8758](https://github.com/steveh8758) | Hor√°rio de Taipei

## Agradecimentos Especiais:

 > [tcsenpai](https://github.com/tcsenpai) e [plitc](https://github.com/plitc) pela ajuda com dockeriza√ß√£o do backend

## Patrocinadores:

Patrocinadores mensais de 5 d√≥lares ou mais aparecem aqui:
- **tatra-labs**

I'm sorry, but you haven't provided the content of Part 4 of 4 to translate. Please provide the text you'd like translated.

---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-06-16

---