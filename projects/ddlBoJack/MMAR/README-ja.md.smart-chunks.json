[
  {
    "Id": 1,
    "Content": "\n# MMAR: A Challenging Benchmark for Deep Reasoning in Speech, Audio, Music, and Their Mix\n[**ğŸ“– arXiv**](https://arxiv.org/abs/2505.13032) | [**ğŸ¬ MMAR Demo Video**](https://www.youtube.com/watch?v=Dab13opIGqU) | [**ğŸ› ï¸ GitHub Code**](https://github.com/ddlBoJack/MMAR) | [**ğŸ”Š MMAR Audio Download (HuggingFace)**](https://huggingface.co/datasets/BoJack/MMAR)\n                                          \n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/ddlBoJack/MMAR/main/assets/logo.png\" alt=\"MMAR Benchmark Logo\" width=\"300\"/></p>\n\n## Overview of MMAR\nWe introduce MMAR, a new benchmark designed to evaluate the deep reasoning capabilities of Audio-Language Models (ALMs) across massive multi-disciplinary tasks. \nMMAR comprises 1,000 meticulously curated audio-question-answer triplets, collected from real-world internet videos and refined through iterative error corrections and quality checks to ensure high quality. \nEach item in the benchmark demands multi-step deep reasoning beyond surface-level understanding. Moreover, a part of the questions requires graduate-level perceptual and domain-specific knowledge, elevating the benchmark's difficulty and depth. \nExamples include:\n\n![Example](https://raw.githubusercontent.com/ddlBoJack/MMAR/main/assets/example.png)\n\nThe metadata for MMAR is available in [this file](https://raw.githubusercontent.com/ddlBoJack/MMAR/main/MMAR-meta.json). Unlike previous benchmarks, MMAR not only covers traditional modalities such as speech, audio, and music, but also extends to their mix, collected from in-the-wild videos. The distribution of data across these modalities is illustrated in the left figure. Furthermore, each question is annotated with a designated category and sub-category, as shown in the right figure.\n\nFor each question, we also provide the URL and corresponding timestamp of the original video, as well as the spoken language (if present) in the clip. To prevent potential data leakage into training for reasoning models, we have withheld reasoning cues and chain-of-thought annotations, which will be released at an appropriate time.\n\n<p float=\"left\">\n  <img src=\"https://raw.githubusercontent.com/ddlBoJack/MMAR/main/assets/modality_pie.png\" width=\"49%\" />\n  <img src=\"https://raw.githubusercontent.com/ddlBoJack/MMAR/main/assets/category_sunburst.png\" width=\"49%\" />\n</p>\n\n## Benchmark Results\nWe benchmark the models on MMAR across five model categories: \n1. Large Audio Language Models (LALMs)\n2. Large Audio Reasoning Models (LARMs)\n3. Omni Language Models (OLMs)\n4. Large Language Models (LLMs) with audio captions as input\n5. Large Reasoning Models (LRMs) with audio captions as input\n\n![Pipeline](https://raw.githubusercontent.com/ddlBoJack/MMAR/main/assets/benchmark.png)\n\n## Dataset Creation\nThe MMAR benchmark was constructed with a comprehensive pipeline. The process includes: \n1. Brainstorming challenging questions\n2. Building a taxonomy through human-LLM collaboration\n3. Heuristic-based data collection and annotation\n4. Crawling audio data and enriching content across multiple slots\n5. Performing iterative correction and quality inspection to ensure high data fidelity\n\n![Pipeline](https://raw.githubusercontent.com/ddlBoJack/MMAR/main/assets/pipeline.png)\n\n## Test Your Model !\n\nTo ensure a smooth integration into existing evaluation pipelines, we adopt an evaluation methodology modified from [MMAU](https://github.com/Sakshi113/MMAU), implemented in [evaluation.py](https://raw.githubusercontent.com/ddlBoJack/MMAR/main/code/evaluation.py). The input to the evaluation script should be the same as [MMAR-meta.json](https://raw.githubusercontent.com/ddlBoJack/MMAR/main/MMAR-meta.json), with an additional key named `model_prediction`, which stores the model prediction for each question. \n  \nTo run the script:",
    "ContentSha": "0WXXg03LR7T7AYyvbQ4hNiaNhD4RgWUJdzg/wwNCkCk=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "<translate-content>\n# MMARï¼šéŸ³å£°ã€ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã€éŸ³æ¥½ã€ãŠã‚ˆã³ãã‚Œã‚‰ã®æ··åˆã«ãŠã‘ã‚‹æ·±å±¤æ¨è«–ã®ãŸã‚ã®æŒ‘æˆ¦çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯\n[**ğŸ“– arXiv**](https://arxiv.org/abs/2505.13032) | [**ğŸ¬ MMAR ãƒ‡ãƒ¢å‹•ç”»**](https://www.youtube.com/watch?v=Dab13opIGqU) | [**ğŸ› ï¸ GitHub ã‚³ãƒ¼ãƒ‰**](https://github.com/ddlBoJack/MMAR) | [**ğŸ”Š MMAR ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆHuggingFaceï¼‰**](https://huggingface.co/datasets/BoJack/MMAR)\n                                          \n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/ddlBoJack/MMAR/main/assets/logo.png\" alt=\"MMAR Benchmark Logo\" width=\"300\"/></p>\n\n## MMARã®æ¦‚è¦\nç§ãŸã¡ã¯MMARã‚’ç´¹ä»‹ã—ã¾ã™ã€‚ã“ã‚Œã¯ã€å¤§è¦æ¨¡ãªå¤šåˆ†é‡ã‚¿ã‚¹ã‚¯ã«ã‚ãŸã‚‹ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªè¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆALMï¼‰ã®æ·±å±¤æ¨è«–èƒ½åŠ›ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã«è¨­è¨ˆã•ã‚ŒãŸæ–°ã—ã„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã™ã€‚  \nMMARã¯ã€å®Ÿéš›ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆå‹•ç”»ã‹ã‚‰åé›†ã•ã‚Œã€åå¾©çš„ãªèª¤ã‚Šä¿®æ­£ã¨å“è³ªãƒã‚§ãƒƒã‚¯ã‚’é€šã˜ã¦é«˜å“è³ªã‚’ä¿è¨¼ã•ã‚ŒãŸã€å³é¸ã•ã‚ŒãŸ1,000ã®ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ»è³ªå•ãƒ»å›ç­”ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚  \nãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®å„é …ç›®ã¯ã€è¡¨é¢çš„ç†è§£ã‚’è¶…ãˆãŸå¤šæ®µéšã®æ·±å±¤æ¨è«–ã‚’è¦æ±‚ã—ã¾ã™ã€‚ã•ã‚‰ã«ã€ä¸€éƒ¨ã®è³ªå•ã¯å¤§å­¦é™¢ãƒ¬ãƒ™ãƒ«ã®çŸ¥è¦šãŠã‚ˆã³å°‚é–€åˆ†é‡çŸ¥è­˜ã‚’å¿…è¦ã¨ã—ã€ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®é›£æ˜“åº¦ã¨æ·±ã•ã‚’é«˜ã‚ã¦ã„ã¾ã™ã€‚  \nä¾‹ã¨ã—ã¦ã¯ä»¥ä¸‹ãŒã‚ã‚Šã¾ã™ï¼š\n\n![Example](https://raw.githubusercontent.com/ddlBoJack/MMAR/main/assets/example.png)\n\nMMARã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¯[ã“ã¡ã‚‰ã®ãƒ•ã‚¡ã‚¤ãƒ«](https://raw.githubusercontent.com/ddlBoJack/MMAR/main/MMAR-meta.json)ã§åˆ©ç”¨å¯èƒ½ã§ã™ã€‚å¾“æ¥ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¨ç•°ãªã‚Šã€MMARã¯éŸ³å£°ã€ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã€éŸ³æ¥½ãªã©ã®å¾“æ¥ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã«åŠ ãˆã€ãã‚Œã‚‰ã®æ··åˆã‚‚å«ã¿ã€é‡å¤–å‹•ç”»ã‹ã‚‰åé›†ã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã‚Œã‚‰ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã«ãŠã‘ã‚‹ãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒã¯å·¦å›³ã«ç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚ã•ã‚‰ã«ã€å„è³ªå•ã«ã¯æŒ‡å®šã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªãŠã‚ˆã³ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªãŒæ³¨é‡ˆã•ã‚Œã¦ãŠã‚Šã€å³å›³ã«ç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚\n\nå„è³ªå•ã«ã¤ã„ã¦ã€å…ƒã®å‹•ç”»ã®URLã¨å¯¾å¿œã™ã‚‹ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã€ãã—ã¦ã‚¯ãƒªãƒƒãƒ—å†…ã®è©±ã•ã‚Œã¦ã„ã‚‹è¨€èªï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰ã‚‚æä¾›ã—ã¦ã„ã¾ã™ã€‚æ¨è«–ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¸ã®æ½œåœ¨çš„ãªãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚’é˜²ããŸã‚ã€æ¨è«–æ‰‹æ›ã‹ã‚ŠãŠã‚ˆã³æ€è€ƒé€£é–æ³¨é‡ˆã¯é©åˆ‡ãªæ™‚æœŸã«ãƒªãƒªãƒ¼ã‚¹ã•ã‚Œã‚‹ã¾ã§ withheld ã—ã¦ã„ã¾ã™ã€‚\n\n<p float=\"left\">\n  <img src=\"https://raw.githubusercontent.com/ddlBoJack/MMAR/main/assets/modality_pie.png\" width=\"49%\" />\n  <img src=\"https://raw.githubusercontent.com/ddlBoJack/MMAR/main/assets/category_sunburst.png\" width=\"49%\" />\n</p>\n\n## ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ\nMMARä¸Šã§ã€ä»¥ä¸‹ã®5ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚«ãƒ†ã‚´ãƒªã«ã‚ãŸã‚Šãƒ¢ãƒ‡ãƒ«ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’è¡Œã„ã¾ã—ãŸï¼š  \n1. å¤§è¦æ¨¡ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªè¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLALMsï¼‰  \n2. å¤§è¦æ¨¡ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªæ¨è«–ãƒ¢ãƒ‡ãƒ«ï¼ˆLARMsï¼‰  \n3. ã‚ªãƒ ãƒ‹è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆOLMsï¼‰  \n4. ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’å…¥åŠ›ã¨ã™ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰  \n5. ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’å…¥åŠ›ã¨ã™ã‚‹å¤§è¦æ¨¡æ¨è«–ãƒ¢ãƒ‡ãƒ«ï¼ˆLRMsï¼‰  \n\n![Pipeline](https://raw.githubusercontent.com/ddlBoJack/MMAR/main/assets/benchmark.png)\n\n## ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ\nMMARãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¯åŒ…æ‹¬çš„ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§æ§‹ç¯‰ã•ã‚Œã¾ã—ãŸã€‚ãƒ—ãƒ­ã‚»ã‚¹ã¯ä»¥ä¸‹ã‚’å«ã¿ã¾ã™ï¼š  \n1. é›£å•ã®ãƒ–ãƒ¬ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒŸãƒ³ã‚°  \n2. äººé–“ã¨LLMã®å”åƒã«ã‚ˆã‚‹åˆ†é¡ä½“ç³»ã®æ§‹ç¯‰  \n3. ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ã«åŸºã¥ããƒ‡ãƒ¼ã‚¿åé›†ã¨æ³¨é‡ˆ  \n4. ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒ­ãƒ¼ãƒ«ã¨è¤‡æ•°ã‚¹ãƒ­ãƒƒãƒˆã«ã‚ãŸã‚‹å†…å®¹ã®å……å®ŸåŒ–  \n5. åå¾©çš„ãªä¿®æ­£ã¨å“è³ªæ¤œæŸ»ã‚’è¡Œã„é«˜ã„ãƒ‡ãƒ¼ã‚¿å¿ å®Ÿåº¦ã‚’ä¿è¨¼  \n\n![Pipeline](https://raw.githubusercontent.com/ddlBoJack/MMAR/main/assets/pipeline.png)\n\n## ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆã—ã‚ˆã†ï¼\n\næ—¢å­˜ã®è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¸ã®ã‚¹ãƒ ãƒ¼ã‚ºãªçµ±åˆã‚’ç¢ºä¿ã™ã‚‹ãŸã‚ã€[MMAU](https://github.com/Sakshi113/MMAU)ã‹ã‚‰ä¿®æ­£ã‚’åŠ ãˆãŸè©•ä¾¡æ‰‹æ³•ã‚’æ¡ç”¨ã—ã€[evaluation.py](https://raw.githubusercontent.com/ddlBoJack/MMAR/main/code/evaluation.py)ã«å®Ÿè£…ã—ã¦ã„ã¾ã™ã€‚è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¸ã®å…¥åŠ›ã¯ã€[MMAR-meta.json](https://raw.githubusercontent.com/ddlBoJack/MMAR/main/MMAR-meta.json)ã¨åŒã˜å½¢å¼ã§ã€å„è³ªå•ã«å¯¾ã™ã‚‹ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬ã‚’æ ¼ç´ã™ã‚‹ `model_prediction` ã¨ã„ã†è¿½åŠ ã‚­ãƒ¼ã‚’å«ã¿ã¾ã™ã€‚  \n  \nã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯ï¼š</translate-content>",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "<translate-content>"
      },
      {
        "row": 2,
        "rowsha": "aXM/4aMjuoV7zeBDSExF299gIW4ZHym5/yoFuZU3vh4=",
        "originContent": "# MMAR: A Challenging Benchmark for Deep Reasoning in Speech, Audio, Music, and Their Mix",
        "translatedContent": "# MMARï¼šéŸ³å£°ã€ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã€éŸ³æ¥½ã€ãŠã‚ˆã³ãã‚Œã‚‰ã®æ··åˆã«ãŠã‘ã‚‹æ·±å±¤æ¨è«–ã®ãŸã‚ã®æŒ‘æˆ¦çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"
      },
      {
        "row": 3,
        "rowsha": "dCfqATOU1WjucDlYXVrYxKbY8+mauhe3dnIQEClsFLc=",
        "originContent": "[**ğŸ“– arXiv**](https://arxiv.org/abs/2505.13032) | [**ğŸ¬ MMAR Demo Video**](https://www.youtube.com/watch?v=Dab13opIGqU) | [**ğŸ› ï¸ GitHub Code**](https://github.com/ddlBoJack/MMAR) | [**ğŸ”Š MMAR Audio Download (HuggingFace)**](https://huggingface.co/datasets/BoJack/MMAR)",
        "translatedContent": "[**ğŸ“– arXiv**](https://arxiv.org/abs/2505.13032) | [**ğŸ¬ MMAR ãƒ‡ãƒ¢å‹•ç”»**](https://www.youtube.com/watch?v=Dab13opIGqU) | [**ğŸ› ï¸ GitHub ã‚³ãƒ¼ãƒ‰**](https://github.com/ddlBoJack/MMAR) | [**ğŸ”Š MMAR ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆHuggingFaceï¼‰**](https://huggingface.co/datasets/BoJack/MMAR)"
      },
      {
        "row": 4,
        "rowsha": "WuCAOlrxXddGHTxJr+Yc6S9Rxii+N/y2NrmS1JPqV78=",
        "originContent": "                                          ",
        "translatedContent": "                                          "
      },
      {
        "row": 5,
        "rowsha": "epU9ZVv2k1NaoKO9ckigBLqMGTuRKyiMVeynli/TTiI=",
        "originContent": "<p align=\"center\"><img src=\"https://raw.githubusercontent.com/ddlBoJack/MMAR/main/assets/logo.png\" alt=\"MMAR Benchmark Logo\" width=\"300\"/></p>",
        "translatedContent": "<p align=\"center\"><img src=\"https://raw.githubusercontent.com/ddlBoJack/MMAR/main/assets/logo.png\" alt=\"MMAR Benchmark Logo\" width=\"300\"/></p>"
      },
      {
        "row": 6,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 7,
        "rowsha": "QN6zNM/THCbcrUf5q8Nmadx/32PlbRQJITh/CNegtnE=",
        "originContent": "## Overview of MMAR",
        "translatedContent": "## MMARã®æ¦‚è¦"
      },
      {
        "row": 8,
        "rowsha": "VzF0g9JZmDyYXl6g6MrO9y/fLO66zR9ZMlyqzdkPLwo=",
        "originContent": "We introduce MMAR, a new benchmark designed to evaluate the deep reasoning capabilities of Audio-Language Models (ALMs) across massive multi-disciplinary tasks. ",
        "translatedContent": "ç§ãŸã¡ã¯MMARã‚’ç´¹ä»‹ã—ã¾ã™ã€‚ã“ã‚Œã¯ã€å¤§è¦æ¨¡ãªå¤šåˆ†é‡ã‚¿ã‚¹ã‚¯ã«ã‚ãŸã‚‹ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªè¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆALMï¼‰ã®æ·±å±¤æ¨è«–èƒ½åŠ›ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã«è¨­è¨ˆã•ã‚ŒãŸæ–°ã—ã„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã™ã€‚  "
      },
      {
        "row": 9,
        "rowsha": "3aGz5j/WjMOhHQijj2uhGB4Q9PUlZ/IIcFfR4Y3zIyc=",
        "originContent": "MMAR comprises 1,000 meticulously curated audio-question-answer triplets, collected from real-world internet videos and refined through iterative error corrections and quality checks to ensure high quality. ",
        "translatedContent": "MMARã¯ã€å®Ÿéš›ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆå‹•ç”»ã‹ã‚‰åé›†ã•ã‚Œã€åå¾©çš„ãªèª¤ã‚Šä¿®æ­£ã¨å“è³ªãƒã‚§ãƒƒã‚¯ã‚’é€šã˜ã¦é«˜å“è³ªã‚’ä¿è¨¼ã•ã‚ŒãŸã€å³é¸ã•ã‚ŒãŸ1,000ã®ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ»è³ªå•ãƒ»å›ç­”ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚  "
      },
      {
        "row": 10,
        "rowsha": "GftgAuH8ua4eoDAvdccw2LRtvuVBKWzY7OEO2X8B7DM=",
        "originContent": "Each item in the benchmark demands multi-step deep reasoning beyond surface-level understanding. Moreover, a part of the questions requires graduate-level perceptual and domain-specific knowledge, elevating the benchmark's difficulty and depth. ",
        "translatedContent": "ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®å„é …ç›®ã¯ã€è¡¨é¢çš„ç†è§£ã‚’è¶…ãˆãŸå¤šæ®µéšã®æ·±å±¤æ¨è«–ã‚’è¦æ±‚ã—ã¾ã™ã€‚ã•ã‚‰ã«ã€ä¸€éƒ¨ã®è³ªå•ã¯å¤§å­¦é™¢ãƒ¬ãƒ™ãƒ«ã®çŸ¥è¦šãŠã‚ˆã³å°‚é–€åˆ†é‡çŸ¥è­˜ã‚’å¿…è¦ã¨ã—ã€ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®é›£æ˜“åº¦ã¨æ·±ã•ã‚’é«˜ã‚ã¦ã„ã¾ã™ã€‚  "
      },
      {
        "row": 11,
        "rowsha": "7Zmdrty2OCDs56+etlgVgRIVelO//GIaD/Ce+Ndiulo=",
        "originContent": "Examples include:",
        "translatedContent": "ä¾‹ã¨ã—ã¦ã¯ä»¥ä¸‹ãŒã‚ã‚Šã¾ã™ï¼š"
      },
      {
        "row": 12,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 13,
        "rowsha": "9IG8u2BDWlgOhJHdJcPn8jmoJK/03X9hKaIyQv2lV9A=",
        "originContent": "![Example](https://raw.githubusercontent.com/ddlBoJack/MMAR/main/assets/example.png)",
        "translatedContent": "![Example](https://raw.githubusercontent.com/ddlBoJack/MMAR/main/assets/example.png)"
      },
      {
        "row": 14,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 15,
        "rowsha": "bmFFN3HNWV9QdTpn8QT9JDY54Dk9aRUiZQOWJAsqfLQ=",
        "originContent": "The metadata for MMAR is available in [this file](https://raw.githubusercontent.com/ddlBoJack/MMAR/main/MMAR-meta.json). Unlike previous benchmarks, MMAR not only covers traditional modalities such as speech, audio, and music, but also extends to their mix, collected from in-the-wild videos. The distribution of data across these modalities is illustrated in the left figure. Furthermore, each question is annotated with a designated category and sub-category, as shown in the right figure.",
        "translatedContent": "MMARã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¯[ã“ã¡ã‚‰ã®ãƒ•ã‚¡ã‚¤ãƒ«](https://raw.githubusercontent.com/ddlBoJack/MMAR/main/MMAR-meta.json)ã§åˆ©ç”¨å¯èƒ½ã§ã™ã€‚å¾“æ¥ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¨ç•°ãªã‚Šã€MMARã¯éŸ³å£°ã€ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã€éŸ³æ¥½ãªã©ã®å¾“æ¥ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã«åŠ ãˆã€ãã‚Œã‚‰ã®æ··åˆã‚‚å«ã¿ã€é‡å¤–å‹•ç”»ã‹ã‚‰åé›†ã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã‚Œã‚‰ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã«ãŠã‘ã‚‹ãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒã¯å·¦å›³ã«ç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚ã•ã‚‰ã«ã€å„è³ªå•ã«ã¯æŒ‡å®šã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªãŠã‚ˆã³ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªãŒæ³¨é‡ˆã•ã‚Œã¦ãŠã‚Šã€å³å›³ã«ç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚"
      },
      {
        "row": 16,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 17,
        "rowsha": "IQs1WsLraNnYbgeAy+xSpbhL9oUGV/0LUQKxzonk0iM=",
        "originContent": "For each question, we also provide the URL and corresponding timestamp of the original video, as well as the spoken language (if present) in the clip. To prevent potential data leakage into training for reasoning models, we have withheld reasoning cues and chain-of-thought annotations, which will be released at an appropriate time.",
        "translatedContent": "å„è³ªå•ã«ã¤ã„ã¦ã€å…ƒã®å‹•ç”»ã®URLã¨å¯¾å¿œã™ã‚‹ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã€ãã—ã¦ã‚¯ãƒªãƒƒãƒ—å†…ã®è©±ã•ã‚Œã¦ã„ã‚‹è¨€èªï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰ã‚‚æä¾›ã—ã¦ã„ã¾ã™ã€‚æ¨è«–ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¸ã®æ½œåœ¨çš„ãªãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚’é˜²ããŸã‚ã€æ¨è«–æ‰‹æ›ã‹ã‚ŠãŠã‚ˆã³æ€è€ƒé€£é–æ³¨é‡ˆã¯é©åˆ‡ãªæ™‚æœŸã«ãƒªãƒªãƒ¼ã‚¹ã•ã‚Œã‚‹ã¾ã§ withheld ã—ã¦ã„ã¾ã™ã€‚"
      },
      {
        "row": 18,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 19,
        "rowsha": "GUbCMidcscNwoOhSkP+X+EwL8KN6omDI0Cory0F0HZc=",
        "originContent": "<p float=\"left\">",
        "translatedContent": "<p float=\"left\">"
      },
      {
        "row": 20,
        "rowsha": "EVF61S76N795S1sL9wFos/F3UEWdyF0/gTUvwI/TPcw=",
        "originContent": "  <img src=\"https://raw.githubusercontent.com/ddlBoJack/MMAR/main/assets/modality_pie.png\" width=\"49%\" />",
        "translatedContent": "  <img src=\"https://raw.githubusercontent.com/ddlBoJack/MMAR/main/assets/modality_pie.png\" width=\"49%\" />"
      },
      {
        "row": 21,
        "rowsha": "0Bh21fPvcHnzwrHguk6aREiwnOMoxKfE+hArykysjR4=",
        "originContent": "  <img src=\"https://raw.githubusercontent.com/ddlBoJack/MMAR/main/assets/category_sunburst.png\" width=\"49%\" />",
        "translatedContent": "  <img src=\"https://raw.githubusercontent.com/ddlBoJack/MMAR/main/assets/category_sunburst.png\" width=\"49%\" />"
      },
      {
        "row": 22,
        "rowsha": "dSdvPNAZSmR86FDDSF6tkQUCVfI9qmACHOR5tThOetY=",
        "originContent": "</p>",
        "translatedContent": "</p>"
      },
      {
        "row": 23,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 24,
        "rowsha": "BzPiSdgeWwyGfxLo1IckxH2q7o+en7fQn7stJtetnMo=",
        "originContent": "## Benchmark Results",
        "translatedContent": "## ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ"
      },
      {
        "row": 25,
        "rowsha": "pmVpBD1MH12Ix3GKMSiOSUZCPVWvGdwcIzeAjk60xXw=",
        "originContent": "We benchmark the models on MMAR across five model categories: ",
        "translatedContent": "MMARä¸Šã§ã€ä»¥ä¸‹ã®5ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚«ãƒ†ã‚´ãƒªã«ã‚ãŸã‚Šãƒ¢ãƒ‡ãƒ«ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’è¡Œã„ã¾ã—ãŸï¼š  "
      },
      {
        "row": 26,
        "rowsha": "oR6e+Y9nPJR+DLoM/Wv9Z/yhJjaFn5eYT4JJiX3cVnc=",
        "originContent": "1. Large Audio Language Models (LALMs)",
        "translatedContent": "1. å¤§è¦æ¨¡ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªè¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLALMsï¼‰  "
      },
      {
        "row": 27,
        "rowsha": "ypEbH8QdZZOVPpPwJIbaBqDtT5CLywbYtCRjSCE7Ti8=",
        "originContent": "2. Large Audio Reasoning Models (LARMs)",
        "translatedContent": "2. å¤§è¦æ¨¡ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªæ¨è«–ãƒ¢ãƒ‡ãƒ«ï¼ˆLARMsï¼‰  "
      },
      {
        "row": 28,
        "rowsha": "XKrTEl2oWEpyVDl5d6BrHN7xvkYhUPH5jnjaym1dcvU=",
        "originContent": "3. Omni Language Models (OLMs)",
        "translatedContent": "3. ã‚ªãƒ ãƒ‹è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆOLMsï¼‰  "
      },
      {
        "row": 29,
        "rowsha": "hOO5atGuLPaYKqwoMe2npLG13TLyeyeKeeScXLxu6hY=",
        "originContent": "4. Large Language Models (LLMs) with audio captions as input",
        "translatedContent": "4. ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’å…¥åŠ›ã¨ã™ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰  "
      },
      {
        "row": 30,
        "rowsha": "lxzWdDIA4ZutU/hDh46rRAXL2WmTuX4KTOh2g/dIB5Q=",
        "originContent": "5. Large Reasoning Models (LRMs) with audio captions as input",
        "translatedContent": "5. ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’å…¥åŠ›ã¨ã™ã‚‹å¤§è¦æ¨¡æ¨è«–ãƒ¢ãƒ‡ãƒ«ï¼ˆLRMsï¼‰  "
      },
      {
        "row": 31,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 32,
        "rowsha": "EqwclIuAiPyH7bJxS+T3YzTT64664Xe7j3a8ZBvlEzM=",
        "originContent": "![Pipeline](https://raw.githubusercontent.com/ddlBoJack/MMAR/main/assets/benchmark.png)",
        "translatedContent": "![Pipeline](https://raw.githubusercontent.com/ddlBoJack/MMAR/main/assets/benchmark.png)"
      },
      {
        "row": 33,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 34,
        "rowsha": "zoNK31UNgZFB8VvbmMZkfZLLM8sF6a3jUxCADcMcTFE=",
        "originContent": "## Dataset Creation",
        "translatedContent": "## ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ"
      },
      {
        "row": 35,
        "rowsha": "jdaMPWQFgktn8Mq2Sakvd+thZUgEY0YxR/GLXawS0F8=",
        "originContent": "The MMAR benchmark was constructed with a comprehensive pipeline. The process includes: ",
        "translatedContent": "MMARãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¯åŒ…æ‹¬çš„ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§æ§‹ç¯‰ã•ã‚Œã¾ã—ãŸã€‚ãƒ—ãƒ­ã‚»ã‚¹ã¯ä»¥ä¸‹ã‚’å«ã¿ã¾ã™ï¼š  "
      },
      {
        "row": 36,
        "rowsha": "jB3vqk6+/ytb3NIoSWQCwOsmI0bCM0BJ2zx1vSz4seY=",
        "originContent": "1. Brainstorming challenging questions",
        "translatedContent": "1. é›£å•ã®ãƒ–ãƒ¬ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒŸãƒ³ã‚°  "
      },
      {
        "row": 37,
        "rowsha": "jCC93EtBVh858X1WbKfFNfPsyMN8vfpqiNa06Xf74kc=",
        "originContent": "2. Building a taxonomy through human-LLM collaboration",
        "translatedContent": "2. äººé–“ã¨LLMã®å”åƒã«ã‚ˆã‚‹åˆ†é¡ä½“ç³»ã®æ§‹ç¯‰  "
      },
      {
        "row": 38,
        "rowsha": "wDHQ8jRU+f1jPvtpopuI1qnx2Nw849hygTkcT7Zkp74=",
        "originContent": "3. Heuristic-based data collection and annotation",
        "translatedContent": "3. ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ã«åŸºã¥ããƒ‡ãƒ¼ã‚¿åé›†ã¨æ³¨é‡ˆ  "
      },
      {
        "row": 39,
        "rowsha": "mRtVMr85bh25S9BNJLXfdG/A7D7ysz0qw9HrwlN1uUc=",
        "originContent": "4. Crawling audio data and enriching content across multiple slots",
        "translatedContent": "4. ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒ­ãƒ¼ãƒ«ã¨è¤‡æ•°ã‚¹ãƒ­ãƒƒãƒˆã«ã‚ãŸã‚‹å†…å®¹ã®å……å®ŸåŒ–  "
      },
      {
        "row": 40,
        "rowsha": "b1g560K97AsIhgMWfm1Zn1loRSMGxhd8iOYsMWFwOfU=",
        "originContent": "5. Performing iterative correction and quality inspection to ensure high data fidelity",
        "translatedContent": "5. åå¾©çš„ãªä¿®æ­£ã¨å“è³ªæ¤œæŸ»ã‚’è¡Œã„é«˜ã„ãƒ‡ãƒ¼ã‚¿å¿ å®Ÿåº¦ã‚’ä¿è¨¼  "
      },
      {
        "row": 41,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 42,
        "rowsha": "9VH0swxibKuI4kBeTL1E+yAl8Ootrqldu/TpclsTuew=",
        "originContent": "![Pipeline](https://raw.githubusercontent.com/ddlBoJack/MMAR/main/assets/pipeline.png)",
        "translatedContent": "![Pipeline](https://raw.githubusercontent.com/ddlBoJack/MMAR/main/assets/pipeline.png)"
      },
      {
        "row": 43,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 44,
        "rowsha": "YcCut4hQV3XmsMwQ7W19jKmMmgnLPVWLZPFahSN1Hi8=",
        "originContent": "## Test Your Model !",
        "translatedContent": "## ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆã—ã‚ˆã†ï¼"
      },
      {
        "row": 45,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 46,
        "rowsha": "Gvm/TYUrCW//dfRDlFd9m7nG2/3qEHHMDrNU9WFvGVQ=",
        "originContent": "To ensure a smooth integration into existing evaluation pipelines, we adopt an evaluation methodology modified from [MMAU](https://github.com/Sakshi113/MMAU), implemented in [evaluation.py](https://raw.githubusercontent.com/ddlBoJack/MMAR/main/code/evaluation.py). The input to the evaluation script should be the same as [MMAR-meta.json](https://raw.githubusercontent.com/ddlBoJack/MMAR/main/MMAR-meta.json), with an additional key named `model_prediction`, which stores the model prediction for each question. ",
        "translatedContent": "æ—¢å­˜ã®è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¸ã®ã‚¹ãƒ ãƒ¼ã‚ºãªçµ±åˆã‚’ç¢ºä¿ã™ã‚‹ãŸã‚ã€[MMAU](https://github.com/Sakshi113/MMAU)ã‹ã‚‰ä¿®æ­£ã‚’åŠ ãˆãŸè©•ä¾¡æ‰‹æ³•ã‚’æ¡ç”¨ã—ã€[evaluation.py](https://raw.githubusercontent.com/ddlBoJack/MMAR/main/code/evaluation.py)ã«å®Ÿè£…ã—ã¦ã„ã¾ã™ã€‚è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¸ã®å…¥åŠ›ã¯ã€[MMAR-meta.json](https://raw.githubusercontent.com/ddlBoJack/MMAR/main/MMAR-meta.json)ã¨åŒã˜å½¢å¼ã§ã€å„è³ªå•ã«å¯¾ã™ã‚‹ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬ã‚’æ ¼ç´ã™ã‚‹ `model_prediction` ã¨ã„ã†è¿½åŠ ã‚­ãƒ¼ã‚’å«ã¿ã¾ã™ã€‚  "
      },
      {
        "row": 47,
        "rowsha": "bBefIeb2K2KQVdirQPRU7QLki2hWORNHO4V9NjjiOyg=",
        "originContent": "  ",
        "translatedContent": "  "
      },
      {
        "row": 48,
        "rowsha": "sOVGM4X2QVAsogLVMc0exuwbmae7iVTx3YcwEQ5bVZ8=",
        "originContent": "To run the script:",
        "translatedContent": "ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯ï¼š</translate-content>"
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 2,
    "Content": "```bash\npython evaluation.py  --input INPUT_JSON_PATH\n```",
    "ContentSha": "/1tAnnCm4pQ5d+OkM+XVnog+9gajhR1v0gP6ezXnfh0=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bash\npython evaluation.py  --input INPUT_JSON_PATH\n```",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "rTMjNc+qNIxb1xXDR5hEYiJw1fc4muBwuBsTXVIaIho=",
        "originContent": "```bash",
        "translatedContent": "```bash"
      },
      {
        "row": 2,
        "rowsha": "w+f+DmLHWp67jFv0ZTQ7WYbafgxfhVHj+BWh3sIZ3R4=",
        "originContent": "python evaluation.py  --input INPUT_JSON_PATH",
        "translatedContent": "python evaluation.py  --input INPUT_JSON_PATH"
      },
      {
        "row": 3,
        "rowsha": "8bkBhHOQsO1+N058HkZOwXtGpCfEh6WtbL0pBkBQg9U=",
        "originContent": "```",
        "translatedContent": "```"
      }
    ],
    "IsCodeBlock": true
  },
  {
    "Id": 3,
    "Content": "\n## Acknowledge\nWe gratefully acknowledge that our evaluation code is modified from the official implementation of [MMAU](https://github.com/Sakshi113/MMAU). \n\n## Citation",
    "ContentSha": "IaUHX6jbGZrgScqQsNh4qROLhDPApEn3/uCPJpM8EE0=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## è¬è¾\nç§ãŸã¡ã¯ã€è©•ä¾¡ã‚³ãƒ¼ãƒ‰ãŒ[MMAU](https://github.com/Sakshi113/MMAU)ã®å…¬å¼å®Ÿè£…ã‚’æ”¹å¤‰ã—ãŸã‚‚ã®ã§ã‚ã‚‹ã“ã¨ã«æ·±ãæ„Ÿè¬ã—ã¾ã™ã€‚\n\n## å¼•ç”¨æ–‡çŒ®\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## è¬è¾"
      },
      {
        "row": 2,
        "rowsha": "xtYBNAto7c9qDmGF7v5SBnjqwPebUyaGM22mxPYxf1s=",
        "originContent": "## Acknowledge",
        "translatedContent": "ç§ãŸã¡ã¯ã€è©•ä¾¡ã‚³ãƒ¼ãƒ‰ãŒ[MMAU](https://github.com/Sakshi113/MMAU)ã®å…¬å¼å®Ÿè£…ã‚’æ”¹å¤‰ã—ãŸã‚‚ã®ã§ã‚ã‚‹ã“ã¨ã«æ·±ãæ„Ÿè¬ã—ã¾ã™ã€‚"
      },
      {
        "row": 3,
        "rowsha": "oE0ZkFOmLcWy6563uXJMl0fCijIMFdN2+8Db89qw5K0=",
        "originContent": "We gratefully acknowledge that our evaluation code is modified from the official implementation of [MMAU](https://github.com/Sakshi113/MMAU). ",
        "translatedContent": ""
      },
      {
        "row": 4,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## å¼•ç”¨æ–‡çŒ®"
      },
      {
        "row": 5,
        "rowsha": "ZwTp5ajUmpHTJefyHhIKzXcG2wnB1jv8iv8cvmdcb/g=",
        "originContent": "## Citation",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 4,
    "Content": "```\n@article{ma2025mmar,\n  title={MMAR: A Challenging Benchmark for Deep Reasoning in Speech, Audio, Music, and Their Mix},\n  author={Ma, Ziyang and Ma, Yinghao and Zhu, Yanqiao and Yang, Chen and Chao, Yi-Wen and Xu, Ruiyang and others},\n  journal={arXiv preprint arXiv:2505.13032},\n  year={2025}\n}\n```",
    "ContentSha": "f8QBDPIREeMqmGubCnVSWJNPU27YVIP/Sx/inKUaNXQ=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```\n@article{ma2025mmar,\n  title={MMAR: A Challenging Benchmark for Deep Reasoning in Speech, Audio, Music, and Their Mix},\n  author={Ma, Ziyang and Ma, Yinghao and Zhu, Yanqiao and Yang, Chen and Chao, Yi-Wen and Xu, Ruiyang and others},\n  journal={arXiv preprint arXiv:2505.13032},\n  year={2025}\n}\n```",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "8bkBhHOQsO1+N058HkZOwXtGpCfEh6WtbL0pBkBQg9U=",
        "originContent": "```",
        "translatedContent": "```"
      },
      {
        "row": 2,
        "rowsha": "dchfcMbraOabyepDeLTuDiuHEiHEFxjFWiA2J0BBi/M=",
        "originContent": "@article{ma2025mmar,",
        "translatedContent": "@article{ma2025mmar,"
      },
      {
        "row": 3,
        "rowsha": "De1CnDsyjg0m4C7vgUJq9iDfx9aezUiGlbO/X1ZAXD0=",
        "originContent": "  title={MMAR: A Challenging Benchmark for Deep Reasoning in Speech, Audio, Music, and Their Mix},",
        "translatedContent": "  title={MMAR: A Challenging Benchmark for Deep Reasoning in Speech, Audio, Music, and Their Mix},"
      },
      {
        "row": 4,
        "rowsha": "GlYTmnQ1X2YGVVxUY+Cnw7Qb3dgzFSP/coLythTl0YQ=",
        "originContent": "  author={Ma, Ziyang and Ma, Yinghao and Zhu, Yanqiao and Yang, Chen and Chao, Yi-Wen and Xu, Ruiyang and others},",
        "translatedContent": "  author={Ma, Ziyang and Ma, Yinghao and Zhu, Yanqiao and Yang, Chen and Chao, Yi-Wen and Xu, Ruiyang and others},"
      },
      {
        "row": 5,
        "rowsha": "3x1DtdPQgWqYMZlwoe9mKZ2pEEZBuktUp7MlyCvcBug=",
        "originContent": "  journal={arXiv preprint arXiv:2505.13032},",
        "translatedContent": "  journal={arXiv preprint arXiv:2505.13032},"
      },
      {
        "row": 6,
        "rowsha": "6oyqJVJ20XoZhlyGjaewQyTQsSBBtVAYmqUtTwjLbAg=",
        "originContent": "  year={2025}",
        "translatedContent": "  year={2025}"
      },
      {
        "row": 7,
        "rowsha": "0Qs2qnSlm89KiBhYN/ZYr682Ru/yuxbDko0OkzXpRdI=",
        "originContent": "}",
        "translatedContent": "}"
      },
      {
        "row": 8,
        "rowsha": "8bkBhHOQsO1+N058HkZOwXtGpCfEh6WtbL0pBkBQg9U=",
        "originContent": "```",
        "translatedContent": "```"
      }
    ],
    "IsCodeBlock": true
  },
  {
    "Id": 5,
    "Content": "",
    "ContentSha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  }
]