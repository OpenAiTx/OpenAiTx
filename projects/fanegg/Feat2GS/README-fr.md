
<h2 align="center"> <a href="https://arxiv.org/abs/2412.09606">Feat2GS : Exploration des mod√®les fondamentaux visuels avec Gaussian Splatting</a>
</h2>

<h5 align="center">

[![arXiv](https://img.shields.io/badge/Arxiv-2412.09606-b31b1b.svg?logo=arXiv)](https://arxiv.org/abs/2412.09606) 
[![Page d'accueil](https://img.shields.io/badge/Project-Website-green.svg)](https://fanegg.github.io/Feat2GS/)  [![youtube](https://img.shields.io/badge/Video-E33122?logo=Youtube)](https://youtu.be/4fT5lzcAJqo?si=_fCSIuXNBSmov2VA)  [![Gradio](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Demo-orange)](https://huggingface.co/spaces/endless-ai/Feat2GS)  [![X](https://img.shields.io/badge/@Yue%20Chen-black?logo=X)](https://twitter.com/faneggchen)  [![Bluesky](https://img.shields.io/badge/@Yue%20Chen-white?logo=Bluesky)](https://bsky.app/profile/fanegg.bsky.social)

[Yue Chen](https://fanegg.github.io/),
[Xingyu Chen](https://rover-xingyu.github.io/),
[Anpei Chen](https://apchenstu.github.io/),
[Gerard Pons-Moll](https://virtualhumans.mpi-inf.mpg.de/),
[Yuliang Xiu](https://xiuyuliang.cn/)
</h5>

<div align="center">
Ce d√©p√¥t est l‚Äôimpl√©mentation officielle de Feat2GS, un cadre unifi√© pour sonder la ¬´ conscience de la texture et de la g√©om√©trie ¬ª des mod√®les fondamentaux visuels. La synth√®se de vues in√©dites sert de proxy efficace pour l‚Äô√©valuation 3D.
</div>
<br>

https://github.com/user-attachments/assets/07ebb8e1-6001-47bf-bf74-984b0032cc17


## Mises √† jour

- [10 juillet 2025] Ajout d‚Äôune nouvelle √©valuation des caract√©ristiques de l‚Äôencodeur et du d√©codeur VGGT. Voir les r√©sultats [ici](https://raw.githubusercontent.com/fanegg/Feat2GS/main/assets/Feat2GS_Benchmark.pdf).

## Commencer

### Installation
1. Clonez Feat2GS et t√©l√©chargez le mod√®le pr√©-entra√Æn√© depuis [DUSt3R](https://github.com/naver/dust3r)/[MASt3R](https://github.com/naver/mast3r).
```bash
git clone https://github.com/fanegg/Feat2GS.git
cd Feat2GS/submodules/mast3r/
mkdir -p checkpoints/
wget https://download.europe.naverlabs.com/ComputerVision/DUSt3R/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth -P checkpoints/
wget https://download.europe.naverlabs.com/ComputerVision/MASt3R/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric.pth -P checkpoints/
cd ../../
```
2. Cr√©ez l‚Äôenvironnement, ici nous montrons un exemple utilisant conda.

```bash
conda create -n feat2gs python=3.11 cmake=3.14.0
conda activate feat2gs
pip install "torch==2.5.1" "torchvision==0.20.1" "numpy<2" --index-url https://download.pytorch.org/whl/cu121  # use the correct version of cuda for your system
cd Feat2GS/
pip install -r requirements.txt
pip install submodules/simple-knn
```
3. Optionnel mais fortement recommand√©, compiler les noyaux cuda pour RoPE (comme dans CroCo v2).

```bash
# DUST3R relies on RoPE positional embeddings for which you can compile some cuda kernels for faster runtime.
cd submodules/mast3r/dust3r/croco/models/curope/
python setup.py build_ext --inplace
cd ../../../../../../
```

4. (Optionnel) suivez [cette instruction](https://github.com/cvlab-columbia/zero123?tab=readme-ov-file#novel-view-synthesis-1) pour installer les d√©pendances n√©cessaires √† l'exploration de [Zero123](https://github.com/cvlab-columbia/zero123).

### Utilisation
1. Pr√©paration des donn√©es (Nous fournissons nos ensembles de donn√©es d'√©valuation et d'inf√©rence : [lien](https://drive.google.com/file/d/1PLTFcvJfiPucrB-pIwfp5QG-AIHcJdjN/view?usp=drive_link))
```bash
  cd <data_root>/Feat2GS/
```

> Si vous souhaitez cr√©er des ensembles de donn√©es personnalis√©s, veuillez suivre et modifier :
> ```
> build_dataset/0_create_json.py ## cr√©er dataset_split.json pour diviser l'ensemble train/test
> build_dataset/1_create_feat2gs_dataset.py ## utiliser dataset_split.json pour cr√©er l'ensemble de donn√©es
> ```


2. √âvaluer les mod√®les fondamentaux visuels :

  | √âtape | Description (lien vers la commande) |
  |-------|------------------------------------|
  | (1)   | [Initialisation DUSt3R & Extraction de caract√©ristiques](https://github.com/fanegg/Feat2GS/blob/b8eadaa54549d34420eba61b388548b8ec8e7325/scripts/run_feat2gs_eval_parallel.sh#L245-L250) |
  | (2)   | [Lecture 3DGS √† partir des caract√©ristiques & Optimisation conjointe de la pose](https://github.com/fanegg/Feat2GS/blob/b8eadaa54549d34420eba61b388548b8ec8e7325/scripts/run_feat2gs_eval_parallel.sh#L253-L262) |
  | (3)   | [Test d'initialisation de la pose](https://github.com/fanegg/Feat2GS/blob/b8eadaa54549d34420eba61b388548b8ec8e7325/scripts/run_feat2gs_eval_parallel.sh#L265-L270) |
  | (4)   | [Rendu de la vue de test pour l'√©valuation](https://github.com/fanegg/Feat2GS/blob/b8eadaa54549d34420eba61b388548b8ec8e7325/scripts/run_feat2gs_eval_parallel.sh#L273-L282) |
  | (5)   | [M√©trique](https://github.com/fanegg/Feat2GS/blob/b8eadaa54549d34420eba61b388548b8ec8e7325/scripts/run_feat2gs_eval_parallel.sh#L298-L301) |
  | (Optionnel) | [Rendu vid√©o avec trajectoire g√©n√©r√©e](https://github.com/fanegg/Feat2GS/blob/b8eadaa54549d34420eba61b388548b8ec8e7325/scripts/run_feat2gs_eval_parallel.sh#L304-L315) |

```bash
  # Run evaluation for all datasets, all VFM features, all probing modes
  bash scripts/run_feat2gs_eval_parallel.sh

  # (Example) Run evaluation for a single scene, DINO feature, Geometry mode
  bash scripts/run_feat2gs_eval.sh
```
> [!NOTE]
> Pour ex√©cuter des exp√©riences en parall√®le, nous avons ajout√© une fonctionnalit√© de **verrou GPU** pour garantir qu'une seule exp√©rience d'√©valuation s'ex√©cute par GPU. Une fois une exp√©rience termin√©e, le GPU est automatiquement d√©verrouill√©. **Si interrompu par `Ctrl+C`, le GPU ne sera pas d√©verrouill√© automatiquement.** Pour corriger cela, supprimez manuellement les fichiers `.lock` dans votre `LOCK_DIR`. Pour d√©sactiver cette fonctionnalit√©, commentez ces lignes dans le script :
    [L4-L5](https://github.com/fanegg/Feat2GS/blob/b8eadaa54549d34420eba61b388548b8ec8e7325/scripts/run_feat2gs_eval_parallel.sh#L4-L5),
    [L9-L22](https://github.com/fanegg/Feat2GS/blob/b8eadaa54549d34420eba61b388548b8ec8e7325/scripts/run_feat2gs_eval_parallel.sh#L9-L22),
    [L223-L233](https://github.com/fanegg/Feat2GS/blob/b8eadaa54549d34420eba61b388548b8ec8e7325/scripts/run_feat2gs_eval_parallel.sh#L223-L233),
    [L330-L331](https://github.com/fanegg/Feat2GS/blob/b8eadaa54549d34420eba61b388548b8ec8e7325/scripts/run_feat2gs_eval_parallel.sh#L330-L331).

  | Config | Op√©ration |
  |--------|-----------------|
  | GPU | Modifier dans [`<AVAILABLE_GPUS>`](https://github.com/fanegg/Feat2GS/blob/b8eadaa54549d34420eba61b388548b8ec8e7325/scripts/run_feat2gs_eval_parallel.sh#L7) |
  | Dataset | Modifier dans [`<SCENES[$Dataset]>`](https://github.com/fanegg/Feat2GS/blob/b8eadaa54549d34420eba61b388548b8ec8e7325/scripts/run_feat2gs_eval_parallel.sh#L105-L111) |
  | Sc√®ne | Modifier dans [`<SCENES_$Dataset>`](https://github.com/fanegg/Feat2GS/blob/b8eadaa54549d34420eba61b388548b8ec8e7325/scripts/run_feat2gs_eval_parallel.sh#L31-L99) |
  | Mod√®le de Fondation Visuelle | Modifier dans [`<FEATURES>`](https://github.com/fanegg/Feat2GS/blob/b8eadaa54549d34420eba61b388548b8ec8e7325/scripts/run_feat2gs_eval_parallel.sh#L120-L162) |
  | Mode de sonde | Modifier dans [`<MODELS>`](https://github.com/fanegg/Feat2GS/blob/b8eadaa54549d34420eba61b388548b8ec8e7325/scripts/run_feat2gs_eval_parallel.sh#L181-L188) |
  | Mode inference uniquement | Commentez les √©tapes (3)(4)(5) dans [`execute_command`](https://github.com/fanegg/Feat2GS/blob/main/scripts/run_feat2gs_eval_parallel.sh#L325-L327) |

```bash
  # Evaluate Visual Foundation Models on DTU dataset
  bash scripts/run_feat2gs_eval_dtu_parallel.sh

  # Run InstantSplat for evaluation
  bash scripts/run_instantsplat_eval_parallel.sh
```
3. Apr√®s l'entra√Ænement, rendre la vid√©o RGB/profondeur/normale avec la trajectoire g√©n√©r√©e.


```bash
  # If render depth/normal, set RENDER_DEPTH_NORMAL=true
  # Set type of generated trjectory by editing <TRAJ_SCENES>
  bash scripts/run_video_render.sh

  # Render video on DTU dataset
  bash scripts/run_video_render_dtu.sh
```
### üéÆ D√©mo interactive

#### üöÄ D√©marrage rapide
1. **Images d'entr√©e**
* T√©l√©chargez 2 images ou plus de la m√™me sc√®ne sous diff√©rents angles
* Pour de meilleurs r√©sultats, assurez-vous que les images ont un bon recouvrement

2. **√âtape 1 : Initialisation DUSt3R & Extraction des caract√©ristiques**
* Cliquez sur "LANCER √âtape 1" pour traiter vos images
* Cette √©tape estime le nuage de points DUSt3R initial et les poses de cam√©ra, et extrait les caract√©ristiques DUSt3R pour chaque pixel

3. **√âtape 2 : Lecture 3DGS √† partir des caract√©ristiques**
* R√©glez le nombre d'it√©rations d'entra√Ænement, un nombre plus √©lev√© am√©liore la qualit√© mais augmente la dur√©e (par d√©faut : 2000, max : 8000)
* Cliquez sur "LANCER √âtape 2" pour optimiser le mod√®le 3D

4. **√âtape 3 : Rendu vid√©o**
* Choisissez une trajectoire de cam√©ra
* Cliquez sur "LANCER √âtape 3" pour g√©n√©rer une vid√©o de votre mod√®le 3D

```bash
gradio demo.py
```

#### üí° Conseils
* Le temps de traitement d√©pend de la r√©solution et de la quantit√© d‚Äôimages
* Pour des performances optimales, testez sur des GPU haut de gamme (A100/4090)
* Utilisez la souris pour interagir avec les mod√®les 3D :
  - Bouton gauche : Rotation
  - Molette : Zoom
  - Bouton droit : D√©placement


## Remerciements

Ce travail repose sur de nombreuses recherches et projets open source remarquables, un grand merci √† tous les auteurs pour leur partage !

- [Gaussian-Splatting](https://github.com/graphdeco-inria/gaussian-splatting) et [diff-gaussian-rasterization](https://github.com/graphdeco-inria/diff-gaussian-rasterization)
- [gsplat](https://github.com/nerfstudio-project/gsplat)
- [DUSt3R](https://github.com/naver/dust3r) et [MASt3R](https://github.com/naver/mast3r)
- [InstantSplat](https://github.com/NVlabs/InstantSplat)
- [Probe3D](https://github.com/mbanani/probe3d)
- [FeatUp](https://github.com/mhamilton723/FeatUp)
- [Shape of Motion](https://github.com/vye16/shape-of-motion/)
- [Splatt3R](https://github.com/btsmart/splatt3r)
- [VGGT](https://github.com/facebookresearch/vggt)

## Citation
Si vous trouvez notre travail utile pour vos recherches, merci de penser √† donner une √©toile :star: et √† citer l‚Äôarticle suivant :pencil:.

```bibTeX
@inproceedings{chen2025feat2gs,
  title={Feat2gs: Probing visual foundation models with gaussian splatting},
  author={Chen, Yue and Chen, Xingyu and Chen, Anpei and Pons-Moll, Gerard and Xiu, Yuliang},
  booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
  pages={6348--6361},
  year={2025}
}
```

## Contact

Pour les retours, questions ou demandes de presse, veuillez contacter [Yue Chen](https://raw.githubusercontent.com/fanegg/Feat2GS/main/mailto:faneggchen@gmail.com) et [Xingyu Chen](https://raw.githubusercontent.com/fanegg/Feat2GS/main/mailto:roverxingyu@gmail.com).


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-10-10

---