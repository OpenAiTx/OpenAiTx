[
  {
    "Id": 1,
    "Content": "# End-to-end Trainable Deep Neural Network for Robotic Grasp Detection and Semantic Segmentation from RGB\n\n<p align=\"center\">\n<img src=\"https://raw.githubusercontent.com/stefan-ainetter/grasp_det_seg_cnn/main/Network.png\" width=\"100%\"/>\n<br>\n<a href=\"https://arxiv.org/abs/2107.05287\">arXiv</a>\n</p>\n\nThis repository contains the code for the ICRA21 paper \"End-to-end Trainable Deep Neural Network for Robotic Grasp Detection\nand Semantic Segmentation from RGB\". \nIt contains the code for training and testing our proposed method in combination with the OCID_grasp dataset. \n\nIf you use our method or dataset extension for your research, please cite:",
    "ContentSha": "SVcL4Yhz0nvluc9l267GviAm+saweS3ClVsW6jV+DdQ=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "# RGB로부터 로봇 그립 감지 및 의미론적 분할을 위한 종단 간 학습 가능한 심층 신경망\n\n<p align=\"center\">\n<img src=\"https://raw.githubusercontent.com/stefan-ainetter/grasp_det_seg_cnn/main/Network.png\" width=\"100%\"/>\n<br>\n<a href=\"https://arxiv.org/abs/2107.05287\">arXiv</a>\n</p>\n\n이 저장소는 ICRA21 논문 \"RGB로부터 로봇 그립 감지 및 의미론적 분할을 위한 종단 간 학습 가능한 심층 신경망\"의 코드를 포함합니다.  \n우리의 제안 방법과 OCID_grasp 데이터셋을 결합한 학습 및 테스트 코드를 포함하고 있습니다.  \n\n만약 연구에 우리 방법이나 데이터셋 확장을 사용한다면, 다음을 인용해 주세요:\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "G6tLOUXN60voRKh47li5k1OEAr+74v2qWuhXZWybQZI=",
        "originContent": "# End-to-end Trainable Deep Neural Network for Robotic Grasp Detection and Semantic Segmentation from RGB",
        "translatedContent": "# RGB로부터 로봇 그립 감지 및 의미론적 분할을 위한 종단 간 학습 가능한 심층 신경망"
      },
      {
        "row": 2,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 3,
        "rowsha": "+/a9XmPwQixGFroME/GMEOLpReZZV4ARosR9orAplJY=",
        "originContent": "<p align=\"center\">",
        "translatedContent": "<p align=\"center\">"
      },
      {
        "row": 4,
        "rowsha": "pH8/wwzCP8rsHxoNAtQrJs1lkMEtd3A2b74TTe8WkTo=",
        "originContent": "<img src=\"https://raw.githubusercontent.com/stefan-ainetter/grasp_det_seg_cnn/main/Network.png\" width=\"100%\"/>",
        "translatedContent": "<img src=\"https://raw.githubusercontent.com/stefan-ainetter/grasp_det_seg_cnn/main/Network.png\" width=\"100%\"/>"
      },
      {
        "row": 5,
        "rowsha": "Za7xrbqGcqX+eTLZ+fDcNcJDWJP2URKsrGKNLcF6C88=",
        "originContent": "<br>",
        "translatedContent": "<br>"
      },
      {
        "row": 6,
        "rowsha": "CuvelhwGQWtDXJ9HTNReH1G5PZyqmjLHKUITZDpgiZU=",
        "originContent": "<a href=\"https://arxiv.org/abs/2107.05287\">arXiv</a>",
        "translatedContent": "<a href=\"https://arxiv.org/abs/2107.05287\">arXiv</a>"
      },
      {
        "row": 7,
        "rowsha": "dSdvPNAZSmR86FDDSF6tkQUCVfI9qmACHOR5tThOetY=",
        "originContent": "</p>",
        "translatedContent": "</p>"
      },
      {
        "row": 8,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 9,
        "rowsha": "fH/zrrkjtyY/2KffanHlIHCcfsouwE7HIkj3vPDPats=",
        "originContent": "This repository contains the code for the ICRA21 paper \"End-to-end Trainable Deep Neural Network for Robotic Grasp Detection",
        "translatedContent": "이 저장소는 ICRA21 논문 \"RGB로부터 로봇 그립 감지 및 의미론적 분할을 위한 종단 간 학습 가능한 심층 신경망\"의 코드를 포함합니다.  "
      },
      {
        "row": 10,
        "rowsha": "WbwkDIQ4KPQmQsaiPSaKkBRQww4Z6PbuOWqDNn+o8as=",
        "originContent": "and Semantic Segmentation from RGB\". ",
        "translatedContent": "우리의 제안 방법과 OCID_grasp 데이터셋을 결합한 학습 및 테스트 코드를 포함하고 있습니다.  "
      },
      {
        "row": 11,
        "rowsha": "wtLPiCjN/XO+eAeYXDG8Tcga1RGaW7GJ0sa43OfzB0c=",
        "originContent": "It contains the code for training and testing our proposed method in combination with the OCID_grasp dataset. ",
        "translatedContent": ""
      },
      {
        "row": 12,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "만약 연구에 우리 방법이나 데이터셋 확장을 사용한다면, 다음을 인용해 주세요:"
      },
      {
        "row": 13,
        "rowsha": "vFCeUREzbEaoLwryxEXs1Myw7Fntzv8Yeu1IgOC6yTU=",
        "originContent": "If you use our method or dataset extension for your research, please cite:",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 2,
    "Content": "```bibtex\n@InProceedings{ainetter2021end,\n  title={End-to-end Trainable Deep Neural Network for Robotic Grasp Detection and Semantic Segmentation from RGB},\n  author={Ainetter, Stefan and Fraundorfer, Friedrich},\n  booktitle={IEEE International Conference on Robotics and Automation (ICRA)},\n  pages={13452--13458}\n  year={2021}\n}\n```",
    "ContentSha": "d3PI1UCnGvc/62HRxTI2RrZbEmHQLkGLWB/8pyqtPKM=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bibtex\n@InProceedings{ainetter2021end,\n  title={End-to-end Trainable Deep Neural Network for Robotic Grasp Detection and Semantic Segmentation from RGB},\n  author={Ainetter, Stefan and Fraundorfer, Friedrich},\n  booktitle={IEEE International Conference on Robotics and Automation (ICRA)},\n  pages={13452--13458}\n  year={2021}\n}\n```",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "o+TmyQ6wneV6/FQB6aUlRSjIGr2/YLJtnz5uxBgsScQ=",
        "originContent": "```bibtex",
        "translatedContent": "```bibtex"
      },
      {
        "row": 2,
        "rowsha": "h1kd4vKaSqjXa1Rhc7YGHRAdCDKI4AH6jjYEo9VdWQ0=",
        "originContent": "@InProceedings{ainetter2021end,",
        "translatedContent": "@InProceedings{ainetter2021end,"
      },
      {
        "row": 3,
        "rowsha": "JyhfHtsN2W5tsN9lYVOubkqpu/qhfSZTsDq3V1tNeYE=",
        "originContent": "  title={End-to-end Trainable Deep Neural Network for Robotic Grasp Detection and Semantic Segmentation from RGB},",
        "translatedContent": "  title={End-to-end Trainable Deep Neural Network for Robotic Grasp Detection and Semantic Segmentation from RGB},"
      },
      {
        "row": 4,
        "rowsha": "f9m2jImP5cv/fNjXVcxgzdWvt95drcIUc8RXzO8gA4I=",
        "originContent": "  author={Ainetter, Stefan and Fraundorfer, Friedrich},",
        "translatedContent": "  author={Ainetter, Stefan and Fraundorfer, Friedrich},"
      },
      {
        "row": 5,
        "rowsha": "ltjqa7zSoDG1Va/fAKC/BpOkmu96NjI1oNDOatNcXXI=",
        "originContent": "  booktitle={IEEE International Conference on Robotics and Automation (ICRA)},",
        "translatedContent": "  booktitle={IEEE International Conference on Robotics and Automation (ICRA)},"
      },
      {
        "row": 6,
        "rowsha": "MtbIAPrXLQzWAamxiTweFAsnQU1+aukDxXgHNCMMeP4=",
        "originContent": "  pages={13452--13458}",
        "translatedContent": "  pages={13452--13458}"
      },
      {
        "row": 7,
        "rowsha": "QRDXTY8rj/7Ra4fyR1kolwdti02Ai8YiXxcKkmvqL6A=",
        "originContent": "  year={2021}",
        "translatedContent": "  year={2021}"
      },
      {
        "row": 8,
        "rowsha": "0Qs2qnSlm89KiBhYN/ZYr682Ru/yuxbDko0OkzXpRdI=",
        "originContent": "}",
        "translatedContent": "}"
      },
      {
        "row": 9,
        "rowsha": "8bkBhHOQsO1+N058HkZOwXtGpCfEh6WtbL0pBkBQg9U=",
        "originContent": "```",
        "translatedContent": "```"
      }
    ],
    "IsCodeBlock": true
  },
  {
    "Id": 3,
    "Content": "\n## Requirements and setup\n\nMain system requirements:\n* CUDA 10.1\n* Linux with GCC 7 or 8\n* PyTorch v1.1.0\n\n**IMPORTANT NOTE**: These requirements are not necessarily stringent, e.g. it might be possible to compile with older\nversions of CUDA, or under Windows. However, we have only tested the code under the above settings and cannot provide support for other setups.\n\nTo install PyTorch, please refer to https://github.com/pytorch/pytorch#installation.\n\nTo install all other dependencies using pip:",
    "ContentSha": "UFXfdTH1VMMvXvoYFInoh3GsLHEfMCOpW692iLxdrhc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## 요구 사항 및 설정\n\n주요 시스템 요구 사항:\n* CUDA 10.1\n* GCC 7 또는 8이 설치된 Linux\n* PyTorch v1.1.0\n\n**중요 참고**: 이 요구 사항들이 반드시 엄격한 것은 아닙니다. 예를 들어, 더 오래된 CUDA 버전이나 Windows 환경에서 컴파일하는 것도 가능할 수 있습니다. 다만, 저희는 위 환경에서만 코드를 테스트했으며 다른 설정에 대한 지원은 제공하지 않습니다.\n\nPyTorch 설치는 https://github.com/pytorch/pytorch#installation 를 참고하세요.\n\npip를 사용하여 모든 기타 의존성을 설치하려면:\n\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## 요구 사항 및 설정"
      },
      {
        "row": 2,
        "rowsha": "1tGc+hUZn8nYqXxFVag9q+5fBxbE/pkH8+yiV3VL5rY=",
        "originContent": "## Requirements and setup",
        "translatedContent": ""
      },
      {
        "row": 3,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "주요 시스템 요구 사항:"
      },
      {
        "row": 4,
        "rowsha": "7r2ACHoGnPV8CntIHsGfA8OBYmpX4Nx+uiaU0z0iwwk=",
        "originContent": "Main system requirements:",
        "translatedContent": "* CUDA 10.1"
      },
      {
        "row": 5,
        "rowsha": "TZPw9SBiMuZiCyyd322q6bhrgV2zOJjUihpQGOzKE1w=",
        "originContent": "* CUDA 10.1",
        "translatedContent": "* GCC 7 또는 8이 설치된 Linux"
      },
      {
        "row": 6,
        "rowsha": "fRGf0OTtlu/bPQRO743G7LDErwd4IdLVpc97C2DeoNg=",
        "originContent": "* Linux with GCC 7 or 8",
        "translatedContent": "* PyTorch v1.1.0"
      },
      {
        "row": 7,
        "rowsha": "HCjVgy9x+EgdYwuUlxmnizotny5QVOtTyGEedMCK11w=",
        "originContent": "* PyTorch v1.1.0",
        "translatedContent": ""
      },
      {
        "row": 8,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "**중요 참고**: 이 요구 사항들이 반드시 엄격한 것은 아닙니다. 예를 들어, 더 오래된 CUDA 버전이나 Windows 환경에서 컴파일하는 것도 가능할 수 있습니다. 다만, 저희는 위 환경에서만 코드를 테스트했으며 다른 설정에 대한 지원은 제공하지 않습니다."
      },
      {
        "row": 9,
        "rowsha": "uycUK8znZrBag4mNqBCLgGvqfbw5e9V23wBsfNp2tyM=",
        "originContent": "**IMPORTANT NOTE**: These requirements are not necessarily stringent, e.g. it might be possible to compile with older",
        "translatedContent": ""
      },
      {
        "row": 10,
        "rowsha": "WwbbY+qo+iTmWP879j87O9aLNFuUXyCIKeYd12o1FDU=",
        "originContent": "versions of CUDA, or under Windows. However, we have only tested the code under the above settings and cannot provide support for other setups.",
        "translatedContent": "PyTorch 설치는 https://github.com/pytorch/pytorch#installation 를 참고하세요."
      },
      {
        "row": 11,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 12,
        "rowsha": "2Eq/W2HxRllVzFi6pbPbn1BUKOHemqLKT9mjEf7nO5s=",
        "originContent": "To install PyTorch, please refer to https://github.com/pytorch/pytorch#installation.",
        "translatedContent": "pip를 사용하여 모든 기타 의존성을 설치하려면:"
      },
      {
        "row": 13,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 14,
        "rowsha": "l+i0lLCSEcqojMGwhreVo9q3Ad1wtnjQfsDlawqDvfc=",
        "originContent": "To install all other dependencies using pip:",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 4,
    "Content": "```bash\npip install -r requirements.txt\n```",
    "ContentSha": "TxMa9uJC0PmBOnm3/TRl4YDLNvSwCWaRNjyXpFhndHU=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bash\npip install -r requirements.txt\n```",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "rTMjNc+qNIxb1xXDR5hEYiJw1fc4muBwuBsTXVIaIho=",
        "originContent": "```bash",
        "translatedContent": "```bash"
      },
      {
        "row": 2,
        "rowsha": "9jQ5Tmvmy0Rca8gZGuieLw3iHyIU3Ba5zS4ICtZgsdw=",
        "originContent": "pip install -r requirements.txt",
        "translatedContent": "pip install -r requirements.txt"
      },
      {
        "row": 3,
        "rowsha": "8bkBhHOQsO1+N058HkZOwXtGpCfEh6WtbL0pBkBQg9U=",
        "originContent": "```",
        "translatedContent": "```"
      }
    ],
    "IsCodeBlock": true
  },
  {
    "Id": 5,
    "Content": "\n### Setup\n\nOur code is split into two main components: a library containing implementations for the various network modules,\nalgorithms and utilities, and a set of scripts to train / test the networks.\n\nThe library, called `grasp_det_seg`, can be installed with:",
    "ContentSha": "SHXHm6LwhsTa8/A96p/4S2FD72Xd+i/4idVqm7HWqh8=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "\n### 설정\n\n우리 코드는 두 가지 주요 구성 요소로 나뉩니다: 다양한 네트워크 모듈, 알고리즘 및 유틸리티 구현을 포함하는 라이브러리와 네트워크를 학습/테스트하는 스크립트 세트입니다.\n\n`grasp_det_seg`라는 라이브러리는 다음과 같이 설치할 수 있습니다:\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 2,
        "rowsha": "iJa409tTUC9P1gTULbIw6Kod+KAUdLl5kgZl7whoChE=",
        "originContent": "### Setup",
        "translatedContent": "### 설정"
      },
      {
        "row": 3,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 4,
        "rowsha": "l+Y9N09haW9XicaL32+pdyUSAZo2Pgqz1ockT35q8M4=",
        "originContent": "Our code is split into two main components: a library containing implementations for the various network modules,",
        "translatedContent": "우리 코드는 두 가지 주요 구성 요소로 나뉩니다: 다양한 네트워크 모듈, 알고리즘 및 유틸리티 구현을 포함하는 라이브러리와 네트워크를 학습/테스트하는 스크립트 세트입니다."
      },
      {
        "row": 5,
        "rowsha": "7GRjc31hbX29IdxC23tIlk5Z/B1qlX+1gcXzAAlSNsI=",
        "originContent": "algorithms and utilities, and a set of scripts to train / test the networks.",
        "translatedContent": ""
      },
      {
        "row": 6,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "`grasp_det_seg`라는 라이브러리는 다음과 같이 설치할 수 있습니다:"
      },
      {
        "row": 7,
        "rowsha": "/IuZZ8KRVzkty19LeVwo1LCv9/UO6cuF8GzGVwuNBnw=",
        "originContent": "The library, called `grasp_det_seg`, can be installed with:",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 6,
    "Content": "```bash\ngit clone https://github.com/stefan-ainetter/grasp_det_seg_cnn.git\ncd grasp_det_seg_cnn\npython setup.py install\n```",
    "ContentSha": "Lgds7avU61M+o+IZAPV/m1Ps2MWZvfxeNyxV1Z0Krfw=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bash\ngit clone https://github.com/stefan-ainetter/grasp_det_seg_cnn.git\ncd grasp_det_seg_cnn\npython setup.py install\n```",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "rTMjNc+qNIxb1xXDR5hEYiJw1fc4muBwuBsTXVIaIho=",
        "originContent": "```bash",
        "translatedContent": "```bash"
      },
      {
        "row": 2,
        "rowsha": "2u6RRIHpvUX2y6/Uw/S92huMpcWyZqT2h1wwqktSjKU=",
        "originContent": "git clone https://github.com/stefan-ainetter/grasp_det_seg_cnn.git",
        "translatedContent": "git clone https://github.com/stefan-ainetter/grasp_det_seg_cnn.git"
      },
      {
        "row": 3,
        "rowsha": "tN9rJige5VjRAS/pNa+BKxlGZEm4j6cLqcnG9FYNvKk=",
        "originContent": "cd grasp_det_seg_cnn",
        "translatedContent": "cd grasp_det_seg_cnn"
      },
      {
        "row": 4,
        "rowsha": "ZTt6UdZddAnjGeKdUdXuKvZdYhrZ72IUjn6s+Z4omHk=",
        "originContent": "python setup.py install",
        "translatedContent": "python setup.py install"
      },
      {
        "row": 5,
        "rowsha": "8bkBhHOQsO1+N058HkZOwXtGpCfEh6WtbL0pBkBQg9U=",
        "originContent": "```",
        "translatedContent": "```"
      }
    ],
    "IsCodeBlock": true
  },
  {
    "Id": 7,
    "Content": "\n## Trained models\n\nThe model files provided are made available under the [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) license.\n\nA trained model for the OCID_grasp dataset can be downloaded [here](https://cloud.tugraz.at/index.php/s/NA7icqiJ5SeNSA6?dir=/Grasp_det_seg_cnn/OCID_pretrained). \nDownload and copy the downloaded weights into the `ckpt_files_OCID/pretrained` folder.\n\nFor re-training the network on OCID_grasp, you need to download weights pretrained on ImageNet \n[here](https://cloud.tugraz.at/index.php/s/NA7icqiJ5SeNSA6?dir=/Grasp_det_seg_cnn/ImageNet_weights) and copy them \ninto the `weights_pretrained` folder.\n\n### Training\n\nTraining involves three main steps: Preparing the dataset, creating a configuration file and running the training\nscript.\n\nTo prepare the dataset:\n1) Download the OCID_grasp dataset [here](https://cloud.tugraz.at/index.php/s/NA7icqiJ5SeNSA6?dir=/Grasp_det_seg_cnn/OCID_grasp).\nUnpack the downloaded `OCID_grasp.zip` file into the `DATA` folder.\n2) The configuration file is a simple text file in `ini` format.\nThe default value of each configuration parameter, as well as a short description of what it does, is available in\n[grasp_det_seg/config/defaults](grasp_det_seg/config/defaults).\n**Note** that these are just an indication of what a \"reasonable\" value for each parameter could be, and are not\nmeant as a way to reproduce any of the results from our paper.\n\n3) To launch the training:",
    "ContentSha": "UnxWxD0oRxePXcKwPwtt7yjvluEUUyEuxyrkoASNGw4=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## 학습된 모델\n\n제공된 모델 파일은 [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) 라이선스 하에 제공됩니다.\n\nOCID_grasp 데이터셋에 대한 학습된 모델은 [여기](https://cloud.tugraz.at/index.php/s/NA7icqiJ5SeNSA6?dir=/Grasp_det_seg_cnn/OCID_pretrained)에서 다운로드할 수 있습니다.  \n다운로드한 가중치를 `ckpt_files_OCID/pretrained` 폴더에 복사하세요.\n\nOCID_grasp에서 네트워크를 재학습하려면 ImageNet에서 사전 학습된 가중치를  \n[여기](https://cloud.tugraz.at/index.php/s/NA7icqiJ5SeNSA6?dir=/Grasp_det_seg_cnn/ImageNet_weights)에서 다운로드하고  \n`weights_pretrained` 폴더에 복사해야 합니다.\n\n### 학습\n\n학습은 세 가지 주요 단계로 구성됩니다: 데이터셋 준비, 설정 파일 작성, 그리고 학습 스크립트 실행.\n\n데이터셋을 준비하려면:  \n1) OCID_grasp 데이터셋을 [여기](https://cloud.tugraz.at/index.php/s/NA7icqiJ5SeNSA6?dir=/Grasp_det_seg_cnn/OCID_grasp)에서 다운로드하세요.  \n다운로드한 `OCID_grasp.zip` 파일을 `DATA` 폴더에 압축 해제합니다.  \n2) 설정 파일은 `ini` 형식의 간단한 텍스트 파일입니다.  \n각 설정 파라미터의 기본값과 그 기능에 대한 간단한 설명은  \n[grasp_det_seg/config/defaults](grasp_det_seg/config/defaults)에서 확인할 수 있습니다.  \n**참고**: 이 값들은 각 파라미터에 대해 \"합리적인\" 값의 예시일 뿐이며, 본 논문의 결과를 재현하기 위한 값은 아닙니다.\n\n3) 학습을 시작하려면:\n\n\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## 학습된 모델"
      },
      {
        "row": 2,
        "rowsha": "Gs54AtwofEk9CwPO7KHNzntI0Od7uMv4wOR82+zDc2c=",
        "originContent": "## Trained models",
        "translatedContent": ""
      },
      {
        "row": 3,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "제공된 모델 파일은 [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) 라이선스 하에 제공됩니다."
      },
      {
        "row": 4,
        "rowsha": "0feZmXrFMgl0/yeAtYaoKgOnVFNoCosrkr/szAMAEHY=",
        "originContent": "The model files provided are made available under the [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) license.",
        "translatedContent": ""
      },
      {
        "row": 5,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "OCID_grasp 데이터셋에 대한 학습된 모델은 [여기](https://cloud.tugraz.at/index.php/s/NA7icqiJ5SeNSA6?dir=/Grasp_det_seg_cnn/OCID_pretrained)에서 다운로드할 수 있습니다.  "
      },
      {
        "row": 6,
        "rowsha": "UPmrEFiHa9kCLhmjsulrUFLiS10PnptZC4ExYAXabvE=",
        "originContent": "A trained model for the OCID_grasp dataset can be downloaded [here](https://cloud.tugraz.at/index.php/s/NA7icqiJ5SeNSA6?dir=/Grasp_det_seg_cnn/OCID_pretrained). ",
        "translatedContent": "다운로드한 가중치를 `ckpt_files_OCID/pretrained` 폴더에 복사하세요."
      },
      {
        "row": 7,
        "rowsha": "txKsBTjA5jCUWxL7pKby+9HNPLSDwGdSRkpru3FsVuc=",
        "originContent": "Download and copy the downloaded weights into the `ckpt_files_OCID/pretrained` folder.",
        "translatedContent": ""
      },
      {
        "row": 8,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "OCID_grasp에서 네트워크를 재학습하려면 ImageNet에서 사전 학습된 가중치를  "
      },
      {
        "row": 9,
        "rowsha": "72kq0hyVh6nY/FQCr3kQzFkQEUD/0xlIJlxYVy0SyJk=",
        "originContent": "For re-training the network on OCID_grasp, you need to download weights pretrained on ImageNet ",
        "translatedContent": "[여기](https://cloud.tugraz.at/index.php/s/NA7icqiJ5SeNSA6?dir=/Grasp_det_seg_cnn/ImageNet_weights)에서 다운로드하고  "
      },
      {
        "row": 10,
        "rowsha": "wZcoZ52PxtS1qHWwqDrpMRr4FJO+OSvr9VuYezjMPww=",
        "originContent": "[here](https://cloud.tugraz.at/index.php/s/NA7icqiJ5SeNSA6?dir=/Grasp_det_seg_cnn/ImageNet_weights) and copy them ",
        "translatedContent": "`weights_pretrained` 폴더에 복사해야 합니다."
      },
      {
        "row": 11,
        "rowsha": "9+SzBNdXOWg35LZ/WogXSVv18gnAI4SyOylfBUZHhTA=",
        "originContent": "into the `weights_pretrained` folder.",
        "translatedContent": ""
      },
      {
        "row": 12,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "### 학습"
      },
      {
        "row": 13,
        "rowsha": "cNKS2XI0q3UNj4+rlFmjA0lefbWomrdh7j5BPCDMKNw=",
        "originContent": "### Training",
        "translatedContent": ""
      },
      {
        "row": 14,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "학습은 세 가지 주요 단계로 구성됩니다: 데이터셋 준비, 설정 파일 작성, 그리고 학습 스크립트 실행."
      },
      {
        "row": 15,
        "rowsha": "eQCAQ6FcBRihX0kFBysSKKnUmSUlgrm5+sqlkkVs1Vc=",
        "originContent": "Training involves three main steps: Preparing the dataset, creating a configuration file and running the training",
        "translatedContent": ""
      },
      {
        "row": 16,
        "rowsha": "HpkQECi3IwVSLmb3kLtCyOav8kaiFi57Bdg7xS6ONqQ=",
        "originContent": "script.",
        "translatedContent": "데이터셋을 준비하려면:  "
      },
      {
        "row": 17,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "1) OCID_grasp 데이터셋을 [여기](https://cloud.tugraz.at/index.php/s/NA7icqiJ5SeNSA6?dir=/Grasp_det_seg_cnn/OCID_grasp)에서 다운로드하세요.  "
      },
      {
        "row": 18,
        "rowsha": "tpVK+Jl785dtv9eRwIdSmlHVOAnppxQp7PdncX7qBic=",
        "originContent": "To prepare the dataset:",
        "translatedContent": "다운로드한 `OCID_grasp.zip` 파일을 `DATA` 폴더에 압축 해제합니다.  "
      },
      {
        "row": 19,
        "rowsha": "u4KQCuZojCvF1PzYEeSJImsRUcCx/xZ6fP7BkUlkJQ0=",
        "originContent": "1) Download the OCID_grasp dataset [here](https://cloud.tugraz.at/index.php/s/NA7icqiJ5SeNSA6?dir=/Grasp_det_seg_cnn/OCID_grasp).",
        "translatedContent": "2) 설정 파일은 `ini` 형식의 간단한 텍스트 파일입니다.  "
      },
      {
        "row": 20,
        "rowsha": "ADwSTE0uOm0YiRN9G7wn4LGZJaKg9TpV0bI7v+0xWe8=",
        "originContent": "Unpack the downloaded `OCID_grasp.zip` file into the `DATA` folder.",
        "translatedContent": "각 설정 파라미터의 기본값과 그 기능에 대한 간단한 설명은  "
      },
      {
        "row": 21,
        "rowsha": "9khvzqP/nuqCIHxURD4M5/RMdgd6fGTbiRytAv/Vu4k=",
        "originContent": "2) The configuration file is a simple text file in `ini` format.",
        "translatedContent": "[grasp_det_seg/config/defaults](grasp_det_seg/config/defaults)에서 확인할 수 있습니다.  "
      },
      {
        "row": 22,
        "rowsha": "zVg4u89O16LTTAWaWzcPeiciH7t/6b5iYPSINnJBr3A=",
        "originContent": "The default value of each configuration parameter, as well as a short description of what it does, is available in",
        "translatedContent": "**참고**: 이 값들은 각 파라미터에 대해 \"합리적인\" 값의 예시일 뿐이며, 본 논문의 결과를 재현하기 위한 값은 아닙니다."
      },
      {
        "row": 23,
        "rowsha": "++SaMnKd9NSnQbJPlemRnnlive33/vKwuVQkRZzsYK8=",
        "originContent": "[grasp_det_seg/config/defaults](grasp_det_seg/config/defaults).",
        "translatedContent": ""
      },
      {
        "row": 24,
        "rowsha": "Y+Wsp7KYCUTP2c8BxrSpJCxfZUctQwLvbQmA/G3CaKs=",
        "originContent": "**Note** that these are just an indication of what a \"reasonable\" value for each parameter could be, and are not",
        "translatedContent": "3) 학습을 시작하려면:"
      },
      {
        "row": 25,
        "rowsha": "bYp7c0oCKqoi3PSHm3X3KzVSkpXRLEvtnf0AYsEpobo=",
        "originContent": "meant as a way to reproduce any of the results from our paper.",
        "translatedContent": ""
      },
      {
        "row": 26,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 27,
        "rowsha": "Xd6eLHO6BJNziG5Vqan0sNlH5nHLKqxfm1C8YsPg3bs=",
        "originContent": "3) To launch the training:",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 8,
    "Content": "```bash\ncd scripts\npython3 -m torch.distributed.launch --nproc_per_node=1 train_det_seg_OCID.py \n--log_dir=LOGDIR CONFIG DATA_DIR\n```",
    "ContentSha": "16PG9/TtAudgTWZKznPTizPiNRRykijVxsruX/FmHSI=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bash\ncd scripts\npython3 -m torch.distributed.launch --nproc_per_node=1 train_det_seg_OCID.py \n--log_dir=LOGDIR CONFIG DATA_DIR\n```",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "rTMjNc+qNIxb1xXDR5hEYiJw1fc4muBwuBsTXVIaIho=",
        "originContent": "```bash",
        "translatedContent": "```bash"
      },
      {
        "row": 2,
        "rowsha": "osdQT8ZHo/O5eva3VI/7Opomtqcc3lffSxdAmcGTCF8=",
        "originContent": "cd scripts",
        "translatedContent": "cd scripts"
      },
      {
        "row": 3,
        "rowsha": "tEgUyhabRdKjjns4U427ynszGjjzBTQtIvQKDgT9h7s=",
        "originContent": "python3 -m torch.distributed.launch --nproc_per_node=1 train_det_seg_OCID.py ",
        "translatedContent": "python3 -m torch.distributed.launch --nproc_per_node=1 train_det_seg_OCID.py "
      },
      {
        "row": 4,
        "rowsha": "q4FS2Y0P/c08fKkLNy8SBnnjSEselS41NuDn9USn/DA=",
        "originContent": "--log_dir=LOGDIR CONFIG DATA_DIR",
        "translatedContent": "--log_dir=LOGDIR CONFIG DATA_DIR"
      },
      {
        "row": 5,
        "rowsha": "8bkBhHOQsO1+N058HkZOwXtGpCfEh6WtbL0pBkBQg9U=",
        "originContent": "```",
        "translatedContent": "```"
      }
    ],
    "IsCodeBlock": true
  },
  {
    "Id": 9,
    "Content": "Training logs, both in text and Tensorboard formats as well as the trained network parameters, will be written \nin `LOG_DIR` (e.g. `ckpt_files_OCID`).\nThe file `CONFIG` contains the network configuration e.g. `grasp_det_seg/config/defaults/det_seg_OCID.ini`, \nand `DATA_DIR` points to the previously downloaded OCID_grasp splits, e.g. `DATA/OCID_grasp/data_split`.\n\nNote that, for now, our code **must** be launched in \"distributed\" mode using PyTorch's `torch.distributed.launch`\nutility.\n\n### Running inference\n\nGiven a trained network, inference can be run on any set of images using\n[scripts/test_det_seg_OCID.py](https://raw.githubusercontent.com/stefan-ainetter/grasp_det_seg_cnn/main/scripts/test_det_seg_OCID.py):",
    "ContentSha": "aSovULCugM/xlZZAJNOk2TmLLBuOFAamaZZHCREkK8s=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "학습 로그는 텍스트 및 Tensorboard 형식으로, 그리고 학습된 네트워크 파라미터는 `LOG_DIR` (예: `ckpt_files_OCID`)에 저장됩니다.  \n파일 `CONFIG`는 네트워크 구성을 포함하며, 예: `grasp_det_seg/config/defaults/det_seg_OCID.ini`  \n`DATA_DIR`는 이전에 다운로드한 OCID_grasp 분할 데이터를 가리킵니다, 예: `DATA/OCID_grasp/data_split`.\n\n참고로, 현재로서는 우리의 코드는 PyTorch의 `torch.distributed.launch`  \n유틸리티를 사용하여 \"distributed\" 모드로 반드시 실행되어야 합니다.\n\n### 추론 실행\n\n학습된 네트워크가 주어지면,  \n[scripts/test_det_seg_OCID.py](https://raw.githubusercontent.com/stefan-ainetter/grasp_det_seg_cnn/main/scripts/test_det_seg_OCID.py)를 사용하여  \n어떤 이미지 세트에 대해서도 추론을 실행할 수 있습니다:",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "yy+xTAi6zMVy+2qRYEUPROQlK8ocDLRAkogfA6UcJjc=",
        "originContent": "Training logs, both in text and Tensorboard formats as well as the trained network parameters, will be written ",
        "translatedContent": "학습 로그는 텍스트 및 Tensorboard 형식으로, 그리고 학습된 네트워크 파라미터는 `LOG_DIR` (예: `ckpt_files_OCID`)에 저장됩니다.  "
      },
      {
        "row": 2,
        "rowsha": "BEx7AI+OB7Jj8OcICE3KLIXz3aKNkh1NsZmctqx2p/8=",
        "originContent": "in `LOG_DIR` (e.g. `ckpt_files_OCID`).",
        "translatedContent": "파일 `CONFIG`는 네트워크 구성을 포함하며, 예: `grasp_det_seg/config/defaults/det_seg_OCID.ini`  "
      },
      {
        "row": 3,
        "rowsha": "Bu7ySMKBqd150OhO7PFMh9uTYcj2accejh0G1a/++i4=",
        "originContent": "The file `CONFIG` contains the network configuration e.g. `grasp_det_seg/config/defaults/det_seg_OCID.ini`, ",
        "translatedContent": "`DATA_DIR`는 이전에 다운로드한 OCID_grasp 분할 데이터를 가리킵니다, 예: `DATA/OCID_grasp/data_split`."
      },
      {
        "row": 4,
        "rowsha": "bBY1sqoGZi709tajOP28z7yiqnjbo8HzF1YtKFzA0Ao=",
        "originContent": "and `DATA_DIR` points to the previously downloaded OCID_grasp splits, e.g. `DATA/OCID_grasp/data_split`.",
        "translatedContent": ""
      },
      {
        "row": 5,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "참고로, 현재로서는 우리의 코드는 PyTorch의 `torch.distributed.launch`  "
      },
      {
        "row": 6,
        "rowsha": "Jw/o3WswNmPBknyZfhLaopDNoPgILYdVgdRaWOh35ag=",
        "originContent": "Note that, for now, our code **must** be launched in \"distributed\" mode using PyTorch's `torch.distributed.launch`",
        "translatedContent": "유틸리티를 사용하여 \"distributed\" 모드로 반드시 실행되어야 합니다."
      },
      {
        "row": 7,
        "rowsha": "lKseaZ5op3JHQbfrM0Q6rX6I4fxKK16JL5kMEY6cpCc=",
        "originContent": "utility.",
        "translatedContent": ""
      },
      {
        "row": 8,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "### 추론 실행"
      },
      {
        "row": 9,
        "rowsha": "YHsJXu+tqYYP3JoETNjFeyieKrZ1zh5YeHPj3BxSra8=",
        "originContent": "### Running inference",
        "translatedContent": ""
      },
      {
        "row": 10,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "학습된 네트워크가 주어지면,  "
      },
      {
        "row": 11,
        "rowsha": "rhC37uBssuCbgoaCmctv8tg/4FTOS0lX8uKlqG5acJs=",
        "originContent": "Given a trained network, inference can be run on any set of images using",
        "translatedContent": "[scripts/test_det_seg_OCID.py](https://raw.githubusercontent.com/stefan-ainetter/grasp_det_seg_cnn/main/scripts/test_det_seg_OCID.py)를 사용하여  "
      },
      {
        "row": 12,
        "rowsha": "L0xtXB/FtI3LuT57l5g7k7p3m80i/bNsHt6AwqEUO/I=",
        "originContent": "[scripts/test_det_seg_OCID.py](https://raw.githubusercontent.com/stefan-ainetter/grasp_det_seg_cnn/main/scripts/test_det_seg_OCID.py):",
        "translatedContent": "어떤 이미지 세트에 대해서도 추론을 실행할 수 있습니다:"
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 10,
    "Content": "```bash\ncd scripts\npython3 -m torch.distributed.launch --nproc_per_node=1 test_det_seg_OCID.py \n--log_dir=LOG_DIR CONFIG MODEL_PARAMS DATA_DIR OUTPUT_DIR\n\n```",
    "ContentSha": "rAAfgkUxfkYKGLFnUxyhhq/9OtPyM6Zb7inzaTXf6tg=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bash\ncd scripts\npython3 -m torch.distributed.launch --nproc_per_node=1 test_det_seg_OCID.py \n--log_dir=LOG_DIR CONFIG MODEL_PARAMS DATA_DIR OUTPUT_DIR\n\n```",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "rTMjNc+qNIxb1xXDR5hEYiJw1fc4muBwuBsTXVIaIho=",
        "originContent": "```bash",
        "translatedContent": "```bash"
      },
      {
        "row": 2,
        "rowsha": "osdQT8ZHo/O5eva3VI/7Opomtqcc3lffSxdAmcGTCF8=",
        "originContent": "cd scripts",
        "translatedContent": "cd scripts"
      },
      {
        "row": 3,
        "rowsha": "yGo/xltjzI9kCDic5lsbvmwZ103W6IopdoA++1FbXS0=",
        "originContent": "python3 -m torch.distributed.launch --nproc_per_node=1 test_det_seg_OCID.py ",
        "translatedContent": "python3 -m torch.distributed.launch --nproc_per_node=1 test_det_seg_OCID.py "
      },
      {
        "row": 4,
        "rowsha": "UlLXky6k0jdCYpJUQdMjSDyNW9NWFhRikAqvRBZ1Sq4=",
        "originContent": "--log_dir=LOG_DIR CONFIG MODEL_PARAMS DATA_DIR OUTPUT_DIR",
        "translatedContent": "--log_dir=LOG_DIR CONFIG MODEL_PARAMS DATA_DIR OUTPUT_DIR"
      },
      {
        "row": 5,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 6,
        "rowsha": "8bkBhHOQsO1+N058HkZOwXtGpCfEh6WtbL0pBkBQg9U=",
        "originContent": "```",
        "translatedContent": "```"
      }
    ],
    "IsCodeBlock": true
  },
  {
    "Id": 11,
    "Content": "Predictions will be written to `OUTPUT_DIR` e.g. the `output` folder. `MODEL_PARAMS` are pre-trained weights e.g. `ckpt_files_OCID/pretrained/model_last.pth.tar`, \n`DATA_DIR` points to the used dateset splits e.g. `DATA/OCID_grasp/data_split`.\n\n## OCID_grasp dataset\nThe OCID_grasp dataset can be downloaded [here](https://cloud.tugraz.at/index.php/s/NA7icqiJ5SeNSA6?dir=/Grasp_det_seg_cnn/OCID_grasp).\nOCID_grasp consists of 1763 selected RGB-D images of the OCID dataset, with over 11.4k segmented object masks and more than 75k hand-annotated \ngrasp candidates. Additionally, each object is classified into one of 31 object classes.\n## Related Work\nOCID_grasp is a dataset extension of the [OCID dataset](https://www.acin.tuwien.ac.at/en/vision-for-robotics/software-tools/object-clutter-indoor-dataset/).\nIf you decide to use OCID_grasp for your research, please also cite the OCID paper:",
    "ContentSha": "Q+rjaJD+yucVKHOa0N6DoyXk+1EPs6Hvapxo2Y2IJag=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "예측 결과는 `OUTPUT_DIR`에 기록됩니다. 예: `output` 폴더. `MODEL_PARAMS`는 사전 학습된 가중치입니다. 예: `ckpt_files_OCID/pretrained/model_last.pth.tar`,  \n`DATA_DIR`는 사용된 데이터셋 분할을 가리킵니다. 예: `DATA/OCID_grasp/data_split`.  \n\n## OCID_grasp 데이터셋  \nOCID_grasp 데이터셋은 [여기](https://cloud.tugraz.at/index.php/s/NA7icqiJ5SeNSA6?dir=/Grasp_det_seg_cnn/OCID_grasp)에서 다운로드할 수 있습니다.  \nOCID_grasp는 OCID 데이터셋의 1763개의 선택된 RGB-D 이미지로 구성되며, 11.4k 이상의 분할된 객체 마스크와 75k 이상의 수작업으로 주석된  \n그립 후보가 포함되어 있습니다. 또한 각 객체는 31개의 객체 클래스 중 하나로 분류됩니다.  \n## 관련 연구  \nOCID_grasp는 [OCID 데이터셋](https://www.acin.tuwien.ac.at/en/vision-for-robotics/software-tools/object-clutter-indoor-dataset/)의 데이터셋 확장입니다.  \n연구에 OCID_grasp를 사용하기로 결정했다면 OCID 논문도 함께 인용해 주세요:",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "LGfuta0BJtREwunwa0cA444cqws6wg2/1Zr9ase6uuE=",
        "originContent": "Predictions will be written to `OUTPUT_DIR` e.g. the `output` folder. `MODEL_PARAMS` are pre-trained weights e.g. `ckpt_files_OCID/pretrained/model_last.pth.tar`, ",
        "translatedContent": "예측 결과는 `OUTPUT_DIR`에 기록됩니다. 예: `output` 폴더. `MODEL_PARAMS`는 사전 학습된 가중치입니다. 예: `ckpt_files_OCID/pretrained/model_last.pth.tar`,  "
      },
      {
        "row": 2,
        "rowsha": "mg/tvYR/kVEYh6aDAm3zEGBiwMfFuhYCywAX+Bg4RiU=",
        "originContent": "`DATA_DIR` points to the used dateset splits e.g. `DATA/OCID_grasp/data_split`.",
        "translatedContent": "`DATA_DIR`는 사용된 데이터셋 분할을 가리킵니다. 예: `DATA/OCID_grasp/data_split`.  "
      },
      {
        "row": 3,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 4,
        "rowsha": "qFhWQWGReCAAodXBX54hbnJArb6K84zlkpenVBoWvzM=",
        "originContent": "## OCID_grasp dataset",
        "translatedContent": "## OCID_grasp 데이터셋  "
      },
      {
        "row": 5,
        "rowsha": "omrqOod/6P5EpDL+v1RdkiqkgG8yUeCY5TABr3iIunI=",
        "originContent": "The OCID_grasp dataset can be downloaded [here](https://cloud.tugraz.at/index.php/s/NA7icqiJ5SeNSA6?dir=/Grasp_det_seg_cnn/OCID_grasp).",
        "translatedContent": "OCID_grasp 데이터셋은 [여기](https://cloud.tugraz.at/index.php/s/NA7icqiJ5SeNSA6?dir=/Grasp_det_seg_cnn/OCID_grasp)에서 다운로드할 수 있습니다.  "
      },
      {
        "row": 6,
        "rowsha": "dZds0w4Cjy/fLl4MXyAUKeztcwpYlkHktDB8Qr/SNZQ=",
        "originContent": "OCID_grasp consists of 1763 selected RGB-D images of the OCID dataset, with over 11.4k segmented object masks and more than 75k hand-annotated ",
        "translatedContent": "OCID_grasp는 OCID 데이터셋의 1763개의 선택된 RGB-D 이미지로 구성되며, 11.4k 이상의 분할된 객체 마스크와 75k 이상의 수작업으로 주석된  "
      },
      {
        "row": 7,
        "rowsha": "AtDjEF79S/csjw819FF1t1GRPg8orHiKdWGQ0NVoZgc=",
        "originContent": "grasp candidates. Additionally, each object is classified into one of 31 object classes.",
        "translatedContent": "그립 후보가 포함되어 있습니다. 또한 각 객체는 31개의 객체 클래스 중 하나로 분류됩니다.  "
      },
      {
        "row": 8,
        "rowsha": "gQgmB+mDPNzuAGWV+6k2andOSH0JX602DNs4sjSZtf0=",
        "originContent": "## Related Work",
        "translatedContent": "## 관련 연구  "
      },
      {
        "row": 9,
        "rowsha": "IHIyhhl867B0xc05sUp+hppl2/MHh3NkI822d7LxUtU=",
        "originContent": "OCID_grasp is a dataset extension of the [OCID dataset](https://www.acin.tuwien.ac.at/en/vision-for-robotics/software-tools/object-clutter-indoor-dataset/).",
        "translatedContent": "OCID_grasp는 [OCID 데이터셋](https://www.acin.tuwien.ac.at/en/vision-for-robotics/software-tools/object-clutter-indoor-dataset/)의 데이터셋 확장입니다.  "
      },
      {
        "row": 10,
        "rowsha": "m1vrFeWhPsndAQdN0iAZHyy+76xelV342JvJJFk6ti0=",
        "originContent": "If you decide to use OCID_grasp for your research, please also cite the OCID paper:",
        "translatedContent": "연구에 OCID_grasp를 사용하기로 결정했다면 OCID 논문도 함께 인용해 주세요:"
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 12,
    "Content": "```bibtex\n@inproceedings{suchi2019easylabel,\n  title={EasyLabel: a semi-automatic pixel-wise object annotation tool for creating robotic RGB-D datasets},\n  author={Suchi, Markus and Patten, Timothy and Fischinger, David and Vincze, Markus},\n  booktitle={2019 International Conference on Robotics and Automation (ICRA)},\n  pages={6678--6684},\n  year={2019},\n  organization={IEEE}\n}\n```",
    "ContentSha": "1iBZXMOeurJZBuKzq4DVpt7Ec/DDgZ5jEGeKmz+hJ/4=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bibtex\n@inproceedings{suchi2019easylabel,\n  title={EasyLabel: a semi-automatic pixel-wise object annotation tool for creating robotic RGB-D datasets},\n  author={Suchi, Markus and Patten, Timothy and Fischinger, David and Vincze, Markus},\n  booktitle={2019 International Conference on Robotics and Automation (ICRA)},\n  pages={6678--6684},\n  year={2019},\n  organization={IEEE}\n}\n```",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "o+TmyQ6wneV6/FQB6aUlRSjIGr2/YLJtnz5uxBgsScQ=",
        "originContent": "```bibtex",
        "translatedContent": "```bibtex"
      },
      {
        "row": 2,
        "rowsha": "am+7fzPMogX57+2No7Zx6g+m/KO6Kpe1VoZRwopV4Kg=",
        "originContent": "@inproceedings{suchi2019easylabel,",
        "translatedContent": "@inproceedings{suchi2019easylabel,"
      },
      {
        "row": 3,
        "rowsha": "3EiZpAxiykBZX3FWPLD330m67Bv96NRgDWCxZ4vuU/4=",
        "originContent": "  title={EasyLabel: a semi-automatic pixel-wise object annotation tool for creating robotic RGB-D datasets},",
        "translatedContent": "  title={EasyLabel: a semi-automatic pixel-wise object annotation tool for creating robotic RGB-D datasets},"
      },
      {
        "row": 4,
        "rowsha": "IxumqeikoFIe6Hndls9+IAJi13XUKKk9bmC7xaFxXm0=",
        "originContent": "  author={Suchi, Markus and Patten, Timothy and Fischinger, David and Vincze, Markus},",
        "translatedContent": "  author={Suchi, Markus and Patten, Timothy and Fischinger, David and Vincze, Markus},"
      },
      {
        "row": 5,
        "rowsha": "IInWFJKqnyVR2m4m2aHt68UI1DxIt9IREQltzcuUIpo=",
        "originContent": "  booktitle={2019 International Conference on Robotics and Automation (ICRA)},",
        "translatedContent": "  booktitle={2019 International Conference on Robotics and Automation (ICRA)},"
      },
      {
        "row": 6,
        "rowsha": "tlaHsVykO12Fo3/PGqoVi3f9pXdsUwtjsrTAFcryMU4=",
        "originContent": "  pages={6678--6684},",
        "translatedContent": "  pages={6678--6684},"
      },
      {
        "row": 7,
        "rowsha": "ocEOTULJTOsDZV6KHGnRSOuaoV0qhbBNO4DJ3ntJ94Q=",
        "originContent": "  year={2019},",
        "translatedContent": "  year={2019},"
      },
      {
        "row": 8,
        "rowsha": "9Hfv4nT468GMOcLLDRYiC872ATBuopr8BNVdeXRMKKo=",
        "originContent": "  organization={IEEE}",
        "translatedContent": "  organization={IEEE}"
      },
      {
        "row": 9,
        "rowsha": "0Qs2qnSlm89KiBhYN/ZYr682Ru/yuxbDko0OkzXpRdI=",
        "originContent": "}",
        "translatedContent": "}"
      },
      {
        "row": 10,
        "rowsha": "8bkBhHOQsO1+N058HkZOwXtGpCfEh6WtbL0pBkBQg9U=",
        "originContent": "```",
        "translatedContent": "```"
      }
    ],
    "IsCodeBlock": true
  },
  {
    "Id": 13,
    "Content": "Our framework is based on the architecture from [Seamless Scene Segmentation](https://github.com/mapillary/seamseg):",
    "ContentSha": "teEAAZH35j5EeglHUPss2l/VI0NSW5CguZHzPiIagvw=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "우리 프레임워크는 [Seamless Scene Segmentation](https://github.com/mapillary/seamseg)의 아키텍처를 기반으로 합니다:",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "teEAAZH35j5EeglHUPss2l/VI0NSW5CguZHzPiIagvw=",
        "originContent": "Our framework is based on the architecture from [Seamless Scene Segmentation](https://github.com/mapillary/seamseg):",
        "translatedContent": "우리 프레임워크는 [Seamless Scene Segmentation](https://github.com/mapillary/seamseg)의 아키텍처를 기반으로 합니다:"
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 14,
    "Content": "```bibtex\n@InProceedings{Porzi_2019_CVPR,\n  author = {Porzi, Lorenzo and Rota Bul\\`o, Samuel and Colovic, Aleksander and Kontschieder, Peter},\n  title = {Seamless Scene Segmentation},\n  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n  month = {June},\n  year = {2019}\n}\n```",
    "ContentSha": "FK+vIm7hQ8n4L2GxlAPqIkSI6lVy6uIKui7hFqn4TDw=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bibtex\n@InProceedings{Porzi_2019_CVPR,\n  author = {Porzi, Lorenzo and Rota Bul\\`o, Samuel and Colovic, Aleksander and Kontschieder, Peter},\n  title = {Seamless Scene Segmentation},\n  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n  month = {June},\n  year = {2019}\n}\n```",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "o+TmyQ6wneV6/FQB6aUlRSjIGr2/YLJtnz5uxBgsScQ=",
        "originContent": "```bibtex",
        "translatedContent": "```bibtex"
      },
      {
        "row": 2,
        "rowsha": "qXDOjNU0Ch+XRTRZ0ydjkWKK/3MJexKAClYsDFO56b0=",
        "originContent": "@InProceedings{Porzi_2019_CVPR,",
        "translatedContent": "@InProceedings{Porzi_2019_CVPR,"
      },
      {
        "row": 3,
        "rowsha": "FfFKa/UbaCH8vG4kEYTyqdUQYPgB8+MveMXV9ea9oN8=",
        "originContent": "  author = {Porzi, Lorenzo and Rota Bul\\`o, Samuel and Colovic, Aleksander and Kontschieder, Peter},",
        "translatedContent": "  author = {Porzi, Lorenzo and Rota Bul\\`o, Samuel and Colovic, Aleksander and Kontschieder, Peter},"
      },
      {
        "row": 4,
        "rowsha": "uGIEzIbt62asefZrx+otnREvHQmBHrAbyQ+caaPx6z8=",
        "originContent": "  title = {Seamless Scene Segmentation},",
        "translatedContent": "  title = {Seamless Scene Segmentation},"
      },
      {
        "row": 5,
        "rowsha": "OzLWvmPmU0HySxSvHnCvlw84cXG0JRG32Y2hANlllNc=",
        "originContent": "  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},",
        "translatedContent": "  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},"
      },
      {
        "row": 6,
        "rowsha": "U1Izw4geK+wX4RkAbLnK9NT0MBNGq5VLS9uL46bPZVs=",
        "originContent": "  month = {June},",
        "translatedContent": "  month = {June},"
      },
      {
        "row": 7,
        "rowsha": "NcyTFqu73UT68Hhp/KhgRz8xkWN2xeRhiy2hedxuQjc=",
        "originContent": "  year = {2019}",
        "translatedContent": "  year = {2019}"
      },
      {
        "row": 8,
        "rowsha": "0Qs2qnSlm89KiBhYN/ZYr682Ru/yuxbDko0OkzXpRdI=",
        "originContent": "}",
        "translatedContent": "}"
      },
      {
        "row": 9,
        "rowsha": "8bkBhHOQsO1+N058HkZOwXtGpCfEh6WtbL0pBkBQg9U=",
        "originContent": "```",
        "translatedContent": "```"
      }
    ],
    "IsCodeBlock": true
  },
  {
    "Id": 15,
    "Content": "---\n## About our latest Research\n### Our paper 'Depth-aware Object Segmentation and Grasp Detection for Robotic Picking Tasks' got accepted at BMVC21\nIn our latest work, we implemented a method for joint grasp detection and class-agnostic object instance segmentation,\nwhich was published at BMVC21. \nMore information can be found [here](https://arxiv.org/pdf/2111.11114).\n",
    "ContentSha": "YG+vtnTyYBCJ3dfscEwf9ciz2CjHp4vry9z6DASBwhU=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "---\n## 최신 연구 소개\n### 저희 논문 'Depth-aware Object Segmentation and Grasp Detection for Robotic Picking Tasks'가 BMVC21에 채택되었습니다\n저희 최신 연구에서는 로봇 픽킹 작업을 위한 깊이 인식 객체 분할과 그립 감지를 공동으로 수행하는 방법을 구현하였으며,\n이는 BMVC21에서 발표되었습니다. \n자세한 내용은 [여기](https://arxiv.org/pdf/2111.11114)에서 확인할 수 있습니다.\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "yz+R1U7uMOU+NbK5mQX3Dxae1Un9eJCdPawt78ntjTs=",
        "originContent": "---",
        "translatedContent": "---"
      },
      {
        "row": 2,
        "rowsha": "kJZYblkECg2EIWGuT746GoZHpfK3HuKzWt/k5GEQrbc=",
        "originContent": "## About our latest Research",
        "translatedContent": "## 최신 연구 소개"
      },
      {
        "row": 3,
        "rowsha": "96M3ewdojOSKPqTVkgan54N+NTpS/EW41y7Ac4TIDdY=",
        "originContent": "### Our paper 'Depth-aware Object Segmentation and Grasp Detection for Robotic Picking Tasks' got accepted at BMVC21",
        "translatedContent": "### 저희 논문 'Depth-aware Object Segmentation and Grasp Detection for Robotic Picking Tasks'가 BMVC21에 채택되었습니다"
      },
      {
        "row": 4,
        "rowsha": "p30lQqbpYb7jbznlL9TlbAztV2qbRHGsS9Teeat5SB8=",
        "originContent": "In our latest work, we implemented a method for joint grasp detection and class-agnostic object instance segmentation,",
        "translatedContent": "저희 최신 연구에서는 로봇 픽킹 작업을 위한 깊이 인식 객체 분할과 그립 감지를 공동으로 수행하는 방법을 구현하였으며,"
      },
      {
        "row": 5,
        "rowsha": "EDlMAC1EObuIzJFQydpJqPXK/0vDMpZHE+tLrO4RXBc=",
        "originContent": "which was published at BMVC21. ",
        "translatedContent": "이는 BMVC21에서 발표되었습니다. "
      },
      {
        "row": 6,
        "rowsha": "U97RtC1JlDrhwc0yd1F9F/THZbwTgS0QCDUrMANUgC4=",
        "originContent": "More information can be found [here](https://arxiv.org/pdf/2111.11114).",
        "translatedContent": "자세한 내용은 [여기](https://arxiv.org/pdf/2111.11114)에서 확인할 수 있습니다."
      },
      {
        "row": 7,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  }
]