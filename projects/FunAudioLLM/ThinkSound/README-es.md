<h1 align="center">ThinkSound</h1>

<p align="center">
  ğŸŒ
  <a href="https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=en">English</a> |
  <a href="https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=zh-CN">ç®€ä½“ä¸­æ–‡</a> |
  <a href="https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=zh-TW">ç¹é«”ä¸­æ–‡</a> |
  <a href="https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=es">EspaÃ±ol</a> |
  <a href="https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=fr">FranÃ§ais</a> |
  <a href="https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=ja">æ—¥æœ¬èª</a>
  
</p>
<p align="center">
  <img src="https://img.shields.io/badge/NeurIPS 2025-Main Conference-blue.svg" alt="NeurIPS 2025"/>
<p align="center">
  <a href="https://arxiv.org/pdf/2506.21448">
    <img src="https://img.shields.io/badge/arXiv-2506.21448-b31b1b.svg" alt="arXiv"/>
  </a>
  &nbsp;
  <a href="https://thinksound-project.github.io/">
    <img src="https://img.shields.io/badge/Online%20Demo-ğŸŒ-blue" alt="Online Demo"/>
  </a>
  &nbsp;
  <a href="https://huggingface.co/spaces/FunAudioLLM/ThinkSound">
    <img src="https://img.shields.io/badge/HuggingFace-Spaces-orange?logo=huggingface" alt="Hugging Face"/>
  </a>
  &nbsp;
  <a href="https://modelscope.cn/studios/iic/ThinkSound">
    <img src="https://img.shields.io/badge/ModelScope-åœ¨çº¿ä½“éªŒ-green" alt="ModelScope"/>
  </a>
</p>

<p align="center">
  Si encuentras Ãºtil este proyecto,<br>
  Â¡un estrella â­ en GitHub serÃ­a muy apreciado!
</p>

---

**ThinkSound** es un marco unificado de generaciÃ³n Any2Audio con coincidencia de flujo guiada por razonamiento de Cadena de Pensamiento (CoT).

ImplementaciÃ³n en PyTorch para generaciÃ³n y ediciÃ³n multimodal de audio: genera o edita audio a partir de video, texto y audio, impulsado por razonamiento paso a paso de Modelos de Lenguaje Multimodal Grandes (MLLMs).

![Avance](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig1_teaser.png)
---

## ğŸ“° Noticias
- **2025.09.19** &nbsp; ğŸ‰ Â¡ThinkSound ha sido aceptado en la **Conferencia Principal de NeurIPS 2025**!
- **2025.09.01** &nbsp; ğŸ”¥ Â¡Nuestro conjunto de datos AudioCoT ahora es de cÃ³digo abierto y estÃ¡ disponible en [Hugging Face](https://huggingface.co/datasets/liuhuadai/AudioCoT)!
- **2025.07.17** &nbsp; ğŸ§  Finetuning habilitado: el cÃ³digo para entrenamiento y ajuste fino ya estÃ¡ disponible pÃºblicamente, junto con instrucciones claras para ayudarte a personalizar y ampliar ThinkSound con tus propios datos.
- **2025.07.15** &nbsp; ğŸ“¦ InstalaciÃ³n y usabilidad simplificadas: dependencias en PyPI para configuraciÃ³n fÃ¡cil multiplataforma; los scripts `.bat` para Windows automatizan la creaciÃ³n de entornos y ejecuciÃ³n de scripts.
- **2025.07.08** &nbsp;Â  ğŸ”§ ActualizaciÃ³n mayor: el modelo ha sido aligerado y la memoria y el uso de GPU optimizados, Â¡ahora admite generaciÃ³n de audio de alto rendimiento a escala!
- **2025.07.01** &nbsp; ğŸ”¥Demo en lÃ­nea en [Hugging Face Spaces](https://huggingface.co/spaces/FunAudioLLM/ThinkSound) y [ModelScope](https://modelscope.cn/studios/iic/ThinkSound) para experiencia interactiva.
- **2025.07.01** &nbsp; ğŸ”¥Se publicaron scripts de inferencia y la interfaz web;
- **2025.06** &nbsp; ğŸ”¥Â¡El artÃ­culo de ThinkSound ([ThinkSound paper](https://arxiv.org/pdf/2506.21448)) estÃ¡ disponible en arXiv!
- **2025.06** &nbsp; ğŸ”¥[Demo en lÃ­nea](http://thinksound-project.github.io/) estÃ¡ activa â€“ Â¡pruÃ©bala ahora!

---


## ğŸš€ CaracterÃ­sticas

- **Any2Audio**: Genera audio desde cualquier modalidad â€” video, texto, audio o combinaciones de estos.
- **Video-a-Audio SOTA**: Logra resultados de Ãºltima generaciÃ³n en mÃºltiples benchmarks V2A.
- **Razonamiento CoT**: Razonamiento en cadena de pensamiento para generaciÃ³n de audio composicional y controlable mediante MLLMs.
- **EdiciÃ³n interactiva centrada en objetos**: Refina o edita eventos sonoros especÃ­ficos haciendo clic en objetos visuales o usando instrucciones de texto.
- **Marco unificado**: Un modelo base soporta generaciÃ³n, ediciÃ³n y flujo de trabajo interactivo.

---

## âœ¨ Resumen del mÃ©todo

ThinkSound descompone la generaciÃ³n y ediciÃ³n de audio en tres etapas interactivas, todas guiadas por razonamiento CoT basado en MLLM:

1. **GeneraciÃ³n Foley:** Genera paisajes sonoros fundamentales, alineados semÃ¡ntica y temporalmente a partir de video.
2. **Refinamiento centrado en objetos:** Refina o aÃ±ade sonidos para objetos especificados por el usuario mediante clics o regiones en el video.
3. **EdiciÃ³n de audio dirigida:** Modifica el audio generado usando instrucciones de lenguaje natural de alto nivel.

![Resumen ThinkSound](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig3_model.png)
<!-- Se utiliza un conjunto de datos a gran escala anotado con CoT (**AudioCoT**) para entrenar tanto el mÃ³dulo de razonamiento como el modelo base de audio unificado.
![AudioCoT Pipeline](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig2_dataset.png) -->

---

## âš¡ Inicio RÃ¡pido

**PreparaciÃ³n del entorno:**
```bash
git clone https://github.com/liuhuadai/ThinkSound.git
cd ThinkSound
conda create -n thinksound python=3.10
conda activate thinksound
pip install thinksound
conda install -y -c conda-forge 'ffmpeg<7'
# Download pretrained weights https://huggingface.co/liuhuadai/ThinkSound to Directory ckpts/
# model weights can be also downloaded from https://www.modelscope.cn/models/iic/ThinkSound
git lfs install
git clone https://huggingface.co/liuhuadai/ThinkSound ckpts
# To improve inference and training speed, you may optionally install a FlashAttention backend compatible with your system and PyTorch version.
```
> âœ… **Consejo para Windows:**  
> Los usuarios de Windows pueden simplemente ejecutar `setup_windows.bat` (o hacer doble clic en Ã©l) para crear automÃ¡ticamente el entorno conda, instalar todas las dependencias (incluido FFmpeg) y descargar el modelo preentrenado â€” no se requiere configuraciÃ³n manual.  
> AsegÃºrate de que `conda` y `git` estÃ©n instalados y disponibles en el PATH de tu sistema antes de ejecutar el script.


### â–¶ï¸ Ejecutar la DemostraciÃ³n

#### **Linux/macOS**


```bash
chmod +x scripts/demo.sh
./scripts/demo.sh <path-to-your-demo-video> <title> <CoT description> [use-half]
```
#### **Windows**

En su lugar, puede usar el script `.bat` proporcionado:


```bash
.\scripts\demo.bat <path-to-your-demo-video> <title> <CoT description> [use-half]
```
**Nota:**

* `<ruta-a-tu-video-demo>`: La ruta a un solo video
* `[use-half]` (opcional): AÃ±ade use-half al final para habilitar la extracciÃ³n de caracterÃ­sticas en media precisiÃ³n.

---

### ğŸ“¦ Inferencia por Lotes

#### **Linux/macOS**


```bash
chmod +x scripts/eval_batch.sh
./scripts/eval_batch.sh <video_path> <csv_path> <save_path (optional)> [use-half]
```
#### **Windows**

Utilice el script `.bat` equivalente:


```bash
.\scripts\eval_batch.bat <video_path> <csv_path> <save_path (optional)> [use-half]
```
**Nota:**

* `<video_path>`: Ruta al directorio raÃ­z que contiene todos los videos .mp4 a procesar (todos los videos deben tener la misma duraciÃ³n).
* `<csv_path>`: Un archivo CSV con indicaciones de texto para cada video (ver `demo_test.csv` para el formato).
* `<save_path>` (opcional): DÃ³nde guardar el audio generado. Por defecto es `results/features`.
* `[use-half]` (opcional): Agregue use-half al final para habilitar la extracciÃ³n de caracterÃ­sticas en precisiÃ³n media.

---


### Uso de la Interfaz Web

Para una experiencia interactiva, inicie la interfaz web de Gradio:


```bash
python app.py
```


## ğŸ‹ï¸ Entrena el Modelo

Consulta [`Training.md`](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/docs/Training.md)


---

## ğŸ“ TODO y Planes Futuros
* - [ ] Lanzar un modelo base mÃ¡s potente que cubra mÃºltiples dominios para ofrecer una creaciÃ³n de foley mÃ¡s atractiva e inmersiva
* - [ ] Agregar soporte para modalidades adicionales y tareas posteriores
* - [ ] Lanzar modelos en diferentes escalas
* - [x] Liberar el conjunto de datos AudioCoT y la canalizaciÃ³n automatizada como cÃ³digo abierto
* - [x] Publicar scripts de entrenamiento para los modelos ThinkSound
* - [x] Un README de inicio rÃ¡pido para Windows, amigable para principiantes
---


## ğŸ“„ Licencia

Este proyecto se publica bajo la Licencia Apache 2.0.

> **Nota:**
> El cÃ³digo, los modelos y el conjunto de datos son **solo para fines de investigaciÃ³n y educativos**.
> **NO se permite el uso comercial.**
> Para licencias comerciales, por favor contacte a los autores.

**ğŸ“¦ Componentes de Terceros**

* **Stable Audio Open VAE** (por Stability AI):
  Este repositorio incluye un VAE ajustado de [Stable Audio Open](https://huggingface.co/stabilityai/stable-audio-open-1.0/), licenciado bajo la [Licencia Comunitaria de Stability AI](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/./third_party/LICENSE_StabilityAI.md).
  **El uso comercial y la redistribuciÃ³n requieren permiso previo de Stability AI.**

* ğŸ“˜ **Todo el resto del cÃ³digo y modelos** se publica bajo la Licencia Apache 2.0.

---

## Agradecimientos

Muchas gracias a:

* **stable-audio-tools** (por Stability AI):
Por proporcionar un marco fÃ¡cil de usar para la generaciÃ³n de audio, asÃ­ como el mÃ³dulo VAE y sus pesos.
* **MMAudio**:
  Por la implementaciÃ³n del backbone MM-DiT en el dominio de audio.

---

## ğŸ“– CitaciÃ³n

Si ThinkSound te resulta Ãºtil en tu investigaciÃ³n o trabajo, por favor cita nuestro artÃ­culo:

```bibtex
@misc{liu2025thinksoundchainofthoughtreasoningmultimodal,
      title={ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language Models for Audio Generation and Editing}, 
      author={Huadai Liu and Jialei Wang and Kaicheng Luo and Wen Wang and Qian Chen and Zhou Zhao and Wei Xue},
      year={2025},
      eprint={2506.21448},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/2506.21448}, 
}
```

---

## ğŸ“¬ Contact


âœ¨ Feel free to [open an issue](https://github.com/liuhuadai/ThinkSound/issues) or contact us via email ([liuhuadai@zju.edu.cn](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/mailto:liuhuadai@zju.edu.cn)) if you have any questions or suggestions!



---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-10-04

---