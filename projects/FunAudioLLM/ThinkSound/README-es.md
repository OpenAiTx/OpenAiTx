# ğŸ¶ ThinkSound

<p align="center">
  Â¡Si encuentras Ãºtil este proyecto, una estrella â­ en GitHub serÃ­a muy apreciada!
</p>

<p align="center">
  <a href="https://arxiv.org/pdf/2506.21448">
    <img src="https://img.shields.io/badge/arXiv-2506.21448-b31b1b.svg" alt="arXiv"/>
  </a>
  &nbsp;
  <a href="https://thinksound-project.github.io/">
    <img src="https://img.shields.io/badge/Online%20Demo-ğŸŒ-blue" alt="Online Demo"/>
  </a>
  &nbsp;
  <a href="https://huggingface.co/spaces/FunAudioLLM/ThinkSound">
    <img src="https://img.shields.io/badge/HuggingFace-Spaces-orange?logo=huggingface" alt="Hugging Face"/>
  </a>
  &nbsp;
  <a href="https://modelscope.cn/studios/iic/ThinkSound">
    <img src="https://img.shields.io/badge/ModelScope-åœ¨çº¿ä½“éªŒ-green" alt="ModelScope"/>
  </a>
</p>

---

**ThinkSound** es un framework unificado de generaciÃ³n Any2Audio con flujo de emparejamiento guiado por razonamiento de Cadena de Pensamiento (Chain-of-Thought, CoT).

ImplementaciÃ³n en PyTorch para generaciÃ³n y ediciÃ³n de audio multimodal: genera o edita audio a partir de video, texto y audio, impulsado por razonamiento paso a paso desde Modelos Multimodales de Lenguaje a Gran Escala (MLLMs).

![Teaser](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig1_teaser.png)
---

## ğŸ“° Noticias
- **2025.07** &nbsp; ğŸ”¥Â¡Demo en lÃ­nea disponible en [Hugging Face Spaces](https://huggingface.co/spaces/FunAudioLLM/ThinkSound) y [ModelScope](https://modelscope.cn/studios/iic/ThinkSound) para experiencia interactiva!
- **2025.07** &nbsp; ğŸ”¥Publicados scripts de inferencia e interfaz web;
- **2025.06** &nbsp; ğŸ”¥Â¡ArtÃ­culo de [ThinkSound](https://arxiv.org/pdf/2506.21448) publicado en arXiv!
- **2025.06** &nbsp; ğŸ”¥[Demo en lÃ­nea](http://thinksound-project.github.io/) disponible - Â¡pruÃ©balo ahora!

---

## ğŸš€ CaracterÃ­sticas

- **Any2Audio**: Genera audio desde cualquier modalidad â€” video, texto, audio o sus combinaciones.
- **Video-a-Audio SOTA**: Alcanza resultados de Ãºltima generaciÃ³n en mÃºltiples benchmarks V2A.
- **Razonamiento impulsado por CoT**: Razonamiento de Cadena de Pensamiento para generaciÃ³n de audio composicional y controlable mediante MLLMs.
- **EdiciÃ³n interactiva centrada en objetos**: Refina o edita eventos sonoros especÃ­ficos haciendo clic en objetos visuales o usando instrucciones de texto.
- **Framework unificado**: Un solo modelo base soporta generaciÃ³n, ediciÃ³n y flujo de trabajo interactivo.

---

## âœ¨ DescripciÃ³n del MÃ©todo

ThinkSound descompone la generaciÃ³n y ediciÃ³n de audio en tres etapas interactivas, todas guiadas por razonamiento CoT basado en MLLM:

1. **GeneraciÃ³n Foley:** Genera paisajes sonoros fundamentales, alineados semÃ¡ntica y temporalmente desde video.
2. **Refinamiento centrado en objetos:** Refina o aÃ±ade sonidos para objetos especificados por el usuario mediante clics o regiones en el video.
3. **EdiciÃ³n de audio dirigida:** Modifica el audio generado usando instrucciones en lenguaje natural de alto nivel.

![ThinkSound Overview](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig3_model.png)
<!-- Un conjunto de datos a gran escala anotado con CoT (**AudioCoT**) se utiliza para entrenar tanto el mÃ³dulo de razonamiento como el modelo base unificado de audio.
![AudioCoT Pipeline](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig2_dataset.png) -->

---

## âš¡ Inicio RÃ¡pido

**PreparaciÃ³n del entorno:**
```bash
git clone https://github.com/liuhuadai/ThinkSound.git
cd ThinkSound
pip install -r requirements.txt
conda install -y -c conda-forge 'ffmpeg<7'
# Descargar los pesos preentrenados https://huggingface.co/liuhuadai/ThinkSound al directorio ckpts/
# los pesos del modelo tambiÃ©n pueden descargarse desde https://www.modelscope.cn/models/iic/ThinkSound
git lfs install
git clone https://huggingface.co/liuhuadai/ThinkSound ckpts
```

**Hazlo ejecutable**
```bash
chmod +x scripts/demo.sh
```

**Ejecuta el script**
```bash
./scripts/demo.sh <video_path> <caption> <CoT description>
```


### Uso de la Interfaz Web

Para una experiencia interactiva, inicia la interfaz web Gradio:

```bash
python app.py
```

---
## ğŸ“ TODO

- â˜ Publicar los scripts de entrenamiento para los modelos ThinkSound
- â˜ Liberar el conjunto de datos AudioCoT y la canalizaciÃ³n automatizada
- â˜ Proporcionar documentaciÃ³n detallada y referencia de la API
- â˜ AÃ±adir soporte para modalidades adicionales y tareas posteriores

---

## ğŸ“„ Licencia

Este proyecto se publica bajo la [Licencia Apache 2.0](LICENSE).

> **Nota:**  
> El cÃ³digo, los modelos y el conjunto de datos son **solo para fines de investigaciÃ³n y educativos**.  
> **No se permite el uso comercial.**
>
> Para licencias comerciales, por favor contacte a los autores.

---

## ğŸ“– CitaciÃ³n

Si encuentra Ãºtil ThinkSound en su investigaciÃ³n o trabajo, cite nuestro artÃ­culo:

```bibtex
@misc{liu2025thinksoundchainofthoughtreasoningmultimodal,
      title={ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language Models for Audio Generation and Editing}, 
      author={Huadai Liu and Jialei Wang and Kaicheng Luo and Wen Wang and Qian Chen and Zhou Zhao and Wei Xue},
      year={2025},
      eprint={2506.21448},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/2506.21448}, 
}
```

---

## ğŸ“¬ Contacto

âœ¨ Â¡No dude en [abrir una incidencia](https://github.com/liuhuadai/ThinkSound/issues) o contactarnos por correo electrÃ³nico ([liuhuadai@zju.edu.cn](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/mailto:liuhuadai@zju.edu.cn)) si tiene alguna pregunta o sugerencia!

---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-03

---