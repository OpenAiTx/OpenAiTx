{
  "id": 1,
  "origin": "# 🎶 ThinkSound\n\n<p align=\"center\">\n  If you find this project useful, a star ⭐ on GitHub would be greatly appreciated!\n</p>\n\n<p align=\"center\">\n  <a href=\"https://arxiv.org/pdf/2506.21448\">\n    <img src=\"https://img.shields.io/badge/arXiv-2506.21448-b31b1b.svg\" alt=\"arXiv\"/>\n  </a>\n  &nbsp;\n  <a href=\"https://thinksound-project.github.io/\">\n    <img src=\"https://img.shields.io/badge/Online%20Demo-🌐-blue\" alt=\"Online Demo\"/>\n  </a>\n  &nbsp;\n  <a href=\"https://huggingface.co/spaces/FunAudioLLM/ThinkSound\">\n    <img src=\"https://img.shields.io/badge/HuggingFace-Spaces-orange?logo=huggingface\" alt=\"Hugging Face\"/>\n  </a>\n  &nbsp;\n  <a href=\"https://modelscope.cn/studios/iic/ThinkSound\">\n    <img src=\"https://img.shields.io/badge/ModelScope-在线体验-green\" alt=\"ModelScope\"/>\n  </a>\n</p>\n\n---\n\n**ThinkSound** is a unified Any2Audio generation framework with flow matching guided by Chain-of-Thought (CoT) reasoning.\n\nPyTorch implementation for multimodal audio generation and editing: generate or edit audio from video, text, and audio, powered by step-by-step reasoning from Multimodal Large Language Models (MLLMs).\n\n![Teaser](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig1_teaser.png)\n---\n\n## 📰 News\n- **2025.07** &nbsp; 🔥Online demo on [Hugging Face Spaces](https://huggingface.co/spaces/FunAudioLLM/ThinkSound) and [ModelScope](https://modelscope.cn/studios/iic/ThinkSound) for interactive experience!\n- **2025.07** &nbsp; 🔥Released inference scripts and web interface; \n- **2025.06** &nbsp; 🔥[ThinkSound paper](https://arxiv.org/pdf/2506.21448) released on arXiv!\n- **2025.06** &nbsp; 🔥[Online Demo](http://thinksound-project.github.io/) is live - try it now!\n\n---\n\n## 🚀 Features\n\n- **Any2Audio**: Generate audio from arbitrary modalities — video, text, audio, or their combinations.\n- **Video-to-Audio SOTA**: Achieves state-of-the-art results on multiple V2A benchmarks.\n- **CoT-Driven Reasoning**: Chain-of-Thought reasoning for compositional and controllable audio generation via MLLMs.\n- **Interactive Object-centric Editing**: Refine or edit specific sound events by clicking on visual objects or using text instructions.\n- **Unified Framework**: One foundation model supports generation, editing, and interactive workflow.\n\n---\n\n## ✨ Method Overview\n\nThinkSound decomposes audio generation and editing into three interactive stages, all guided by MLLM-based Chain-of-Thought (CoT) reasoning:\n\n1. **Foley Generation:** Generate foundational, semantically and temporally aligned soundscapes from video.\n2. **Object-Centric Refinement:** Refine or add sounds for user-specified objects via clicks or regions in the video.\n3. **Targeted Audio Editing:** Modify generated audio using high-level natural language instructions.\n\n![ThinkSound Overview](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig3_model.png)\n<!-- A large-scale CoT-annotated dataset (**AudioCoT**) is used to train both the reasoning module and the unified audio foundation model.\n![AudioCoT Pipeline](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig2_dataset.png) -->\n\n---\n\n## ⚡ Quick Start\n\n**Environment Preparation:**\n```bash\ngit clone https://github.com/liuhuadai/ThinkSound.git\ncd ThinkSound\npip install -r requirements.txt\nconda install -y -c conda-forge 'ffmpeg<7'\n# Download pretrained weights https://huggingface.co/liuhuadai/ThinkSound to Directory ckpts/\n# model weights can be also downloaded from https://www.modelscope.cn/models/iic/ThinkSound\ngit lfs install\ngit clone https://huggingface.co/liuhuadai/ThinkSound ckpts\n```\n\n**Make it executable**\n```bash\nchmod +x scripts/demo.sh\n```\n\n**Run the script**\n```bash\n./scripts/demo.sh <video_path> <caption> <CoT description>\n```\n\n\n### Web Interface Usage\n\nFor an interactive experience, launch the Gradio web interface:\n\n```bash\npython app.py\n```\n\n---\n",
  "origin_sha": "vYi4X/38TyDFihDOfSBGZIgKnu2bd0a73Iz5+m+7pd0=",
  "translate": "# 🎶 ThinkSound\n\n<p align=\"center\">\n  如果您觉得这个项目有用，非常感谢在 GitHub 上点个星 ⭐！\n</p>\n\n<p align=\"center\">\n  <a href=\"https://arxiv.org/pdf/2506.21448\">\n    <img src=\"https://img.shields.io/badge/arXiv-2506.21448-b31b1b.svg\" alt=\"arXiv\"/>\n  </a>\n  &nbsp;\n  <a href=\"https://thinksound-project.github.io/\">\n    <img src=\"https://img.shields.io/badge/Online%20Demo-🌐-blue\" alt=\"Online Demo\"/>\n  </a>\n  &nbsp;\n  <a href=\"https://huggingface.co/spaces/FunAudioLLM/ThinkSound\">\n    <img src=\"https://img.shields.io/badge/HuggingFace-Spaces-orange?logo=huggingface\" alt=\"Hugging Face\"/>\n  </a>\n  &nbsp;\n  <a href=\"https://modelscope.cn/studios/iic/ThinkSound\">\n    <img src=\"https://img.shields.io/badge/ModelScope-在线体验-green\" alt=\"ModelScope\"/>\n  </a>\n</p>\n\n---\n\n**ThinkSound** 是一个统一的 Any2Audio 生成框架，通过链式思维（Chain-of-Thought, CoT）推理进行流匹配指导。\n\n基于 PyTorch 的多模态音频生成与编辑实现：可基于视频、文本、音频及其组合，生成或编辑音频，底层由多模态大语言模型（MLLMs）逐步推理驱动。\n\n![Teaser](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig1_teaser.png)\n---\n\n## 📰 最新动态\n- **2025.07** &nbsp; 🔥[Hugging Face Spaces](https://huggingface.co/spaces/FunAudioLLM/ThinkSound) 及 [ModelScope](https://modelscope.cn/studios/iic/ThinkSound) 在线交互体验发布！\n- **2025.07** &nbsp; 🔥推理脚本及网页界面已发布；\n- **2025.06** &nbsp; 🔥[ThinkSound 论文](https://arxiv.org/pdf/2506.21448) 已在 arXiv 发布！\n- **2025.06** &nbsp; 🔥[在线演示](http://thinksound-project.github.io/) 上线，欢迎体验！\n\n---\n\n## 🚀 主要特性\n\n- **Any2Audio**：支持任意模态（视频、文本、音频或其组合）生成音频。\n- **视频转音频 SOTA**：在多个 V2A 基准上取得最新最优结果。\n- **CoT 驱动推理**：基于链式思维推理，实现可组合、可控的音频生成。\n- **交互式面向对象编辑**：通过点击视觉对象或文本指令，细化或编辑特定声音事件。\n- **统一框架**：单一基础模型，支持生成、编辑与交互式工作流。\n\n---\n\n## ✨ 方法概述\n\nThinkSound 将音频生成与编辑分为三个交互式阶段，均由基于 MLLM 的链式思维（CoT）推理指导：\n\n1. **拟音生成（Foley Generation）：** 从视频生成基础、语义与时序对齐的声景。\n2. **面向对象的细化：** 通过点击或选择视频中的对象区域，对用户指定对象的声音进行细化或添加。\n3. **定向音频编辑：** 使用高级自然语言指令对生成音频进行修改。\n\n![ThinkSound Overview](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig3_model.png)\n<!-- 大规模 CoT 注释数据集（**AudioCoT**）用于训练推理模块和统一音频基础模型。\n![AudioCoT Pipeline](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig2_dataset.png) -->\n\n---\n\n## ⚡ 快速开始\n\n**环境准备：**\n```bash\ngit clone https://github.com/liuhuadai/ThinkSound.git\ncd ThinkSound\npip install -r requirements.txt\nconda install -y -c conda-forge 'ffmpeg<7'\n# 下载预训练权重 https://huggingface.co/liuhuadai/ThinkSound 到目录 ckpts/\n# 模型权重也可从 https://www.modelscope.cn/models/iic/ThinkSound 下载\ngit lfs install\ngit clone https://huggingface.co/liuhuadai/ThinkSound ckpts\n```\n\n**赋予可执行权限**\n```bash\nchmod +x scripts/demo.sh\n```\n\n**运行脚本**\n```bash\n./scripts/demo.sh <video_path> <caption> <CoT description>\n```\n\n\n### 网页界面使用\n\n如需交互体验，可启动 Gradio 网页界面：\n\n```bash\npython app.py\n```\n\n---",
  "status": "ok"
}