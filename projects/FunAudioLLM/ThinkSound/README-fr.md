<h1 align="center">ThinkSound</h1>

<p align="center">
  ğŸŒ
  <a href="https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=en">English</a> |
  <a href="https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=zh-CN">ç®€ä½“ä¸­æ–‡</a> |
  <a href="https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=zh-TW">ç¹é«”ä¸­æ–‡</a> |
  <a href="https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=es">EspaÃ±ol</a> |
  <a href="https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=fr">FranÃ§ais</a> |
  <a href="https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=ja">æ—¥æœ¬èª</a>
  
</p>
<p align="center">
  <img src="https://img.shields.io/badge/NeurIPS 2025-Main Conference-blue.svg" alt="NeurIPS 2025"/>
<p align="center">
  <a href="https://arxiv.org/pdf/2506.21448">
    <img src="https://img.shields.io/badge/arXiv-2506.21448-b31b1b.svg" alt="arXiv"/>
  </a>
  &nbsp;
  <a href="https://thinksound-project.github.io/">
    <img src="https://img.shields.io/badge/Online%20Demo-ğŸŒ-blue" alt="Online Demo"/>
  </a>
  &nbsp;
  <a href="https://huggingface.co/spaces/FunAudioLLM/ThinkSound">
    <img src="https://img.shields.io/badge/HuggingFace-Spaces-orange?logo=huggingface" alt="Hugging Face"/>
  </a>
  &nbsp;
  <a href="https://modelscope.cn/studios/iic/ThinkSound">
    <img src="https://img.shields.io/badge/ModelScope-åœ¨çº¿ä½“éªŒ-green" alt="ModelScope"/>
  </a>
</p>

<p align="center">
  Si vous trouvez ce projet utile,<br>
  une Ã©toile â­ sur GitHub serait trÃ¨s apprÃ©ciÃ©e !
</p>

---

**ThinkSound** est un cadre unifiÃ© de gÃ©nÃ©ration Any2Audio avec un appariement de flux guidÃ© par le raisonnement Chain-of-Thought (CoT).

ImplÃ©mentation PyTorch pour la gÃ©nÃ©ration et lâ€™Ã©dition audio multimodales : gÃ©nÃ©rez ou Ã©ditez de lâ€™audio Ã  partir de vidÃ©o, texte et audio, propulsÃ© par un raisonnement Ã©tape par Ã©tape issu des Grands ModÃ¨les de Langage Multimodaux (MLLMs).

![Teaser](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig1_teaser.png)
---

## ğŸ“° ActualitÃ©s
- **2025.11.25** &nbsp; ğŸ”¥[DÃ©mo PrismAudio en ligne](http://prismaudio-project.github.io/) disponible - essayez-la dÃ¨s maintenant !
- **2025.11.25** &nbsp; ğŸ”¥[Article PrismAudio](https://arxiv.org/pdf/2511.18833) publiÃ© sur arXiv, premier cadre CoT-RL multidimensionnel pour la gÃ©nÃ©ration VidÃ©o-vers-Audio !
- **2025.09.19** &nbsp; ğŸ‰ ThinkSound a Ã©tÃ© acceptÃ© Ã  la **confÃ©rence principale NeurIPS 2025** !
- **2025.09.01** &nbsp; Notre jeu de donnÃ©es AudioCoT est dÃ©sormais open source et disponible sur [Hugging Face](https://huggingface.co/datasets/liuhuadai/AudioCoT) !
- **2025.07.17** &nbsp; ğŸ§  Affinage activÃ© : code dâ€™entraÃ®nement et dâ€™affinage maintenant public, avec instructions claires pour personnaliser et Ã©tendre ThinkSound avec vos propres donnÃ©es.
- **2025.07.15** &nbsp; ğŸ“¦ Installation et utilisation simplifiÃ©es : dÃ©pendances sur PyPI pour une configuration multiplateforme facile ; scripts Windows `.bat` pour automatiser la crÃ©ation dâ€™environnement et lâ€™exÃ©cution des scripts.
- **2025.07.08** &nbsp;Â  ğŸ”§ Mise Ã  jour majeure : modÃ¨le allÃ©gÃ© et optimisation de la mÃ©moire et de lâ€™utilisation du GPU, prise en charge de la gÃ©nÃ©ration audio Ã  haut dÃ©bit Ã  grande Ã©chelle !
- **2025.07.01** &nbsp; DÃ©mo en ligne sur [Hugging Face Spaces](https://huggingface.co/spaces/FunAudioLLM/ThinkSound) et [ModelScope](https://modelscope.cn/studios/iic/ThinkSound) pour une expÃ©rience interactive !
- **2025.07.01** &nbsp; Scripts dâ€™infÃ©rence et interface web publiÃ©s ;
- **2025.06** &nbsp; [Article ThinkSound](https://arxiv.org/pdf/2506.21448) publiÃ© sur arXiv !
- **2025.06** &nbsp; [DÃ©mo en ligne](http://thinksound-project.github.io/) disponible - essayez-la dÃ¨s maintenant !

---


## ğŸš€ FonctionnalitÃ©s

- **Any2Audio** : GÃ©nÃ©rez de lâ€™audio Ã  partir de nâ€™importe quelle modalitÃ© â€” vidÃ©o, texte, audio ou leurs combinaisons.
- **Video-to-Audio SOTA** : Atteint lâ€™Ã©tat de lâ€™art sur de nombreux benchmarks V2A.
- **Raisonnement CoT** : Raisonnement par ChaÃ®ne-de-PensÃ©e pour une gÃ©nÃ©ration audio compositionnelle et contrÃ´lable via MLLMs.
- **Ã‰dition interactive centrÃ©e objet** : Affinez ou Ã©ditez des Ã©vÃ©nements sonores spÃ©cifiques par clic sur des objets visuels ou instructions textuelles.
- **Cadre unifiÃ©** : Un modÃ¨le fondamental prend en charge la gÃ©nÃ©ration, lâ€™Ã©dition et les flux interactifs.

---

## âœ¨ Vue dâ€™ensemble de la mÃ©thode

ThinkSound dÃ©compose la gÃ©nÃ©ration et lâ€™Ã©dition audio en trois Ã©tapes interactives, toutes guidÃ©es par un raisonnement CoT basÃ© sur les MLLMs :

1. **GÃ©nÃ©ration Foley :** GÃ©nÃ©rer des paysages sonores fondamentaux, alignÃ©s sÃ©mantiquement et temporellement Ã  partir de la vidÃ©o.
2. **Affinage centrÃ© objet :** Affiner ou ajouter des sons pour des objets spÃ©cifiÃ©s par lâ€™utilisateur via des clics ou des rÃ©gions dans la vidÃ©o.
3. **Ã‰dition audio ciblÃ©e :** Modifier lâ€™audio gÃ©nÃ©rÃ© Ã  lâ€™aide dâ€™instructions en langage naturel de haut niveau.

![AperÃ§u ThinkSound](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig3_model.png)
<!-- Un jeu de donnÃ©es annotÃ© Ã  grande Ã©chelle CoT (**AudioCoT**) est utilisÃ© pour entraÃ®ner Ã  la fois le module de raisonnement et le modÃ¨le audio fondation unifiÃ©.
![Pipeline AudioCoT](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig2_dataset.png) -->

---

## âš¡ DÃ©marrage rapide

**PrÃ©paration de l'environnement :**
```bash
git clone https://github.com/liuhuadai/ThinkSound.git
cd ThinkSound
conda create -n thinksound python=3.10
conda activate thinksound
pip install thinksound
conda install -y -c conda-forge 'ffmpeg<7'
# Download pretrained weights https://huggingface.co/liuhuadai/ThinkSound to Directory ckpts/
# model weights can be also downloaded from https://www.modelscope.cn/models/iic/ThinkSound
git lfs install
git clone https://huggingface.co/liuhuadai/ThinkSound ckpts
# To improve inference and training speed, you may optionally install a FlashAttention backend compatible with your system and PyTorch version.
```
> âœ… **Astuce Windows :**  
> Les utilisateurs de Windows peuvent simplement exÃ©cuter `setup_windows.bat` (ou double-cliquer dessus) pour crÃ©er automatiquement l'environnement conda, installer toutes les dÃ©pendances (y compris FFmpeg) et tÃ©lÃ©charger le modÃ¨le prÃ©-entraÃ®nÃ© â€” aucune configuration manuelle n'est requise.  
> Assurez-vous que `conda` et `git` sont installÃ©s et disponibles dans le PATH de votre systÃ¨me avant d'exÃ©cuter le script.


### â–¶ï¸ ExÃ©cuter la dÃ©mo

#### **Linux/macOS**


```bash
chmod +x scripts/demo.sh
./scripts/demo.sh <path-to-your-demo-video> <title> <CoT description> [use-half]
```
#### **Windows**

Vous pouvez utiliser le script `.bat` fourni Ã  la placeÂ :


```bash
.\scripts\demo.bat <path-to-your-demo-video> <title> <CoT description> [use-half]
```
**Remarque :**

* `<chemin-vers-votre-vidÃ©o-dÃ©mo>` : Le chemin vers une seule vidÃ©o
* `[utiliser-half]` (optionnel) : Ajoutez utiliser-half Ã  la fin pour activer lâ€™extraction de caractÃ©ristiques en demi-prÃ©cision.

---

### ğŸ“¦ InfÃ©rence par lot

#### **Linux/macOS**


```bash
chmod +x scripts/eval_batch.sh
./scripts/eval_batch.sh <video_path> <csv_path> <save_path (optional)> [use-half]
```
#### **Windows**

Utilisez le script `.bat` Ã©quivalentÂ :


```bash
.\scripts\eval_batch.bat <video_path> <csv_path> <save_path (optional)> [use-half]
```
**Remarque :**

* `<video_path>` : Chemin vers le rÃ©pertoire racine contenant toutes les vidÃ©os .mp4 Ã  traiter (toutes les vidÃ©os doivent avoir la mÃªme durÃ©e).
* `<csv_path>` : Un fichier CSV avec les invites textuelles pour chaque vidÃ©o (voir `demo_test.csv` pour le format).
* `<save_path>` (optionnel) : Emplacement oÃ¹ enregistrer l'audio gÃ©nÃ©rÃ©. Par dÃ©faut `results/features`.
* `[use-half]` (optionnel) : Ajoutez use-half Ã  la fin pour activer l'extraction de caractÃ©ristiques en demi-prÃ©cision.

---


### Utilisation de lâ€™interface Web

Pour une expÃ©rience interactive, lancez lâ€™interface web Gradio :


```bash
python app.py
```


## ğŸ‹ï¸ EntraÃ®nez le modÃ¨le

Voir [`Training.md`](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/docs/Training.md)


---

## ğŸ“ Ã€ faire & Plans futurs
* - [ ] Publier un modÃ¨le de base plus puissant couvrant plusieurs domaines pour offrir une crÃ©ation de foley plus engageante et immersive
* - [ ] Ajouter la prise en charge de modalitÃ©s supplÃ©mentaires et de tÃ¢ches aval
* - [ ] Publier des modÃ¨les Ã  diffÃ©rentes Ã©chelles
* - [x] Open source du jeu de donnÃ©es AudioCoT et du pipeline automatisÃ©
* - [x] Publication des scripts d'entraÃ®nement pour les modÃ¨les ThinkSound
* - [x] Un README de dÃ©marrage rapide convivial pour Windows
---


## ğŸ“„ Licence

Ce projet est publiÃ© sous la licence Apache 2.0.

> **Remarque :**
> Le code, les modÃ¨les et le jeu de donnÃ©es sont **uniquement destinÃ©s Ã  la recherche et Ã  l'Ã©ducation**.
> **L'utilisation commerciale n'est PAS autorisÃ©e.**
> Pour une licence commerciale, veuillez contacter les auteurs.

**ğŸ“¦ Composants tiers**

* **Stable Audio Open VAE** (par Stability AI) :
  Ce dÃ©pÃ´t inclut un VAE affinÃ© issu de [Stable Audio Open](https://huggingface.co/stabilityai/stable-audio-open-1.0/), sous licence [Stability AI Community License](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/./third_party/LICENSE_StabilityAI.md).
  **L'utilisation commerciale et la redistribution nÃ©cessitent l'autorisation prÃ©alable de Stability AI.**

* ğŸ“˜ **Tout le reste du code et des modÃ¨les** est publiÃ© sous la licence Apache 2.0.

---

## Remerciements

Un grand merci Ã  :

* **stable-audio-tools** (par Stability AI) :
Pour avoir fourni un cadre facile Ã  utiliser pour la gÃ©nÃ©ration audio, ainsi que le module VAE et les poids.
* **MMAudio** :
  Pour lâ€™implÃ©mentation de lâ€™architecture MM-DiT dans le domaine audio.

---

## ğŸ“– Citation

Si ThinkSound vous est utile dans vos recherches ou travaux, veuillez citer notre article :

```bibtex
@misc{liu2025thinksoundchainofthoughtreasoningmultimodal,
      title={ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language Models for Audio Generation and Editing}, 
      author={Huadai Liu and Jialei Wang and Kaicheng Luo and Wen Wang and Qian Chen and Zhou Zhao and Wei Xue},
      year={2025},
      eprint={2506.21448},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/2506.21448}, 
}
```

---

## ğŸ“¬ Contact


âœ¨ Feel free to [open an issue](https://github.com/liuhuadai/ThinkSound/issues) or contact us via email ([liuhuadai@zju.edu.cn](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/mailto:liuhuadai@zju.edu.cn)) if you have any questions or suggestions!




---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2026-01-07

---