# ğŸ¶ ThinkSound

<p align="center">
  Wenn Sie dieses Projekt nÃ¼tzlich finden, wÃ¼rden wir uns sehr Ã¼ber einen Stern â­ auf GitHub freuen!
</p>

<p align="center">
  <a href="https://arxiv.org/pdf/2506.21448">
    <img src="https://img.shields.io/badge/arXiv-2506.21448-b31b1b.svg" alt="arXiv"/>
  </a>
  &nbsp;
  <a href="https://thinksound-project.github.io/">
    <img src="https://img.shields.io/badge/Online%20Demo-ğŸŒ-blue" alt="Online Demo"/>
  </a>
  &nbsp;
  <a href="https://huggingface.co/spaces/FunAudioLLM/ThinkSound">
    <img src="https://img.shields.io/badge/HuggingFace-Spaces-orange?logo=huggingface" alt="Hugging Face"/>
  </a>
  &nbsp;
  <a href="https://modelscope.cn/studios/iic/ThinkSound">
    <img src="https://img.shields.io/badge/ModelScope-åœ¨çº¿ä½“éªŒ-green" alt="ModelScope"/>
  </a>
</p>

---

**ThinkSound** ist ein einheitliches Any2Audio-Generierungsframework mit Flow Matching, das durch Chain-of-Thought (CoT)-Reasoning gefÃ¼hrt wird.

PyTorch-Implementierung fÃ¼r multimodale Audiogenerierung und -bearbeitung: Erzeugen oder Bearbeiten von Audio aus Video, Text und Audio, unterstÃ¼tzt durch schrittweises Reasoning von Multimodalen Large Language Models (MLLMs).

![Teaser](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig1_teaser.png)
---

## ğŸ“° Neuigkeiten
- **2025.07** &nbsp; ğŸ”¥Online-Demo auf [Hugging Face Spaces](https://huggingface.co/spaces/FunAudioLLM/ThinkSound) und [ModelScope](https://modelscope.cn/studios/iic/ThinkSound) fÃ¼r interaktive Erfahrungen!
- **2025.07** &nbsp; ğŸ”¥Inference-Skripte und Web-OberflÃ¤che verÃ¶ffentlicht;
- **2025.06** &nbsp; ğŸ”¥[ThinkSound Paper](https://arxiv.org/pdf/2506.21448) auf arXiv verÃ¶ffentlicht!
- **2025.06** &nbsp; ğŸ”¥[Online-Demo](http://thinksound-project.github.io/) ist live â€“ jetzt ausprobieren!

---

## ğŸš€ Funktionen

- **Any2Audio**: Erzeuge Audio aus beliebigen ModalitÃ¤ten â€“ Video, Text, Audio oder deren Kombinationen.
- **Video-zu-Audio SOTA**: Erreicht den Stand der Technik auf mehreren V2A-Benchmarks.
- **CoT-gesteuertes Reasoning**: Chain-of-Thought-Reasoning fÃ¼r kompositionelle und kontrollierbare Audiogenerierung via MLLMs.
- **Interaktive objektzentrierte Bearbeitung**: Verfeinere oder bearbeite spezifische Klangereignisse durch Klicken auf visuelle Objekte oder durch Textanweisungen.
- **Einheitliches Framework**: Ein Foundation-Modell unterstÃ¼tzt Generierung, Bearbeitung und interaktiven Workflow.

---

## âœ¨ MethodenÃ¼bersicht

ThinkSound zerlegt die Audiogenerierung und -bearbeitung in drei interaktive Phasen, die alle durch MLLM-basiertes Chain-of-Thought (CoT)-Reasoning gefÃ¼hrt werden:

1. **Foley-Generierung:** Generiere grundlegende, semantisch und zeitlich ausgerichtete Klanglandschaften aus Video.
2. **Objektzentrierte Verfeinerung:** Verfeinere oder fÃ¼ge KlÃ¤nge fÃ¼r benutzerspezifizierte Objekte durch Klicks oder Regionen im Video hinzu.
3. **Gezielte Audiobearbeitung:** Bearbeite generiertes Audio mithilfe von hochrangigen Anweisungen in natÃ¼rlicher Sprache.

![ThinkSound Ãœbersicht](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig3_model.png)
<!-- Ein groÃŸ angelegter CoT-annotierter Datensatz (**AudioCoT**) wird zum Training sowohl des Reasoning-Moduls als auch des einheitlichen Audio-Foundation-Modells verwendet.
![AudioCoT Pipeline](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig2_dataset.png) -->

---

## âš¡ Schnellstart

**Umgebung vorbereiten:**
```bash
git clone https://github.com/liuhuadai/ThinkSound.git
cd ThinkSound
pip install -r requirements.txt
conda install -y -c conda-forge 'ffmpeg<7'
# Vorgefertigte Gewichte herunterladen https://huggingface.co/liuhuadai/ThinkSound in das Verzeichnis ckpts/
# Modellgewichte kÃ¶nnen auch von https://www.modelscope.cn/models/iic/ThinkSound heruntergeladen werden
git lfs install
git clone https://huggingface.co/liuhuadai/ThinkSound ckpts
```

**AusfÃ¼hrbar machen**
```bash
chmod +x scripts/demo.sh
```

**Skript ausfÃ¼hren**
```bash
./scripts/demo.sh <video_path> <caption> <CoT description>
```

### Nutzung der Web-OberflÃ¤che

FÃ¼r ein interaktives Erlebnis starte die Gradio-WeboberflÃ¤che:

```bash
python app.py
```

---
## ğŸ“ TODO

- â˜ VerÃ¶ffentlichung der Trainingsskripte fÃ¼r ThinkSound-Modelle
- â˜ Open-Source des AudioCoT-Datensatzes und der automatisierten Pipeline
- â˜ Bereitstellung ausfÃ¼hrlicher Dokumentation und API-Referenz
- â˜ UnterstÃ¼tzung weiterer ModalitÃ¤ten und nachgelagerter Aufgaben hinzufÃ¼gen

---

## ğŸ“„ Lizenz

Dieses Projekt wird unter der [Apache 2.0 Lizenz](LICENSE) verÃ¶ffentlicht.

> **Hinweis:**  
> Der Code, die Modelle und der Datensatz sind **ausschlieÃŸlich fÃ¼r Forschungs- und Bildungszwecke** bestimmt.  
> **Kommerzielle Nutzung ist NICHT gestattet.**
>
> FÃ¼r kommerzielle Lizenzen kontaktieren Sie bitte die Autoren.

---

## ğŸ“– Zitation

Wenn Sie ThinkSound in Ihrer Forschung oder Arbeit verwenden, zitieren Sie bitte unsere Publikation:

```bibtex
@misc{liu2025thinksoundchainofthoughtreasoningmultimodal,
      title={ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language Models for Audio Generation and Editing}, 
      author={Huadai Liu and Jialei Wang and Kaicheng Luo und Wen Wang und Qian Chen und Zhou Zhao und Wei Xue},
      year={2025},
      eprint={2506.21448},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/2506.21448}, 
}
```

---

## ğŸ“¬ Kontakt

âœ¨ Ã–ffnen Sie gerne [ein Issue](https://github.com/liuhuadai/ThinkSound/issues) oder kontaktieren Sie uns per E-Mail ([liuhuadai@zju.edu.cn](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/mailto:liuhuadai@zju.edu.cn)), wenn Sie Fragen oder Anregungen haben!

---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-03

---