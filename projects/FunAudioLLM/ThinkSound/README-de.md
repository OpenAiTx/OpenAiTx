<h1 align="center">ThinkSound</h1>

<p align="center">
  ğŸŒ
  <a href="https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=en">English</a> |
  <a href="https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=zh-CN">ç®€ä½“ä¸­æ–‡</a> |
  <a href="https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=zh-TW">ç¹é«”ä¸­æ–‡</a> |
  <a href="https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=es">EspaÃ±ol</a> |
  <a href="https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=fr">FranÃ§ais</a> |
  <a href="https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=ja">æ—¥æœ¬èª</a>
  
</p>
<p align="center">
  <img src="https://img.shields.io/badge/NeurIPS 2025-Main Conference-blue.svg" alt="NeurIPS 2025"/>
<p align="center">
  <a href="https://arxiv.org/pdf/2506.21448">
    <img src="https://img.shields.io/badge/arXiv-2506.21448-b31b1b.svg" alt="arXiv"/>
  </a>
  &nbsp;
  <a href="https://thinksound-project.github.io/">
    <img src="https://img.shields.io/badge/Online%20Demo-ğŸŒ-blue" alt="Online Demo"/>
  </a>
  &nbsp;
  <a href="https://huggingface.co/spaces/FunAudioLLM/ThinkSound">
    <img src="https://img.shields.io/badge/HuggingFace-Spaces-orange?logo=huggingface" alt="Hugging Face"/>
  </a>
  &nbsp;
  <a href="https://modelscope.cn/studios/iic/ThinkSound">
    <img src="https://img.shields.io/badge/ModelScope-åœ¨çº¿ä½“éªŒ-green" alt="ModelScope"/>
  </a>
</p>

<p align="center">
  Wenn Sie dieses Projekt nÃ¼tzlich finden,<br>
  wÃ¼rden wir uns sehr Ã¼ber einen Stern â­ auf GitHub freuen!
</p>

---

**ThinkSound** ist ein einheitliches Any2Audio-Generierungsframework mit Flow Matching, das durch Chain-of-Thought (CoT)-Reasoning gesteuert wird.

PyTorch-Implementierung fÃ¼r multimodale Audiogenerierung und -bearbeitung: Erstellen oder Bearbeiten von Audio aus Video, Text und Audio, unterstÃ¼tzt durch schrittweise Argumentation von Multimodalen GroÃŸen Sprachmodellen (MLLMs).

![Teaser](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig1_teaser.png)
---

## ğŸ“° Neuigkeiten
- **2025.11.25** &nbsp; ğŸ”¥[Online PrismAudio Demo](http://prismaudio-project.github.io/) ist live â€“ jetzt ausprobieren!
- **2025.11.25** &nbsp; ğŸ”¥[PrismAudio Paper](https://arxiv.org/pdf/2511.18833) verÃ¶ffentlicht auf arXiv, das erste multidimensionale CoT-RL-Framework fÃ¼r Video-zu-Audio-Generierung!
- **2025.09.19** &nbsp; ğŸ‰ ThinkSound wurde auf der **NeurIPS 2025 Hauptkonferenz** angenommen!
- **2025.09.01** &nbsp; Unser AudioCoT-Datensatz ist jetzt quelloffen und auf [Hugging Face](https://huggingface.co/datasets/liuhuadai/AudioCoT) verfÃ¼gbar!
- **2025.07.17** &nbsp; ğŸ§  Finetuning aktiviert: Trainings- und Finetuning-Code ist jetzt Ã¶ffentlich verfÃ¼gbar, mit klaren Anweisungen zur individuellen Anpassung und Erweiterung von ThinkSound mit eigenen Daten.
- **2025.07.15** &nbsp; ğŸ“¦ Vereinfachte Installation und Benutzerfreundlichkeit: AbhÃ¤ngigkeiten auf PyPI fÃ¼r einfache plattformÃ¼bergreifende Einrichtung; Windows-`.bat`-Skripte automatisieren die Erstellung von Umgebungen und das AusfÃ¼hren von Skripten.
- **2025.07.08** &nbsp;Â  ğŸ”§ GroÃŸes Update: Modell verschlankt und optimierter Speicher- sowie GPU-Verbrauch, unterstÃ¼tzt jetzt hochskalierbare Audiogenerierung!
- **2025.07.01** &nbsp; Online-Demo auf [Hugging Face Spaces](https://huggingface.co/spaces/FunAudioLLM/ThinkSound) und [ModelScope](https://modelscope.cn/studios/iic/ThinkSound) fÃ¼r interaktive Erfahrungen!
- **2025.07.01** &nbsp; VerÃ¶ffentlichte Inferenz-Skripte und WeboberflÃ¤che;
- **2025.06** &nbsp; [ThinkSound Paper](https://arxiv.org/pdf/2506.21448) auf arXiv erschienen!
- **2025.06** &nbsp; [Online-Demo](http://thinksound-project.github.io/) ist live â€“ jetzt ausprobieren!

---


## ğŸš€ Funktionen

- **Any2Audio**: Generiere Audio aus beliebigen ModalitÃ¤ten â€“ Video, Text, Audio oder deren Kombinationen.
- **Video-zu-Audio SOTA**: Erzielt State-of-the-Art-Ergebnisse auf mehreren V2A-Benchmarks.
- **CoT-gesteuertes Reasoning**: Chain-of-Thought-Reasoning fÃ¼r kompositionelle und steuerbare Audiogenerierung Ã¼ber MLLMs.
- **Interaktive objektzentrierte Bearbeitung**: Verfeinere oder bearbeite spezifische Klangereignisse durch Klicken auf visuelle Objekte oder mit Textanweisungen.
- **Einheitliches Framework**: Ein Grundmodell unterstÃ¼tzt Generierung, Bearbeitung und interaktiven Workflow.

---

## âœ¨ MethodenÃ¼berblick

ThinkSound unterteilt Audiogenerierung und -bearbeitung in drei interaktive Phasen, alle gesteuert durch MLLM-basiertes Chain-of-Thought (CoT)-Reasoning:

1. **Foley-Generierung:** Erzeuge grundlegende, semantisch und zeitlich ausgerichtete Klanglandschaften aus Video.
2. **Objektzentrierte Verfeinerung:** Verfeinere oder fÃ¼ge KlÃ¤nge fÃ¼r benutzerspezifizierte Objekte durch Klicks oder Regionen im Video hinzu.
3. **Gezielte Audiobearbeitung:** Modifiziere generiertes Audio mit hochrangigen natÃ¼rlichen Sprachinstruktionen.

![ThinkSound Ãœbersicht](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig3_model.png)
<!-- Ein groÃŸ angelegter, mit CoT annotierter Datensatz (**AudioCoT**) wird sowohl zum Trainieren des Reasoning-Moduls als auch des einheitlichen Audio-Foundation-Modells verwendet.
![AudioCoT Pipeline](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig2_dataset.png) -->

---

## âš¡ Schnellstart

**Umgebungsvorbereitung:**
```bash
git clone https://github.com/liuhuadai/ThinkSound.git
cd ThinkSound
conda create -n thinksound python=3.10
conda activate thinksound
pip install thinksound
conda install -y -c conda-forge 'ffmpeg<7'
# Download pretrained weights https://huggingface.co/liuhuadai/ThinkSound to Directory ckpts/
# model weights can be also downloaded from https://www.modelscope.cn/models/iic/ThinkSound
git lfs install
git clone https://huggingface.co/liuhuadai/ThinkSound ckpts
# To improve inference and training speed, you may optionally install a FlashAttention backend compatible with your system and PyTorch version.
```
> âœ… **Windows-Tipp:**  
> Windows-Benutzer kÃ¶nnen einfach `setup_windows.bat` ausfÃ¼hren (oder doppelt anklicken), um automatisch die Conda-Umgebung zu erstellen, alle AbhÃ¤ngigkeiten zu installieren (einschlieÃŸlich FFmpeg) und das vortrainierte Modell herunterzuladen â€” keine manuelle Einrichtung erforderlich.  
> Stellen Sie sicher, dass `conda` und `git` installiert sind und sich in Ihrem System-PATH befinden, bevor Sie das Skript ausfÃ¼hren.


### â–¶ï¸ Demo ausfÃ¼hren

#### **Linux/macOS**


```bash
chmod +x scripts/demo.sh
./scripts/demo.sh <path-to-your-demo-video> <title> <CoT description> [use-half]
```
#### **Windows**

Stattdessen kÃ¶nnen Sie das bereitgestellte `.bat`-Skript verwenden:


```bash
.\scripts\demo.bat <path-to-your-demo-video> <title> <CoT description> [use-half]
```
**Hinweis:**

* `<path-to-your-demo-video>`: Der Pfad zu einem einzelnen Video
* `[use-half]` (optional): FÃ¼gen Sie use-half am Ende hinzu, um die Extraktion von Merkmalen mit halber Genauigkeit zu aktivieren.

---

### ğŸ“¦ Stapel-Inferenz

#### **Linux/macOS**


```bash
chmod +x scripts/eval_batch.sh
./scripts/eval_batch.sh <video_path> <csv_path> <save_path (optional)> [use-half]
```
#### **Windows**

Verwenden Sie das entsprechende `.bat`-Skript:


```bash
.\scripts\eval_batch.bat <video_path> <csv_path> <save_path (optional)> [use-half]
```
**Hinweis:**

* `<video_path>`: Pfad zum Stammverzeichnis, das alle zu verarbeitenden .mp4-Videos enthÃ¤lt (alle Videos mÃ¼ssen die gleiche Dauer haben).
* `<csv_path>`: Eine CSV-Datei mit Text-Inputs fÃ¼r jedes Video (siehe `demo_test.csv` fÃ¼r das Format).
* `<save_path>` (optional): Speicherort fÃ¼r die generierte Audiodatei. StandardmÃ¤ÃŸig `results/features`.
* `[use-half]` (optional): FÃ¼gen Sie use-half am Ende hinzu, um die HalbprÃ¤zisions-Feature-Extraktion zu aktivieren.

---


### Nutzung der WeboberflÃ¤che

FÃ¼r eine interaktive Erfahrung starten Sie die Gradio-WeboberflÃ¤che:


```bash
python app.py
```


## ğŸ‹ï¸ Modell trainieren

Siehe [`Training.md`](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/docs/Training.md)


---

## ğŸ“ TODO & ZukunftsplÃ¤ne
* - [ ] VerÃ¶ffentlichung eines leistungsfÃ¤higeren Foundation-Modells fÃ¼r mehrere DomÃ¤nen zur Bereitstellung einer ansprechenderen und immersiveren Foley-Erstellung
* - [ ] UnterstÃ¼tzung fÃ¼r zusÃ¤tzliche ModalitÃ¤ten und nachgelagerte Aufgaben hinzufÃ¼gen
* - [ ] VerÃ¶ffentlichung von Modellen in verschiedenen GrÃ¶ÃŸen
* - [x] Open-Source AudioCoT-Datensatz und automatisierte Pipeline
* - [x] VerÃ¶ffentlichung von Trainingsskripten fÃ¼r ThinkSound-Modelle
* - [x] Ein einsteigerfreundliches Windows-Quickstart-README
---


## ğŸ“„ Lizenz

Dieses Projekt wird unter der Apache 2.0 Lizenz verÃ¶ffentlicht.

> **Hinweis:**
> Der Code, die Modelle und der Datensatz sind **ausschlieÃŸlich fÃ¼r Forschungs- und Bildungszwecke**.
> **Kommerzielle Nutzung ist NICHT gestattet.**
> FÃ¼r kommerzielle Lizenzen kontaktieren Sie bitte die Autoren.

**ğŸ“¦ Komponenten Dritter**

* **Stable Audio Open VAE** (von Stability AI):
  Dieses Repository enthÃ¤lt ein feinabgestimmtes VAE von [Stable Audio Open](https://huggingface.co/stabilityai/stable-audio-open-1.0/), lizenziert unter der [Stability AI Community License](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/./third_party/LICENSE_StabilityAI.md).
  **Kommerzielle Nutzung und Weiterverteilung erfordern vorherige Genehmigung durch Stability AI.**

* ğŸ“˜ **Alle anderen Codes und Modelle** sind unter der Apache License 2.0 verÃ¶ffentlicht.

---

## Danksagungen

Vielen Dank an:

* **stable-audio-tools** (von Stability AI):
FÃ¼r die Bereitstellung eines benutzerfreundlichen Frameworks zur Audiogenerierung sowie des VAE-Moduls und der Gewichte.
* **MMAudio**:
  FÃ¼r die Implementierung des MM-DiT-Backbones im Audio-Bereich.

---

## ğŸ“– Zitation

Wenn ThinkSound fÃ¼r Ihre Forschung oder Arbeit nÃ¼tzlich ist, zitieren Sie bitte unser Paper:

```bibtex
@misc{liu2025thinksoundchainofthoughtreasoningmultimodal,
      title={ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language Models for Audio Generation and Editing}, 
      author={Huadai Liu and Jialei Wang and Kaicheng Luo and Wen Wang and Qian Chen and Zhou Zhao and Wei Xue},
      year={2025},
      eprint={2506.21448},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/2506.21448}, 
}
```

---

## ğŸ“¬ Contact


âœ¨ Feel free to [open an issue](https://github.com/liuhuadai/ThinkSound/issues) or contact us via email ([liuhuadai@zju.edu.cn](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/mailto:liuhuadai@zju.edu.cn)) if you have any questions or suggestions!




---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2026-01-07

---