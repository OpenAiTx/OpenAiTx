[
  {
    "Id": 1,
    "Content": "<h1 align=\"center\">ThinkSound</h1>\n\n<p align=\"center\">\n  ğŸŒ\n  <a href=\"https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=en\">English</a> |\n  <a href=\"https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=zh-CN\">ç®€ä½“ä¸­æ–‡</a> |\n  <a href=\"https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=zh-TW\">ç¹é«”ä¸­æ–‡</a> |\n  <a href=\"https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=es\">EspaÃ±ol</a> |\n  <a href=\"https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=fr\">FranÃ§ais</a> |\n  <a href=\"https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=ja\">æ—¥æœ¬èª</a>\n  \n</p>\n<p align=\"center\">\n  <img src=\"https://img.shields.io/badge/NeurIPS 2025-Main Conference-blue.svg\" alt=\"NeurIPS 2025\"/>\n<p align=\"center\">\n  <a href=\"https://arxiv.org/pdf/2506.21448\">\n    <img src=\"https://img.shields.io/badge/arXiv-2506.21448-b31b1b.svg\" alt=\"arXiv\"/>\n  </a>\n  &nbsp;\n  <a href=\"https://thinksound-project.github.io/\">\n    <img src=\"https://img.shields.io/badge/Online%20Demo-ğŸŒ-blue\" alt=\"Online Demo\"/>\n  </a>\n  &nbsp;\n  <a href=\"https://huggingface.co/spaces/FunAudioLLM/ThinkSound\">\n    <img src=\"https://img.shields.io/badge/HuggingFace-Spaces-orange?logo=huggingface\" alt=\"Hugging Face\"/>\n  </a>\n  &nbsp;\n  <a href=\"https://modelscope.cn/studios/iic/ThinkSound\">\n    <img src=\"https://img.shields.io/badge/ModelScope-åœ¨çº¿ä½“éªŒ-green\" alt=\"ModelScope\"/>\n  </a>\n</p>\n\n<p align=\"center\">\n  If you find this project useful,<br>\n  a star â­ on GitHub would be greatly appreciated!\n</p>\n\n---\n\n**ThinkSound** is a unified Any2Audio generation framework with flow matching guided by Chain-of-Thought (CoT) reasoning.",
    "ContentSha": "3764VshEOMedejxsV+uo8k5R5Emk0MLuYfIOX8JadcY=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "<h1 align=\"center\">ThinkSound</h1>\n\n<p align=\"center\">\n  ğŸŒ\n  <a href=\"https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=en\">English</a> |\n  <a href=\"https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=zh-CN\">ç®€ä½“ä¸­æ–‡</a> |\n  <a href=\"https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=zh-TW\">ç¹é«”ä¸­æ–‡</a> |\n  <a href=\"https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=es\">EspaÃ±ol</a> |\n  <a href=\"https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=fr\">FranÃ§ais</a> |\n  <a href=\"https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=ja\">æ—¥æœ¬èª</a>\n  \n</p>\n<p align=\"center\">\n  <img src=\"https://img.shields.io/badge/NeurIPS 2025-Main Conference-blue.svg\" alt=\"NeurIPS 2025\"/>\n<p align=\"center\">\n  <a href=\"https://arxiv.org/pdf/2506.21448\">\n    <img src=\"https://img.shields.io/badge/arXiv-2506.21448-b31b1b.svg\" alt=\"arXiv\"/>\n  </a>\n  &nbsp;\n  <a href=\"https://thinksound-project.github.io/\">\n    <img src=\"https://img.shields.io/badge/Online%20Demo-ğŸŒ-blue\" alt=\"Online Demo\"/>\n  </a>\n  &nbsp;\n  <a href=\"https://huggingface.co/spaces/FunAudioLLM/ThinkSound\">\n    <img src=\"https://img.shields.io/badge/HuggingFace-Spaces-orange?logo=huggingface\" alt=\"Hugging Face\"/>\n  </a>\n  &nbsp;\n  <a href=\"https://modelscope.cn/studios/iic/ThinkSound\">\n    <img src=\"https://img.shields.io/badge/ModelScope-åœ¨çº¿ä½“éªŒ-green\" alt=\"ModelScope\"/>\n  </a>\n</p>\n\n<p align=\"center\">\n  ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒå½¹ç«‹ã¤ã¨æ„Ÿã˜ãŸå ´åˆã€<br>\n  GitHubã§ã‚¹ã‚¿ãƒ¼ â­ ã‚’ã„ãŸã ã‘ã‚‹ã¨å¬‰ã—ã„ã§ã™ï¼\n</p>\n\n---\n\n**ThinkSound** ã¯ã€Chain-of-Thoughtï¼ˆCoTï¼‰æ¨è«–ã«ã‚ˆã‚‹ãƒ•ãƒ­ãƒ¼ãƒ»ãƒãƒƒãƒãƒ³ã‚°ã‚’æ´»ç”¨ã—ãŸçµ±åˆå‹Any2Audioç”Ÿæˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 2,
    "Content": "\nPyTorch implementation for multimodal audio generation and editing: generate or edit audio from video, text, and audio, powered by step-by-step reasoning from Multimodal Large Language Models (MLLMs).\n\n![Teaser](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig1_teaser.png)\n---\n\n## ğŸ“° News\n- **2025.11.25** &nbsp; ğŸ”¥[Online PrismAudio Demo](http://prismaudio-project.github.io/) is live - try it now!\n- **2025.11.25** &nbsp; ğŸ”¥[PrismAudio paper](https://arxiv.org/pdf/2511.18833) released on arXiv, the first multi-dimensional CoT-RL framework for Video-to-Audio Generation!\n- **2025.09.19** &nbsp; ğŸ‰ ThinkSound has been accepted to the **NeurIPS 2025 Main Conference**!\n- **2025.09.01** &nbsp; Our AudioCoT dataset is now open-sourced and available on [Hugging Face](https://huggingface.co/datasets/liuhuadai/AudioCoT)!\n- **2025.07.17** &nbsp; ğŸ§  Finetuning enabled: training and finetuning code is now publicly available, along with clear usage instructions to help you customize and extend ThinkSound with your own data.\n- **2025.07.15** &nbsp; ğŸ“¦ Simplified installation and usability: dependencies on PyPI for easy cross-platform setup; Windows `.bat` scripts automate environment creation and script running.\n- **2025.07.08** &nbsp;Â  ğŸ”§ Major update: model lightweighted and optimized memory and GPU usage, now supports high-throughput audio generation at scale!\n- **2025.07.01** &nbsp; Online demo on [Hugging Face Spaces](https://huggingface.co/spaces/FunAudioLLM/ThinkSound) and [ModelScope](https://modelscope.cn/studios/iic/ThinkSound) for interactive experience!\n- **2025.07.01** &nbsp; Released inference scripts and web interface; \n- **2025.06** &nbsp; [ThinkSound paper](https://arxiv.org/pdf/2506.21448) released on arXiv!\n- **2025.06** &nbsp; [Online Demo](http://thinksound-project.github.io/) is live - try it now!\n\n---\n\n\n## ğŸš€ Features\n\n- **Any2Audio**: Generate audio from arbitrary modalities â€” video, text, audio, or their combinations.\n- **Video-to-Audio SOTA**: Achieves state-of-the-art results on multiple V2A benchmarks.\n- **CoT-Driven Reasoning**: Chain-of-Thought reasoning for compositional and controllable audio generation via MLLMs.\n- **Interactive Object-centric Editing**: Refine or edit specific sound events by clicking on visual objects or using text instructions.\n- **Unified Framework**: One foundation model supports generation, editing, and interactive workflow.\n\n---\n\n## âœ¨ Method Overview\n\nThinkSound decomposes audio generation and editing into three interactive stages, all guided by MLLM-based Chain-of-Thought (CoT) reasoning:\n\n1. **Foley Generation:** Generate foundational, semantically and temporally aligned soundscapes from video.\n2. **Object-Centric Refinement:** Refine or add sounds for user-specified objects via clicks or regions in the video.\n3. **Targeted Audio Editing:** Modify generated audio using high-level natural language instructions.\n",
    "ContentSha": "Vp81xeUGr9WCESp62x1lkoIptJHl0dM1R8kleL4V10c=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "PyTorchã«ã‚ˆã‚‹ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«éŸ³å£°ç”ŸæˆãŠã‚ˆã³ç·¨é›†ã®å®Ÿè£…ï¼šå‹•ç”»ã€ãƒ†ã‚­ã‚¹ãƒˆã€éŸ³å£°ã‹ã‚‰éŸ³å£°ã‚’ç”Ÿæˆãƒ»ç·¨é›†å¯èƒ½ã€‚ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆMLLMï¼‰ã®æ®µéšçš„ãªæ¨è«–ã«ã‚ˆã£ã¦å¼·åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚\n\n![Teaser](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig1_teaser.png)\n---\n\n## ğŸ“° ãƒ‹ãƒ¥ãƒ¼ã‚¹\n- **2025.11.25** &nbsp; ğŸ”¥[ã‚ªãƒ³ãƒ©ã‚¤ãƒ³PrismAudioãƒ‡ãƒ¢](http://prismaudio-project.github.io/)å…¬é–‹ - ä»Šã™ããŠè©¦ã—ãã ã•ã„ï¼\n- **2025.11.25** &nbsp; ğŸ”¥[PrismAudioè«–æ–‡](https://arxiv.org/pdf/2511.18833)ãŒarXivã«å…¬é–‹ã€åˆã®å¤šæ¬¡å…ƒCoT-RLãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«ã‚ˆã‚‹Video-to-Audioç”Ÿæˆï¼\n- **2025.09.19** &nbsp; ğŸ‰ ThinkSoundãŒ**NeurIPS 2025æœ¬ä¼šè­°**ã«æ¡æŠã•ã‚Œã¾ã—ãŸï¼\n- **2025.09.01** &nbsp; AudioCoTãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åŒ–ã•ã‚Œã€[Hugging Face](https://huggingface.co/datasets/liuhuadai/AudioCoT)ã§å…¥æ‰‹å¯èƒ½ã«ãªã‚Šã¾ã—ãŸï¼\n- **2025.07.17** &nbsp; ğŸ§  ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¯¾å¿œï¼šãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŠã‚ˆã³ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚³ãƒ¼ãƒ‰ã‚’å…¬é–‹ã€ç‹¬è‡ªãƒ‡ãƒ¼ã‚¿ã§ThinkSoundã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºãƒ»æ‹¡å¼µã™ã‚‹ãŸã‚ã®æ˜ç¢ºãªä½¿ç”¨æ–¹æ³•ã‚‚æä¾›ã€‚\n- **2025.07.15** &nbsp; ğŸ“¦ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãƒ»åˆ©ç”¨ãŒç°¡å˜ã«ï¼šPyPIä¾å­˜é–¢ä¿‚ã§ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ç’°å¢ƒæ§‹ç¯‰ãŒå®¹æ˜“ã«ã€‚Windowsç”¨`.bat`ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ç’°å¢ƒä½œæˆãƒ»ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œã‚’è‡ªå‹•åŒ–ã€‚\n- **2025.07.08** &nbsp;Â  ğŸ”§ å¤§å¹…ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆï¼šãƒ¢ãƒ‡ãƒ«ã®è»½é‡åŒ–ã€ãƒ¡ãƒ¢ãƒªãƒ»GPUä½¿ç”¨æœ€é©åŒ–ã«ã‚ˆã‚Šã€å¤§è¦æ¨¡ãªé«˜ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆéŸ³å£°ç”Ÿæˆã‚’å®Ÿç¾ï¼\n- **2025.07.01** &nbsp; [Hugging Face Spaces](https://huggingface.co/spaces/FunAudioLLM/ThinkSound)ãŠã‚ˆã³[ModelScope](https://modelscope.cn/studios/iic/ThinkSound)ã§ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¢å…¬é–‹ã€ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ä½“é¨“ãŒå¯èƒ½ï¼\n- **2025.07.01** &nbsp; æ¨è«–ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹å…¬é–‹ï¼› \n- **2025.06** &nbsp; [ThinkSoundè«–æ–‡](https://arxiv.org/pdf/2506.21448)ãŒarXivã«å…¬é–‹ï¼\n- **2025.06** &nbsp; [ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¢](http://thinksound-project.github.io/)å…¬é–‹ - ä»Šã™ããŠè©¦ã—ãã ã•ã„ï¼\n\n---\n\n\n## ğŸš€ ç‰¹å¾´\n\n- **Any2Audio**: ä»»æ„ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ï¼ˆå‹•ç”»ã€ãƒ†ã‚­ã‚¹ãƒˆã€éŸ³å£°ã¾ãŸã¯çµ„ã¿åˆã‚ã›ï¼‰ã‹ã‚‰éŸ³å£°ã‚’ç”Ÿæˆã€‚\n- **Video-to-Audio SOTA**: è¤‡æ•°ã®V2Aãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§æœ€å…ˆç«¯ã®çµæœã‚’é”æˆã€‚\n- **CoTé§†å‹•æ¨è«–**: MLLMã‚’åˆ©ç”¨ã—ãŸChain-of-Thoughtæ¨è«–ã«ã‚ˆã‚‹æ§‹æˆçš„ã‹ã¤åˆ¶å¾¡å¯èƒ½ãªéŸ³å£°ç”Ÿæˆã€‚\n- **ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä¸­å¿ƒç·¨é›†**: è¦–è¦šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã‚¯ãƒªãƒƒã‚¯ã‚„ãƒ†ã‚­ã‚¹ãƒˆæŒ‡ç¤ºã§ç‰¹å®šã®éŸ³ã‚¤ãƒ™ãƒ³ãƒˆã‚’ç·¨é›†ãƒ»æ”¹è‰¯ã€‚\n- **çµ±ä¸€ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**: ã²ã¨ã¤ã®åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã§ç”Ÿæˆã€ç·¨é›†ã€ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ã‚µãƒãƒ¼ãƒˆã€‚\n\n---\n\n## âœ¨ æ‰‹æ³•æ¦‚è¦\n\nThinkSoundã¯éŸ³å£°ç”Ÿæˆã¨ç·¨é›†ã‚’ã€MLLMãƒ™ãƒ¼ã‚¹ã®Chain-of-Thoughtï¼ˆCoTï¼‰æ¨è«–ã§å°ã‹ã‚Œã‚‹3ã¤ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªæ®µéšã«åˆ†è§£ã—ã¾ã™ï¼š\n\n1. **ãƒ•ã‚©ãƒ¼ãƒªãƒ¼ç”Ÿæˆ:** å‹•ç”»ã‹ã‚‰æ„å‘³çš„ãƒ»æ™‚é–“çš„ã«æ•´åˆã—ãŸåŸºç¤çš„ãªã‚µã‚¦ãƒ³ãƒ‰ã‚¹ã‚±ãƒ¼ãƒ—ã‚’ç”Ÿæˆã€‚\n2. **ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä¸­å¿ƒã®æ”¹è‰¯:** å‹•ç”»å†…ã®ã‚¯ãƒªãƒƒã‚¯ã‚„é ˜åŸŸæŒ‡å®šã§ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®éŸ³ã‚’è¿½åŠ ãƒ»æ”¹è‰¯ã€‚\n3. **ã‚¿ãƒ¼ã‚²ãƒƒãƒˆéŸ³å£°ç·¨é›†:** é«˜ãƒ¬ãƒ™ãƒ«ãªè‡ªç„¶è¨€èªæŒ‡ç¤ºã§ç”Ÿæˆæ¸ˆã¿éŸ³å£°ã‚’ä¿®æ­£ã€‚\n\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "PyTorchã«ã‚ˆã‚‹ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«éŸ³å£°ç”ŸæˆãŠã‚ˆã³ç·¨é›†ã®å®Ÿè£…ï¼šå‹•ç”»ã€ãƒ†ã‚­ã‚¹ãƒˆã€éŸ³å£°ã‹ã‚‰éŸ³å£°ã‚’ç”Ÿæˆãƒ»ç·¨é›†å¯èƒ½ã€‚ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆMLLMï¼‰ã®æ®µéšçš„ãªæ¨è«–ã«ã‚ˆã£ã¦å¼·åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚"
      },
      {
        "row": 2,
        "rowsha": "eexREJOMCITdo96Iwg4N4PQqu1ux/UwupfjVRj0l/L8=",
        "originContent": "PyTorch implementation for multimodal audio generation and editing: generate or edit audio from video, text, and audio, powered by step-by-step reasoning from Multimodal Large Language Models (MLLMs).",
        "translatedContent": ""
      },
      {
        "row": 3,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "![Teaser](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig1_teaser.png)"
      },
      {
        "row": 4,
        "rowsha": "+7Cx1rdpWiIOeWRA/RM6HsKf7pNA3hHsQk2Bpadt4II=",
        "originContent": "![Teaser](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig1_teaser.png)",
        "translatedContent": "---"
      },
      {
        "row": 5,
        "rowsha": "yz+R1U7uMOU+NbK5mQX3Dxae1Un9eJCdPawt78ntjTs=",
        "originContent": "---",
        "translatedContent": ""
      },
      {
        "row": 6,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## ğŸ“° ãƒ‹ãƒ¥ãƒ¼ã‚¹"
      },
      {
        "row": 7,
        "rowsha": "0YeCyxpcm7/4RDbaM+OQoI8YfqEHQzDkpGW15VZdi1U=",
        "originContent": "## ğŸ“° News",
        "translatedContent": "- **2025.11.25** &nbsp; ğŸ”¥[ã‚ªãƒ³ãƒ©ã‚¤ãƒ³PrismAudioãƒ‡ãƒ¢](http://prismaudio-project.github.io/)å…¬é–‹ - ä»Šã™ããŠè©¦ã—ãã ã•ã„ï¼"
      },
      {
        "row": 8,
        "rowsha": "EeDKxV0PYB9iw6GxOqwnsP3C385ykbqk8PtEVoVhAcc=",
        "originContent": "- **2025.11.25** &nbsp; ğŸ”¥[Online PrismAudio Demo](http://prismaudio-project.github.io/) is live - try it now!",
        "translatedContent": "- **2025.11.25** &nbsp; ğŸ”¥[PrismAudioè«–æ–‡](https://arxiv.org/pdf/2511.18833)ãŒarXivã«å…¬é–‹ã€åˆã®å¤šæ¬¡å…ƒCoT-RLãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«ã‚ˆã‚‹Video-to-Audioç”Ÿæˆï¼"
      },
      {
        "row": 9,
        "rowsha": "Q6c68QE34xLPSZ5xAL4zofEn/u8umRYYiH3ddCcy5vk=",
        "originContent": "- **2025.11.25** &nbsp; ğŸ”¥[PrismAudio paper](https://arxiv.org/pdf/2511.18833) released on arXiv, the first multi-dimensional CoT-RL framework for Video-to-Audio Generation!",
        "translatedContent": "- **2025.09.19** &nbsp; ğŸ‰ ThinkSoundãŒ**NeurIPS 2025æœ¬ä¼šè­°**ã«æ¡æŠã•ã‚Œã¾ã—ãŸï¼"
      },
      {
        "row": 10,
        "rowsha": "vj2hbV/6MacDcxRmFA0AtTog5xU6CHoqBFO3YMO+yJo=",
        "originContent": "- **2025.09.19** &nbsp; ğŸ‰ ThinkSound has been accepted to the **NeurIPS 2025 Main Conference**!",
        "translatedContent": "- **2025.09.01** &nbsp; AudioCoTãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åŒ–ã•ã‚Œã€[Hugging Face](https://huggingface.co/datasets/liuhuadai/AudioCoT)ã§å…¥æ‰‹å¯èƒ½ã«ãªã‚Šã¾ã—ãŸï¼"
      },
      {
        "row": 11,
        "rowsha": "+guVQwDlJDqR1pBGMGc4FQ11nryItaEtKQ3etaCBcn0=",
        "originContent": "- **2025.09.01** &nbsp; Our AudioCoT dataset is now open-sourced and available on [Hugging Face](https://huggingface.co/datasets/liuhuadai/AudioCoT)!",
        "translatedContent": "- **2025.07.17** &nbsp; ğŸ§  ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¯¾å¿œï¼šãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŠã‚ˆã³ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚³ãƒ¼ãƒ‰ã‚’å…¬é–‹ã€ç‹¬è‡ªãƒ‡ãƒ¼ã‚¿ã§ThinkSoundã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºãƒ»æ‹¡å¼µã™ã‚‹ãŸã‚ã®æ˜ç¢ºãªä½¿ç”¨æ–¹æ³•ã‚‚æä¾›ã€‚"
      },
      {
        "row": 12,
        "rowsha": "4Jq9g83O8cYV4fVKsetfTpI+JrSepLrjK6J7Xg9tSqo=",
        "originContent": "- **2025.07.17** &nbsp; ğŸ§  Finetuning enabled: training and finetuning code is now publicly available, along with clear usage instructions to help you customize and extend ThinkSound with your own data.",
        "translatedContent": "- **2025.07.15** &nbsp; ğŸ“¦ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãƒ»åˆ©ç”¨ãŒç°¡å˜ã«ï¼šPyPIä¾å­˜é–¢ä¿‚ã§ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ç’°å¢ƒæ§‹ç¯‰ãŒå®¹æ˜“ã«ã€‚Windowsç”¨`.bat`ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ç’°å¢ƒä½œæˆãƒ»ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œã‚’è‡ªå‹•åŒ–ã€‚"
      },
      {
        "row": 13,
        "rowsha": "Ae5w+cTrCd9E8qidC11IWJAgg+LuOgUxA4czDPNG/G0=",
        "originContent": "- **2025.07.15** &nbsp; ğŸ“¦ Simplified installation and usability: dependencies on PyPI for easy cross-platform setup; Windows `.bat` scripts automate environment creation and script running.",
        "translatedContent": "- **2025.07.08** &nbsp;Â  ğŸ”§ å¤§å¹…ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆï¼šãƒ¢ãƒ‡ãƒ«ã®è»½é‡åŒ–ã€ãƒ¡ãƒ¢ãƒªãƒ»GPUä½¿ç”¨æœ€é©åŒ–ã«ã‚ˆã‚Šã€å¤§è¦æ¨¡ãªé«˜ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆéŸ³å£°ç”Ÿæˆã‚’å®Ÿç¾ï¼"
      },
      {
        "row": 14,
        "rowsha": "nYxNhwgSqjwYLuWsfHAqP5sx2PnzYwoFrwcf9U+Fdss=",
        "originContent": "- **2025.07.08** &nbsp;Â  ğŸ”§ Major update: model lightweighted and optimized memory and GPU usage, now supports high-throughput audio generation at scale!",
        "translatedContent": "- **2025.07.01** &nbsp; [Hugging Face Spaces](https://huggingface.co/spaces/FunAudioLLM/ThinkSound)ãŠã‚ˆã³[ModelScope](https://modelscope.cn/studios/iic/ThinkSound)ã§ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¢å…¬é–‹ã€ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ä½“é¨“ãŒå¯èƒ½ï¼"
      },
      {
        "row": 15,
        "rowsha": "RPL6cU3/Jgu90ib4PkeN5Q/ALrnjq9hK0ZUCranb/PA=",
        "originContent": "- **2025.07.01** &nbsp; Online demo on [Hugging Face Spaces](https://huggingface.co/spaces/FunAudioLLM/ThinkSound) and [ModelScope](https://modelscope.cn/studios/iic/ThinkSound) for interactive experience!",
        "translatedContent": "- **2025.07.01** &nbsp; æ¨è«–ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹å…¬é–‹ï¼› "
      },
      {
        "row": 16,
        "rowsha": "0PEL3eOyUmX2U56FPEPvYfaltZ/P/wbH0uO6GcLPlUE=",
        "originContent": "- **2025.07.01** &nbsp; Released inference scripts and web interface; ",
        "translatedContent": "- **2025.06** &nbsp; [ThinkSoundè«–æ–‡](https://arxiv.org/pdf/2506.21448)ãŒarXivã«å…¬é–‹ï¼"
      },
      {
        "row": 17,
        "rowsha": "XNdJ/DN741rXoJAruiGiRueQILXIUHRXzBlp+HVWM88=",
        "originContent": "- **2025.06** &nbsp; [ThinkSound paper](https://arxiv.org/pdf/2506.21448) released on arXiv!",
        "translatedContent": "- **2025.06** &nbsp; [ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¢](http://thinksound-project.github.io/)å…¬é–‹ - ä»Šã™ããŠè©¦ã—ãã ã•ã„ï¼"
      },
      {
        "row": 18,
        "rowsha": "W45oflUmoUAksoDc1WjcR2hErqh9UIi738PFVipiDg0=",
        "originContent": "- **2025.06** &nbsp; [Online Demo](http://thinksound-project.github.io/) is live - try it now!",
        "translatedContent": ""
      },
      {
        "row": 19,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "---"
      },
      {
        "row": 20,
        "rowsha": "yz+R1U7uMOU+NbK5mQX3Dxae1Un9eJCdPawt78ntjTs=",
        "originContent": "---",
        "translatedContent": ""
      },
      {
        "row": 21,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 22,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## ğŸš€ ç‰¹å¾´"
      },
      {
        "row": 23,
        "rowsha": "f4oQIFLM2EJQxJ65F4oMEA7yWOIqs0eBtiIvGxI+GgI=",
        "originContent": "## ğŸš€ Features",
        "translatedContent": ""
      },
      {
        "row": 24,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "- **Any2Audio**: ä»»æ„ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ï¼ˆå‹•ç”»ã€ãƒ†ã‚­ã‚¹ãƒˆã€éŸ³å£°ã¾ãŸã¯çµ„ã¿åˆã‚ã›ï¼‰ã‹ã‚‰éŸ³å£°ã‚’ç”Ÿæˆã€‚"
      },
      {
        "row": 25,
        "rowsha": "8TbVluJJPR3C0fX41HklpO/vH9/k/rHC+jlvmHHfzQU=",
        "originContent": "- **Any2Audio**: Generate audio from arbitrary modalities â€” video, text, audio, or their combinations.",
        "translatedContent": "- **Video-to-Audio SOTA**: è¤‡æ•°ã®V2Aãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§æœ€å…ˆç«¯ã®çµæœã‚’é”æˆã€‚"
      },
      {
        "row": 26,
        "rowsha": "+Jf5LrYz+d1ShYCuQe8UF9taEJUpuGhDk6fAlYK3Kj4=",
        "originContent": "- **Video-to-Audio SOTA**: Achieves state-of-the-art results on multiple V2A benchmarks.",
        "translatedContent": "- **CoTé§†å‹•æ¨è«–**: MLLMã‚’åˆ©ç”¨ã—ãŸChain-of-Thoughtæ¨è«–ã«ã‚ˆã‚‹æ§‹æˆçš„ã‹ã¤åˆ¶å¾¡å¯èƒ½ãªéŸ³å£°ç”Ÿæˆã€‚"
      },
      {
        "row": 27,
        "rowsha": "mU7qXkjW1YifKoYXJedYo9l64NsTBaiXsgoGFRF/g+E=",
        "originContent": "- **CoT-Driven Reasoning**: Chain-of-Thought reasoning for compositional and controllable audio generation via MLLMs.",
        "translatedContent": "- **ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä¸­å¿ƒç·¨é›†**: è¦–è¦šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã‚¯ãƒªãƒƒã‚¯ã‚„ãƒ†ã‚­ã‚¹ãƒˆæŒ‡ç¤ºã§ç‰¹å®šã®éŸ³ã‚¤ãƒ™ãƒ³ãƒˆã‚’ç·¨é›†ãƒ»æ”¹è‰¯ã€‚"
      },
      {
        "row": 28,
        "rowsha": "RComOCBBrXsZf9RHmLginqKTh9eI/bKUZuUunQEmD5M=",
        "originContent": "- **Interactive Object-centric Editing**: Refine or edit specific sound events by clicking on visual objects or using text instructions.",
        "translatedContent": "- **çµ±ä¸€ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**: ã²ã¨ã¤ã®åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã§ç”Ÿæˆã€ç·¨é›†ã€ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ã‚µãƒãƒ¼ãƒˆã€‚"
      },
      {
        "row": 29,
        "rowsha": "C3sf87sy73G/XZft+TDo5NjXo5XcrtJB805ayHRAXoQ=",
        "originContent": "- **Unified Framework**: One foundation model supports generation, editing, and interactive workflow.",
        "translatedContent": ""
      },
      {
        "row": 30,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "---"
      },
      {
        "row": 31,
        "rowsha": "yz+R1U7uMOU+NbK5mQX3Dxae1Un9eJCdPawt78ntjTs=",
        "originContent": "---",
        "translatedContent": ""
      },
      {
        "row": 32,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## âœ¨ æ‰‹æ³•æ¦‚è¦"
      },
      {
        "row": 33,
        "rowsha": "gjgLOIAU2x83BBZdLUdgEB+F64ajt/QuLYQXM1hDBLE=",
        "originContent": "## âœ¨ Method Overview",
        "translatedContent": ""
      },
      {
        "row": 34,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "ThinkSoundã¯éŸ³å£°ç”Ÿæˆã¨ç·¨é›†ã‚’ã€MLLMãƒ™ãƒ¼ã‚¹ã®Chain-of-Thoughtï¼ˆCoTï¼‰æ¨è«–ã§å°ã‹ã‚Œã‚‹3ã¤ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªæ®µéšã«åˆ†è§£ã—ã¾ã™ï¼š"
      },
      {
        "row": 35,
        "rowsha": "v5//GqG/smYIGoUpN+12k+9/3GWH3GdYD+jLqScb7AM=",
        "originContent": "ThinkSound decomposes audio generation and editing into three interactive stages, all guided by MLLM-based Chain-of-Thought (CoT) reasoning:",
        "translatedContent": ""
      },
      {
        "row": 36,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "1. **ãƒ•ã‚©ãƒ¼ãƒªãƒ¼ç”Ÿæˆ:** å‹•ç”»ã‹ã‚‰æ„å‘³çš„ãƒ»æ™‚é–“çš„ã«æ•´åˆã—ãŸåŸºç¤çš„ãªã‚µã‚¦ãƒ³ãƒ‰ã‚¹ã‚±ãƒ¼ãƒ—ã‚’ç”Ÿæˆã€‚"
      },
      {
        "row": 37,
        "rowsha": "vTObEWb7f5gCU681X3dTkwqhsaSW89TLw3GgMJ5I3bo=",
        "originContent": "1. **Foley Generation:** Generate foundational, semantically and temporally aligned soundscapes from video.",
        "translatedContent": "2. **ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä¸­å¿ƒã®æ”¹è‰¯:** å‹•ç”»å†…ã®ã‚¯ãƒªãƒƒã‚¯ã‚„é ˜åŸŸæŒ‡å®šã§ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®éŸ³ã‚’è¿½åŠ ãƒ»æ”¹è‰¯ã€‚"
      },
      {
        "row": 38,
        "rowsha": "LTBpIQQHtEkNF8StAa+ZEDASGmRhmHIKDQOdZ4ExJWM=",
        "originContent": "2. **Object-Centric Refinement:** Refine or add sounds for user-specified objects via clicks or regions in the video.",
        "translatedContent": "3. **ã‚¿ãƒ¼ã‚²ãƒƒãƒˆéŸ³å£°ç·¨é›†:** é«˜ãƒ¬ãƒ™ãƒ«ãªè‡ªç„¶è¨€èªæŒ‡ç¤ºã§ç”Ÿæˆæ¸ˆã¿éŸ³å£°ã‚’ä¿®æ­£ã€‚"
      },
      {
        "row": 39,
        "rowsha": "8wrAo7X7dPC6Sgpfrlq2ziv/Wg/3+JnlFYvO+RMxADQ=",
        "originContent": "3. **Targeted Audio Editing:** Modify generated audio using high-level natural language instructions.",
        "translatedContent": ""
      },
      {
        "row": 40,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 3,
    "Content": "![ThinkSound Overview](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig3_model.png)\n<!-- A large-scale CoT-annotated dataset (**AudioCoT**) is used to train both the reasoning module and the unified audio foundation model.\n![AudioCoT Pipeline](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig2_dataset.png) -->\n\n---\n\n## âš¡ Quick Start\n\n**Environment Preparation:**",
    "ContentSha": "QpULU62syvvJhbUWGR7NuQMiHmHmeeiFfKqP+ZpqFOk=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "![ThinkSound æ¦‚è¦](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig3_model.png)\n<!-- å¤§è¦æ¨¡ãªCoTã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ**AudioCoT**ï¼‰ã‚’ä½¿ç”¨ã—ã¦ã€æ¨è«–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨çµ±åˆéŸ³å£°åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã®ä¸¡æ–¹ã‚’è¨“ç·´ã—ã¾ã™ã€‚\n![AudioCoT ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig2_dataset.png) -->\n\n---\n\n## âš¡ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ\n\n**ç’°å¢ƒæº–å‚™ï¼š**",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "4UKlvFW3Xb0bSAVjcBNeekH/MMiYS0XDg9w4mCuPy/Q=",
        "originContent": "![ThinkSound Overview](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig3_model.png)",
        "translatedContent": "![ThinkSound æ¦‚è¦](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig3_model.png)"
      },
      {
        "row": 2,
        "rowsha": "GaujeIM3x7+YcFy07LNNyITlujhkgpgeIaOiKHJkYnE=",
        "originContent": "<!-- A large-scale CoT-annotated dataset (**AudioCoT**) is used to train both the reasoning module and the unified audio foundation model.",
        "translatedContent": "<!-- å¤§è¦æ¨¡ãªCoTã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ**AudioCoT**ï¼‰ã‚’ä½¿ç”¨ã—ã¦ã€æ¨è«–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨çµ±åˆéŸ³å£°åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã®ä¸¡æ–¹ã‚’è¨“ç·´ã—ã¾ã™ã€‚"
      },
      {
        "row": 3,
        "rowsha": "qYOXaaTiYkoaPFcpTXE5xdSqqiW3ebi//EW/RfSXd9g=",
        "originContent": "![AudioCoT Pipeline](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig2_dataset.png) -->",
        "translatedContent": "![AudioCoT ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig2_dataset.png) -->"
      },
      {
        "row": 4,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 5,
        "rowsha": "yz+R1U7uMOU+NbK5mQX3Dxae1Un9eJCdPawt78ntjTs=",
        "originContent": "---",
        "translatedContent": "---"
      },
      {
        "row": 6,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 7,
        "rowsha": "PrY/jc4yAHdS+Sr+s+Yhab477/BDp3GAzMJ8+WyumyI=",
        "originContent": "## âš¡ Quick Start",
        "translatedContent": "## âš¡ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ"
      },
      {
        "row": 8,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 9,
        "rowsha": "mD63DGEpdc7FlccTEps0KPAiwsJpO/C3yjV+SKIi/vE=",
        "originContent": "**Environment Preparation:**",
        "translatedContent": "**ç’°å¢ƒæº–å‚™ï¼š**"
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 4,
    "Content": "```bash\ngit clone https://github.com/liuhuadai/ThinkSound.git\ncd ThinkSound\nconda create -n thinksound python=3.10\nconda activate thinksound\npip install thinksound\nconda install -y -c conda-forge 'ffmpeg<7'\n# Download pretrained weights https://huggingface.co/liuhuadai/ThinkSound to Directory ckpts/\n# model weights can be also downloaded from https://www.modelscope.cn/models/iic/ThinkSound\ngit lfs install\ngit clone https://huggingface.co/liuhuadai/ThinkSound ckpts\n# To improve inference and training speed, you may optionally install a FlashAttention backend compatible with your system and PyTorch version.\n```",
    "ContentSha": "CSBCDvBmuatxDa1cNMeHEBTJJzdLjK6wyO9v0LrETM8=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bash\ngit clone https://github.com/liuhuadai/ThinkSound.git\ncd ThinkSound\nconda create -n thinksound python=3.10\nconda activate thinksound\npip install thinksound\nconda install -y -c conda-forge 'ffmpeg<7'\n# Download pretrained weights https://huggingface.co/liuhuadai/ThinkSound to Directory ckpts/\n# model weights can be also downloaded from https://www.modelscope.cn/models/iic/ThinkSound\ngit lfs install\ngit clone https://huggingface.co/liuhuadai/ThinkSound ckpts\n# To improve inference and training speed, you may optionally install a FlashAttention backend compatible with your system and PyTorch version.\n```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 5,
    "Content": "\n> âœ… **Windows Tip:**  \n> Windows users can simply run `setup_windows.bat` (or double-click it) to automatically create the conda environment, install all dependencies (including FFmpeg), and download the pretrained model â€” no manual setup required.  \n> Make sure `conda` and `git` are installed and available in your system PATH before running the script.\n\n\n### â–¶ï¸ Run the Demo\n\n#### **Linux/macOS**\n",
    "ContentSha": "YGhm7lbBNPq6xLS6zXlFTPszO8rc4QZCwsScPNskcto=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "> âœ… **Windowsã®ãƒ’ãƒ³ãƒˆï¼š**  \n> Windowsãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ã€`setup_windows.bat` ã‚’å®Ÿè¡Œï¼ˆã¾ãŸã¯ãƒ€ãƒ–ãƒ«ã‚¯ãƒªãƒƒã‚¯ï¼‰ã™ã‚‹ã ã‘ã§ã€è‡ªå‹•çš„ã«condaç’°å¢ƒãŒä½œæˆã•ã‚Œã€ã™ã¹ã¦ã®ä¾å­˜é–¢ä¿‚ï¼ˆFFmpegã‚’å«ã‚€ï¼‰ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã€å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã™ â€” æ‰‹å‹•ã§ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã¯ä¸è¦ã§ã™ã€‚  \n> ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œã™ã‚‹å‰ã«ã€`conda` ãŠã‚ˆã³ `git` ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ãŠã‚Šã€ã‚·ã‚¹ãƒ†ãƒ PATHã«è¿½åŠ ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n\n\n### â–¶ï¸ ãƒ‡ãƒ¢ã®å®Ÿè¡Œ\n\n#### **Linux/macOS**\n\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 6,
    "Content": "```bash\nchmod +x scripts/demo.sh\n./scripts/demo.sh <path-to-your-demo-video> <title> <CoT description> [use-half]\n```",
    "ContentSha": "EW6OKf+6hdOehT5SO7gfI7wR8oAoMckp60MRfIA1jHc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bash\nchmod +x scripts/demo.sh\n./scripts/demo.sh <path-to-your-demo-video> <title> <CoT description> [use-half]\n```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 7,
    "Content": "\n#### **Windows**\n\nYou can use the provided `.bat` script instead:\n",
    "ContentSha": "zXqRZWTEWOuKZG1GOlqqZff+IH24zUwdPtSfwESqS9E=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "#### **Windows**\n\nä»£ã‚ã‚Šã«ã€ç”¨æ„ã•ã‚ŒãŸ `.bat` ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½¿ç”¨ã§ãã¾ã™ã€‚\n\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 8,
    "Content": "```bash\n.\\scripts\\demo.bat <path-to-your-demo-video> <title> <CoT description> [use-half]\n```",
    "ContentSha": "A2a1kVuIPNs8ht1a6LBYTEijJjnfjiTN0r+2n7VEJSg=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bash\n.\\scripts\\demo.bat <path-to-your-demo-video> <title> <CoT description> [use-half]\n```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 9,
    "Content": "\n**Note:**\n\n* `<path-to-your-demo-video>`: The path to a single video\n* `[use-half]` (optional): Add use-half at the end to enable half precision feature extraction.\n\n---\n\n### ğŸ“¦ Batch Inference\n\n#### **Linux/macOS**\n",
    "ContentSha": "T7owm3ZZW7sVjKwFivgiuYX2+RVuNBl0RYTSnIcxxbM=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "**æ³¨æ„:**\n\n* `<path-to-your-demo-video>`: å˜ä¸€ãƒ“ãƒ‡ã‚ªã¸ã®ãƒ‘ã‚¹\n* `[use-half]`ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰: æœ€å¾Œã«use-halfã‚’è¿½åŠ ã—ã¦åŠç²¾åº¦ç‰¹å¾´æŠ½å‡ºã‚’æœ‰åŠ¹ã«ã—ã¾ã™ã€‚\n\n---\n\n### ğŸ“¦ ãƒãƒƒãƒæ¨è«–\n\n#### **Linux/macOS**\n\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 10,
    "Content": "```bash\nchmod +x scripts/eval_batch.sh\n./scripts/eval_batch.sh <video_path> <csv_path> <save_path (optional)> [use-half]\n```",
    "ContentSha": "EQ4HuSYii55aHfgphESvOXMz2+Fq39+Xquxg6Z6uzdU=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bash\nchmod +x scripts/eval_batch.sh\n./scripts/eval_batch.sh <video_path> <csv_path> <save_path (optional)> [use-half]\n```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 11,
    "Content": "\n#### **Windows**\n\nUse the equivalent `.bat` script:\n",
    "ContentSha": "njm5i6o3MR7AV4Q3WLctbe3LN1njFn89fPfTlo+zSmc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "#### **Windows**\n\nåŒç­‰ã® `.bat` ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„:\n\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 12,
    "Content": "```bash\n.\\scripts\\eval_batch.bat <video_path> <csv_path> <save_path (optional)> [use-half]\n```",
    "ContentSha": "XLkAqxYBZeJiF6XnpshI6naENFsr5yFAH7af132cgb0=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bash\n.\\scripts\\eval_batch.bat <video_path> <csv_path> <save_path (optional)> [use-half]\n```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 13,
    "Content": "\n**Note:**\n\n* `<video_path>`: Path to the root directory containing all .mp4 videos to be processed (all videos must be of equal duration).\n* `<csv_path>`: A CSV file with text prompts for each video (see `demo_test.csv` for format).\n* `<save_path>` (optional): Where to save generated audio. Defaults to `results/features`.\n* `[use-half]` (optional): Add use-half at the end to enable half precision feature extraction.\n\n---\n\n\n### Web Interface Usage\n\nFor an interactive experience, launch the Gradio web interface:\n",
    "ContentSha": "yT/y6PXpYV8wS4qmKJfVNVGDLOwreTxCdCDFDv2VbLo=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "**æ³¨æ„:**\n\n* `<video_path>`: å‡¦ç†ã™ã‚‹ã™ã¹ã¦ã®.mp4å‹•ç”»ã‚’å«ã‚€ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸ã®ãƒ‘ã‚¹ï¼ˆã™ã¹ã¦ã®å‹•ç”»ã¯åŒã˜é•·ã•ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼‰ã€‚\n* `<csv_path>`: å„å‹•ç”»ã«å¯¾å¿œã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒè¨˜è¼‰ã•ã‚ŒãŸCSVãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆå½¢å¼ã¯`demo_test.csv`ã‚’å‚ç…§ï¼‰ã€‚\n* `<save_path>`ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰: ç”Ÿæˆã•ã‚ŒãŸéŸ³å£°ã®ä¿å­˜å…ˆã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯`results/features`ã€‚\n* `[use-half]`ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰: æœ€å¾Œã«use-halfã‚’è¿½åŠ ã™ã‚‹ã¨åŠç²¾åº¦ç‰¹å¾´é‡æŠ½å‡ºã‚’æœ‰åŠ¹åŒ–ã—ã¾ã™ã€‚\n\n---\n\n\n### Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®åˆ©ç”¨\n\nã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªæ“ä½œã‚’è¡Œã†ã«ã¯ã€Gradioã®Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’èµ·å‹•ã—ã¦ãã ã•ã„ã€‚\n\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 14,
    "Content": "```bash\npython app.py\n```",
    "ContentSha": "2nQFYMHYtsOO4+egbu20DhxqoaxfzoH8CneeM8qTEb0=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bash\npython app.py\n```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 15,
    "Content": "\n\n## ğŸ‹ï¸ Train the Model\n\nSee [`Training.md`](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/docs/Training.md)\n\n\n---\n\n## ğŸ“ TODO & Future Plans\n* - [ ] Release a more powerful foundation model covering multiple domains to provide more engaging and immersive foley creation\n* - [ ] Add support for additional modalities and downstream tasks\n* - [ ] Release models at different scales\n* - [x] Open-source AudioCoT dataset and automated pipeline\n* - [x] Release training scripts for ThinkSound models\n* - [x] A beginner-friendly Windows quick-start README\n---\n\n\n## ğŸ“„ License\n\nThis project is released under the Apache 2.0 License.\n\n> **Note:**\n> The code, models, and dataset are **for research and educational purposes only**.\n> **Commercial use is NOT permitted.**\n> For commercial licensing, please contact the authors.\n\n**ğŸ“¦ Third-Party Components**\n\n* **Stable Audio Open VAE** (by Stability AI):\n  This repository includes a fine-tuned VAE from [Stable Audio Open](https://huggingface.co/stabilityai/stable-audio-open-1.0/), licensed under the [Stability AI Community License](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/./third_party/LICENSE_StabilityAI.md).\n  **Commercial use and redistribution require prior permission from Stability AI.**\n\n* ğŸ“˜ **All other code and models** are released under the Apache License 2.0.\n\n---\n\n## Acknowledgements\n",
    "ContentSha": "j3jq6Afpr38oSd7nSvDiMiQ889Z6kSywM0DVVT9ieNA=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "\n\n## ğŸ‹ï¸ ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°\n\n[`Training.md`](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/docs/Training.md) ã‚’å‚ç…§ã—ã¦ãã ã•ã„\n\n\n---\n\n## ğŸ“ TODO & ä»Šå¾Œã®è¨ˆç”»\n* - [ ] ã‚ˆã‚Šå¼·åŠ›ãªåŸºç¤ãƒ¢ãƒ‡ãƒ«ã‚’å…¬é–‹ã—ã€è¤‡æ•°ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’ã‚«ãƒãƒ¼ã—ã¦ã‚ˆã‚Šé­…åŠ›çš„ã§æ²¡å…¥æ„Ÿã®ã‚ã‚‹ãƒ•ã‚©ãƒ¼ãƒªãƒ¼ä½œæˆã‚’æä¾›\n* - [ ] è¿½åŠ ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ãŠã‚ˆã³ä¸‹æµã‚¿ã‚¹ã‚¯ã¸ã®å¯¾å¿œã‚’è¿½åŠ \n* - [ ] ç•°ãªã‚‹ã‚¹ã‚±ãƒ¼ãƒ«ã®ãƒ¢ãƒ‡ãƒ«ã‚’å…¬é–‹\n* - [x] AudioCoTãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŠã‚ˆã³è‡ªå‹•åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åŒ–\n* - [x] ThinkSoundãƒ¢ãƒ‡ãƒ«ç”¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆå…¬é–‹\n* - [x] åˆå¿ƒè€…å‘ã‘Windowsã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆREADME\n---\n\n\n## ğŸ“„ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹\n\nã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯Apache 2.0ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã®ä¸‹ã§å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚\n\n> **æ³¨æ„:**\n> ã‚³ãƒ¼ãƒ‰ã€ãƒ¢ãƒ‡ãƒ«ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ **ç ”ç©¶ãŠã‚ˆã³æ•™è‚²ç›®çš„ã®ã¿** ã§åˆ©ç”¨å¯èƒ½ã§ã™ã€‚\n> **å•†ç”¨åˆ©ç”¨ã¯è¨±å¯ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚**\n> å•†ç”¨ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã«ã¤ã„ã¦ã¯è‘—è€…ã«ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚\n\n**ğŸ“¦ ã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ**\n\n* **Stable Audio Open VAE**ï¼ˆStability AIã«ã‚ˆã‚‹ï¼‰:\n  ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã«ã¯ [Stable Audio Open](https://huggingface.co/stabilityai/stable-audio-open-1.0/) ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸVAEãŒå«ã¾ã‚Œã¦ãŠã‚Šã€[Stability AI Community License](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/./third_party/LICENSE_StabilityAI.md)ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚\n  **å•†ç”¨åˆ©ç”¨ãŠã‚ˆã³å†é…å¸ƒã«ã¯Stability AIã®äº‹å‰è¨±å¯ãŒå¿…è¦ã§ã™ã€‚**\n\n* ğŸ“˜ **ãã®ä»–ã®ã‚³ãƒ¼ãƒ‰ãŠã‚ˆã³ãƒ¢ãƒ‡ãƒ«** ã¯Apache License 2.0ã®ä¸‹ã§å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚\n\n---\n\n## è¬è¾\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 16,
    "Content": "Many thanks to:\n\n* **stable-audio-tools** (by Stability AI):\nFor providing an easy-to-use framework for audio generation, as well as the VAE module and weights.\n* **MMAudio**:\n  For the implementation of the MM-DiT backbone in the audio domain.\n\n---\n\n## ğŸ“– Citation\n\nIf you find ThinkSound useful in your research or work, please cite our paper:\n",
    "ContentSha": "FsK5U++tkthvkZ/Gd4G7gn74YKpB282Oxnkt96u9C1k=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "æ„Ÿè¬ï¼š\n\n* **stable-audio-tools**ï¼ˆStability AI æä¾›ï¼‰ï¼š\néŸ³å£°ç”Ÿæˆã®ãŸã‚ã®ä½¿ã„ã‚„ã™ã„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€ãªã‚‰ã³ã«VAEãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨é‡ã¿ã‚’ã”æä¾›ã„ãŸã ãã¾ã—ãŸã€‚\n* **MMAudio**ï¼š\n  ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªé ˜åŸŸã«ãŠã‘ã‚‹MM-DiTãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã®å®Ÿè£…ã«æ„Ÿè¬ã—ã¾ã™ã€‚\n\n---\n\n## ğŸ“– å¼•ç”¨\n\nã‚‚ã—ThinkSoundãŒç ”ç©¶ã‚„æ¥­å‹™ã§å½¹ç«‹ã£ãŸå ´åˆã¯ã€ãœã²ç§ãŸã¡ã®è«–æ–‡ã‚’å¼•ç”¨ã—ã¦ãã ã•ã„ï¼š\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 17,
    "Content": "```bibtex\n@misc{liu2025thinksoundchainofthoughtreasoningmultimodal,\n      title={ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language Models for Audio Generation and Editing}, \n      author={Huadai Liu and Jialei Wang and Kaicheng Luo and Wen Wang and Qian Chen and Zhou Zhao and Wei Xue},\n      year={2025},\n      eprint={2506.21448},\n      archivePrefix={arXiv},\n      primaryClass={eess.AS},\n      url={https://arxiv.org/abs/2506.21448}, \n}\n```",
    "ContentSha": "KKv35iBt6IDF1ifN04L+6lkh0BHkbObnW/+m50Wufrs=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bibtex\n@misc{liu2025thinksoundchainofthoughtreasoningmultimodal,\n      title={ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language Models for Audio Generation and Editing}, \n      author={Huadai Liu and Jialei Wang and Kaicheng Luo and Wen Wang and Qian Chen and Zhou Zhao and Wei Xue},\n      year={2025},\n      eprint={2506.21448},\n      archivePrefix={arXiv},\n      primaryClass={eess.AS},\n      url={https://arxiv.org/abs/2506.21448}, \n}\n```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 18,
    "Content": "\n---\n\n## ğŸ“¬ Contact\n\n\nâœ¨ Feel free to [open an issue](https://github.com/liuhuadai/ThinkSound/issues) or contact us via email ([liuhuadai@zju.edu.cn](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/mailto:liuhuadai@zju.edu.cn)) if you have any questions or suggestions!\n\n\n",
    "ContentSha": "QMNRHPzbmsL2YxrLNEPneJCBrTj4/XiY9XGTX01NZl8=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "\n---\n\n## ğŸ“¬ Contact\n\n\nâœ¨ Feel free to [open an issue](https://github.com/liuhuadai/ThinkSound/issues) or contact us via email ([liuhuadai@zju.edu.cn](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/mailto:liuhuadai@zju.edu.cn)) if you have any questions or suggestions!\n\n\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 2,
        "rowsha": "yz+R1U7uMOU+NbK5mQX3Dxae1Un9eJCdPawt78ntjTs=",
        "originContent": "---",
        "translatedContent": "---"
      },
      {
        "row": 3,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 4,
        "rowsha": "V0ea1xQLKG+cGj5kHVv5f15HDd+yj0ulkcBQnvErdJc=",
        "originContent": "## ğŸ“¬ Contact",
        "translatedContent": "## ğŸ“¬ Contact"
      },
      {
        "row": 5,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 6,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 7,
        "rowsha": "QsDuVwX1DXlTNejKuNftx4k1x7yNHjfP9/1HS85hJng=",
        "originContent": "âœ¨ Feel free to [open an issue](https://github.com/liuhuadai/ThinkSound/issues) or contact us via email ([liuhuadai@zju.edu.cn](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/mailto:liuhuadai@zju.edu.cn)) if you have any questions or suggestions!",
        "translatedContent": "âœ¨ Feel free to [open an issue](https://github.com/liuhuadai/ThinkSound/issues) or contact us via email ([liuhuadai@zju.edu.cn](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/mailto:liuhuadai@zju.edu.cn)) if you have any questions or suggestions!"
      },
      {
        "row": 8,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 9,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 10,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  }
]