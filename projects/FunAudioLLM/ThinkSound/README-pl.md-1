{
  "id": 1,
  "origin": "# ğŸ¶ ThinkSound\n\n<p align=\"center\">\n  If you find this project useful, a star â­ on GitHub would be greatly appreciated!\n</p>\n\n<p align=\"center\">\n  <a href=\"https://arxiv.org/pdf/2506.21448\">\n    <img src=\"https://img.shields.io/badge/arXiv-2506.21448-b31b1b.svg\" alt=\"arXiv\"/>\n  </a>\n  &nbsp;\n  <a href=\"https://thinksound-project.github.io/\">\n    <img src=\"https://img.shields.io/badge/Online%20Demo-ğŸŒ-blue\" alt=\"Online Demo\"/>\n  </a>\n  &nbsp;\n  <a href=\"https://huggingface.co/spaces/FunAudioLLM/ThinkSound\">\n    <img src=\"https://img.shields.io/badge/HuggingFace-Spaces-orange?logo=huggingface\" alt=\"Hugging Face\"/>\n  </a>\n  &nbsp;\n  <a href=\"https://modelscope.cn/studios/iic/ThinkSound\">\n    <img src=\"https://img.shields.io/badge/ModelScope-åœ¨çº¿ä½“éªŒ-green\" alt=\"ModelScope\"/>\n  </a>\n</p>\n\n---\n\n**ThinkSound** is a unified Any2Audio generation framework with flow matching guided by Chain-of-Thought (CoT) reasoning.\n\nPyTorch implementation for multimodal audio generation and editing: generate or edit audio from video, text, and audio, powered by step-by-step reasoning from Multimodal Large Language Models (MLLMs).\n\n![Teaser](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig1_teaser.png)\n---\n\n## ğŸ“° News\n- **2025.07** &nbsp; ğŸ”¥Online demo on [Hugging Face Spaces](https://huggingface.co/spaces/FunAudioLLM/ThinkSound) and [ModelScope](https://modelscope.cn/studios/iic/ThinkSound) for interactive experience!\n- **2025.07** &nbsp; ğŸ”¥Released inference scripts and web interface; \n- **2025.06** &nbsp; ğŸ”¥[ThinkSound paper](https://arxiv.org/pdf/2506.21448) released on arXiv!\n- **2025.06** &nbsp; ğŸ”¥[Online Demo](http://thinksound-project.github.io/) is live - try it now!\n\n---\n\n## ğŸš€ Features\n\n- **Any2Audio**: Generate audio from arbitrary modalities â€” video, text, audio, or their combinations.\n- **Video-to-Audio SOTA**: Achieves state-of-the-art results on multiple V2A benchmarks.\n- **CoT-Driven Reasoning**: Chain-of-Thought reasoning for compositional and controllable audio generation via MLLMs.\n- **Interactive Object-centric Editing**: Refine or edit specific sound events by clicking on visual objects or using text instructions.\n- **Unified Framework**: One foundation model supports generation, editing, and interactive workflow.\n\n---\n\n## âœ¨ Method Overview\n\nThinkSound decomposes audio generation and editing into three interactive stages, all guided by MLLM-based Chain-of-Thought (CoT) reasoning:\n\n1. **Foley Generation:** Generate foundational, semantically and temporally aligned soundscapes from video.\n2. **Object-Centric Refinement:** Refine or add sounds for user-specified objects via clicks or regions in the video.\n3. **Targeted Audio Editing:** Modify generated audio using high-level natural language instructions.\n\n![ThinkSound Overview](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig3_model.png)\n<!-- A large-scale CoT-annotated dataset (**AudioCoT**) is used to train both the reasoning module and the unified audio foundation model.\n![AudioCoT Pipeline](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig2_dataset.png) -->\n\n---\n\n## âš¡ Quick Start\n\n**Environment Preparation:**\n```bash\ngit clone https://github.com/liuhuadai/ThinkSound.git\ncd ThinkSound\npip install -r requirements.txt\nconda install -y -c conda-forge 'ffmpeg<7'\n# Download pretrained weights https://huggingface.co/liuhuadai/ThinkSound to Directory ckpts/\n# model weights can be also downloaded from https://www.modelscope.cn/models/iic/ThinkSound\ngit lfs install\ngit clone https://huggingface.co/liuhuadai/ThinkSound ckpts\n```\n\n**Make it executable**\n```bash\nchmod +x scripts/demo.sh\n```\n\n**Run the script**\n```bash\n./scripts/demo.sh <video_path> <caption> <CoT description>\n```\n\n\n### Web Interface Usage\n\nFor an interactive experience, launch the Gradio web interface:\n\n```bash\npython app.py\n```\n\n---\n",
  "origin_sha": "vYi4X/38TyDFihDOfSBGZIgKnu2bd0a73Iz5+m+7pd0=",
  "translate": "# ğŸ¶ ThinkSound\n\n<p align=\"center\">\n  JeÅ›li uznasz ten projekt za przydatny, gwiazdka â­ na GitHubie bÄ™dzie bardzo mile widziana!\n</p>\n\n<p align=\"center\">\n  <a href=\"https://arxiv.org/pdf/2506.21448\">\n    <img src=\"https://img.shields.io/badge/arXiv-2506.21448-b31b1b.svg\" alt=\"arXiv\"/>\n  </a>\n  &nbsp;\n  <a href=\"https://thinksound-project.github.io/\">\n    <img src=\"https://img.shields.io/badge/Online%20Demo-ğŸŒ-blue\" alt=\"Online Demo\"/>\n  </a>\n  &nbsp;\n  <a href=\"https://huggingface.co/spaces/FunAudioLLM/ThinkSound\">\n    <img src=\"https://img.shields.io/badge/HuggingFace-Spaces-orange?logo=huggingface\" alt=\"Hugging Face\"/>\n  </a>\n  &nbsp;\n  <a href=\"https://modelscope.cn/studios/iic/ThinkSound\">\n    <img src=\"https://img.shields.io/badge/ModelScope-åœ¨çº¿ä½“éªŒ-green\" alt=\"ModelScope\"/>\n  </a>\n</p>\n\n---\n\n**ThinkSound** to zunifikowane Å›rodowisko generowania Any2Audio z dopasowaniem przepÅ‚ywu sterowanym przez rozumowanie Chain-of-Thought (CoT).\n\nImplementacja PyTorch do multimodalnej generacji i edycji dÅºwiÄ™ku: generuj lub edytuj dÅºwiÄ™k na podstawie wideo, tekstu oraz dÅºwiÄ™ku, napÄ™dzana przez rozumowanie krok po kroku z wykorzystaniem Multimodalnych DuÅ¼ych Modeli JÄ™zykowych (MLLM).\n\n![Teaser](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig1_teaser.png)\n---\n\n## ğŸ“° AktualnoÅ›ci\n- **2025.07** &nbsp; ğŸ”¥Demo online na [Hugging Face Spaces](https://huggingface.co/spaces/FunAudioLLM/ThinkSound) oraz [ModelScope](https://modelscope.cn/studios/iic/ThinkSound) do interaktywnego korzystania!\n- **2025.07** &nbsp; ğŸ”¥Wydano skrypty do wnioskowania i interfejs webowy;\n- **2025.06** &nbsp; ğŸ”¥[ArtykuÅ‚ ThinkSound](https://arxiv.org/pdf/2506.21448) opublikowany na arXiv!\n- **2025.06** &nbsp; ğŸ”¥[Demo online](http://thinksound-project.github.io/) juÅ¼ dostÄ™pne â€“ wyprÃ³buj teraz!\n\n---\n\n## ğŸš€ Funkcje\n\n- **Any2Audio**: Generuj dÅºwiÄ™k z dowolnej modalnoÅ›ci â€” wideo, tekstu, dÅºwiÄ™ku lub ich kombinacji.\n- **Video-to-Audio SOTA**: OsiÄ…ga najnowsze wyniki na wielu benchmarkach V2A.\n- **CoT-Driven Reasoning**: Rozumowanie Chain-of-Thought dla kompozycyjnej i kontrolowanej generacji dÅºwiÄ™ku przez MLLM.\n- **Interaktywna Edycja Skoncentrowana na Obiektach**: Ulepszaj lub edytuj konkretne zdarzenia dÅºwiÄ™kowe poprzez klikniÄ™cie na obiekt wizualny lub uÅ¼ycie instrukcji tekstowych.\n- **Zunifikowany Framework**: Jeden model bazowy obsÅ‚uguje generowanie, edycjÄ™ oraz interaktywny workflow.\n\n---\n\n## âœ¨ PrzeglÄ…d Metody\n\nThinkSound rozkÅ‚ada generowanie i edycjÄ™ dÅºwiÄ™ku na trzy interaktywne etapy, wszystkie sterowane rozumowaniem Chain-of-Thought (CoT) opartym na MLLM:\n\n1. **Generowanie Foley:** Tworzenie podstawowych, semantycznie i czasowo dopasowanych pejzaÅ¼y dÅºwiÄ™kowych na podstawie wideo.\n2. **Refinacja Skoncentrowana na Obiekcie:** Ulepszaj lub dodawaj dÅºwiÄ™ki dla obiektÃ³w wskazanych przez uÅ¼ytkownika za pomocÄ… klikniÄ™Ä‡ lub zaznaczeÅ„ na wideo.\n3. **Celowana Edycja DÅºwiÄ™ku:** Modyfikuj wygenerowany dÅºwiÄ™k za pomocÄ… zÅ‚oÅ¼onych instrukcji w jÄ™zyku naturalnym.\n\n![ThinkSound Overview](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig3_model.png)\n<!-- DuÅ¼y, oznaczony danymi CoT zbiÃ³r danych (**AudioCoT**) wykorzystywany jest do trenowania zarÃ³wno moduÅ‚u rozumowania, jak i zunifikowanego modelu bazowego dÅºwiÄ™ku.\n![AudioCoT Pipeline](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig2_dataset.png) -->\n\n---\n\n## âš¡ Szybki Start\n\n**Przygotowanie Å›rodowiska:**\n```bash\ngit clone https://github.com/liuhuadai/ThinkSound.git\ncd ThinkSound\npip install -r requirements.txt\nconda install -y -c conda-forge 'ffmpeg<7'\n# Pobierz wytrenowane wagi https://huggingface.co/liuhuadai/ThinkSound do katalogu ckpts/\n# Wagi modelu moÅ¼na rÃ³wnieÅ¼ pobraÄ‡ z https://www.modelscope.cn/models/iic/ThinkSound\ngit lfs install\ngit clone https://huggingface.co/liuhuadai/ThinkSound ckpts\n```\n\n**Ustaw jako wykonywalny**\n```bash\nchmod +x scripts/demo.sh\n```\n\n**Uruchom skrypt**\n```bash\n./scripts/demo.sh <Å›cieÅ¼ka_do_wideo> <opis> <opis_CoT>\n```\n\n### Korzystanie z interfejsu webowego\n\nAby uzyskaÄ‡ interaktywne doÅ›wiadczenie, uruchom webowy interfejs Gradio:\n\n```bash\npython app.py\n```\n\n---",
  "status": "ok"
}