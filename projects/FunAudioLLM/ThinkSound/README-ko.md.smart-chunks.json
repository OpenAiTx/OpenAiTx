[
  {
    "Id": 1,
    "Content": "<h1 align=\"center\">ThinkSound</h1>\n\n<p align=\"center\">\n  ğŸŒ\n  <a href=\"https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=en\">English</a> |\n  <a href=\"https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=zh-CN\">ç®€ä½“ä¸­æ–‡</a> |\n  <a href=\"https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=zh-TW\">ç¹é«”ä¸­æ–‡</a> |\n  <a href=\"https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=es\">EspaÃ±ol</a> |\n  <a href=\"https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=fr\">FranÃ§ais</a> |\n  <a href=\"https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=ja\">æ—¥æœ¬èª</a>\n  \n</p>\n<p align=\"center\">\n  <img src=\"https://img.shields.io/badge/NeurIPS 2025-Main Conference-blue.svg\" alt=\"NeurIPS 2025\"/>\n<p align=\"center\">\n  <a href=\"https://arxiv.org/pdf/2506.21448\">\n    <img src=\"https://img.shields.io/badge/arXiv-2506.21448-b31b1b.svg\" alt=\"arXiv\"/>\n  </a>\n  &nbsp;\n  <a href=\"https://thinksound-project.github.io/\">\n    <img src=\"https://img.shields.io/badge/Online%20Demo-ğŸŒ-blue\" alt=\"Online Demo\"/>\n  </a>\n  &nbsp;\n  <a href=\"https://huggingface.co/spaces/FunAudioLLM/ThinkSound\">\n    <img src=\"https://img.shields.io/badge/HuggingFace-Spaces-orange?logo=huggingface\" alt=\"Hugging Face\"/>\n  </a>\n  &nbsp;\n  <a href=\"https://modelscope.cn/studios/iic/ThinkSound\">\n    <img src=\"https://img.shields.io/badge/ModelScope-åœ¨çº¿ä½“éªŒ-green\" alt=\"ModelScope\"/>\n  </a>\n</p>\n\n<p align=\"center\">\n  If you find this project useful,<br>\n  a star â­ on GitHub would be greatly appreciated!\n</p>\n\n---\n\n**ThinkSound** is a unified Any2Audio generation framework with flow matching guided by Chain-of-Thought (CoT) reasoning.",
    "ContentSha": "3764VshEOMedejxsV+uo8k5R5Emk0MLuYfIOX8JadcY=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "<h1 align=\"center\">ThinkSound</h1>\n\n<p align=\"center\">\n  ğŸŒ\n  <a href=\"https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=en\">English</a> |\n  <a href=\"https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=zh-CN\">ç®€ä½“ä¸­æ–‡</a> |\n  <a href=\"https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=zh-TW\">ç¹é«”ä¸­æ–‡</a> |\n  <a href=\"https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=es\">EspaÃ±ol</a> |\n  <a href=\"https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=fr\">FranÃ§ais</a> |\n  <a href=\"https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=ja\">æ—¥æœ¬èª</a>\n  \n</p>\n<p align=\"center\">\n  <img src=\"https://img.shields.io/badge/NeurIPS 2025-Main Conference-blue.svg\" alt=\"NeurIPS 2025\"/>\n<p align=\"center\">\n  <a href=\"https://arxiv.org/pdf/2506.21448\">\n    <img src=\"https://img.shields.io/badge/arXiv-2506.21448-b31b1b.svg\" alt=\"arXiv\"/>\n  </a>\n  &nbsp;\n  <a href=\"https://thinksound-project.github.io/\">\n    <img src=\"https://img.shields.io/badge/Online%20Demo-ğŸŒ-blue\" alt=\"Online Demo\"/>\n  </a>\n  &nbsp;\n  <a href=\"https://huggingface.co/spaces/FunAudioLLM/ThinkSound\">\n    <img src=\"https://img.shields.io/badge/HuggingFace-Spaces-orange?logo=huggingface\" alt=\"Hugging Face\"/>\n  </a>\n  &nbsp;\n  <a href=\"https://modelscope.cn/studios/iic/ThinkSound\">\n    <img src=\"https://img.shields.io/badge/ModelScope-ì˜¨ë¼ì¸ ì²´í—˜-green\" alt=\"ModelScope\"/>\n  </a>\n</p>\n\n<p align=\"center\">\n  ì´ í”„ë¡œì íŠ¸ê°€ ë„ì›€ì´ ë˜ì—ˆë‹¤ë©´,<br>\n  GitHubì— ë³„í‘œ â­ë¥¼ ë‚¨ê²¨ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤!\n</p>\n\n---\n\n**ThinkSound**ëŠ” Chain-of-Thought (CoT) ì¶”ë¡ ì„ í™œìš©í•œ íë¦„ ë§¤ì¹­ ê¸°ë°˜ì˜ í†µí•© Any2Audio ìƒì„± í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 2,
    "Content": "\nPyTorch implementation for multimodal audio generation and editing: generate or edit audio from video, text, and audio, powered by step-by-step reasoning from Multimodal Large Language Models (MLLMs).\n\n![Teaser](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig1_teaser.png)\n---\n\n## ğŸ“° News\n- **2025.11.25** &nbsp; ğŸ”¥[Online PrismAudio Demo](http://prismaudio-project.github.io/) is live - try it now!\n- **2025.11.25** &nbsp; ğŸ”¥[PrismAudio paper](https://arxiv.org/pdf/2511.18833) released on arXiv, the first multi-dimensional CoT-RL framework for Video-to-Audio Generation!\n- **2025.09.19** &nbsp; ğŸ‰ ThinkSound has been accepted to the **NeurIPS 2025 Main Conference**!\n- **2025.09.01** &nbsp; Our AudioCoT dataset is now open-sourced and available on [Hugging Face](https://huggingface.co/datasets/liuhuadai/AudioCoT)!\n- **2025.07.17** &nbsp; ğŸ§  Finetuning enabled: training and finetuning code is now publicly available, along with clear usage instructions to help you customize and extend ThinkSound with your own data.\n- **2025.07.15** &nbsp; ğŸ“¦ Simplified installation and usability: dependencies on PyPI for easy cross-platform setup; Windows `.bat` scripts automate environment creation and script running.\n- **2025.07.08** &nbsp;Â  ğŸ”§ Major update: model lightweighted and optimized memory and GPU usage, now supports high-throughput audio generation at scale!\n- **2025.07.01** &nbsp; Online demo on [Hugging Face Spaces](https://huggingface.co/spaces/FunAudioLLM/ThinkSound) and [ModelScope](https://modelscope.cn/studios/iic/ThinkSound) for interactive experience!\n- **2025.07.01** &nbsp; Released inference scripts and web interface; \n- **2025.06** &nbsp; [ThinkSound paper](https://arxiv.org/pdf/2506.21448) released on arXiv!\n- **2025.06** &nbsp; [Online Demo](http://thinksound-project.github.io/) is live - try it now!\n\n---\n\n\n## ğŸš€ Features\n\n- **Any2Audio**: Generate audio from arbitrary modalities â€” video, text, audio, or their combinations.\n- **Video-to-Audio SOTA**: Achieves state-of-the-art results on multiple V2A benchmarks.\n- **CoT-Driven Reasoning**: Chain-of-Thought reasoning for compositional and controllable audio generation via MLLMs.\n- **Interactive Object-centric Editing**: Refine or edit specific sound events by clicking on visual objects or using text instructions.\n- **Unified Framework**: One foundation model supports generation, editing, and interactive workflow.\n\n---\n\n## âœ¨ Method Overview\n\nThinkSound decomposes audio generation and editing into three interactive stages, all guided by MLLM-based Chain-of-Thought (CoT) reasoning:\n\n1. **Foley Generation:** Generate foundational, semantically and temporally aligned soundscapes from video.\n2. **Object-Centric Refinement:** Refine or add sounds for user-specified objects via clicks or regions in the video.\n3. **Targeted Audio Editing:** Modify generated audio using high-level natural language instructions.\n",
    "ContentSha": "Vp81xeUGr9WCESp62x1lkoIptJHl0dM1R8kleL4V10c=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "\në©€í‹°ëª¨ë‹¬ ì˜¤ë””ì˜¤ ìƒì„± ë° í¸ì§‘ì„ ìœ„í•œ PyTorch êµ¬í˜„: ë¹„ë””ì˜¤, í…ìŠ¤íŠ¸, ì˜¤ë””ì˜¤ë¡œë¶€í„° ì˜¤ë””ì˜¤ë¥¼ ìƒì„±í•˜ê±°ë‚˜ í¸ì§‘í•˜ë©°, ë©€í‹°ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(MLLM)ì˜ ë‹¨ê³„ë³„ ì¶”ë¡ ì— ì˜í•´ êµ¬ë™ë©ë‹ˆë‹¤.\n\n![Teaser](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig1_teaser.png)\n---\n\n## ğŸ“° ì†Œì‹\n- **2025.11.25** &nbsp; ğŸ”¥[ì˜¨ë¼ì¸ PrismAudio ë°ëª¨](http://prismaudio-project.github.io/) ì˜¤í”ˆ - ì§€ê¸ˆ ë°”ë¡œ ì‚¬ìš©í•´ë³´ì„¸ìš”!\n- **2025.11.25** &nbsp; ğŸ”¥[PrismAudio ë…¼ë¬¸](https://arxiv.org/pdf/2511.18833)ì´ arXivì— ê³µê°œë˜ì—ˆìŠµë‹ˆë‹¤. ìµœì´ˆì˜ ë‹¤ì°¨ì› CoT-RL ê¸°ë°˜ ì˜ìƒ-ì˜¤ë””ì˜¤ ìƒì„± í”„ë ˆì„ì›Œí¬!\n- **2025.09.19** &nbsp; ğŸ‰ ThinkSoundê°€ **NeurIPS 2025 ë©”ì¸ ì»¨í¼ëŸ°ìŠ¤**ì— ì±„íƒë˜ì—ˆìŠµë‹ˆë‹¤!\n- **2025.09.01** &nbsp; AudioCoT ë°ì´í„°ì…‹ì´ ì˜¤í”ˆì†ŒìŠ¤ë¡œ ê³µê°œë˜ì—ˆìœ¼ë©° [Hugging Face](https://huggingface.co/datasets/liuhuadai/AudioCoT)ì—ì„œ ì´ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤!\n- **2025.07.17** &nbsp; ğŸ§  íŒŒì¸íŠœë‹ ì§€ì›: í•™ìŠµ ë° íŒŒì¸íŠœë‹ ì½”ë“œê°€ ê³µê°œë˜ì–´, ì‚¬ìš©ìê°€ ì§ì ‘ ë°ì´í„°ë¡œ ThinkSoundë¥¼ ë§ì¶¤ ë° í™•ì¥í•  ìˆ˜ ìˆë„ë¡ ì‚¬ìš©ë²•ì´ ì œê³µë©ë‹ˆë‹¤.\n- **2025.07.15** &nbsp; ğŸ“¦ ê°„í¸í•œ ì„¤ì¹˜ ë° ì‚¬ìš©ì„±: PyPI ê¸°ë°˜ ì˜ì¡´ì„± ê´€ë¦¬ë¡œ í¬ë¡œìŠ¤í”Œë«í¼ ì„¤ì¹˜ ê°€ëŠ¥; Windows `.bat` ìŠ¤í¬ë¦½íŠ¸ë¡œ í™˜ê²½ ìƒì„±ê³¼ ì‹¤í–‰ ìë™í™”.\n- **2025.07.08** &nbsp;Â  ğŸ”§ ì£¼ìš” ì—…ë°ì´íŠ¸: ëª¨ë¸ ê²½ëŸ‰í™” ë° ë©”ëª¨ë¦¬Â·GPU ìµœì í™”, ëŒ€ê·œëª¨ ê³ ì„±ëŠ¥ ì˜¤ë””ì˜¤ ìƒì„± ì§€ì›!\n- **2025.07.01** &nbsp; [Hugging Face Spaces](https://huggingface.co/spaces/FunAudioLLM/ThinkSound) ë° [ModelScope](https://modelscope.cn/studios/iic/ThinkSound)ì—ì„œ ì˜¨ë¼ì¸ ë°ëª¨ ì²´í—˜ ì œê³µ!\n- **2025.07.01** &nbsp; ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸ì™€ ì›¹ ì¸í„°í˜ì´ìŠ¤ ê³µê°œ;\n- **2025.06** &nbsp; [ThinkSound ë…¼ë¬¸](https://arxiv.org/pdf/2506.21448)ì´ arXivì— ê³µê°œë˜ì—ˆìŠµë‹ˆë‹¤!\n- **2025.06** &nbsp; [ì˜¨ë¼ì¸ ë°ëª¨](http://thinksound-project.github.io/) ì˜¤í”ˆ - ì§€ê¸ˆ ë°”ë¡œ ì²´í—˜í•´ë³´ì„¸ìš”!\n\n---\n\n\n## ğŸš€ ì£¼ìš” ê¸°ëŠ¥\n\n- **Any2Audio**: ë¹„ë””ì˜¤, í…ìŠ¤íŠ¸, ì˜¤ë””ì˜¤ ë° ê·¸ ì¡°í•© ë“± ì„ì˜ì˜ ëª¨ë‹¬ë¦¬í‹°ì—ì„œ ì˜¤ë””ì˜¤ ìƒì„±.\n- **Video-to-Audio SOTA**: ë‹¤ìˆ˜ì˜ V2A ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìµœì‹  ì„±ëŠ¥ ë‹¬ì„±.\n- **CoT ê¸°ë°˜ ì¶”ë¡ **: MLLMì„ í†µí•œ ì¡°í•©ì Â·ì œì–´ ê°€ëŠ¥í•œ ì˜¤ë””ì˜¤ ìƒì„±ì„ ìœ„í•œ Chain-of-Thought ì¶”ë¡ .\n- **ëŒ€í™”í˜• ì˜¤ë¸Œì íŠ¸ ì¤‘ì‹¬ í¸ì§‘**: ì‹œê° ê°ì²´ í´ë¦­ ë˜ëŠ” í…ìŠ¤íŠ¸ ëª…ë ¹ìœ¼ë¡œ íŠ¹ì • ì†Œë¦¬ ì´ë²¤íŠ¸ ì„¸ë°€ ì¡°ì • ë° í¸ì§‘.\n- **í†µí•© í”„ë ˆì„ì›Œí¬**: í•˜ë‚˜ì˜ ê¸°ë°˜ ëª¨ë¸ë¡œ ìƒì„±, í¸ì§‘, ëŒ€í™”í˜• ì›Œí¬í”Œë¡œìš° ì§€ì›.\n\n---\n\n## âœ¨ ë°©ë²• ê°œìš”\n\nThinkSoundëŠ” ì˜¤ë””ì˜¤ ìƒì„± ë° í¸ì§‘ ê³¼ì •ì„ MLLM ê¸°ë°˜ Chain-of-Thought(CoT) ì¶”ë¡ ì— ì˜í•´ ì•ˆë‚´ë˜ëŠ” ì„¸ ê°€ì§€ ëŒ€í™”í˜• ë‹¨ê³„ë¡œ ë¶„í•´í•©ë‹ˆë‹¤:\n\n1. **í´ë¦¬ ìƒì„±:** ë¹„ë””ì˜¤ë¡œë¶€í„° ì˜ë¯¸ì Â·ì‹œê°„ì ìœ¼ë¡œ ì •ë ¬ëœ ê¸°ë³¸ ì‚¬ìš´ë“œìŠ¤ì¼€ì´í”„ ìƒì„±.\n2. **ì˜¤ë¸Œì íŠ¸ ì¤‘ì‹¬ ì •ì œ:** ë¹„ë””ì˜¤ ë‚´ í´ë¦­ ë˜ëŠ” ì˜ì—­ ì§€ì •ìœ¼ë¡œ ì‚¬ìš©ìê°€ ì„ íƒí•œ ì˜¤ë¸Œì íŠ¸ì˜ ì‚¬ìš´ë“œ ì •ì œ ë˜ëŠ” ì¶”ê°€.\n3. **íƒ€ê¹ƒ ì˜¤ë””ì˜¤ í¸ì§‘:** ê³ ìˆ˜ì¤€ ìì—°ì–´ ëª…ë ¹ì„ ì‚¬ìš©í•œ ìƒì„± ì˜¤ë””ì˜¤ ìˆ˜ì •.\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 2,
        "rowsha": "eexREJOMCITdo96Iwg4N4PQqu1ux/UwupfjVRj0l/L8=",
        "originContent": "PyTorch implementation for multimodal audio generation and editing: generate or edit audio from video, text, and audio, powered by step-by-step reasoning from Multimodal Large Language Models (MLLMs).",
        "translatedContent": "ë©€í‹°ëª¨ë‹¬ ì˜¤ë””ì˜¤ ìƒì„± ë° í¸ì§‘ì„ ìœ„í•œ PyTorch êµ¬í˜„: ë¹„ë””ì˜¤, í…ìŠ¤íŠ¸, ì˜¤ë””ì˜¤ë¡œë¶€í„° ì˜¤ë””ì˜¤ë¥¼ ìƒì„±í•˜ê±°ë‚˜ í¸ì§‘í•˜ë©°, ë©€í‹°ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(MLLM)ì˜ ë‹¨ê³„ë³„ ì¶”ë¡ ì— ì˜í•´ êµ¬ë™ë©ë‹ˆë‹¤."
      },
      {
        "row": 3,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 4,
        "rowsha": "+7Cx1rdpWiIOeWRA/RM6HsKf7pNA3hHsQk2Bpadt4II=",
        "originContent": "![Teaser](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig1_teaser.png)",
        "translatedContent": "![Teaser](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig1_teaser.png)"
      },
      {
        "row": 5,
        "rowsha": "yz+R1U7uMOU+NbK5mQX3Dxae1Un9eJCdPawt78ntjTs=",
        "originContent": "---",
        "translatedContent": "---"
      },
      {
        "row": 6,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 7,
        "rowsha": "0YeCyxpcm7/4RDbaM+OQoI8YfqEHQzDkpGW15VZdi1U=",
        "originContent": "## ğŸ“° News",
        "translatedContent": "## ğŸ“° ì†Œì‹"
      },
      {
        "row": 8,
        "rowsha": "EeDKxV0PYB9iw6GxOqwnsP3C385ykbqk8PtEVoVhAcc=",
        "originContent": "- **2025.11.25** &nbsp; ğŸ”¥[Online PrismAudio Demo](http://prismaudio-project.github.io/) is live - try it now!",
        "translatedContent": "- **2025.11.25** &nbsp; ğŸ”¥[ì˜¨ë¼ì¸ PrismAudio ë°ëª¨](http://prismaudio-project.github.io/) ì˜¤í”ˆ - ì§€ê¸ˆ ë°”ë¡œ ì‚¬ìš©í•´ë³´ì„¸ìš”!"
      },
      {
        "row": 9,
        "rowsha": "Q6c68QE34xLPSZ5xAL4zofEn/u8umRYYiH3ddCcy5vk=",
        "originContent": "- **2025.11.25** &nbsp; ğŸ”¥[PrismAudio paper](https://arxiv.org/pdf/2511.18833) released on arXiv, the first multi-dimensional CoT-RL framework for Video-to-Audio Generation!",
        "translatedContent": "- **2025.11.25** &nbsp; ğŸ”¥[PrismAudio ë…¼ë¬¸](https://arxiv.org/pdf/2511.18833)ì´ arXivì— ê³µê°œë˜ì—ˆìŠµë‹ˆë‹¤. ìµœì´ˆì˜ ë‹¤ì°¨ì› CoT-RL ê¸°ë°˜ ì˜ìƒ-ì˜¤ë””ì˜¤ ìƒì„± í”„ë ˆì„ì›Œí¬!"
      },
      {
        "row": 10,
        "rowsha": "vj2hbV/6MacDcxRmFA0AtTog5xU6CHoqBFO3YMO+yJo=",
        "originContent": "- **2025.09.19** &nbsp; ğŸ‰ ThinkSound has been accepted to the **NeurIPS 2025 Main Conference**!",
        "translatedContent": "- **2025.09.19** &nbsp; ğŸ‰ ThinkSoundê°€ **NeurIPS 2025 ë©”ì¸ ì»¨í¼ëŸ°ìŠ¤**ì— ì±„íƒë˜ì—ˆìŠµë‹ˆë‹¤!"
      },
      {
        "row": 11,
        "rowsha": "+guVQwDlJDqR1pBGMGc4FQ11nryItaEtKQ3etaCBcn0=",
        "originContent": "- **2025.09.01** &nbsp; Our AudioCoT dataset is now open-sourced and available on [Hugging Face](https://huggingface.co/datasets/liuhuadai/AudioCoT)!",
        "translatedContent": "- **2025.09.01** &nbsp; AudioCoT ë°ì´í„°ì…‹ì´ ì˜¤í”ˆì†ŒìŠ¤ë¡œ ê³µê°œë˜ì—ˆìœ¼ë©° [Hugging Face](https://huggingface.co/datasets/liuhuadai/AudioCoT)ì—ì„œ ì´ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤!"
      },
      {
        "row": 12,
        "rowsha": "4Jq9g83O8cYV4fVKsetfTpI+JrSepLrjK6J7Xg9tSqo=",
        "originContent": "- **2025.07.17** &nbsp; ğŸ§  Finetuning enabled: training and finetuning code is now publicly available, along with clear usage instructions to help you customize and extend ThinkSound with your own data.",
        "translatedContent": "- **2025.07.17** &nbsp; ğŸ§  íŒŒì¸íŠœë‹ ì§€ì›: í•™ìŠµ ë° íŒŒì¸íŠœë‹ ì½”ë“œê°€ ê³µê°œë˜ì–´, ì‚¬ìš©ìê°€ ì§ì ‘ ë°ì´í„°ë¡œ ThinkSoundë¥¼ ë§ì¶¤ ë° í™•ì¥í•  ìˆ˜ ìˆë„ë¡ ì‚¬ìš©ë²•ì´ ì œê³µë©ë‹ˆë‹¤."
      },
      {
        "row": 13,
        "rowsha": "Ae5w+cTrCd9E8qidC11IWJAgg+LuOgUxA4czDPNG/G0=",
        "originContent": "- **2025.07.15** &nbsp; ğŸ“¦ Simplified installation and usability: dependencies on PyPI for easy cross-platform setup; Windows `.bat` scripts automate environment creation and script running.",
        "translatedContent": "- **2025.07.15** &nbsp; ğŸ“¦ ê°„í¸í•œ ì„¤ì¹˜ ë° ì‚¬ìš©ì„±: PyPI ê¸°ë°˜ ì˜ì¡´ì„± ê´€ë¦¬ë¡œ í¬ë¡œìŠ¤í”Œë«í¼ ì„¤ì¹˜ ê°€ëŠ¥; Windows `.bat` ìŠ¤í¬ë¦½íŠ¸ë¡œ í™˜ê²½ ìƒì„±ê³¼ ì‹¤í–‰ ìë™í™”."
      },
      {
        "row": 14,
        "rowsha": "nYxNhwgSqjwYLuWsfHAqP5sx2PnzYwoFrwcf9U+Fdss=",
        "originContent": "- **2025.07.08** &nbsp;Â  ğŸ”§ Major update: model lightweighted and optimized memory and GPU usage, now supports high-throughput audio generation at scale!",
        "translatedContent": "- **2025.07.08** &nbsp;Â  ğŸ”§ ì£¼ìš” ì—…ë°ì´íŠ¸: ëª¨ë¸ ê²½ëŸ‰í™” ë° ë©”ëª¨ë¦¬Â·GPU ìµœì í™”, ëŒ€ê·œëª¨ ê³ ì„±ëŠ¥ ì˜¤ë””ì˜¤ ìƒì„± ì§€ì›!"
      },
      {
        "row": 15,
        "rowsha": "RPL6cU3/Jgu90ib4PkeN5Q/ALrnjq9hK0ZUCranb/PA=",
        "originContent": "- **2025.07.01** &nbsp; Online demo on [Hugging Face Spaces](https://huggingface.co/spaces/FunAudioLLM/ThinkSound) and [ModelScope](https://modelscope.cn/studios/iic/ThinkSound) for interactive experience!",
        "translatedContent": "- **2025.07.01** &nbsp; [Hugging Face Spaces](https://huggingface.co/spaces/FunAudioLLM/ThinkSound) ë° [ModelScope](https://modelscope.cn/studios/iic/ThinkSound)ì—ì„œ ì˜¨ë¼ì¸ ë°ëª¨ ì²´í—˜ ì œê³µ!"
      },
      {
        "row": 16,
        "rowsha": "0PEL3eOyUmX2U56FPEPvYfaltZ/P/wbH0uO6GcLPlUE=",
        "originContent": "- **2025.07.01** &nbsp; Released inference scripts and web interface; ",
        "translatedContent": "- **2025.07.01** &nbsp; ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸ì™€ ì›¹ ì¸í„°í˜ì´ìŠ¤ ê³µê°œ;"
      },
      {
        "row": 17,
        "rowsha": "XNdJ/DN741rXoJAruiGiRueQILXIUHRXzBlp+HVWM88=",
        "originContent": "- **2025.06** &nbsp; [ThinkSound paper](https://arxiv.org/pdf/2506.21448) released on arXiv!",
        "translatedContent": "- **2025.06** &nbsp; [ThinkSound ë…¼ë¬¸](https://arxiv.org/pdf/2506.21448)ì´ arXivì— ê³µê°œë˜ì—ˆìŠµë‹ˆë‹¤!"
      },
      {
        "row": 18,
        "rowsha": "W45oflUmoUAksoDc1WjcR2hErqh9UIi738PFVipiDg0=",
        "originContent": "- **2025.06** &nbsp; [Online Demo](http://thinksound-project.github.io/) is live - try it now!",
        "translatedContent": "- **2025.06** &nbsp; [ì˜¨ë¼ì¸ ë°ëª¨](http://thinksound-project.github.io/) ì˜¤í”ˆ - ì§€ê¸ˆ ë°”ë¡œ ì²´í—˜í•´ë³´ì„¸ìš”!"
      },
      {
        "row": 19,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 20,
        "rowsha": "yz+R1U7uMOU+NbK5mQX3Dxae1Un9eJCdPawt78ntjTs=",
        "originContent": "---",
        "translatedContent": "---"
      },
      {
        "row": 21,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 22,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 23,
        "rowsha": "f4oQIFLM2EJQxJ65F4oMEA7yWOIqs0eBtiIvGxI+GgI=",
        "originContent": "## ğŸš€ Features",
        "translatedContent": "## ğŸš€ ì£¼ìš” ê¸°ëŠ¥"
      },
      {
        "row": 24,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 25,
        "rowsha": "8TbVluJJPR3C0fX41HklpO/vH9/k/rHC+jlvmHHfzQU=",
        "originContent": "- **Any2Audio**: Generate audio from arbitrary modalities â€” video, text, audio, or their combinations.",
        "translatedContent": "- **Any2Audio**: ë¹„ë””ì˜¤, í…ìŠ¤íŠ¸, ì˜¤ë””ì˜¤ ë° ê·¸ ì¡°í•© ë“± ì„ì˜ì˜ ëª¨ë‹¬ë¦¬í‹°ì—ì„œ ì˜¤ë””ì˜¤ ìƒì„±."
      },
      {
        "row": 26,
        "rowsha": "+Jf5LrYz+d1ShYCuQe8UF9taEJUpuGhDk6fAlYK3Kj4=",
        "originContent": "- **Video-to-Audio SOTA**: Achieves state-of-the-art results on multiple V2A benchmarks.",
        "translatedContent": "- **Video-to-Audio SOTA**: ë‹¤ìˆ˜ì˜ V2A ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìµœì‹  ì„±ëŠ¥ ë‹¬ì„±."
      },
      {
        "row": 27,
        "rowsha": "mU7qXkjW1YifKoYXJedYo9l64NsTBaiXsgoGFRF/g+E=",
        "originContent": "- **CoT-Driven Reasoning**: Chain-of-Thought reasoning for compositional and controllable audio generation via MLLMs.",
        "translatedContent": "- **CoT ê¸°ë°˜ ì¶”ë¡ **: MLLMì„ í†µí•œ ì¡°í•©ì Â·ì œì–´ ê°€ëŠ¥í•œ ì˜¤ë””ì˜¤ ìƒì„±ì„ ìœ„í•œ Chain-of-Thought ì¶”ë¡ ."
      },
      {
        "row": 28,
        "rowsha": "RComOCBBrXsZf9RHmLginqKTh9eI/bKUZuUunQEmD5M=",
        "originContent": "- **Interactive Object-centric Editing**: Refine or edit specific sound events by clicking on visual objects or using text instructions.",
        "translatedContent": "- **ëŒ€í™”í˜• ì˜¤ë¸Œì íŠ¸ ì¤‘ì‹¬ í¸ì§‘**: ì‹œê° ê°ì²´ í´ë¦­ ë˜ëŠ” í…ìŠ¤íŠ¸ ëª…ë ¹ìœ¼ë¡œ íŠ¹ì • ì†Œë¦¬ ì´ë²¤íŠ¸ ì„¸ë°€ ì¡°ì • ë° í¸ì§‘."
      },
      {
        "row": 29,
        "rowsha": "C3sf87sy73G/XZft+TDo5NjXo5XcrtJB805ayHRAXoQ=",
        "originContent": "- **Unified Framework**: One foundation model supports generation, editing, and interactive workflow.",
        "translatedContent": "- **í†µí•© í”„ë ˆì„ì›Œí¬**: í•˜ë‚˜ì˜ ê¸°ë°˜ ëª¨ë¸ë¡œ ìƒì„±, í¸ì§‘, ëŒ€í™”í˜• ì›Œí¬í”Œë¡œìš° ì§€ì›."
      },
      {
        "row": 30,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 31,
        "rowsha": "yz+R1U7uMOU+NbK5mQX3Dxae1Un9eJCdPawt78ntjTs=",
        "originContent": "---",
        "translatedContent": "---"
      },
      {
        "row": 32,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 33,
        "rowsha": "gjgLOIAU2x83BBZdLUdgEB+F64ajt/QuLYQXM1hDBLE=",
        "originContent": "## âœ¨ Method Overview",
        "translatedContent": "## âœ¨ ë°©ë²• ê°œìš”"
      },
      {
        "row": 34,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 35,
        "rowsha": "v5//GqG/smYIGoUpN+12k+9/3GWH3GdYD+jLqScb7AM=",
        "originContent": "ThinkSound decomposes audio generation and editing into three interactive stages, all guided by MLLM-based Chain-of-Thought (CoT) reasoning:",
        "translatedContent": "ThinkSoundëŠ” ì˜¤ë””ì˜¤ ìƒì„± ë° í¸ì§‘ ê³¼ì •ì„ MLLM ê¸°ë°˜ Chain-of-Thought(CoT) ì¶”ë¡ ì— ì˜í•´ ì•ˆë‚´ë˜ëŠ” ì„¸ ê°€ì§€ ëŒ€í™”í˜• ë‹¨ê³„ë¡œ ë¶„í•´í•©ë‹ˆë‹¤:"
      },
      {
        "row": 36,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 37,
        "rowsha": "vTObEWb7f5gCU681X3dTkwqhsaSW89TLw3GgMJ5I3bo=",
        "originContent": "1. **Foley Generation:** Generate foundational, semantically and temporally aligned soundscapes from video.",
        "translatedContent": "1. **í´ë¦¬ ìƒì„±:** ë¹„ë””ì˜¤ë¡œë¶€í„° ì˜ë¯¸ì Â·ì‹œê°„ì ìœ¼ë¡œ ì •ë ¬ëœ ê¸°ë³¸ ì‚¬ìš´ë“œìŠ¤ì¼€ì´í”„ ìƒì„±."
      },
      {
        "row": 38,
        "rowsha": "LTBpIQQHtEkNF8StAa+ZEDASGmRhmHIKDQOdZ4ExJWM=",
        "originContent": "2. **Object-Centric Refinement:** Refine or add sounds for user-specified objects via clicks or regions in the video.",
        "translatedContent": "2. **ì˜¤ë¸Œì íŠ¸ ì¤‘ì‹¬ ì •ì œ:** ë¹„ë””ì˜¤ ë‚´ í´ë¦­ ë˜ëŠ” ì˜ì—­ ì§€ì •ìœ¼ë¡œ ì‚¬ìš©ìê°€ ì„ íƒí•œ ì˜¤ë¸Œì íŠ¸ì˜ ì‚¬ìš´ë“œ ì •ì œ ë˜ëŠ” ì¶”ê°€."
      },
      {
        "row": 39,
        "rowsha": "8wrAo7X7dPC6Sgpfrlq2ziv/Wg/3+JnlFYvO+RMxADQ=",
        "originContent": "3. **Targeted Audio Editing:** Modify generated audio using high-level natural language instructions.",
        "translatedContent": "3. **íƒ€ê¹ƒ ì˜¤ë””ì˜¤ í¸ì§‘:** ê³ ìˆ˜ì¤€ ìì—°ì–´ ëª…ë ¹ì„ ì‚¬ìš©í•œ ìƒì„± ì˜¤ë””ì˜¤ ìˆ˜ì •."
      },
      {
        "row": 40,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 3,
    "Content": "![ThinkSound Overview](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig3_model.png)\n<!-- A large-scale CoT-annotated dataset (**AudioCoT**) is used to train both the reasoning module and the unified audio foundation model.\n![AudioCoT Pipeline](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig2_dataset.png) -->\n\n---\n\n## âš¡ Quick Start\n\n**Environment Preparation:**",
    "ContentSha": "QpULU62syvvJhbUWGR7NuQMiHmHmeeiFfKqP+ZpqFOk=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "![ThinkSound ê°œìš”](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig3_model.png)\n<!-- ëŒ€ê·œëª¨ CoT ì£¼ì„ ë°ì´í„°ì…‹(**AudioCoT**)ì´ ì¶”ë¡  ëª¨ë“ˆê³¼ í†µí•© ì˜¤ë””ì˜¤ ê¸°ë°˜ ëª¨ë¸ì„ ëª¨ë‘ í•™ìŠµí•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.\n![AudioCoT íŒŒì´í”„ë¼ì¸](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig2_dataset.png) -->\n\n---\n\n## âš¡ ë¹ ë¥¸ ì‹œì‘\n\n**í™˜ê²½ ì¤€ë¹„:**",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "4UKlvFW3Xb0bSAVjcBNeekH/MMiYS0XDg9w4mCuPy/Q=",
        "originContent": "![ThinkSound Overview](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig3_model.png)",
        "translatedContent": "![ThinkSound ê°œìš”](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig3_model.png)"
      },
      {
        "row": 2,
        "rowsha": "GaujeIM3x7+YcFy07LNNyITlujhkgpgeIaOiKHJkYnE=",
        "originContent": "<!-- A large-scale CoT-annotated dataset (**AudioCoT**) is used to train both the reasoning module and the unified audio foundation model.",
        "translatedContent": "<!-- ëŒ€ê·œëª¨ CoT ì£¼ì„ ë°ì´í„°ì…‹(**AudioCoT**)ì´ ì¶”ë¡  ëª¨ë“ˆê³¼ í†µí•© ì˜¤ë””ì˜¤ ê¸°ë°˜ ëª¨ë¸ì„ ëª¨ë‘ í•™ìŠµí•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤."
      },
      {
        "row": 3,
        "rowsha": "qYOXaaTiYkoaPFcpTXE5xdSqqiW3ebi//EW/RfSXd9g=",
        "originContent": "![AudioCoT Pipeline](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig2_dataset.png) -->",
        "translatedContent": "![AudioCoT íŒŒì´í”„ë¼ì¸](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig2_dataset.png) -->"
      },
      {
        "row": 4,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 5,
        "rowsha": "yz+R1U7uMOU+NbK5mQX3Dxae1Un9eJCdPawt78ntjTs=",
        "originContent": "---",
        "translatedContent": "---"
      },
      {
        "row": 6,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 7,
        "rowsha": "PrY/jc4yAHdS+Sr+s+Yhab477/BDp3GAzMJ8+WyumyI=",
        "originContent": "## âš¡ Quick Start",
        "translatedContent": "## âš¡ ë¹ ë¥¸ ì‹œì‘"
      },
      {
        "row": 8,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 9,
        "rowsha": "mD63DGEpdc7FlccTEps0KPAiwsJpO/C3yjV+SKIi/vE=",
        "originContent": "**Environment Preparation:**",
        "translatedContent": "**í™˜ê²½ ì¤€ë¹„:**"
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 4,
    "Content": "```bash\ngit clone https://github.com/liuhuadai/ThinkSound.git\ncd ThinkSound\nconda create -n thinksound python=3.10\nconda activate thinksound\npip install thinksound\nconda install -y -c conda-forge 'ffmpeg<7'\n# Download pretrained weights https://huggingface.co/liuhuadai/ThinkSound to Directory ckpts/\n# model weights can be also downloaded from https://www.modelscope.cn/models/iic/ThinkSound\ngit lfs install\ngit clone https://huggingface.co/liuhuadai/ThinkSound ckpts\n# To improve inference and training speed, you may optionally install a FlashAttention backend compatible with your system and PyTorch version.\n```",
    "ContentSha": "CSBCDvBmuatxDa1cNMeHEBTJJzdLjK6wyO9v0LrETM8=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bash\ngit clone https://github.com/liuhuadai/ThinkSound.git\ncd ThinkSound\nconda create -n thinksound python=3.10\nconda activate thinksound\npip install thinksound\nconda install -y -c conda-forge 'ffmpeg<7'\n# Download pretrained weights https://huggingface.co/liuhuadai/ThinkSound to Directory ckpts/\n# model weights can be also downloaded from https://www.modelscope.cn/models/iic/ThinkSound\ngit lfs install\ngit clone https://huggingface.co/liuhuadai/ThinkSound ckpts\n# To improve inference and training speed, you may optionally install a FlashAttention backend compatible with your system and PyTorch version.\n```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 5,
    "Content": "\n> âœ… **Windows Tip:**  \n> Windows users can simply run `setup_windows.bat` (or double-click it) to automatically create the conda environment, install all dependencies (including FFmpeg), and download the pretrained model â€” no manual setup required.  \n> Make sure `conda` and `git` are installed and available in your system PATH before running the script.\n\n\n### â–¶ï¸ Run the Demo\n\n#### **Linux/macOS**\n",
    "ContentSha": "YGhm7lbBNPq6xLS6zXlFTPszO8rc4QZCwsScPNskcto=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "> âœ… **Windows íŒ:**  \n> Windows ì‚¬ìš©ìëŠ” `setup_windows.bat` íŒŒì¼ì„ ì‹¤í–‰(ë˜ëŠ” ë”ë¸” í´ë¦­)í•˜ë©´ ì½˜ë‹¤ í™˜ê²½ì´ ìë™ìœ¼ë¡œ ìƒì„±ë˜ê³ , ëª¨ë“  ì¢…ì†ì„±(FFmpeg í¬í•¨)ê³¼ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì´ ë‹¤ìš´ë¡œë“œë©ë‹ˆë‹¤ â€” ë³„ë„ì˜ ìˆ˜ë™ ì„¤ì •ì´ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.  \n> ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ê¸° ì „ì— `conda`ì™€ `git`ì´ ì‹œìŠ¤í…œ PATHì— ì„¤ì¹˜ë˜ì–´ ìˆê³  ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸í•˜ì„¸ìš”.\n\n\n### â–¶ï¸ ë°ëª¨ ì‹¤í–‰\n\n#### **Linux/macOS**\n\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 6,
    "Content": "```bash\nchmod +x scripts/demo.sh\n./scripts/demo.sh <path-to-your-demo-video> <title> <CoT description> [use-half]\n```",
    "ContentSha": "EW6OKf+6hdOehT5SO7gfI7wR8oAoMckp60MRfIA1jHc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bash\nchmod +x scripts/demo.sh\n./scripts/demo.sh <path-to-your-demo-video> <title> <CoT description> [use-half]\n```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 7,
    "Content": "\n#### **Windows**\n\nYou can use the provided `.bat` script instead:\n",
    "ContentSha": "zXqRZWTEWOuKZG1GOlqqZff+IH24zUwdPtSfwESqS9E=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "#### **Windows**\n\nëŒ€ì‹  ì œê³µëœ `.bat` ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 8,
    "Content": "```bash\n.\\scripts\\demo.bat <path-to-your-demo-video> <title> <CoT description> [use-half]\n```",
    "ContentSha": "A2a1kVuIPNs8ht1a6LBYTEijJjnfjiTN0r+2n7VEJSg=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bash\n.\\scripts\\demo.bat <path-to-your-demo-video> <title> <CoT description> [use-half]\n```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 9,
    "Content": "\n**Note:**\n\n* `<path-to-your-demo-video>`: The path to a single video\n* `[use-half]` (optional): Add use-half at the end to enable half precision feature extraction.\n\n---\n\n### ğŸ“¦ Batch Inference\n\n#### **Linux/macOS**\n",
    "ContentSha": "T7owm3ZZW7sVjKwFivgiuYX2+RVuNBl0RYTSnIcxxbM=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "**ì°¸ê³ :**\n\n* `<path-to-your-demo-video>`: ë‹¨ì¼ ë¹„ë””ì˜¤ì˜ ê²½ë¡œ\n* `[use-half]` (ì„ íƒì‚¬í•­): ë§ˆì§€ë§‰ì— use-halfë¥¼ ì¶”ê°€í•˜ì—¬ í•˜í”„ í”„ë¦¬ì‹œì „ íŠ¹ì§• ì¶”ì¶œì„ í™œì„±í™”í•©ë‹ˆë‹¤.\n\n---\n\n### ğŸ“¦ ë°°ì¹˜ ì¶”ë¡ \n\n#### **Linux/macOS**\n\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 10,
    "Content": "```bash\nchmod +x scripts/eval_batch.sh\n./scripts/eval_batch.sh <video_path> <csv_path> <save_path (optional)> [use-half]\n```",
    "ContentSha": "EQ4HuSYii55aHfgphESvOXMz2+Fq39+Xquxg6Z6uzdU=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bash\nchmod +x scripts/eval_batch.sh\n./scripts/eval_batch.sh <video_path> <csv_path> <save_path (optional)> [use-half]\n```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 11,
    "Content": "\n#### **Windows**\n\nUse the equivalent `.bat` script:\n",
    "ContentSha": "njm5i6o3MR7AV4Q3WLctbe3LN1njFn89fPfTlo+zSmc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "#### **Windows**\n\në™ë“±í•œ `.bat` ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”:\n\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 12,
    "Content": "```bash\n.\\scripts\\eval_batch.bat <video_path> <csv_path> <save_path (optional)> [use-half]\n```",
    "ContentSha": "XLkAqxYBZeJiF6XnpshI6naENFsr5yFAH7af132cgb0=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bash\n.\\scripts\\eval_batch.bat <video_path> <csv_path> <save_path (optional)> [use-half]\n```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 13,
    "Content": "\n**Note:**\n\n* `<video_path>`: Path to the root directory containing all .mp4 videos to be processed (all videos must be of equal duration).\n* `<csv_path>`: A CSV file with text prompts for each video (see `demo_test.csv` for format).\n* `<save_path>` (optional): Where to save generated audio. Defaults to `results/features`.\n* `[use-half]` (optional): Add use-half at the end to enable half precision feature extraction.\n\n---\n\n\n### Web Interface Usage\n\nFor an interactive experience, launch the Gradio web interface:\n",
    "ContentSha": "yT/y6PXpYV8wS4qmKJfVNVGDLOwreTxCdCDFDv2VbLo=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "**ì°¸ê³ :**\n\n* `<video_path>`: ì²˜ë¦¬í•  ëª¨ë“  .mp4 ë¹„ë””ì˜¤ê°€ í¬í•¨ëœ ë£¨íŠ¸ ë””ë ‰í„°ë¦¬ì˜ ê²½ë¡œ (ëª¨ë“  ë¹„ë””ì˜¤ëŠ” ë™ì¼í•œ ê¸¸ì´ì—¬ì•¼ í•©ë‹ˆë‹¤).\n* `<csv_path>`: ê° ë¹„ë””ì˜¤ì— ëŒ€í•œ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ê°€ í¬í•¨ëœ CSV íŒŒì¼ (`demo_test.csv`ì—ì„œ í˜•ì‹ ì°¸ê³ ).\n* `<save_path>` (ì„ íƒì‚¬í•­): ìƒì„±ëœ ì˜¤ë””ì˜¤ë¥¼ ì €ì¥í•  ìœ„ì¹˜. ê¸°ë³¸ê°’ì€ `results/features`ì…ë‹ˆë‹¤.\n* `[use-half]` (ì„ íƒì‚¬í•­): ë§ˆì§€ë§‰ì— use-halfë¥¼ ì¶”ê°€í•˜ë©´ í•˜í”„ í”„ë¦¬ì‹œì „ íŠ¹ì§• ì¶”ì¶œì´ í™œì„±í™”ë©ë‹ˆë‹¤.\n\n---\n\n\n### ì›¹ ì¸í„°í˜ì´ìŠ¤ ì‚¬ìš©ë²•\n\nì¸í„°ë™í‹°ë¸Œí•œ ê²½í—˜ì„ ìœ„í•´ Gradio ì›¹ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:\n\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 14,
    "Content": "```bash\npython app.py\n```",
    "ContentSha": "2nQFYMHYtsOO4+egbu20DhxqoaxfzoH8CneeM8qTEb0=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bash\npython app.py\n```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 15,
    "Content": "\n\n## ğŸ‹ï¸ Train the Model\n\nSee [`Training.md`](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/docs/Training.md)\n\n\n---\n\n## ğŸ“ TODO & Future Plans\n* - [ ] Release a more powerful foundation model covering multiple domains to provide more engaging and immersive foley creation\n* - [ ] Add support for additional modalities and downstream tasks\n* - [ ] Release models at different scales\n* - [x] Open-source AudioCoT dataset and automated pipeline\n* - [x] Release training scripts for ThinkSound models\n* - [x] A beginner-friendly Windows quick-start README\n---\n\n\n## ğŸ“„ License\n\nThis project is released under the Apache 2.0 License.\n\n> **Note:**\n> The code, models, and dataset are **for research and educational purposes only**.\n> **Commercial use is NOT permitted.**\n> For commercial licensing, please contact the authors.\n\n**ğŸ“¦ Third-Party Components**\n\n* **Stable Audio Open VAE** (by Stability AI):\n  This repository includes a fine-tuned VAE from [Stable Audio Open](https://huggingface.co/stabilityai/stable-audio-open-1.0/), licensed under the [Stability AI Community License](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/./third_party/LICENSE_StabilityAI.md).\n  **Commercial use and redistribution require prior permission from Stability AI.**\n\n* ğŸ“˜ **All other code and models** are released under the Apache License 2.0.\n\n---\n\n## Acknowledgements\n",
    "ContentSha": "j3jq6Afpr38oSd7nSvDiMiQ889Z6kSywM0DVVT9ieNA=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "\n\n## ğŸ‹ï¸ ëª¨ë¸ í•™ìŠµ\n\n[`Training.md`](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/docs/Training.md) ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\n\n\n---\n\n## ğŸ“ TODO & í–¥í›„ ê³„íš\n* - [ ] ë‹¤ì–‘í•œ ë„ë©”ì¸ì„ ì•„ìš°ë¥´ëŠ” ë” ê°•ë ¥í•œ ê¸°ë°˜ ëª¨ë¸ì„ ì¶œì‹œí•˜ì—¬ ë”ìš± ëª°ì…ê° ìˆê³  í¥ë¯¸ë¡œìš´ í´ë¦¬ ì‚¬ìš´ë“œ ìƒì„± ì œê³µ\n* - [ ] ì¶”ê°€ì ì¸ ëª¨ë‹¬ë¦¬í‹°ì™€ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—… ì§€ì› ì¶”ê°€\n* - [ ] ë‹¤ì–‘í•œ ìŠ¤ì¼€ì¼ì˜ ëª¨ë¸ ì¶œì‹œ\n* - [x] AudioCoT ë°ì´í„°ì…‹ ë° ìë™í™” íŒŒì´í”„ë¼ì¸ ì˜¤í”ˆì†ŒìŠ¤ ê³µê°œ\n* - [x] ThinkSound ëª¨ë¸ìš© í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ê³µê°œ\n* - [x] ì´ˆë³´ì ì¹œí™”ì  ìœˆë„ìš° ë¹ ë¥¸ ì‹œì‘ README ì œê³µ\n---\n\n\n## ğŸ“„ ë¼ì´ì„ ìŠ¤\n\nì´ í”„ë¡œì íŠ¸ëŠ” Apache 2.0 ë¼ì´ì„ ìŠ¤ í•˜ì— ê³µê°œë©ë‹ˆë‹¤.\n\n> **ì°¸ê³ :**\n> ì½”ë“œ, ëª¨ë¸, ë°ì´í„°ì…‹ì€ **ì—°êµ¬ ë° êµìœ¡ ëª©ì ì—ë§Œ ì‚¬ìš©**ë©ë‹ˆë‹¤.\n> **ìƒì—…ì  ì‚¬ìš©ì€ í—ˆê°€ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.**\n> ìƒì—…ì  ë¼ì´ì„ ìŠ¤ê°€ í•„ìš”í•  ê²½ìš° ì €ìì—ê²Œ ë¬¸ì˜í•´ ì£¼ì„¸ìš”.\n\n**ğŸ“¦ ì„œë“œíŒŒí‹° ì»´í¬ë„ŒíŠ¸**\n\n* **Stable Audio Open VAE** (Stability AI ì œê³µ):\n  ë³¸ ì €ì¥ì†Œì—ëŠ” [Stable Audio Open](https://huggingface.co/stabilityai/stable-audio-open-1.0/)ì—ì„œ íŒŒì¸íŠœë‹ëœ VAEê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©°, [Stability AI ì»¤ë®¤ë‹ˆí‹° ë¼ì´ì„ ìŠ¤](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/./third_party/LICENSE_StabilityAI.md) í•˜ì— ë¼ì´ì„ ìŠ¤ë©ë‹ˆë‹¤.\n  **ìƒì—…ì  ì‚¬ìš© ë° ì¬ë°°í¬ëŠ” Stability AIì˜ ì‚¬ì „ í—ˆê°€ê°€ í•„ìš”í•©ë‹ˆë‹¤.**\n\n* ğŸ“˜ **ê·¸ ì™¸ ëª¨ë“  ì½”ë“œ ë° ëª¨ë¸**ì€ Apache License 2.0 í•˜ì— ê³µê°œë©ë‹ˆë‹¤.\n\n---\n\n## ê°ì‚¬ì˜ ë§ì”€\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 16,
    "Content": "Many thanks to:\n\n* **stable-audio-tools** (by Stability AI):\nFor providing an easy-to-use framework for audio generation, as well as the VAE module and weights.\n* **MMAudio**:\n  For the implementation of the MM-DiT backbone in the audio domain.\n\n---\n\n## ğŸ“– Citation\n\nIf you find ThinkSound useful in your research or work, please cite our paper:\n",
    "ContentSha": "FsK5U++tkthvkZ/Gd4G7gn74YKpB282Oxnkt96u9C1k=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "ê°ì‚¬í•©ë‹ˆë‹¤:\n\n* **stable-audio-tools** (Stability AI ì œê³µ):\nì˜¤ë””ì˜¤ ìƒì„±ì— ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í”„ë ˆì„ì›Œí¬ì™€ VAE ëª¨ë“ˆ ë° ê°€ì¤‘ì¹˜ë¥¼ ì œê³µí•´ ì£¼ì…¨ìŠµë‹ˆë‹¤.\n* **MMAudio**:\n  ì˜¤ë””ì˜¤ ë„ë©”ì¸ì—ì„œ MM-DiT ë°±ë³¸ êµ¬í˜„ì„ ì œê³µí•´ ì£¼ì…¨ìŠµë‹ˆë‹¤.\n\n---\n\n## ğŸ“– ì¸ìš©\n\nThinkSoundê°€ ê·€í•˜ì˜ ì—°êµ¬ë‚˜ ì‘ì—…ì— ìœ ìš©í–ˆë‹¤ë©´, ì €í¬ ë…¼ë¬¸ì„ ì¸ìš©í•´ ì£¼ì„¸ìš”:\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 17,
    "Content": "```bibtex\n@misc{liu2025thinksoundchainofthoughtreasoningmultimodal,\n      title={ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language Models for Audio Generation and Editing}, \n      author={Huadai Liu and Jialei Wang and Kaicheng Luo and Wen Wang and Qian Chen and Zhou Zhao and Wei Xue},\n      year={2025},\n      eprint={2506.21448},\n      archivePrefix={arXiv},\n      primaryClass={eess.AS},\n      url={https://arxiv.org/abs/2506.21448}, \n}\n```",
    "ContentSha": "KKv35iBt6IDF1ifN04L+6lkh0BHkbObnW/+m50Wufrs=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bibtex\n@misc{liu2025thinksoundchainofthoughtreasoningmultimodal,\n      title={ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language Models for Audio Generation and Editing}, \n      author={Huadai Liu and Jialei Wang and Kaicheng Luo and Wen Wang and Qian Chen and Zhou Zhao and Wei Xue},\n      year={2025},\n      eprint={2506.21448},\n      archivePrefix={arXiv},\n      primaryClass={eess.AS},\n      url={https://arxiv.org/abs/2506.21448}, \n}\n```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 18,
    "Content": "\n---\n\n## ğŸ“¬ Contact\n\n\nâœ¨ Feel free to [open an issue](https://github.com/liuhuadai/ThinkSound/issues) or contact us via email ([liuhuadai@zju.edu.cn](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/mailto:liuhuadai@zju.edu.cn)) if you have any questions or suggestions!\n\n\n",
    "ContentSha": "QMNRHPzbmsL2YxrLNEPneJCBrTj4/XiY9XGTX01NZl8=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "\n---\n\n## ğŸ“¬ Contact\n\n\nâœ¨ Feel free to [open an issue](https://github.com/liuhuadai/ThinkSound/issues) or contact us via email ([liuhuadai@zju.edu.cn](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/mailto:liuhuadai@zju.edu.cn)) if you have any questions or suggestions!\n\n\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 2,
        "rowsha": "yz+R1U7uMOU+NbK5mQX3Dxae1Un9eJCdPawt78ntjTs=",
        "originContent": "---",
        "translatedContent": "---"
      },
      {
        "row": 3,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 4,
        "rowsha": "V0ea1xQLKG+cGj5kHVv5f15HDd+yj0ulkcBQnvErdJc=",
        "originContent": "## ğŸ“¬ Contact",
        "translatedContent": "## ğŸ“¬ Contact"
      },
      {
        "row": 5,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 6,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 7,
        "rowsha": "QsDuVwX1DXlTNejKuNftx4k1x7yNHjfP9/1HS85hJng=",
        "originContent": "âœ¨ Feel free to [open an issue](https://github.com/liuhuadai/ThinkSound/issues) or contact us via email ([liuhuadai@zju.edu.cn](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/mailto:liuhuadai@zju.edu.cn)) if you have any questions or suggestions!",
        "translatedContent": "âœ¨ Feel free to [open an issue](https://github.com/liuhuadai/ThinkSound/issues) or contact us via email ([liuhuadai@zju.edu.cn](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/mailto:liuhuadai@zju.edu.cn)) if you have any questions or suggestions!"
      },
      {
        "row": 8,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 9,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 10,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  }
]