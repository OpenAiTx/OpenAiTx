<h1 align="center">ThinkSound</h1>

<p align="center">
  ğŸŒ
  <a href="https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=en">English</a> |
  <a href="https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=zh-CN">ç®€ä½“ä¸­æ–‡</a> |
  <a href="https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=zh-TW">ç¹é«”ä¸­æ–‡</a> |
  <a href="https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=es">EspaÃ±ol</a> |
  <a href="https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=fr">FranÃ§ais</a> |
  <a href="https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=ja">æ—¥æœ¬èª</a>
  
</p>
<p align="center">
  <img src="https://img.shields.io/badge/NeurIPS 2025-Main Conference-blue.svg" alt="NeurIPS 2025"/>
<p align="center">
  <a href="https://arxiv.org/pdf/2506.21448">
    <img src="https://img.shields.io/badge/arXiv-2506.21448-b31b1b.svg" alt="arXiv"/>
  </a>
  &nbsp;
  <a href="https://thinksound-project.github.io/">
    <img src="https://img.shields.io/badge/Online%20Demo-ğŸŒ-blue" alt="Online Demo"/>
  </a>
  &nbsp;
  <a href="https://huggingface.co/spaces/FunAudioLLM/ThinkSound">
    <img src="https://img.shields.io/badge/HuggingFace-Spaces-orange?logo=huggingface" alt="Hugging Face"/>
  </a>
  &nbsp;
  <a href="https://modelscope.cn/studios/iic/ThinkSound">
    <img src="https://img.shields.io/badge/ModelScope-åœ¨çº¿ä½“éªŒ-green" alt="ModelScope"/>
  </a>
</p>

<p align="center">
  Se vocÃª achar este projeto Ãºtil,<br>
  um star â­ no GitHub seria muito apreciado!
</p>

---

**ThinkSound** Ã© uma estrutura unificada de geraÃ§Ã£o Any2Audio com fluxo guiado por raciocÃ­nio Chain-of-Thought (CoT).

ImplementaÃ§Ã£o em PyTorch para geraÃ§Ã£o e ediÃ§Ã£o multimodal de Ã¡udio: gere ou edite Ã¡udio a partir de vÃ­deo, texto e Ã¡udio, impulsionado por raciocÃ­nio passo a passo de Modelos de Linguagem Multimodal de Grande Escala (MLLMs).

![Teaser](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig1_teaser.png)
---

## ğŸ“° Novidades
- **2025.09.19** &nbsp; ğŸ‰ ThinkSound foi aceito na **ConferÃªncia Principal NeurIPS 2025**!
- **2025.09.01** &nbsp; ğŸ”¥ Nosso conjunto de dados AudioCoT agora Ã© open-source e estÃ¡ disponÃ­vel no [Hugging Face](https://huggingface.co/datasets/liuhuadai/AudioCoT)!
- **2025.07.17** &nbsp; ğŸ§  Finetuning habilitado: cÃ³digo para treinamento e ajuste fino agora disponÃ­vel publicamente, juntamente com instruÃ§Ãµes claras para ajudar vocÃª a customizar e ampliar o ThinkSound com seus prÃ³prios dados.
- **2025.07.15** &nbsp; ğŸ“¦ InstalaÃ§Ã£o e usabilidade simplificadas: dependÃªncias no PyPI para configuraÃ§Ã£o fÃ¡cil em mÃºltiplas plataformas; scripts `.bat` para Windows automatizam a criaÃ§Ã£o do ambiente e execuÃ§Ã£o dos scripts.
- **2025.07.08** &nbsp;Â  ğŸ”§ Grande atualizaÃ§Ã£o: modelo otimizado e leve, com uso aprimorado de memÃ³ria e GPU, agora suporta geraÃ§Ã£o de Ã¡udio em larga escala!
- **2025.07.01** &nbsp; ğŸ”¥Demo online no [Hugging Face Spaces](https://huggingface.co/spaces/FunAudioLLM/ThinkSound) e [ModelScope](https://modelscope.cn/studios/iic/ThinkSound) para experiÃªncia interativa!
- **2025.07.01** &nbsp; ğŸ”¥Scripts de inferÃªncia e interface web lanÃ§ados;
- **2025.06** &nbsp; ğŸ”¥[Artigo ThinkSound](https://arxiv.org/pdf/2506.21448) publicado no arXiv!
- **2025.06** &nbsp; ğŸ”¥[Demo Online](http://thinksound-project.github.io/) disponÃ­vel - experimente agora!

---

## ğŸš€ Funcionalidades

- **Any2Audio**: Gere Ã¡udio a partir de qualquer modalidade â€” vÃ­deo, texto, Ã¡udio ou suas combinaÃ§Ãµes.
- **Video-to-Audio SOTA**: AlcanÃ§a resultados de Ãºltima geraÃ§Ã£o em mÃºltiplos benchmarks V2A.
- **RaciocÃ­nio CoT-Driven**: RaciocÃ­nio em cadeia para geraÃ§Ã£o de Ã¡udio composicional e controlÃ¡vel via MLLMs.
- **EdiÃ§Ã£o Interativa Centrada em Objetos**: Refine ou edite eventos sonoros especÃ­ficos clicando em objetos visuais ou usando instruÃ§Ãµes de texto.
- **Framework Unificado**: Um modelo base suporta geraÃ§Ã£o, ediÃ§Ã£o e fluxo de trabalho interativo.

---

## âœ¨ VisÃ£o Geral do MÃ©todo

ThinkSound decompÃµe a geraÃ§Ã£o e ediÃ§Ã£o de Ã¡udio em trÃªs etapas interativas, todas guiadas pelo raciocÃ­nio Chain-of-Thought (CoT) baseado em MLLM:

1. **GeraÃ§Ã£o Foley:** Gere paisagens sonoras fundamentais, semanticamente e temporalmente alinhadas a partir de vÃ­deo.
2. **Refinamento Centrado em Objetos:** Refine ou adicione sons para objetos especificados pelo usuÃ¡rio via cliques ou regiÃµes no vÃ­deo.
3. **EdiÃ§Ã£o de Ãudio Direcionada:** Modifique o Ã¡udio gerado usando instruÃ§Ãµes em linguagem natural de alto nÃ­vel.

![VisÃ£o Geral ThinkSound](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig3_model.png)
<!-- Um grande conjunto de dados anotado com CoT (**AudioCoT**) Ã© utilizado para treinar tanto o mÃ³dulo de raciocÃ­nio quanto o modelo base de Ã¡udio unificado.

![AudioCoT Pipeline](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig2_dataset.png) -->

---

## âš¡ InÃ­cio RÃ¡pido

**PreparaÃ§Ã£o do Ambiente:**
```bash
git clone https://github.com/liuhuadai/ThinkSound.git
cd ThinkSound
conda create -n thinksound python=3.10
conda activate thinksound
pip install thinksound
conda install -y -c conda-forge 'ffmpeg<7'
# Download pretrained weights https://huggingface.co/liuhuadai/ThinkSound to Directory ckpts/
# model weights can be also downloaded from https://www.modelscope.cn/models/iic/ThinkSound
git lfs install
git clone https://huggingface.co/liuhuadai/ThinkSound ckpts
# To improve inference and training speed, you may optionally install a FlashAttention backend compatible with your system and PyTorch version.
```
> âœ… **Dica para Windows:**  
> UsuÃ¡rios do Windows podem simplesmente executar `setup_windows.bat` (ou dar um duplo clique) para criar automaticamente o ambiente conda, instalar todas as dependÃªncias (incluindo o FFmpeg) e baixar o modelo prÃ©-treinado â€” nÃ£o Ã© necessÃ¡rio configuraÃ§Ã£o manual.  
> Certifique-se de que `conda` e `git` estejam instalados e disponÃ­veis no PATH do sistema antes de executar o script.


### â–¶ï¸ Execute a DemonstraÃ§Ã£o

#### **Linux/macOS**


```bash
chmod +x scripts/demo.sh
./scripts/demo.sh <path-to-your-demo-video> <title> <CoT description> [use-half]
```
#### **Windows**

VocÃª pode usar o script `.bat` fornecido em vez disso:


```bash
.\scripts\demo.bat <path-to-your-demo-video> <title> <CoT description> [use-half]
```
**Nota:**

* `<caminho-para-seu-vÃ­deo-demo>`: O caminho para um Ãºnico vÃ­deo
* `[use-half]` (opcional): Adicione use-half ao final para ativar a extraÃ§Ã£o de recursos em precisÃ£o reduzida.

---

### ğŸ“¦ InferÃªncia em Lote

#### **Linux/macOS**


```bash
chmod +x scripts/eval_batch.sh
./scripts/eval_batch.sh <video_path> <csv_path> <save_path (optional)> [use-half]
```
#### **Windows**

Use o script `.bat` equivalente:


```bash
.\scripts\eval_batch.bat <video_path> <csv_path> <save_path (optional)> [use-half]
```
**Nota:**

* `<video_path>`: Caminho para o diretÃ³rio raiz contendo todos os vÃ­deos .mp4 a serem processados (todos os vÃ­deos devem ter a mesma duraÃ§Ã£o).
* `<csv_path>`: Um arquivo CSV com prompts de texto para cada vÃ­deo (veja `demo_test.csv` para o formato).
* `<save_path>` (opcional): Onde salvar o Ã¡udio gerado. O padrÃ£o Ã© `results/features`.
* `[use-half]` (opcional): Adicione use-half ao final para habilitar a extraÃ§Ã£o de recursos em meia precisÃ£o.

---


### Uso da Interface Web

Para uma experiÃªncia interativa, inicie a interface web do Gradio:


```bash
python app.py
```


## ğŸ‹ï¸ Treinar o Modelo

Veja [`Training.md`](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/docs/Training.md)


---

## ğŸ“ TODO & Planos Futuros
* - [ ] LanÃ§ar um modelo de fundaÃ§Ã£o mais poderoso cobrindo mÃºltiplos domÃ­nios para oferecer criaÃ§Ã£o de foley mais envolvente e imersiva
* - [ ] Adicionar suporte para modalidades adicionais e tarefas downstream
* - [ ] LanÃ§ar modelos em diferentes escalas
* - [x] Open-source do conjunto de dados AudioCoT e pipeline automatizado
* - [x] LanÃ§ar scripts de treinamento para os modelos ThinkSound
* - [x] Um README de inÃ­cio rÃ¡pido para Windows, amigÃ¡vel para iniciantes
---


## ğŸ“„ LicenÃ§a

Este projeto Ã© lanÃ§ado sob a LicenÃ§a Apache 2.0.

> **Nota:**
> O cÃ³digo, modelos e conjunto de dados sÃ£o **apenas para fins de pesquisa e educaÃ§Ã£o**.
> **Uso comercial NÃƒO Ã© permitido.**
> Para licenciamento comercial, entre em contato com os autores.

**ğŸ“¦ Componentes de Terceiros**

* **Stable Audio Open VAE** (por Stability AI):
  Este repositÃ³rio inclui um VAE ajustado do [Stable Audio Open](https://huggingface.co/stabilityai/stable-audio-open-1.0/), licenciado sob a [LicenÃ§a de Comunidade Stability AI](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/./third_party/LICENSE_StabilityAI.md).
  **Uso comercial e redistribuiÃ§Ã£o requerem permissÃ£o prÃ©via da Stability AI.**

* ğŸ“˜ **Todo o restante do cÃ³digo e modelos** sÃ£o lanÃ§ados sob a LicenÃ§a Apache 2.0.

---

## Agradecimentos

Muito obrigado a:

* **stable-audio-tools** (por Stability AI):
Por fornecer uma estrutura de fÃ¡cil utilizaÃ§Ã£o para geraÃ§Ã£o de Ã¡udio, bem como o mÃ³dulo VAE e os pesos.
* **MMAudio**:
  Pela implementaÃ§Ã£o do backbone MM-DiT no domÃ­nio de Ã¡udio.

---

## ğŸ“– CitaÃ§Ã£o

Se vocÃª achar o ThinkSound Ãºtil em sua pesquisa ou trabalho, por favor cite nosso artigo:

```bibtex
@misc{liu2025thinksoundchainofthoughtreasoningmultimodal,
      title={ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language Models for Audio Generation and Editing}, 
      author={Huadai Liu and Jialei Wang and Kaicheng Luo and Wen Wang and Qian Chen and Zhou Zhao and Wei Xue},
      year={2025},
      eprint={2506.21448},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/2506.21448}, 
}
```

---

## ğŸ“¬ Contact


âœ¨ Feel free to [open an issue](https://github.com/liuhuadai/ThinkSound/issues) or contact us via email ([liuhuadai@zju.edu.cn](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/mailto:liuhuadai@zju.edu.cn)) if you have any questions or suggestions!



---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-10-04

---