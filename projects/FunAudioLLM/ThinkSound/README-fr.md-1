{
  "id": 1,
  "origin": "# üé∂ ThinkSound\n\n<p align=\"center\">\n  If you find this project useful, a star ‚≠ê on GitHub would be greatly appreciated!\n</p>\n\n<p align=\"center\">\n  <a href=\"https://arxiv.org/pdf/2506.21448\">\n    <img src=\"https://img.shields.io/badge/arXiv-2506.21448-b31b1b.svg\" alt=\"arXiv\"/>\n  </a>\n  &nbsp;\n  <a href=\"https://thinksound-project.github.io/\">\n    <img src=\"https://img.shields.io/badge/Online%20Demo-üåê-blue\" alt=\"Online Demo\"/>\n  </a>\n  &nbsp;\n  <a href=\"https://huggingface.co/spaces/FunAudioLLM/ThinkSound\">\n    <img src=\"https://img.shields.io/badge/HuggingFace-Spaces-orange?logo=huggingface\" alt=\"Hugging Face\"/>\n  </a>\n  &nbsp;\n  <a href=\"https://modelscope.cn/studios/iic/ThinkSound\">\n    <img src=\"https://img.shields.io/badge/ModelScope-Âú®Á∫ø‰ΩìÈ™å-green\" alt=\"ModelScope\"/>\n  </a>\n</p>\n\n---\n\n**ThinkSound** is a unified Any2Audio generation framework with flow matching guided by Chain-of-Thought (CoT) reasoning.\n\nPyTorch implementation for multimodal audio generation and editing: generate or edit audio from video, text, and audio, powered by step-by-step reasoning from Multimodal Large Language Models (MLLMs).\n\n![Teaser](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig1_teaser.png)\n---\n\n## üì∞ News\n- **2025.07** &nbsp; üî•Online demo on [Hugging Face Spaces](https://huggingface.co/spaces/FunAudioLLM/ThinkSound) and [ModelScope](https://modelscope.cn/studios/iic/ThinkSound) for interactive experience!\n- **2025.07** &nbsp; üî•Released inference scripts and web interface; \n- **2025.06** &nbsp; üî•[ThinkSound paper](https://arxiv.org/pdf/2506.21448) released on arXiv!\n- **2025.06** &nbsp; üî•[Online Demo](http://thinksound-project.github.io/) is live - try it now!\n\n---\n\n## üöÄ Features\n\n- **Any2Audio**: Generate audio from arbitrary modalities ‚Äî video, text, audio, or their combinations.\n- **Video-to-Audio SOTA**: Achieves state-of-the-art results on multiple V2A benchmarks.\n- **CoT-Driven Reasoning**: Chain-of-Thought reasoning for compositional and controllable audio generation via MLLMs.\n- **Interactive Object-centric Editing**: Refine or edit specific sound events by clicking on visual objects or using text instructions.\n- **Unified Framework**: One foundation model supports generation, editing, and interactive workflow.\n\n---\n\n## ‚ú® Method Overview\n\nThinkSound decomposes audio generation and editing into three interactive stages, all guided by MLLM-based Chain-of-Thought (CoT) reasoning:\n\n1. **Foley Generation:** Generate foundational, semantically and temporally aligned soundscapes from video.\n2. **Object-Centric Refinement:** Refine or add sounds for user-specified objects via clicks or regions in the video.\n3. **Targeted Audio Editing:** Modify generated audio using high-level natural language instructions.\n\n![ThinkSound Overview](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig3_model.png)\n<!-- A large-scale CoT-annotated dataset (**AudioCoT**) is used to train both the reasoning module and the unified audio foundation model.\n![AudioCoT Pipeline](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig2_dataset.png) -->\n\n---\n\n## ‚ö° Quick Start\n\n**Environment Preparation:**\n```bash\ngit clone https://github.com/liuhuadai/ThinkSound.git\ncd ThinkSound\npip install -r requirements.txt\nconda install -y -c conda-forge 'ffmpeg<7'\n# Download pretrained weights https://huggingface.co/liuhuadai/ThinkSound to Directory ckpts/\n# model weights can be also downloaded from https://www.modelscope.cn/models/iic/ThinkSound\ngit lfs install\ngit clone https://huggingface.co/liuhuadai/ThinkSound ckpts\n```\n\n**Make it executable**\n```bash\nchmod +x scripts/demo.sh\n```\n\n**Run the script**\n```bash\n./scripts/demo.sh <video_path> <caption> <CoT description>\n```\n\n\n### Web Interface Usage\n\nFor an interactive experience, launch the Gradio web interface:\n\n```bash\npython app.py\n```\n\n---\n",
  "origin_sha": "vYi4X/38TyDFihDOfSBGZIgKnu2bd0a73Iz5+m+7pd0=",
  "translate": "# üé∂ ThinkSound\n\n<p align=\"center\">\n  Si vous trouvez ce projet utile, une √©toile ‚≠ê sur GitHub serait grandement appr√©ci√©e !\n</p>\n\n<p align=\"center\">\n  <a href=\"https://arxiv.org/pdf/2506.21448\">\n    <img src=\"https://img.shields.io/badge/arXiv-2506.21448-b31b1b.svg\" alt=\"arXiv\"/>\n  </a>\n  &nbsp;\n  <a href=\"https://thinksound-project.github.io/\">\n    <img src=\"https://img.shields.io/badge/Online%20Demo-üåê-blue\" alt=\"Online Demo\"/>\n  </a>\n  &nbsp;\n  <a href=\"https://huggingface.co/spaces/FunAudioLLM/ThinkSound\">\n    <img src=\"https://img.shields.io/badge/HuggingFace-Spaces-orange?logo=huggingface\" alt=\"Hugging Face\"/>\n  </a>\n  &nbsp;\n  <a href=\"https://modelscope.cn/studios/iic/ThinkSound\">\n    <img src=\"https://img.shields.io/badge/ModelScope-Âú®Á∫ø‰ΩìÈ™å-green\" alt=\"ModelScope\"/>\n  </a>\n</p>\n\n---\n\n**ThinkSound** est un cadre unifi√© de g√©n√©ration Any2Audio avec un guidage par appariement de flux bas√© sur le raisonnement Chain-of-Thought (CoT).\n\nImpl√©mentation PyTorch pour la g√©n√©ration et l'√©dition audio multimodales : g√©n√©rez ou √©ditez de l‚Äôaudio √† partir de vid√©o, de texte et d‚Äôaudio, aliment√© par un raisonnement √©tape par √©tape provenant de Mod√®les de Langage Multimodaux de Grande Taille (MLLMs).\n\n![Teaser](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig1_teaser.png)\n---\n\n## üì∞ Actualit√©s\n- **2025.07** &nbsp; üî•D√©mo en ligne sur [Hugging Face Spaces](https://huggingface.co/spaces/FunAudioLLM/ThinkSound) et [ModelScope](https://modelscope.cn/studios/iic/ThinkSound) pour une exp√©rience interactive !\n- **2025.07** &nbsp; üî•Scripts d‚Äôinf√©rence et interface web publi√©s ; \n- **2025.06** &nbsp; üî•[Article ThinkSound](https://arxiv.org/pdf/2506.21448) publi√© sur arXiv !\n- **2025.06** &nbsp; üî•[D√©mo en ligne](http://thinksound-project.github.io/) disponible - essayez-la d√®s maintenant !\n\n---\n\n## üöÄ Fonctionnalit√©s\n\n- **Any2Audio** : G√©n√©ration audio √† partir de modalit√©s arbitraires ‚Äî vid√©o, texte, audio, ou leurs combinaisons.\n- **Vid√©o-vers-Audio SOTA** : R√©sultats √† la pointe de la technologie sur plusieurs benchmarks V2A.\n- **Raisonnement guid√© par CoT** : Raisonnement Chain-of-Thought pour une g√©n√©ration audio compositionnelle et contr√¥lable via les MLLMs.\n- **√âdition interactive centr√©e objet** : Affinez ou √©ditez des √©v√©nements sonores sp√©cifiques en cliquant sur des objets visuels ou via des instructions textuelles.\n- **Cadre unifi√©** : Un mod√®le fondamental unique prend en charge la g√©n√©ration, l‚Äô√©dition et le flux de travail interactif.\n\n---\n\n## ‚ú® Aper√ßu de la m√©thode\n\nThinkSound d√©compose la g√©n√©ration et l‚Äô√©dition audio en trois √©tapes interactives, toutes guid√©es par le raisonnement Chain-of-Thought (CoT) bas√© sur les MLLMs :\n\n1. **G√©n√©ration Foley :** G√©n√©rer des paysages sonores fondamentaux, s√©mantiquement et temporellement align√©s √† partir de la vid√©o.\n2. **Affinement centr√© objet :** Affiner ou ajouter des sons pour des objets sp√©cifi√©s par l‚Äôutilisateur via des clics ou des r√©gions dans la vid√©o.\n3. **√âdition audio cibl√©e :** Modifier l‚Äôaudio g√©n√©r√© √† l‚Äôaide d‚Äôinstructions en langage naturel de haut niveau.\n\n![ThinkSound Overview](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig3_model.png)\n<!-- Un jeu de donn√©es annot√© CoT √† grande √©chelle (**AudioCoT**) est utilis√© pour entra√Æner √† la fois le module de raisonnement et le mod√®le audio fondamental unifi√©.\n![AudioCoT Pipeline](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig2_dataset.png) -->\n\n---\n\n## ‚ö° D√©marrage rapide\n\n**Pr√©paration de l‚Äôenvironnement :**\n```bash\ngit clone https://github.com/liuhuadai/ThinkSound.git\ncd ThinkSound\npip install -r requirements.txt\nconda install -y -c conda-forge 'ffmpeg<7'\n# T√©l√©charger les poids pr√©-entra√Æn√©s https://huggingface.co/liuhuadai/ThinkSound dans le dossier ckpts/\n# Les poids du mod√®le peuvent √©galement √™tre t√©l√©charg√©s depuis https://www.modelscope.cn/models/iic/ThinkSound\ngit lfs install\ngit clone https://huggingface.co/liuhuadai/ThinkSound ckpts\n```\n\n**Rendez-le ex√©cutable**\n```bash\nchmod +x scripts/demo.sh\n```\n\n**Lancer le script**\n```bash\n./scripts/demo.sh <chemin_vers_video> <l√©gende> <description_CoT>\n```\n\n\n### Utilisation de l‚Äôinterface web\n\nPour une exp√©rience interactive, lancez l‚Äôinterface web Gradio :\n\n```bash\npython app.py\n```\n\n---",
  "status": "ok"
}