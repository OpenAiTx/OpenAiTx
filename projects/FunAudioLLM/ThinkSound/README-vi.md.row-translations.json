[
  {
    "row": 1,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 2,
    "rowsha": "eexREJOMCITdo96Iwg4N4PQqu1ux/UwupfjVRj0l/L8=",
    "originContent": "PyTorch implementation for multimodal audio generation and editing: generate or edit audio from video, text, and audio, powered by step-by-step reasoning from Multimodal Large Language Models (MLLMs).",
    "translatedContent": "Tri·ªÉn khai PyTorch cho t·∫°o v√† ch·ªânh s·ª≠a √¢m thanh ƒëa ph∆∞∆°ng th·ª©c: t·∫°o ho·∫∑c ch·ªânh s·ª≠a √¢m thanh t·ª´ video, vƒÉn b·∫£n v√† √¢m thanh, ƒë∆∞·ª£c h·ªó tr·ª£ b·ªüi suy lu·∫≠n t·ª´ng b∆∞·ªõc t·ª´ c√°c M√¥ h√¨nh Ng√¥n ng·ªØ L·ªõn ƒêa ph∆∞∆°ng th·ª©c (MLLMs)."
  },
  {
    "row": 3,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 4,
    "rowsha": "+7Cx1rdpWiIOeWRA/RM6HsKf7pNA3hHsQk2Bpadt4II=",
    "originContent": "![Teaser](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig1_teaser.png)",
    "translatedContent": "![Teaser](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig1_teaser.png)"
  },
  {
    "row": 5,
    "rowsha": "yz+R1U7uMOU+NbK5mQX3Dxae1Un9eJCdPawt78ntjTs=",
    "originContent": "---",
    "translatedContent": "---"
  },
  {
    "row": 6,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 7,
    "rowsha": "0YeCyxpcm7/4RDbaM+OQoI8YfqEHQzDkpGW15VZdi1U=",
    "originContent": "## üì∞ News",
    "translatedContent": "## üì∞ Tin t·ª©c"
  },
  {
    "row": 8,
    "rowsha": "EeDKxV0PYB9iw6GxOqwnsP3C385ykbqk8PtEVoVhAcc=",
    "originContent": "- **2025.11.25** &nbsp; üî•[Online PrismAudio Demo](http://prismaudio-project.github.io/) is live - try it now!",
    "translatedContent": "- **2025.11.25** &nbsp; üî•[Demo PrismAudio tr·ª±c tuy·∫øn](http://prismaudio-project.github.io/) ƒë√£ ra m·∫Øt - h√£y th·ª≠ ngay!"
  },
  {
    "row": 9,
    "rowsha": "Q6c68QE34xLPSZ5xAL4zofEn/u8umRYYiH3ddCcy5vk=",
    "originContent": "- **2025.11.25** &nbsp; üî•[PrismAudio paper](https://arxiv.org/pdf/2511.18833) released on arXiv, the first multi-dimensional CoT-RL framework for Video-to-Audio Generation!",
    "translatedContent": "- **2025.11.25** &nbsp; üî•[B√†i b√°o PrismAudio](https://arxiv.org/pdf/2511.18833) ƒë∆∞·ª£c ph√°t h√†nh tr√™n arXiv, l√† khu√¥n kh·ªï CoT-RL ƒëa chi·ªÅu ƒë·∫ßu ti√™n cho T·∫°o √¢m thanh t·ª´ Video!"
  },
  {
    "row": 10,
    "rowsha": "vj2hbV/6MacDcxRmFA0AtTog5xU6CHoqBFO3YMO+yJo=",
    "originContent": "- **2025.09.19** &nbsp; üéâ ThinkSound has been accepted to the **NeurIPS 2025 Main Conference**!",
    "translatedContent": "- **2025.09.19** &nbsp; üéâ ThinkSound ƒë√£ ƒë∆∞·ª£c ch·∫•p nh·∫≠n v√†o **H·ªôi ngh·ªã NeurIPS 2025**!"
  },
  {
    "row": 11,
    "rowsha": "+guVQwDlJDqR1pBGMGc4FQ11nryItaEtKQ3etaCBcn0=",
    "originContent": "- **2025.09.01** &nbsp; Our AudioCoT dataset is now open-sourced and available on [Hugging Face](https://huggingface.co/datasets/liuhuadai/AudioCoT)!",
    "translatedContent": "- **2025.09.01** &nbsp; B·ªô d·ªØ li·ªáu AudioCoT c·ªßa ch√∫ng t√¥i hi·ªán ƒë√£ m√£ ngu·ªìn m·ªü v√† c√≥ s·∫µn tr√™n [Hugging Face](https://huggingface.co/datasets/liuhuadai/AudioCoT)!"
  },
  {
    "row": 12,
    "rowsha": "4Jq9g83O8cYV4fVKsetfTpI+JrSepLrjK6J7Xg9tSqo=",
    "originContent": "- **2025.07.17** &nbsp; üß† Finetuning enabled: training and finetuning code is now publicly available, along with clear usage instructions to help you customize and extend ThinkSound with your own data.",
    "translatedContent": "- **2025.07.17** &nbsp; üß† ƒê√£ h·ªó tr·ª£ tinh ch·ªânh: m√£ hu·∫•n luy·ªán v√† tinh ch·ªânh hi·ªán ƒë√£ ƒë∆∞·ª£c c√¥ng khai, k√®m h∆∞·ªõng d·∫´n s·ª≠ d·ª•ng r√µ r√†ng gi√∫p b·∫°n t√πy ch·ªânh v√† m·ªü r·ªông ThinkSound v·ªõi d·ªØ li·ªáu c·ªßa ri√™ng m√¨nh."
  },
  {
    "row": 13,
    "rowsha": "Ae5w+cTrCd9E8qidC11IWJAgg+LuOgUxA4czDPNG/G0=",
    "originContent": "- **2025.07.15** &nbsp; üì¶ Simplified installation and usability: dependencies on PyPI for easy cross-platform setup; Windows `.bat` scripts automate environment creation and script running.",
    "translatedContent": "- **2025.07.15** &nbsp; üì¶ ƒê∆°n gi·∫£n h√≥a c√†i ƒë·∫∑t v√† s·ª≠ d·ª•ng: c√°c ph·ª• thu·ªôc tr√™n PyPI gi√∫p thi·∫øt l·∫≠p ƒëa n·ªÅn t·∫£ng d·ªÖ d√†ng; script `.bat` cho Windows t·ª± ƒë·ªông t·∫°o m√¥i tr∆∞·ªùng v√† ch·∫°y script."
  },
  {
    "row": 14,
    "rowsha": "nYxNhwgSqjwYLuWsfHAqP5sx2PnzYwoFrwcf9U+Fdss=",
    "originContent": "- **2025.07.08** &nbsp;¬† üîß Major update: model lightweighted and optimized memory and GPU usage, now supports high-throughput audio generation at scale!",
    "translatedContent": "- **2025.07.08** &nbsp;¬† üîß C·∫≠p nh·∫≠t l·ªõn: t·ªëi ∆∞u h√≥a b·ªô nh·ªõ v√† GPU, gi·∫£m nh·∫π m√¥ h√¨nh, nay h·ªó tr·ª£ t·∫°o √¢m thanh t·ªëc ƒë·ªô cao ·ªü quy m√¥ l·ªõn!"
  },
  {
    "row": 15,
    "rowsha": "RPL6cU3/Jgu90ib4PkeN5Q/ALrnjq9hK0ZUCranb/PA=",
    "originContent": "- **2025.07.01** &nbsp; Online demo on [Hugging Face Spaces](https://huggingface.co/spaces/FunAudioLLM/ThinkSound) and [ModelScope](https://modelscope.cn/studios/iic/ThinkSound) for interactive experience!",
    "translatedContent": "- **2025.07.01** &nbsp; Demo tr·ª±c tuy·∫øn tr√™n [Hugging Face Spaces](https://huggingface.co/spaces/FunAudioLLM/ThinkSound) v√† [ModelScope](https://modelscope.cn/studios/iic/ThinkSound) cho tr·∫£i nghi·ªám t∆∞∆°ng t√°c!"
  },
  {
    "row": 16,
    "rowsha": "0PEL3eOyUmX2U56FPEPvYfaltZ/P/wbH0uO6GcLPlUE=",
    "originContent": "- **2025.07.01** &nbsp; Released inference scripts and web interface; ",
    "translatedContent": "- **2025.07.01** &nbsp; ƒê√£ ph√°t h√†nh script suy lu·∫≠n v√† giao di·ªán web;"
  },
  {
    "row": 17,
    "rowsha": "XNdJ/DN741rXoJAruiGiRueQILXIUHRXzBlp+HVWM88=",
    "originContent": "- **2025.06** &nbsp; [ThinkSound paper](https://arxiv.org/pdf/2506.21448) released on arXiv!",
    "translatedContent": "- **2025.06** &nbsp; [B√†i b√°o ThinkSound](https://arxiv.org/pdf/2506.21448) ƒë∆∞·ª£c c√¥ng b·ªë tr√™n arXiv!"
  },
  {
    "row": 18,
    "rowsha": "W45oflUmoUAksoDc1WjcR2hErqh9UIi738PFVipiDg0=",
    "originContent": "- **2025.06** &nbsp; [Online Demo](http://thinksound-project.github.io/) is live - try it now!",
    "translatedContent": "- **2025.06** &nbsp; [Demo tr·ª±c tuy·∫øn](http://thinksound-project.github.io/) ƒë√£ ra m·∫Øt - h√£y th·ª≠ ngay!"
  },
  {
    "row": 19,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 20,
    "rowsha": "yz+R1U7uMOU+NbK5mQX3Dxae1Un9eJCdPawt78ntjTs=",
    "originContent": "---",
    "translatedContent": "---"
  },
  {
    "row": 21,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 22,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 23,
    "rowsha": "f4oQIFLM2EJQxJ65F4oMEA7yWOIqs0eBtiIvGxI+GgI=",
    "originContent": "## üöÄ Features",
    "translatedContent": "## üöÄ T√≠nh nƒÉng"
  },
  {
    "row": 24,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 25,
    "rowsha": "8TbVluJJPR3C0fX41HklpO/vH9/k/rHC+jlvmHHfzQU=",
    "originContent": "- **Any2Audio**: Generate audio from arbitrary modalities ‚Äî video, text, audio, or their combinations.",
    "translatedContent": "- **Any2Audio**: T·∫°o √¢m thanh t·ª´ b·∫•t k·ª≥ ph∆∞∆°ng th·ª©c n√†o ‚Äî video, vƒÉn b·∫£n, √¢m thanh ho·∫∑c k·∫øt h·ª£p c√°c ph∆∞∆°ng th·ª©c."
  },
  {
    "row": 26,
    "rowsha": "+Jf5LrYz+d1ShYCuQe8UF9taEJUpuGhDk6fAlYK3Kj4=",
    "originContent": "- **Video-to-Audio SOTA**: Achieves state-of-the-art results on multiple V2A benchmarks.",
    "translatedContent": "- **Video-to-Audio SOTA**: ƒê·∫°t k·∫øt qu·∫£ t·ªët nh·∫•t tr√™n nhi·ªÅu benchmark V2A."
  },
  {
    "row": 27,
    "rowsha": "mU7qXkjW1YifKoYXJedYo9l64NsTBaiXsgoGFRF/g+E=",
    "originContent": "- **CoT-Driven Reasoning**: Chain-of-Thought reasoning for compositional and controllable audio generation via MLLMs.",
    "translatedContent": "- **Suy lu·∫≠n CoT**: Suy lu·∫≠n Chu·ªói T∆∞ duy cho t·∫°o √¢m thanh c√≥ t√≠nh th√†nh ph·∫ßn v√† ki·ªÉm so√°t th√¥ng qua MLLMs."
  },
  {
    "row": 28,
    "rowsha": "RComOCBBrXsZf9RHmLginqKTh9eI/bKUZuUunQEmD5M=",
    "originContent": "- **Interactive Object-centric Editing**: Refine or edit specific sound events by clicking on visual objects or using text instructions.",
    "translatedContent": "- **Ch·ªânh s·ª≠a t·∫≠p trung ƒë·ªëi t∆∞·ª£ng t∆∞∆°ng t√°c**: Tinh ch·ªânh ho·∫∑c ch·ªânh s·ª≠a s·ª± ki·ªán √¢m thanh c·ª• th·ªÉ b·∫±ng c√°ch nh·∫•p v√†o ƒë·ªëi t∆∞·ª£ng tr·ª±c quan ho·∫∑c s·ª≠ d·ª•ng h∆∞·ªõng d·∫´n vƒÉn b·∫£n."
  },
  {
    "row": 29,
    "rowsha": "C3sf87sy73G/XZft+TDo5NjXo5XcrtJB805ayHRAXoQ=",
    "originContent": "- **Unified Framework**: One foundation model supports generation, editing, and interactive workflow.",
    "translatedContent": "- **Khung th·ªëng nh·∫•t**: M·ªôt m√¥ h√¨nh n·ªÅn t·∫£ng h·ªó tr·ª£ t·∫°o, ch·ªânh s·ª≠a v√† quy tr√¨nh t∆∞∆°ng t√°c."
  },
  {
    "row": 30,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 31,
    "rowsha": "yz+R1U7uMOU+NbK5mQX3Dxae1Un9eJCdPawt78ntjTs=",
    "originContent": "---",
    "translatedContent": "---"
  },
  {
    "row": 32,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 33,
    "rowsha": "gjgLOIAU2x83BBZdLUdgEB+F64ajt/QuLYQXM1hDBLE=",
    "originContent": "## ‚ú® Method Overview",
    "translatedContent": "## ‚ú® T·ªïng quan ph∆∞∆°ng ph√°p"
  },
  {
    "row": 34,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 35,
    "rowsha": "v5//GqG/smYIGoUpN+12k+9/3GWH3GdYD+jLqScb7AM=",
    "originContent": "ThinkSound decomposes audio generation and editing into three interactive stages, all guided by MLLM-based Chain-of-Thought (CoT) reasoning:",
    "translatedContent": "ThinkSound ph√¢n t√°ch qu√° tr√¨nh t·∫°o v√† ch·ªânh s·ª≠a √¢m thanh th√†nh ba giai ƒëo·∫°n t∆∞∆°ng t√°c, t·∫•t c·∫£ ƒë·ªÅu ƒë∆∞·ª£c h∆∞·ªõng d·∫´n b·ªüi suy lu·∫≠n Chu·ªói T∆∞ duy (CoT) d·ª±a tr√™n MLLM:"
  },
  {
    "row": 36,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 37,
    "rowsha": "vTObEWb7f5gCU681X3dTkwqhsaSW89TLw3GgMJ5I3bo=",
    "originContent": "1. **Foley Generation:** Generate foundational, semantically and temporally aligned soundscapes from video.",
    "translatedContent": "1. **T·∫°o Foley:** T·∫°o c·∫£nh √¢m thanh n·ªÅn, cƒÉn ch·ªânh ng·ªØ nghƒ©a v√† th·ªùi gian t·ª´ video."
  },
  {
    "row": 38,
    "rowsha": "LTBpIQQHtEkNF8StAa+ZEDASGmRhmHIKDQOdZ4ExJWM=",
    "originContent": "2. **Object-Centric Refinement:** Refine or add sounds for user-specified objects via clicks or regions in the video.",
    "translatedContent": "2. **Tinh ch·ªânh t·∫≠p trung ƒë·ªëi t∆∞·ª£ng:** Tinh ch·ªânh ho·∫∑c th√™m √¢m thanh cho c√°c ƒë·ªëi t∆∞·ª£ng do ng∆∞·ªùi d√πng ch·ªâ ƒë·ªãnh qua nh·∫•p chu·ªôt ho·∫∑c v√πng trong video."
  },
  {
    "row": 39,
    "rowsha": "8wrAo7X7dPC6Sgpfrlq2ziv/Wg/3+JnlFYvO+RMxADQ=",
    "originContent": "3. **Targeted Audio Editing:** Modify generated audio using high-level natural language instructions.",
    "translatedContent": "3. **Ch·ªânh s·ª≠a √¢m thanh c√≥ m·ª•c ti√™u:** S·ª≠a ƒë·ªïi √¢m thanh ƒë√£ t·∫°o b·∫±ng h∆∞·ªõng d·∫´n ng√¥n ng·ªØ t·ª± nhi√™n c·∫•p cao."
  },
  {
    "row": 40,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 41,
    "rowsha": "4UKlvFW3Xb0bSAVjcBNeekH/MMiYS0XDg9w4mCuPy/Q=",
    "originContent": "![ThinkSound Overview](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig3_model.png)",
    "translatedContent": "![T·ªïng quan ThinkSound](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig3_model.png)"
  },
  {
    "row": 42,
    "rowsha": "GaujeIM3x7+YcFy07LNNyITlujhkgpgeIaOiKHJkYnE=",
    "originContent": "<!-- A large-scale CoT-annotated dataset (**AudioCoT**) is used to train both the reasoning module and the unified audio foundation model.",
    "translatedContent": "<!-- M·ªôt b·ªô d·ªØ li·ªáu ch√∫ th√≠ch CoT quy m√¥ l·ªõn (**AudioCoT**) ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ hu·∫•n luy·ªán c·∫£ m√¥-ƒëun suy lu·∫≠n v√† m√¥ h√¨nh n·ªÅn t·∫£ng √¢m thanh h·ª£p nh·∫•t."
  },
  {
    "row": 43,
    "rowsha": "qYOXaaTiYkoaPFcpTXE5xdSqqiW3ebi//EW/RfSXd9g=",
    "originContent": "![AudioCoT Pipeline](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig2_dataset.png) -->",
    "translatedContent": "![Quy tr√¨nh AudioCoT](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig2_dataset.png) -->"
  },
  {
    "row": 44,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 45,
    "rowsha": "yz+R1U7uMOU+NbK5mQX3Dxae1Un9eJCdPawt78ntjTs=",
    "originContent": "---",
    "translatedContent": "---"
  },
  {
    "row": 46,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 47,
    "rowsha": "PrY/jc4yAHdS+Sr+s+Yhab477/BDp3GAzMJ8+WyumyI=",
    "originContent": "## ‚ö° Quick Start",
    "translatedContent": "## ‚ö° B·∫Øt ƒë·∫ßu nhanh"
  },
  {
    "row": 48,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 49,
    "rowsha": "mD63DGEpdc7FlccTEps0KPAiwsJpO/C3yjV+SKIi/vE=",
    "originContent": "**Environment Preparation:**",
    "translatedContent": "**Chu·∫©n b·ªã m√¥i tr∆∞·ªùng:**"
  },
  {
    "row": 50,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 51,
    "rowsha": "yz+R1U7uMOU+NbK5mQX3Dxae1Un9eJCdPawt78ntjTs=",
    "originContent": "---",
    "translatedContent": "---"
  },
  {
    "row": 52,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 53,
    "rowsha": "V0ea1xQLKG+cGj5kHVv5f15HDd+yj0ulkcBQnvErdJc=",
    "originContent": "## üì¨ Contact",
    "translatedContent": "## üì¨ Contact"
  },
  {
    "row": 54,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 55,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 56,
    "rowsha": "QsDuVwX1DXlTNejKuNftx4k1x7yNHjfP9/1HS85hJng=",
    "originContent": "‚ú® Feel free to [open an issue](https://github.com/liuhuadai/ThinkSound/issues) or contact us via email ([liuhuadai@zju.edu.cn](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/mailto:liuhuadai@zju.edu.cn)) if you have any questions or suggestions!",
    "translatedContent": "‚ú® Feel free to [open an issue](https://github.com/liuhuadai/ThinkSound/issues) or contact us via email ([liuhuadai@zju.edu.cn](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/mailto:liuhuadai@zju.edu.cn)) if you have any questions or suggestions!"
  },
  {
    "row": 57,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 58,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 59,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  }
]