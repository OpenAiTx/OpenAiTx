{
  "id": 1,
  "origin": "# 🎶 ThinkSound\n\n<p align=\"center\">\n  If you find this project useful, a star ⭐ on GitHub would be greatly appreciated!\n</p>\n\n<p align=\"center\">\n  <a href=\"https://arxiv.org/pdf/2506.21448\">\n    <img src=\"https://img.shields.io/badge/arXiv-2506.21448-b31b1b.svg\" alt=\"arXiv\"/>\n  </a>\n  &nbsp;\n  <a href=\"https://thinksound-project.github.io/\">\n    <img src=\"https://img.shields.io/badge/Online%20Demo-🌐-blue\" alt=\"Online Demo\"/>\n  </a>\n  &nbsp;\n  <a href=\"https://huggingface.co/spaces/FunAudioLLM/ThinkSound\">\n    <img src=\"https://img.shields.io/badge/HuggingFace-Spaces-orange?logo=huggingface\" alt=\"Hugging Face\"/>\n  </a>\n  &nbsp;\n  <a href=\"https://modelscope.cn/studios/iic/ThinkSound\">\n    <img src=\"https://img.shields.io/badge/ModelScope-在线体验-green\" alt=\"ModelScope\"/>\n  </a>\n</p>\n\n---\n\n**ThinkSound** is a unified Any2Audio generation framework with flow matching guided by Chain-of-Thought (CoT) reasoning.\n\nPyTorch implementation for multimodal audio generation and editing: generate or edit audio from video, text, and audio, powered by step-by-step reasoning from Multimodal Large Language Models (MLLMs).\n\n![Teaser](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig1_teaser.png)\n---\n\n## 📰 News\n- **2025.07** &nbsp; 🔥Online demo on [Hugging Face Spaces](https://huggingface.co/spaces/FunAudioLLM/ThinkSound) and [ModelScope](https://modelscope.cn/studios/iic/ThinkSound) for interactive experience!\n- **2025.07** &nbsp; 🔥Released inference scripts and web interface; \n- **2025.06** &nbsp; 🔥[ThinkSound paper](https://arxiv.org/pdf/2506.21448) released on arXiv!\n- **2025.06** &nbsp; 🔥[Online Demo](http://thinksound-project.github.io/) is live - try it now!\n\n---\n\n## 🚀 Features\n\n- **Any2Audio**: Generate audio from arbitrary modalities — video, text, audio, or their combinations.\n- **Video-to-Audio SOTA**: Achieves state-of-the-art results on multiple V2A benchmarks.\n- **CoT-Driven Reasoning**: Chain-of-Thought reasoning for compositional and controllable audio generation via MLLMs.\n- **Interactive Object-centric Editing**: Refine or edit specific sound events by clicking on visual objects or using text instructions.\n- **Unified Framework**: One foundation model supports generation, editing, and interactive workflow.\n\n---\n\n## ✨ Method Overview\n\nThinkSound decomposes audio generation and editing into three interactive stages, all guided by MLLM-based Chain-of-Thought (CoT) reasoning:\n\n1. **Foley Generation:** Generate foundational, semantically and temporally aligned soundscapes from video.\n2. **Object-Centric Refinement:** Refine or add sounds for user-specified objects via clicks or regions in the video.\n3. **Targeted Audio Editing:** Modify generated audio using high-level natural language instructions.\n\n![ThinkSound Overview](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig3_model.png)\n<!-- A large-scale CoT-annotated dataset (**AudioCoT**) is used to train both the reasoning module and the unified audio foundation model.\n![AudioCoT Pipeline](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig2_dataset.png) -->\n\n---\n\n## ⚡ Quick Start\n\n**Environment Preparation:**\n```bash\ngit clone https://github.com/liuhuadai/ThinkSound.git\ncd ThinkSound\npip install -r requirements.txt\nconda install -y -c conda-forge 'ffmpeg<7'\n# Download pretrained weights https://huggingface.co/liuhuadai/ThinkSound to Directory ckpts/\n# model weights can be also downloaded from https://www.modelscope.cn/models/iic/ThinkSound\ngit lfs install\ngit clone https://huggingface.co/liuhuadai/ThinkSound ckpts\n```\n\n**Make it executable**\n```bash\nchmod +x scripts/demo.sh\n```\n\n**Run the script**\n```bash\n./scripts/demo.sh <video_path> <caption> <CoT description>\n```\n\n\n### Web Interface Usage\n\nFor an interactive experience, launch the Gradio web interface:\n\n```bash\npython app.py\n```\n\n---\n",
  "origin_sha": "vYi4X/38TyDFihDOfSBGZIgKnu2bd0a73Iz5+m+7pd0=",
  "translate": "# 🎶 ThinkSound\n\n<p align=\"center\">\n  이 프로젝트가 유용하다면, GitHub에서 별표 ⭐를 눌러주시면 큰 힘이 됩니다!\n</p>\n\n<p align=\"center\">\n  <a href=\"https://arxiv.org/pdf/2506.21448\">\n    <img src=\"https://img.shields.io/badge/arXiv-2506.21448-b31b1b.svg\" alt=\"arXiv\"/>\n  </a>\n  &nbsp;\n  <a href=\"https://thinksound-project.github.io/\">\n    <img src=\"https://img.shields.io/badge/Online%20Demo-🌐-blue\" alt=\"Online Demo\"/>\n  </a>\n  &nbsp;\n  <a href=\"https://huggingface.co/spaces/FunAudioLLM/ThinkSound\">\n    <img src=\"https://img.shields.io/badge/HuggingFace-Spaces-orange?logo=huggingface\" alt=\"Hugging Face\"/>\n  </a>\n  &nbsp;\n  <a href=\"https://modelscope.cn/studios/iic/ThinkSound\">\n    <img src=\"https://img.shields.io/badge/ModelScope-在线体验-green\" alt=\"ModelScope\"/>\n  </a>\n</p>\n\n---\n\n**ThinkSound**는 Chain-of-Thought (CoT) 추론이 적용된 Flow Matching 기반의 통합 Any2Audio 생성 프레임워크입니다.\n\n멀티모달 대형 언어 모델(MLLM)의 단계별 추론을 활용하여, 비디오, 텍스트, 오디오로부터 오디오를 생성하거나 편집하는 PyTorch 구현체입니다.\n\n![Teaser](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig1_teaser.png)\n---\n\n## 📰 새 소식\n- **2025.07** &nbsp; 🔥[Hugging Face Spaces](https://huggingface.co/spaces/FunAudioLLM/ThinkSound) 및 [ModelScope](https://modelscope.cn/studios/iic/ThinkSound)에서 온라인 데모 및 인터랙티브 체험 제공!\n- **2025.07** &nbsp; 🔥추론 스크립트 및 웹 인터페이스 공개; \n- **2025.06** &nbsp; 🔥[ThinkSound 논문](https://arxiv.org/pdf/2506.21448) arXiv 공개!\n- **2025.06** &nbsp; 🔥[온라인 데모](http://thinksound-project.github.io/) 오픈 - 지금 바로 체험해 보세요!\n\n---\n\n## 🚀 주요 특징\n\n- **Any2Audio**: 비디오, 텍스트, 오디오 및 이들의 조합 등 임의의 모달리티로부터 오디오 생성.\n- **비디오-투-오디오 SOTA**: 다양한 V2A 벤치마크에서 최첨단 성능 달성.\n- **CoT 기반 추론**: MLLM 기반 단계별 Chain-of-Thought 추론을 통한 합성 및 제어 가능한 오디오 생성.\n- **인터랙티브 객체 중심 편집**: 비주얼 객체 클릭 또는 텍스트 지시어로 특정 사운드 이벤트를 세밀하게 보정 및 편집.\n- **통합 프레임워크**: 하나의 기반 모델로 생성, 편집, 인터랙티브 워크플로우 지원.\n\n---\n\n## ✨ 방법 개요\n\nThinkSound는 오디오 생성 및 편집을 MLLM 기반 Chain-of-Thought(CoT) 추론이 이끄는 세 가지 인터랙티브 단계로 분해합니다:\n\n1. **폴리(foley) 생성:** 비디오와 의미적·시간적으로 정렬된 기초 사운드스케이프 생성.\n2. **객체 중심 정제:** 비디오 내 사용자가 지정한 객체(클릭 또는 영역)를 기준으로 사운드 추가/정제.\n3. **타겟 오디오 편집:** 고수준 자연어 지시어를 활용해 생성된 오디오를 수정.\n\n![ThinkSound Overview](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig3_model.png)\n<!-- 대규모 CoT 주석 데이터셋(**AudioCoT**)으로 추론 모듈과 통합 오디오 기반 모델 모두를 학습.\n![AudioCoT Pipeline](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig2_dataset.png) -->\n\n---\n\n## ⚡ 빠른 시작\n\n**환경 준비:**\n```bash\ngit clone https://github.com/liuhuadai/ThinkSound.git\ncd ThinkSound\npip install -r requirements.txt\nconda install -y -c conda-forge 'ffmpeg<7'\n# 사전학습 가중치 다운로드 https://huggingface.co/liuhuadai/ThinkSound → ckpts/ 디렉토리\n# 모델 가중치는 https://www.modelscope.cn/models/iic/ThinkSound 에서도 다운로드 가능\ngit lfs install\ngit clone https://huggingface.co/liuhuadai/ThinkSound ckpts\n```\n\n**실행 권한 부여**\n```bash\nchmod +x scripts/demo.sh\n```\n\n**스크립트 실행**\n```bash\n./scripts/demo.sh <video_path> <caption> <CoT description>\n```\n\n\n### 웹 인터페이스 사용법\n\n인터랙티브 체험을 원한다면, Gradio 웹 인터페이스를 실행하세요:\n\n```bash\npython app.py\n```\n\n---",
  "status": "ok"
}