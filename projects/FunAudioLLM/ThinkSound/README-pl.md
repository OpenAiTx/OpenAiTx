# ğŸ¶ ThinkSound

<p align="center">
  JeÅ›li uznasz ten projekt za przydatny, gwiazdka â­ na GitHubie bÄ™dzie bardzo mile widziana!
</p>

<p align="center">
  <a href="https://arxiv.org/pdf/2506.21448">
    <img src="https://img.shields.io/badge/arXiv-2506.21448-b31b1b.svg" alt="arXiv"/>
  </a>
  &nbsp;
  <a href="https://thinksound-project.github.io/">
    <img src="https://img.shields.io/badge/Online%20Demo-ğŸŒ-blue" alt="Online Demo"/>
  </a>
  &nbsp;
  <a href="https://huggingface.co/spaces/FunAudioLLM/ThinkSound">
    <img src="https://img.shields.io/badge/HuggingFace-Spaces-orange?logo=huggingface" alt="Hugging Face"/>
  </a>
  &nbsp;
  <a href="https://modelscope.cn/studios/iic/ThinkSound">
    <img src="https://img.shields.io/badge/ModelScope-åœ¨çº¿ä½“éªŒ-green" alt="ModelScope"/>
  </a>
</p>

---

**ThinkSound** to zunifikowane Å›rodowisko generowania Any2Audio z dopasowaniem przepÅ‚ywu sterowanym przez rozumowanie Chain-of-Thought (CoT).

Implementacja PyTorch do multimodalnej generacji i edycji dÅºwiÄ™ku: generuj lub edytuj dÅºwiÄ™k na podstawie wideo, tekstu oraz dÅºwiÄ™ku, napÄ™dzana przez rozumowanie krok po kroku z wykorzystaniem Multimodalnych DuÅ¼ych Modeli JÄ™zykowych (MLLM).

![Teaser](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig1_teaser.png)
---

## ğŸ“° AktualnoÅ›ci
- **2025.07** &nbsp; ğŸ”¥Demo online na [Hugging Face Spaces](https://huggingface.co/spaces/FunAudioLLM/ThinkSound) oraz [ModelScope](https://modelscope.cn/studios/iic/ThinkSound) do interaktywnego korzystania!
- **2025.07** &nbsp; ğŸ”¥Wydano skrypty do wnioskowania i interfejs webowy;
- **2025.06** &nbsp; ğŸ”¥[ArtykuÅ‚ ThinkSound](https://arxiv.org/pdf/2506.21448) opublikowany na arXiv!
- **2025.06** &nbsp; ğŸ”¥[Demo online](http://thinksound-project.github.io/) juÅ¼ dostÄ™pne â€“ wyprÃ³buj teraz!

---

## ğŸš€ Funkcje

- **Any2Audio**: Generuj dÅºwiÄ™k z dowolnej modalnoÅ›ci â€” wideo, tekstu, dÅºwiÄ™ku lub ich kombinacji.
- **Video-to-Audio SOTA**: OsiÄ…ga najnowsze wyniki na wielu benchmarkach V2A.
- **CoT-Driven Reasoning**: Rozumowanie Chain-of-Thought dla kompozycyjnej i kontrolowanej generacji dÅºwiÄ™ku przez MLLM.
- **Interaktywna Edycja Skoncentrowana na Obiektach**: Ulepszaj lub edytuj konkretne zdarzenia dÅºwiÄ™kowe poprzez klikniÄ™cie na obiekt wizualny lub uÅ¼ycie instrukcji tekstowych.
- **Zunifikowany Framework**: Jeden model bazowy obsÅ‚uguje generowanie, edycjÄ™ oraz interaktywny workflow.

---

## âœ¨ PrzeglÄ…d Metody

ThinkSound rozkÅ‚ada generowanie i edycjÄ™ dÅºwiÄ™ku na trzy interaktywne etapy, wszystkie sterowane rozumowaniem Chain-of-Thought (CoT) opartym na MLLM:

1. **Generowanie Foley:** Tworzenie podstawowych, semantycznie i czasowo dopasowanych pejzaÅ¼y dÅºwiÄ™kowych na podstawie wideo.
2. **Refinacja Skoncentrowana na Obiekcie:** Ulepszaj lub dodawaj dÅºwiÄ™ki dla obiektÃ³w wskazanych przez uÅ¼ytkownika za pomocÄ… klikniÄ™Ä‡ lub zaznaczeÅ„ na wideo.
3. **Celowana Edycja DÅºwiÄ™ku:** Modyfikuj wygenerowany dÅºwiÄ™k za pomocÄ… zÅ‚oÅ¼onych instrukcji w jÄ™zyku naturalnym.

![ThinkSound Overview](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig3_model.png)
<!-- DuÅ¼y, oznaczony danymi CoT zbiÃ³r danych (**AudioCoT**) wykorzystywany jest do trenowania zarÃ³wno moduÅ‚u rozumowania, jak i zunifikowanego modelu bazowego dÅºwiÄ™ku.
![AudioCoT Pipeline](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig2_dataset.png) -->

---

## âš¡ Szybki Start

**Przygotowanie Å›rodowiska:**
```bash
git clone https://github.com/liuhuadai/ThinkSound.git
cd ThinkSound
pip install -r requirements.txt
conda install -y -c conda-forge 'ffmpeg<7'
# Pobierz wytrenowane wagi https://huggingface.co/liuhuadai/ThinkSound do katalogu ckpts/
# Wagi modelu moÅ¼na rÃ³wnieÅ¼ pobraÄ‡ z https://www.modelscope.cn/models/iic/ThinkSound
git lfs install
git clone https://huggingface.co/liuhuadai/ThinkSound ckpts
```

**Ustaw jako wykonywalny**
```bash
chmod +x scripts/demo.sh
```

**Uruchom skrypt**
```bash
./scripts/demo.sh <Å›cieÅ¼ka_do_wideo> <opis> <opis_CoT>
```

### Korzystanie z interfejsu webowego

Aby uzyskaÄ‡ interaktywne doÅ›wiadczenie, uruchom webowy interfejs Gradio:

```bash
python app.py
```

---
## ğŸ“ DO ZROBIENIA

- â˜ UdostÄ™pniÄ‡ skrypty treningowe dla modeli ThinkSound
- â˜ UdostÄ™pniÄ‡ jako open-source zbiÃ³r danych AudioCoT oraz zautomatyzowany pipeline
- â˜ ZapewniÄ‡ szczegÃ³Å‚owÄ… dokumentacjÄ™ oraz referencjÄ™ API
- â˜ DodaÄ‡ wsparcie dla dodatkowych modalnoÅ›ci i zadaÅ„ koÅ„cowych

---

## ğŸ“„ Licencja

Ten projekt jest udostÄ™pniany na licencji [Apache 2.0](LICENSE).

> **Uwaga:**  
> Kod, modele i zbiÃ³r danych sÄ… **wyÅ‚Ä…cznie do celÃ³w badawczych i edukacyjnych**.  
> **UÅ¼ycie komercyjne NIE jest dozwolone.**
>
> W sprawie licencji komercyjnej prosimy o kontakt z autorami.

---

## ğŸ“– Cytowanie

JeÅ›li ThinkSound okaÅ¼e siÄ™ przydatny w Twoich badaniach lub pracy, prosimy o cytowanie naszej publikacji:

```bibtex
@misc{liu2025thinksoundchainofthoughtreasoningmultimodal,
      title={ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language Models for Audio Generation and Editing}, 
      author={Huadai Liu and Jialei Wang and Kaicheng Luo and Wen Wang and Qian Chen and Zhou Zhao and Wei Xue},
      year={2025},
      eprint={2506.21448},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/2506.21448}, 
}
```

---

## ğŸ“¬ Kontakt

âœ¨ ZachÄ™camy do [otwarcia zgÅ‚oszenia](https://github.com/liuhuadai/ThinkSound/issues) lub kontaktu mailowego ([liuhuadai@zju.edu.cn](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/mailto:liuhuadai@zju.edu.cn)), jeÅ›li masz pytania lub sugestie!

---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-03

---