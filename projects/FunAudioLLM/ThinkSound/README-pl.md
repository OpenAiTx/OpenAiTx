<h1 align="center">ThinkSound</h1>

<p align="center">
  ğŸŒ
  <a href="https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=en">English</a> |
  <a href="https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=zh-CN">ç®€ä½“ä¸­æ–‡</a> |
  <a href="https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=zh-TW">ç¹é«”ä¸­æ–‡</a> |
  <a href="https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=es">EspaÃ±ol</a> |
  <a href="https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=fr">FranÃ§ais</a> |
  <a href="https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=ja">æ—¥æœ¬èª</a>
  
</p>

<p align="center">
  <a href="https://arxiv.org/pdf/2506.21448">
    <img src="https://img.shields.io/badge/arXiv-2506.21448-b31b1b.svg" alt="arXiv"/>
  </a>
  &nbsp;
  <a href="https://thinksound-project.github.io/">
    <img src="https://img.shields.io/badge/Online%20Demo-ğŸŒ-blue" alt="Online Demo"/>
  </a>
  &nbsp;
  <a href="https://huggingface.co/spaces/FunAudioLLM/ThinkSound">
    <img src="https://img.shields.io/badge/HuggingFace-Spaces-orange?logo=huggingface" alt="Hugging Face"/>
  </a>
  &nbsp;
  <a href="https://modelscope.cn/studios/iic/ThinkSound">
    <img src="https://img.shields.io/badge/ModelScope-åœ¨çº¿ä½“éªŒ-green" alt="ModelScope"/>
  </a>
</p>

<p align="center">
  JeÅ›li ten projekt jest dla Ciebie przydatny,<br>
  bÄ™dziemy wdziÄ™czni za gwiazdkÄ™ â­ na GitHubie!
</p>

---

**ThinkSound** to zunifikowany framework Any2Audio do generowania dÅºwiÄ™ku z przepÅ‚ywem sterowanym przez Chain-of-Thought (CoT).

Implementacja w PyTorch do multimodalnego generowania i edycji dÅºwiÄ™ku: generuj lub edytuj dÅºwiÄ™k na podstawie wideo, tekstu i audio, napÄ™dzana przez krok po kroku rozumowanie modeli Multimodal Large Language Models (MLLMs).

![Teaser](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig1_teaser.png)
---

## ğŸ“° NowoÅ›ci
- **2025.07.17** &nbsp; ğŸ§  DostÄ™pny finetuning: kod treningu i dostrajania jest juÅ¼ publicznie dostÄ™pny, wraz z jasnymi instrukcjami, jak dostosowaÄ‡ i rozbudowaÄ‡ ThinkSound na wÅ‚asnych danych.
- **2025.07.15** &nbsp; ğŸ“¦ Uproszczona instalacja i obsÅ‚uga: zaleÅ¼noÅ›ci dostÄ™pne na PyPI dla Å‚atwej instalacji miÄ™dzyplatformowej; skrypty Windows `.bat` automatyzujÄ… tworzenie Å›rodowiska i uruchamianie skryptÃ³w.
- **2025.07.08** &nbsp;Â  ğŸ”§ DuÅ¼a aktualizacja: model odchudzony i zoptymalizowany pod kÄ…tem pamiÄ™ci i GPU, teraz wspiera masowe generowanie dÅºwiÄ™ku na duÅ¼Ä… skalÄ™!
- **2025.07.01** &nbsp; ğŸ”¥Demo online na [Hugging Face Spaces](https://huggingface.co/spaces/FunAudioLLM/ThinkSound) i [ModelScope](https://modelscope.cn/studios/iic/ThinkSound) â€“ interaktywne doÅ›wiadczenie!
- **2025.07.01** &nbsp; ğŸ”¥UdostÄ™pnione skrypty inferencyjne i interfejs webowy; 
- **2025.06** &nbsp; ğŸ”¥[ArtykuÅ‚ ThinkSound](https://arxiv.org/pdf/2506.21448) opublikowany na arXiv!
- **2025.06** &nbsp; ğŸ”¥[Demo online](http://thinksound-project.github.io/) jest juÅ¼ dostÄ™pne â€“ wyprÃ³buj teraz!

---


## ğŸš€ Funkcje

- **Any2Audio**: Generuj dÅºwiÄ™k z dowolnych modalnoÅ›ci â€” wideo, tekst, dÅºwiÄ™k lub ich kombinacji.
- **Video-to-Audio SOTA**: OsiÄ…ga najlepsze wyniki na wielu benchmarkach V2A.
- **CoT-Driven Reasoning**: Chain-of-Thought do kompozycyjnego i kontrolowanego generowania dÅºwiÄ™ku przez MLLM.
- **Interaktywna edycja obiektowa**: Udoskonalaj lub edytuj konkretne zdarzenia dÅºwiÄ™kowe, klikajÄ…c obiekty wizualne lub korzystajÄ…c z instrukcji tekstowych.
- **Zunifikowany framework**: Jeden model bazowy wspiera generowanie, edycjÄ™ i interaktywny workflow.

---

## âœ¨ PrzeglÄ…d metody

ThinkSound rozkÅ‚ada generowanie i edycjÄ™ dÅºwiÄ™ku na trzy interaktywne etapy, wszystkie prowadzone przez CoT oparty na MLLM:

1. **Generowanie Foley:** Tworzenie podstawowych, semantycznie i czasowo dopasowanych pejzaÅ¼y dÅºwiÄ™kowych z wideo.
2. **Obiektowe udoskonalenie:** Udoskonalaj lub dodawaj dÅºwiÄ™ki dla wybranych przez uÅ¼ytkownika obiektÃ³w poprzez klikniÄ™cia lub zaznaczenia na wideo.
3. **Ukierunkowana edycja audio:** Modyfikuj wygenerowany dÅºwiÄ™k przy uÅ¼yciu wysokopoziomowych instrukcji w jÄ™zyku naturalnym.

![ThinkSound Overview](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig3_model.png)
<!-- DuÅ¼y, oznaczony CoT zbiÃ³r danych (**AudioCoT**) sÅ‚uÅ¼y do trenowania zarÃ³wno moduÅ‚u rozumowania, jak i zunifikowanego modelu audio.
![AudioCoT Pipeline](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig2_dataset.png) -->

---

## âš¡ Szybki start

**Przygotowanie Å›rodowiska:**
```bash
git clone https://github.com/liuhuadai/ThinkSound.git
cd ThinkSound
conda create -n thinksound python=3.10
conda activate thinksound
pip install thinksound
conda install -y -c conda-forge 'ffmpeg<7'
# Download pretrained weights https://huggingface.co/liuhuadai/ThinkSound to Directory ckpts/
# model weights can be also downloaded from https://www.modelscope.cn/models/iic/ThinkSound
git lfs install
git clone https://huggingface.co/liuhuadai/ThinkSound ckpts
# To improve inference and training speed, you may optionally install a FlashAttention backend compatible with your system and PyTorch version.
```
> âœ… **WskazÃ³wka dla Windows:**  
> UÅ¼ytkownicy Windows mogÄ… po prostu uruchomiÄ‡ `setup_windows.bat` (lub kliknÄ…Ä‡ go dwukrotnie), aby automatycznie utworzyÄ‡ Å›rodowisko conda, zainstalowaÄ‡ wszystkie zaleÅ¼noÅ›ci (w tym FFmpeg) i pobraÄ‡ wytrenowany model â€” bez koniecznoÅ›ci rÄ™cznej konfiguracji.  
> Przed uruchomieniem skryptu upewnij siÄ™, Å¼e `conda` i `git` sÄ… zainstalowane i dostÄ™pne w zmiennej PATH systemu.


### â–¶ï¸ Uruchom Demo

#### **Linux/macOS**


```bash
chmod +x scripts/demo.sh
./scripts/demo.sh <path-to-your-demo-video> <title> <CoT description> [use-half]
```
#### **Windows**

MoÅ¼esz zamiast tego uÅ¼yÄ‡ doÅ‚Ä…czonego skryptu `.bat`:


```bash
.\scripts\demo.bat <path-to-your-demo-video> <title> <CoT description> [use-half]
```
**Uwaga:**

* `<Å›cieÅ¼ka-do-twojego-wideo-demo>`: ÅšcieÅ¼ka do pojedynczego wideo
* `[use-half]` (opcjonalnie): Dodaj use-half na koÅ„cu, aby wÅ‚Ä…czyÄ‡ ekstrakcjÄ™ cech w poÅ‚owie precyzji.

---

### ğŸ“¦ Przetwarzanie wsadowe

#### **Linux/macOS**


```bash
chmod +x scripts/eval_batch.sh
./scripts/eval_batch.sh <video_path> <csv_path> <save_path (optional)> [use-half]
```
#### **Windows**

UÅ¼yj rÃ³wnowaÅ¼nego skryptu `.bat`:


```bash
.\scripts\eval_batch.bat <video_path> <csv_path> <save_path (optional)> [use-half]
```
**Uwaga:**

* `<video_path>`: ÅšcieÅ¼ka do katalogu gÅ‚Ã³wnego zawierajÄ…cego wszystkie filmy .mp4 do przetworzenia (wszystkie filmy muszÄ… mieÄ‡ takÄ… samÄ… dÅ‚ugoÅ›Ä‡).
* `<csv_path>`: Plik CSV z tekstowymi podpowiedziami dla kaÅ¼dego filmu (zobacz `demo_test.csv` dla formatu).
* `<save_path>` (opcjonalnie): Gdzie zapisaÄ‡ wygenerowane audio. DomyÅ›lnie `results/features`.
* `[use-half]` (opcjonalnie): Dodaj use-half na koÅ„cu, aby wÅ‚Ä…czyÄ‡ ekstrakcjÄ™ cech w trybie pÃ³Å‚precyzyjnym.

---


### UÅ¼ycie interfejsu webowego

Aby uzyskaÄ‡ interaktywne doÅ›wiadczenie, uruchom interfejs webowy Gradio:


```bash
python app.py
```
## ğŸ‹ï¸ Trening modelu

Zobacz [`Training.md`](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/docs/Training.md)


---

## ğŸ“ TODO i Plany na PrzyszÅ‚oÅ›Ä‡
* - [ ] Otwarcie kodu zbioru danych AudioCoT i zautomatyzowanego pipelineâ€™u (Oczekiwane przed 23.07.2025)
* - [ ] Wydanie potÄ™Å¼niejszego modelu bazowego obejmujÄ…cego wiele dziedzin, aby zapewniÄ‡ bardziej angaÅ¼ujÄ…ce i immersyjne tworzenie efektÃ³w foley (Oczekiwane do koÅ„ca sierpnia 2025)
* - [ ] Dodanie wsparcia dla dodatkowych modalnoÅ›ci i zadaÅ„ downstream (Oczekiwane przed koÅ„cem lipca 2025)
* - [ ] Wydanie modeli w rÃ³Å¼nych skalach (Oczekiwane przed koÅ„cem lipca 2025)
* - [x] UdostÄ™pnienie skryptÃ³w treningowych dla modeli ThinkSound
* - [x] Przyjazny dla poczÄ…tkujÄ…cych README do szybkiego startu na Windows
---


## ğŸ“„ Licencja

Ten projekt jest wydany na licencji Apache 2.0.

> **Uwaga:**
> Kod, modele i zbiÃ³r danych sÄ… **wyÅ‚Ä…cznie do celÃ³w badawczych i edukacyjnych**.
> **UÅ¼ycie komercyjne NIE jest dozwolone.**
> W sprawie licencji komercyjnej prosimy o kontakt z autorami.

**ğŸ“¦ Komponenty zewnÄ™trzne**

* **Stable Audio Open VAE** (by Stability AI):
  To repozytorium zawiera dostrojony VAE z [Stable Audio Open](https://huggingface.co/stabilityai/stable-audio-open-1.0/), licencjonowany na [Stability AI Community License](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/./third_party/LICENSE_StabilityAI.md).
  **UÅ¼ycie komercyjne i redystrybucja wymagajÄ… wczeÅ›niejszej zgody Stability AI.**

* ğŸ“˜ **Wszystkie pozostaÅ‚e kody i modele** sÄ… wydawane na licencji Apache 2.0.

---

## PodziÄ™kowania

Serdeczne podziÄ™kowania dla:

* **stable-audio-tools** (by Stability AI):
Za udostÄ™pnienie Å‚atwego w uÅ¼yciu frameworka do generowania audio oraz moduÅ‚u VAE i wag.
* **MMAudio**:
  Za implementacjÄ™ backboneâ€™u MM-DiT w dziedzinie audio.

---

## ğŸ“– Cytowanie

JeÅ›li ThinkSound okazaÅ‚ siÄ™ przydatny w Twoich badaniach lub pracy, prosimy o cytowanie naszej pracy:



```bibtex
@misc{liu2025thinksoundchainofthoughtreasoningmultimodal,
      title={ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language Models for Audio Generation and Editing}, 
      author={Huadai Liu and Jialei Wang and Kaicheng Luo and Wen Wang and Qian Chen and Zhou Zhao and Wei Xue},
      year={2025},
      eprint={2506.21448},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/2506.21448}, 
}
```
---

## ğŸ“¬ Kontakt

âœ¨ ZachÄ™camy do [zgÅ‚oszenia problemu](https://github.com/liuhuadai/ThinkSound/issues) lub kontaktu mailowego ([liuhuadai@zju.edu.cn](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/mailto:liuhuadai@zju.edu.cn)), jeÅ›li masz jakiekolwiek pytania lub sugestie!


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-17

---