<h1 align="center">ThinkSound</h1>

<p align="center">
  üåê
  <a href="https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=en">English</a> |
  <a href="https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a> |
  <a href="https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=zh-TW">ÁπÅÈ´î‰∏≠Êñá</a> |
  <a href="https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=es">Espa√±ol</a> |
  <a href="https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=fr">Fran√ßais</a> |
  <a href="https://openaitx.github.io/view.html?user=FunAudioLLM&project=ThinkSound&lang=ja">Êó•Êú¨Ë™û</a>
  
</p>
<p align="center">
  <img src="https://img.shields.io/badge/NeurIPS 2025-Main Conference-blue.svg" alt="NeurIPS 2025"/>
<p align="center">
  <a href="https://arxiv.org/pdf/2506.21448">
    <img src="https://img.shields.io/badge/arXiv-2506.21448-b31b1b.svg" alt="arXiv"/>
  </a>
  &nbsp;
  <a href="https://thinksound-project.github.io/">
    <img src="https://img.shields.io/badge/Online%20Demo-üåê-blue" alt="Online Demo"/>
  </a>
  &nbsp;
  <a href="https://huggingface.co/spaces/FunAudioLLM/ThinkSound">
    <img src="https://img.shields.io/badge/HuggingFace-Spaces-orange?logo=huggingface" alt="Hugging Face"/>
  </a>
  &nbsp;
  <a href="https://modelscope.cn/studios/iic/ThinkSound">
    <img src="https://img.shields.io/badge/ModelScope-Âú®Á∫ø‰ΩìÈ™å-green" alt="ModelScope"/>
  </a>
</p>

<p align="center">
  Se trovi utile questo progetto,<br>
  una stella ‚≠ê su GitHub sarebbe molto apprezzata!
</p>

---

**ThinkSound** √® un framework unificato per la generazione Any2Audio con flow matching guidato dal ragionamento Chain-of-Thought (CoT).

Implementazione PyTorch per la generazione e modifica multimodale di audio: genera o modifica audio da video, testo e audio, grazie al ragionamento passo-passo dei Modelli Multimodali di Linguaggio di Grandi Dimensioni (MLLMs).

![Teaser](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig1_teaser.png)
---

## üì∞ Novit√†
- **2025.09.19** &nbsp; üéâ ThinkSound √® stato accettato alla **Conferenza Principale NeurIPS 2025**!
- **2025.09.01** &nbsp; üî• Il nostro dataset AudioCoT √® ora open source e disponibile su [Hugging Face](https://huggingface.co/datasets/liuhuadai/AudioCoT)!
- **2025.07.17** &nbsp; üß† Fine-tuning abilitato: il codice per l‚Äôaddestramento e il fine-tuning √® ora pubblico, con istruzioni chiare per aiutarti a personalizzare ed estendere ThinkSound con i tuoi dati.
- **2025.07.15** &nbsp; üì¶ Installazione e utilizzo semplificati: dipendenze su PyPI per una configurazione semplice cross-platform; script `.bat` per Windows automatizzano la creazione dell‚Äôambiente e l‚Äôesecuzione degli script.
- **2025.07.08** &nbsp;¬† üîß Aggiornamento importante: il modello √® stato alleggerito e ottimizzato per l‚Äôuso di memoria e GPU, ora supporta la generazione audio ad alto rendimento su larga scala!
- **2025.07.01** &nbsp; üî•Demo online su [Hugging Face Spaces](https://huggingface.co/spaces/FunAudioLLM/ThinkSound) e [ModelScope](https://modelscope.cn/studios/iic/ThinkSound) per un‚Äôesperienza interattiva!
- **2025.07.01** &nbsp; üî•Rilasciati script di inferenza e interfaccia web;
- **2025.06** &nbsp; üî•[Paper ThinkSound](https://arxiv.org/pdf/2506.21448) pubblicato su arXiv!
- **2025.06** &nbsp; üî•[Demo Online](http://thinksound-project.github.io/) √® attiva - provala subito!

---


## üöÄ Caratteristiche

- **Any2Audio**: Genera audio da qualsiasi modalit√† ‚Äî video, testo, audio, o loro combinazioni.
- **Video-to-Audio SOTA**: Risultati all‚Äôavanguardia su molteplici benchmark V2A.
- **Ragionamento CoT-Driven**: Ragionamento Chain-of-Thought per generazione audio composizionale e controllabile tramite MLLMs.
- **Editing Interattivo Object-centric**: Raffina o modifica eventi sonori specifici cliccando su oggetti visivi o usando istruzioni testuali.
- **Framework Unificato**: Un modello base supporta generazione, modifica e flusso di lavoro interattivo.

---

## ‚ú® Panoramica del Metodo

ThinkSound suddivide la generazione e modifica audio in tre fasi interattive, tutte guidate dal ragionamento Chain-of-Thought (CoT) basato su MLLM:

1. **Generazione Foley:** Genera paesaggi sonori fondamentali, semanticamente e temporalmente allineati dal video.
2. **Raffinamento Object-Centric:** Raffina o aggiungi suoni per oggetti specificati dall‚Äôutente tramite click o regioni nel video.
3. **Editing Audio Mirato:** Modifica l‚Äôaudio generato usando istruzioni in linguaggio naturale di alto livello.

![ThinkSound Overview](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig3_model.png)
<!-- Un dataset su larga scala annotato CoT (**AudioCoT**) viene utilizzato per addestrare sia il modulo di ragionamento sia il modello base audio unificato.
![AudioCoT Pipeline](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig2_dataset.png) -->

---

## ‚ö° Avvio rapido

**Preparazione dell'ambiente:**
```bash
git clone https://github.com/liuhuadai/ThinkSound.git
cd ThinkSound
conda create -n thinksound python=3.10
conda activate thinksound
pip install thinksound
conda install -y -c conda-forge 'ffmpeg<7'
# Download pretrained weights https://huggingface.co/liuhuadai/ThinkSound to Directory ckpts/
# model weights can be also downloaded from https://www.modelscope.cn/models/iic/ThinkSound
git lfs install
git clone https://huggingface.co/liuhuadai/ThinkSound ckpts
# To improve inference and training speed, you may optionally install a FlashAttention backend compatible with your system and PyTorch version.
```
> ‚úÖ **Suggerimento per Windows:**  
> Gli utenti Windows possono semplicemente eseguire `setup_windows.bat` (o farci doppio clic) per creare automaticamente l'ambiente conda, installare tutte le dipendenze (incluso FFmpeg) e scaricare il modello pre-addestrato ‚Äî nessuna configurazione manuale richiesta.  
> Assicurati che `conda` e `git` siano installati e disponibili nel PATH di sistema prima di eseguire lo script.


### ‚ñ∂Ô∏è Esegui la Demo

#### **Linux/macOS**


```bash
chmod +x scripts/demo.sh
./scripts/demo.sh <path-to-your-demo-video> <title> <CoT description> [use-half]
```
#### **Windows**

In alternativa, puoi utilizzare lo script `.bat` fornito:


```bash
.\scripts\demo.bat <path-to-your-demo-video> <title> <CoT description> [use-half]
```
**Nota:**

* `<percorso-al-tuo-video-demo>`: Il percorso a un singolo video
* `[usa-half]` (opzionale): Aggiungi usa-half alla fine per abilitare l'estrazione delle caratteristiche in mezza precisione.

---

### üì¶ Inferenza in Batch

#### **Linux/macOS**


```bash
chmod +x scripts/eval_batch.sh
./scripts/eval_batch.sh <video_path> <csv_path> <save_path (optional)> [use-half]
```
#### **Windows**

Usa lo script `.bat` equivalente:


```bash
.\scripts\eval_batch.bat <video_path> <csv_path> <save_path (optional)> [use-half]
```
**Nota:**

* `<video_path>`: Percorso alla directory principale contenente tutti i video .mp4 da elaborare (tutti i video devono avere la stessa durata).
* `<csv_path>`: Un file CSV con i prompt di testo per ciascun video (vedi `demo_test.csv` per il formato).
* `<save_path>` (opzionale): Dove salvare l'audio generato. Il valore predefinito √® `results/features`.
* `[use-half]` (opzionale): Aggiungi use-half alla fine per abilitare l'estrazione delle feature in mezza precisione.

---


### Utilizzo dell'interfaccia web

Per un'esperienza interattiva, avvia l'interfaccia web Gradio:


```bash
python app.py
```


## üèãÔ∏è Allena il Modello

Vedi [`Training.md`](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/docs/Training.md)


---

## üìù TODO & Piani Futuri
* - [ ] Rilasciare un modello di base pi√π potente che copra pi√π domini per offrire una creazione di foley pi√π coinvolgente e immersiva
* - [ ] Aggiungere supporto per ulteriori modalit√† e compiti downstream
* - [ ] Rilasciare modelli di diverse dimensioni
* - [x] Open-source del dataset AudioCoT e pipeline automatizzata
* - [x] Rilasciare gli script di addestramento per i modelli ThinkSound
* - [x] Un README rapido per Windows adatto ai principianti
---


## üìÑ Licenza

Questo progetto √® rilasciato sotto la Licenza Apache 2.0.

> **Nota:**
> Il codice, i modelli e il dataset sono **solo per scopi di ricerca ed educativi**.
> **L'uso commerciale NON √® consentito.**
> Per la licenza commerciale, si prega di contattare gli autori.

**üì¶ Componenti di Terze Parti**

* **Stable Audio Open VAE** (di Stability AI):
  Questo repository include un VAE ottimizzato da [Stable Audio Open](https://huggingface.co/stabilityai/stable-audio-open-1.0/), concesso in licenza secondo la [Stability AI Community License](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/./third_party/LICENSE_StabilityAI.md).
  **L'uso commerciale e la redistribuzione richiedono il permesso preventivo di Stability AI.**

* üìò **Tutto il resto del codice e dei modelli** √® rilasciato sotto la Licenza Apache 2.0.

---

## Ringraziamenti

Molte grazie a:

* **stable-audio-tools** (di Stability AI):
Per aver fornito un framework facile da usare per la generazione audio, oltre al modulo VAE e ai relativi pesi.
* **MMAudio**:
  Per l'implementazione dell'architettura MM-DiT nel dominio audio.

---

## üìñ Citazione

Se trovi ThinkSound utile per la tua ricerca o lavoro, cita il nostro articolo:

```bibtex
@misc{liu2025thinksoundchainofthoughtreasoningmultimodal,
      title={ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language Models for Audio Generation and Editing}, 
      author={Huadai Liu and Jialei Wang and Kaicheng Luo and Wen Wang and Qian Chen and Zhou Zhao and Wei Xue},
      year={2025},
      eprint={2506.21448},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/2506.21448}, 
}
```

---

## üì¨ Contact


‚ú® Feel free to [open an issue](https://github.com/liuhuadai/ThinkSound/issues) or contact us via email ([liuhuadai@zju.edu.cn](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/mailto:liuhuadai@zju.edu.cn)) if you have any questions or suggestions!



---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-10-04

---