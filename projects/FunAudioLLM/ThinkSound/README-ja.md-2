{
  "id": 2,
  "origin": "## ğŸ“ TODO\n\n- â˜ Release training scripts for ThinkSound models\n- â˜ Open-source AudioCoT dataset and automated pipeline\n- â˜ Provide detailed documentation and API reference\n- â˜ Add support for additional modalities and downstream tasks\n\n---\n\n## ğŸ“„ License\n\nThis project is released under the [Apache 2.0 License](LICENSE).\n\n> **Note:**  \n> The code, models, and dataset are **for research and educational purposes only**.  \n> **Commercial use is NOT permitted.**\n>\n> For commercial licensing, please contact the authors.\n\n---\n\n## ğŸ“– Citation\n\nIf you find ThinkSound useful in your research or work, please cite our paper:\n\n```bibtex\n@misc{liu2025thinksoundchainofthoughtreasoningmultimodal,\n      title={ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language Models for Audio Generation and Editing}, \n      author={Huadai Liu and Jialei Wang and Kaicheng Luo and Wen Wang and Qian Chen and Zhou Zhao and Wei Xue},\n      year={2025},\n      eprint={2506.21448},\n      archivePrefix={arXiv},\n      primaryClass={eess.AS},\n      url={https://arxiv.org/abs/2506.21448}, \n}\n```\n\n---\n\n## ğŸ“¬ Contact\n\nâœ¨ Feel free to [open an issue](https://github.com/liuhuadai/ThinkSound/issues) or contact us via email ([liuhuadai@zju.edu.cn](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/mailto:liuhuadai@zju.edu.cn)) if you have any questions or suggestions!\n",
  "origin_sha": "NEFlwyN+fN3dUarHAXDiIoJazYSSuWRByH696fda+qA=",
  "translate": "## ğŸ“ TODO\n\n- â˜ ThinkSoundãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å…¬é–‹\n- â˜ AudioCoTãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŠã‚ˆã³è‡ªå‹•åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åŒ–\n- â˜ è©³ç´°ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨APIãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ã‚’æä¾›\n- â˜ è¿½åŠ ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ãŠã‚ˆã³ä¸‹æµã‚¿ã‚¹ã‚¯ã¸ã®å¯¾å¿œã‚’è¿½åŠ \n\n---\n\n## ğŸ“„ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹\n\næœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ [Apache 2.0 License](LICENSE) ã®ã‚‚ã¨ã§å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚\n\n> **æ³¨æ„:**  \n> ã‚³ãƒ¼ãƒ‰ã€ãƒ¢ãƒ‡ãƒ«ã€ãŠã‚ˆã³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯**ç ”ç©¶ãŠã‚ˆã³æ•™è‚²ç›®çš„ã«é™ã‚Šä½¿ç”¨å¯èƒ½**ã§ã™ã€‚  \n> **å•†ç”¨åˆ©ç”¨ã¯ç¦æ­¢ã•ã‚Œã¦ã„ã¾ã™ã€‚**\n>\n> å•†ç”¨ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã«ã¤ã„ã¦ã¯ã€è‘—è€…ã¾ã§ã”é€£çµ¡ãã ã•ã„ã€‚\n\n---\n\n## ğŸ“– å¼•ç”¨\n\nThinkSoundãŒã‚ãªãŸã®ç ”ç©¶ã‚„æ¥­å‹™ã«å½¹ç«‹ã£ãŸå ´åˆã¯ã€ä»¥ä¸‹ã®è«–æ–‡ã‚’å¼•ç”¨ã—ã¦ãã ã•ã„:\n\n```bibtex\n@misc{liu2025thinksoundchainofthoughtreasoningmultimodal,\n      title={ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language Models for Audio Generation and Editing}, \n      author={Huadai Liu and Jialei Wang and Kaicheng Luo and Wen Wang and Qian Chen and Zhou Zhao and Wei Xue},\n      year={2025},\n      eprint={2506.21448},\n      archivePrefix={arXiv},\n      primaryClass={eess.AS},\n      url={https://arxiv.org/abs/2506.21448}, \n}\n```\n\n---\n\n## ğŸ“¬ ãŠå•ã„åˆã‚ã›\n\nâœ¨ ã”è³ªå•ã‚„ã”ææ¡ˆãŒã‚ã‚Šã¾ã—ãŸã‚‰ã€[issueã‚’ã‚ªãƒ¼ãƒ—ãƒ³](https://github.com/liuhuadai/ThinkSound/issues)ã„ãŸã ãã‹ã€ãƒ¡ãƒ¼ãƒ«ï¼ˆ[liuhuadai@zju.edu.cn](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/mailto:liuhuadai@zju.edu.cn)ï¼‰ã§ãŠæ°—è»½ã«ã”é€£çµ¡ãã ã•ã„ï¼",
  "status": "ok"
}