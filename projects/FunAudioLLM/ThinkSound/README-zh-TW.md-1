{
  "id": 1,
  "origin": "# ğŸ¶ ThinkSound\n\n<p align=\"center\">\n  If you find this project useful, a star â­ on GitHub would be greatly appreciated!\n</p>\n\n<p align=\"center\">\n  <a href=\"https://arxiv.org/pdf/2506.21448\">\n    <img src=\"https://img.shields.io/badge/arXiv-2506.21448-b31b1b.svg\" alt=\"arXiv\"/>\n  </a>\n  &nbsp;\n  <a href=\"https://thinksound-project.github.io/\">\n    <img src=\"https://img.shields.io/badge/Online%20Demo-ğŸŒ-blue\" alt=\"Online Demo\"/>\n  </a>\n  &nbsp;\n  <a href=\"https://huggingface.co/spaces/FunAudioLLM/ThinkSound\">\n    <img src=\"https://img.shields.io/badge/HuggingFace-Spaces-orange?logo=huggingface\" alt=\"Hugging Face\"/>\n  </a>\n  &nbsp;\n  <a href=\"https://modelscope.cn/studios/iic/ThinkSound\">\n    <img src=\"https://img.shields.io/badge/ModelScope-åœ¨çº¿ä½“éªŒ-green\" alt=\"ModelScope\"/>\n  </a>\n</p>\n\n---\n\n**ThinkSound** is a unified Any2Audio generation framework with flow matching guided by Chain-of-Thought (CoT) reasoning.\n\nPyTorch implementation for multimodal audio generation and editing: generate or edit audio from video, text, and audio, powered by step-by-step reasoning from Multimodal Large Language Models (MLLMs).\n\n![Teaser](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig1_teaser.png)\n---\n\n## ğŸ“° News\n- **2025.07** &nbsp; ğŸ”¥Online demo on [Hugging Face Spaces](https://huggingface.co/spaces/FunAudioLLM/ThinkSound) and [ModelScope](https://modelscope.cn/studios/iic/ThinkSound) for interactive experience!\n- **2025.07** &nbsp; ğŸ”¥Released inference scripts and web interface; \n- **2025.06** &nbsp; ğŸ”¥[ThinkSound paper](https://arxiv.org/pdf/2506.21448) released on arXiv!\n- **2025.06** &nbsp; ğŸ”¥[Online Demo](http://thinksound-project.github.io/) is live - try it now!\n\n---\n\n## ğŸš€ Features\n\n- **Any2Audio**: Generate audio from arbitrary modalities â€” video, text, audio, or their combinations.\n- **Video-to-Audio SOTA**: Achieves state-of-the-art results on multiple V2A benchmarks.\n- **CoT-Driven Reasoning**: Chain-of-Thought reasoning for compositional and controllable audio generation via MLLMs.\n- **Interactive Object-centric Editing**: Refine or edit specific sound events by clicking on visual objects or using text instructions.\n- **Unified Framework**: One foundation model supports generation, editing, and interactive workflow.\n\n---\n\n## âœ¨ Method Overview\n\nThinkSound decomposes audio generation and editing into three interactive stages, all guided by MLLM-based Chain-of-Thought (CoT) reasoning:\n\n1. **Foley Generation:** Generate foundational, semantically and temporally aligned soundscapes from video.\n2. **Object-Centric Refinement:** Refine or add sounds for user-specified objects via clicks or regions in the video.\n3. **Targeted Audio Editing:** Modify generated audio using high-level natural language instructions.\n\n![ThinkSound Overview](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig3_model.png)\n<!-- A large-scale CoT-annotated dataset (**AudioCoT**) is used to train both the reasoning module and the unified audio foundation model.\n![AudioCoT Pipeline](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig2_dataset.png) -->\n\n---\n\n## âš¡ Quick Start\n\n**Environment Preparation:**\n```bash\ngit clone https://github.com/liuhuadai/ThinkSound.git\ncd ThinkSound\npip install -r requirements.txt\nconda install -y -c conda-forge 'ffmpeg<7'\n# Download pretrained weights https://huggingface.co/liuhuadai/ThinkSound to Directory ckpts/\n# model weights can be also downloaded from https://www.modelscope.cn/models/iic/ThinkSound\ngit lfs install\ngit clone https://huggingface.co/liuhuadai/ThinkSound ckpts\n```\n\n**Make it executable**\n```bash\nchmod +x scripts/demo.sh\n```\n\n**Run the script**\n```bash\n./scripts/demo.sh <video_path> <caption> <CoT description>\n```\n\n\n### Web Interface Usage\n\nFor an interactive experience, launch the Gradio web interface:\n\n```bash\npython app.py\n```\n\n---\n",
  "origin_sha": "vYi4X/38TyDFihDOfSBGZIgKnu2bd0a73Iz5+m+7pd0=",
  "translate": "# ğŸ¶ ThinkSound\n\n<p align=\"center\">\n  å¦‚æœä½ è¦ºå¾—æœ¬å°ˆæ¡ˆæœ‰ç”¨ï¼Œè«‹åœ¨ GitHub çµ¦æˆ‘å€‘ä¸€é¡†æ˜Ÿ â­ï¼Œéå¸¸æ„Ÿè¬ï¼\n</p>\n\n<p align=\"center\">\n  <a href=\"https://arxiv.org/pdf/2506.21448\">\n    <img src=\"https://img.shields.io/badge/arXiv-2506.21448-b31b1b.svg\" alt=\"arXiv\"/>\n  </a>\n  &nbsp;\n  <a href=\"https://thinksound-project.github.io/\">\n    <img src=\"https://img.shields.io/badge/Online%20Demo-ğŸŒ-blue\" alt=\"Online Demo\"/>\n  </a>\n  &nbsp;\n  <a href=\"https://huggingface.co/spaces/FunAudioLLM/ThinkSound\">\n    <img src=\"https://img.shields.io/badge/HuggingFace-Spaces-orange?logo=huggingface\" alt=\"Hugging Face\"/>\n  </a>\n  &nbsp;\n  <a href=\"https://modelscope.cn/studios/iic/ThinkSound\">\n    <img src=\"https://img.shields.io/badge/ModelScope-åœ¨çº¿ä½“éªŒ-green\" alt=\"ModelScope\"/>\n  </a>\n</p>\n\n---\n\n**ThinkSound** æ˜¯ä¸€å€‹çµ±ä¸€çš„ Any2Audio ç”Ÿæˆæ¡†æ¶ï¼Œæ¡ç”¨ Chain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†æŒ‡å°çš„ Flow Matchingã€‚\n\nåŸºæ–¼ PyTorch çš„å¤šæ¨¡æ…‹éŸ³è¨Šç”Ÿæˆèˆ‡ç·¨è¼¯å¯¦ç¾ï¼šå¯å¾å½±ç‰‡ã€æ–‡å­—ã€éŸ³è¨Šç”Ÿæˆæˆ–ç·¨è¼¯éŸ³è¨Šï¼Œä¸¦ç”±å¤šæ¨¡æ…‹å¤§å‹èªè¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é€æ­¥æ¨ç†é©…å‹•ã€‚\n\n![Teaser](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig1_teaser.png)\n---\n\n## ğŸ“° æœ€æ–°æ¶ˆæ¯\n- **2025.07** &nbsp; ğŸ”¥[Hugging Face Spaces](https://huggingface.co/spaces/FunAudioLLM/ThinkSound) åŠ [ModelScope](https://modelscope.cn/studios/iic/ThinkSound) ä¸Šç·šäº’å‹•é«”é©—ï¼\n- **2025.07** &nbsp; ğŸ”¥æ¨ç†è…³æœ¬èˆ‡ç¶²é ä»‹é¢é–‹æºç™¼å¸ƒï¼›\n- **2025.06** &nbsp; ğŸ”¥[ThinkSound è«–æ–‡](https://arxiv.org/pdf/2506.21448) ç™¼ä½ˆæ–¼ arXivï¼\n- **2025.06** &nbsp; ğŸ”¥[ç·šä¸Š Demo](http://thinksound-project.github.io/) å·²ä¸Šç·š - æ­¡è¿é«”é©—ï¼\n\n---\n\n## ğŸš€ ç‰¹è‰²\n\n- **Any2Audio**ï¼šå¯å¾ä»»æ„æ¨¡æ…‹ï¼ˆå½±ç‰‡ã€æ–‡å­—ã€éŸ³è¨Šæˆ–å…¶çµ„åˆï¼‰ç”ŸæˆéŸ³è¨Šã€‚\n- **å½±ç‰‡è½‰éŸ³è¨Š SOTA**ï¼šåœ¨å¤šå€‹ V2A åŸºæº–ä¸Šé”åˆ°æœ€å…ˆé€²è¡¨ç¾ã€‚\n- **CoT é©…å‹•æ¨ç†**ï¼šé€éå¤šæ¨¡æ…‹å¤§å‹èªè¨€æ¨¡å‹å¯¦ç¾çµ„åˆå¼èˆ‡å¯æ§éŸ³è¨Šç”Ÿæˆçš„ Chain-of-Thought æ¨ç†ã€‚\n- **äº’å‹•å¼ç‰©ä»¶å°å‘ç·¨è¼¯**ï¼šå¯é€éé»æ“Šè¦–è¦ºç‰©ä»¶æˆ–æ–‡å­—æŒ‡ä»¤ç´°ç·»ç·¨è¼¯æˆ–å®Œå–„ç‰¹å®šè²éŸ³äº‹ä»¶ã€‚\n- **çµ±ä¸€æ¡†æ¶**ï¼šä¸€å€‹åŸºç¤æ¨¡å‹å³æ”¯æ´ç”Ÿæˆã€ç·¨è¼¯èˆ‡äº’å‹•å¼æµç¨‹ã€‚\n\n---\n\n## âœ¨ æ–¹æ³•æ¦‚è¦½\n\nThinkSound å°‡éŸ³è¨Šç”Ÿæˆèˆ‡ç·¨è¼¯åˆ†ç‚ºä¸‰å€‹äº’å‹•éšæ®µï¼Œçš†ç”±åŸºæ–¼ MLLM çš„ Chain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†æŒ‡å°ï¼š\n\n1. **Foley ç”Ÿæˆï¼š** å¾å½±ç‰‡ç”Ÿæˆèªæ„èˆ‡æ™‚é–“å°é½Šçš„åŸºç¤éŸ³æ™¯ã€‚\n2. **ç‰©ä»¶å°å‘ç´°ç·»åŒ–ï¼š** é€éé»é¸æˆ–åœˆé¸å½±ç‰‡ä¸­æŒ‡å®šç‰©ä»¶ï¼Œç´°ç·»åŒ–æˆ–æ–°å¢è²éŸ³ã€‚\n3. **ç›®æ¨™éŸ³è¨Šç·¨è¼¯ï¼š** ä½¿ç”¨é«˜éšè‡ªç„¶èªè¨€æŒ‡ä»¤ä¿®æ”¹ç”ŸæˆéŸ³è¨Šã€‚\n\n![ThinkSound Overview](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig3_model.png)\n<!-- ä¸€å€‹å¤§è¦æ¨¡ CoT è¨»é‡‹è³‡æ–™é›†ï¼ˆ**AudioCoT**ï¼‰ç”¨æ–¼è¨“ç·´æ¨ç†æ¨¡çµ„èˆ‡çµ±ä¸€éŸ³è¨ŠåŸºç¤æ¨¡å‹ã€‚\n![AudioCoT Pipeline](https://raw.githubusercontent.com/FunAudioLLM/ThinkSound/master/assets/figs/fig2_dataset.png) -->\n\n---\n\n## âš¡ å¿«é€Ÿé–‹å§‹\n\n**ç’°å¢ƒæº–å‚™ï¼š**\n```bash\ngit clone https://github.com/liuhuadai/ThinkSound.git\ncd ThinkSound\npip install -r requirements.txt\nconda install -y -c conda-forge 'ffmpeg<7'\n# ä¸‹è¼‰é è¨“ç·´æ¬Šé‡ https://huggingface.co/liuhuadai/ThinkSound è‡³ç›®éŒ„ ckpts/\n# æ¨¡å‹æ¬Šé‡ä¹Ÿå¯å¾ https://www.modelscope.cn/models/iic/ThinkSound ä¸‹è¼‰\ngit lfs install\ngit clone https://huggingface.co/liuhuadai/ThinkSound ckpts\n```\n\n**è¨­ç‚ºå¯åŸ·è¡Œ**\n```bash\nchmod +x scripts/demo.sh\n```\n\n**åŸ·è¡Œè…³æœ¬**\n```bash\n./scripts/demo.sh <video_path> <caption> <CoT description>\n```\n\n\n### ç¶²é ä»‹é¢ä½¿ç”¨\n\nå¦‚éœ€äº’å‹•é«”é©—ï¼Œå•Ÿå‹• Gradio ç¶²é ä»‹é¢ï¼š\n\n```bash\npython app.py\n```\n\n---",
  "status": "ok"
}