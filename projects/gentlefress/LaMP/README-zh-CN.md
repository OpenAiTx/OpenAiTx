# :bulb: LaMPï¼šç”¨äºåŠ¨ä½œç”Ÿæˆã€æ£€ç´¢å’Œæè¿°çš„è¯­è¨€-åŠ¨ä½œé¢„è®­ç»ƒï¼ˆICLR 2025ï¼‰
### [[é¡¹ç›®é¡µé¢]](https://aigc3d.github.io/LaMP/) [[è®ºæ–‡]](https://arxiv.org/abs/2410.07093)
![teaser_image](https://github.com/gentlefress/LaMP/blob/main/teaser.png)

å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„ä»£ç æˆ–è®ºæ–‡æœ‰å¸®åŠ©ï¼Œè¯·è€ƒè™‘ä¸ºæˆ‘ä»¬çš„ä»“åº“ç‚¹èµå¹¶å¼•ç”¨ï¼š
```
@article{li2024lamp,
  title={LaMP: Language-Motion Pretraining for Motion Generation, Retrieval, and Captioning},
  author={Li, Zhe and Yuan, Weihao and He, Yisheng and Qiu, Lingteng and Zhu, Shenhao and Gu, Xiaodong and Shen, Weichao and Dong, Yuan and Dong, Zilong and Yang, Laurence T},
  journal={arXiv preprint arXiv:2410.07093},
  year={2024}
}
```
<translate-content>
## :postbox: æ–°é—»
ğŸ“¢ **2025-01-22** --- ğŸ”¥ğŸ”¥ğŸ”¥ ç¥è´ºï¼LaMPè¢«ICLR 2025å½•ç”¨ã€‚

ğŸ“¢ **2025-4-28** --- å‘å¸ƒLaMPçš„ä»£ç å’Œæ¨¡å‹ã€‚åŒ…æ‹¬è®­ç»ƒ/è¯„ä¼°/ç”Ÿæˆè„šæœ¬ã€‚

ğŸ“¢ **2025-4-28** --- åˆå§‹åŒ–ç½‘é¡µå’Œgité¡¹ç›®ã€‚  


## :1st_place_medal: å‡†å¤‡å°±ç»ª

<details>
  
### 1. Conda ç¯å¢ƒ</translate-content>
```
conda env create -f environment.yml
conda activate lamp
pip install git+https://github.com/openai/CLIP.git
```
æˆ‘ä»¬åœ¨ Python 3.9.12 å’Œ PyTorch 1.12.1 ä¸Šæµ‹è¯•æˆ‘ä»¬çš„ä»£ç 

### 2. æ¨¡å‹ä¸ä¾èµ–

#### ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹
```
bash prepare/download_models.sh
```
#### ä¸‹è½½è¯„ä¼°æ¨¡å‹å’Œæ‰‹å¥—  
ä»…ä¾›è¯„ä¼°ä½¿ç”¨ã€‚

```
bash prepare/download_evaluator.sh
bash prepare/download_glove.sh
```
#### ï¼ˆå¯é€‰ï¼‰æ‰‹åŠ¨ä¸‹è½½  
##### VQVAE é¢„è®­ç»ƒæƒé‡ï¼š  
https://virutalbuy-public.oss-cn-hangzhou.aliyuncs.com/share/aigc3d/lamp/vq.tar  
##### LaMP é¢„è®­ç»ƒæƒé‡ï¼š  
HumanML3D: https://virutalbuy-public.oss-cn-hangzhou.aliyuncs.com/share/aigc3d/lamp/h3d-qformer.tar  

KIT-ML: https://virutalbuy-public.oss-cn-hangzhou.aliyuncs.com/share/aigc3d/lamp/kit-qformer.tar  
##### LaMP-T2M é¢„è®­ç»ƒæƒé‡ï¼š  
https://virutalbuy-public.oss-cn-hangzhou.aliyuncs.com/share/aigc3d/lamp/t2m.tar  
##### M2T-LaMP é¢„è®­ç»ƒæƒé‡ï¼š  
https://virutalbuy-public.oss-cn-hangzhou.aliyuncs.com/share/aigc3d/lamp/m2t.pth  
### 3. è·å–æ•°æ®  

ä½ æœ‰ä¸¤ä¸ªé€‰æ‹©ï¼š  
* **è·³è¿‡è·å–æ•°æ®**ï¼Œå¦‚æœä½ åªæƒ³ä½¿ç”¨*è‡ªå·±çš„*æè¿°ç”ŸæˆåŠ¨ä½œã€‚  
* **è·å–å®Œæ•´æ•°æ®**ï¼Œå¦‚æœä½ æƒ³*é‡æ–°è®­ç»ƒ*å’Œ*è¯„ä¼°*æ¨¡å‹ã€‚  

**ï¼ˆaï¼‰å®Œæ•´æ•°æ®ï¼ˆæ–‡æœ¬ + åŠ¨ä½œï¼‰**  

**HumanML3D** - æŒ‰ç…§ [HumanML3D](https://github.com/EricGuo5513/HumanML3D.git) ä¸­çš„è¯´æ˜æ“ä½œï¼Œç„¶åå°†å¾—åˆ°çš„æ•°æ®é›†å¤åˆ¶åˆ°æˆ‘ä»¬çš„ä»“åº“ï¼š


```
cp -r ../HumanML3D/HumanML3D ./dataset/HumanML3D
```
**KIT**-ä» [HumanML3D](https://github.com/EricGuo5513/HumanML3D.git) ä¸‹è½½ï¼Œç„¶åå°†ç»“æœæ”¾ç½®åœ¨ `./dataset/KIT-ML`

#### 

</details>

## :fire: æ¼”ç¤º
<details>

### (a) ä»å•ä¸ªæç¤ºç”Ÿæˆ</details>
```
python gen_t2m.py --gpu_id 1 --ext exp1 --text_prompt "A person is running on a treadmill."
```
### (b) ä»æç¤ºæ–‡ä»¶ç”Ÿæˆ  
æç¤ºæ–‡ä»¶çš„ç¤ºä¾‹è§ `./assets/text_prompt.txt`ã€‚è¯·éµå¾ªæ¯è¡Œ `<æ–‡æœ¬æè¿°>#<åŠ¨ä½œé•¿åº¦>` çš„æ ¼å¼ã€‚åŠ¨ä½œé•¿åº¦è¡¨ç¤ºå§¿åŠ¿æ•°é‡ï¼Œå¿…é¡»æ˜¯æ•´æ•°ä¸”å°†å››èˆäº”å…¥ä¸º4çš„å€æ•°ã€‚åœ¨æˆ‘ä»¬çš„å·¥ä½œä¸­ï¼ŒåŠ¨ä½œä»¥20å¸§æ¯ç§’ä¸ºå•ä½ã€‚  
å¦‚æœä½ å†™ `<æ–‡æœ¬æè¿°>#NA`ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å°†è‡ªåŠ¨ç¡®å®šé•¿åº¦ã€‚æ³¨æ„ï¼Œä¸€æ—¦æœ‰**ä¸€ä¸ª**NAï¼Œå…¶ä»–çš„ä¹Ÿå°†è‡ªåŠ¨å˜ä¸º**NA**ã€‚  


```
python gen_t2m.py --gpu_id 1 --ext exp2 --text_path ./assets/text_prompt.txt
```
æ‚¨å¯èƒ½æ„Ÿå…´è¶£çš„å‡ ä¸ªå‚æ•°ï¼š
* `--repeat_times`ï¼šç”Ÿæˆçš„å¤åˆ¶æ¬¡æ•°ï¼Œé»˜è®¤å€¼ä¸º `1`ã€‚
* `--motion_length`ï¼šæŒ‡å®šç”Ÿæˆçš„å§¿åŠ¿æ•°é‡ï¼Œä»…é€‚ç”¨äºï¼ˆaï¼‰ã€‚

è¾“å‡ºæ–‡ä»¶å­˜å‚¨åœ¨æ–‡ä»¶å¤¹ `./generation/<ext>/` ä¸‹ã€‚å®ƒä»¬åŒ…æ‹¬ï¼š
* `numpy æ–‡ä»¶`ï¼šç”Ÿæˆçš„åŠ¨ä½œï¼Œå½¢çŠ¶ä¸º (nframe, 22, 3)ï¼Œå­˜æ”¾åœ¨å­æ–‡ä»¶å¤¹ `./joints` ä¸‹ã€‚
* `è§†é¢‘æ–‡ä»¶`ï¼šmp4 æ ¼å¼çš„çº¿æ¡åŠ¨ç”»ï¼Œå­˜æ”¾åœ¨å­æ–‡ä»¶å¤¹ `./animation` ä¸‹ã€‚
* `bvh æ–‡ä»¶`ï¼šç”ŸæˆåŠ¨ä½œçš„ bvh æ–‡ä»¶ï¼Œå­˜æ”¾åœ¨å­æ–‡ä»¶å¤¹ `./animation` ä¸‹ã€‚

æˆ‘ä»¬è¿˜å¯¹ç”Ÿæˆçš„åŠ¨ä½œåº”ç”¨äº†ç®€å•çš„è„šéƒ¨é€†å‘è¿åŠ¨å­¦ï¼ŒæŸ¥çœ‹å¸¦æœ‰åç¼€ `_ik` çš„æ–‡ä»¶ã€‚æœ‰æ—¶æ•ˆæœè‰¯å¥½ï¼Œæœ‰æ—¶ä¼šå¤±è´¥ã€‚
  
</details>

## :basketball_man: å¯è§†åŒ–
<details>

æ‰€æœ‰åŠ¨ç”»å‡åœ¨ Blender ä¸­æ‰‹åŠ¨æ¸²æŸ“ã€‚æˆ‘ä»¬ä½¿ç”¨æ¥è‡ª [mixamo](https://www.mixamo.com/#/) çš„è§’è‰²ã€‚æ‚¨éœ€è¦ä¸‹è½½å¸¦éª¨éª¼çš„ T å½¢å§¿åŠ¿è§’è‰²ã€‚

### é‡å®šå‘
å…³äºé‡å®šå‘ï¼Œæˆ‘ä»¬å‘ç° rokoko é€šå¸¸ä¼šå¯¼è‡´è„šéƒ¨è¾ƒå¤§è¯¯å·®ã€‚å¦ä¸€æ–¹é¢ï¼Œ[keemap.rig.transfer](https://github.com/nkeeline/Keemap-Blender-Rig-ReTargeting-Addon/releases) æ˜¾ç¤ºäº†æ›´ç²¾ç¡®çš„é‡å®šå‘æ•ˆæœã€‚æ‚¨å¯ä»¥è§‚çœ‹æ­¤å¤„çš„[æ•™ç¨‹](https://www.youtube.com/watch?v=EG-VCMkVpxg)ã€‚

æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š
* ä» GitHub ä¸‹è½½ keemap.rig.transferï¼Œå¹¶åœ¨ Blender ä¸­å®‰è£…ã€‚
* åœ¨ Blender ä¸­å¯¼å…¥åŠ¨ä½œæ–‡ä»¶ï¼ˆ.bvhï¼‰å’Œè§’è‰²æ–‡ä»¶ï¼ˆ.fbxï¼‰ã€‚
* `Shift + é€‰æ‹©` æºéª¨éª¼å’Œç›®æ ‡éª¨éª¼ã€‚ï¼ˆæ— éœ€å¤„äºé™æ­¢å§¿åŠ¿ï¼‰
* åˆ‡æ¢åˆ° `Pose Mode`ï¼Œç„¶åå±•å¼€è§†å›¾çª—å£å³ä¸Šè§’çš„ `KeeMapRig` å·¥å…·ã€‚
* å¯¹äº `bone mapping file`ï¼ŒæŒ‡å‘ `./assets/mapping.json`ï¼ˆå¦‚æœæ— æ•ˆï¼Œåˆ™ä¸º `mapping6.json`ï¼‰ï¼Œç‚¹å‡» `Read In Bone Mapping File`ã€‚è¯¥æ–‡ä»¶ç”±æˆ‘ä»¬æ‰‹åŠ¨åˆ¶ä½œï¼Œé€‚ç”¨äºå¤§å¤šæ•° mixamo è§’è‰²ã€‚
* ï¼ˆå¯é€‰ï¼‰æ‚¨å¯ä»¥æ‰‹åŠ¨å¡«å†™éª¨éª¼æ˜ å°„å¹¶è°ƒæ•´æ—‹è½¬ï¼Œä»¥é€‚åº”æ‚¨çš„è§’è‰²ã€‚ç‚¹å‡» `Save Bone Mapping File` å¯ä»¥å°†æ˜ å°„é…ç½®ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ï¼Œè·¯å¾„ç”±æ˜ å°„æ–‡ä»¶è·¯å¾„æŒ‡å®šã€‚
* è°ƒæ•´ `Number of Samples`ã€`Source Rig`ã€`Destination Rig Name`ã€‚
* ç‚¹å‡» `Transfer Animation from Source Destination`ï¼Œç­‰å¾…å‡ ç§’é’Ÿã€‚

æˆ‘ä»¬æœªå°è¯•å…¶ä»–é‡å®šå‘å·¥å…·ã€‚å¦‚æœæ‚¨å‘ç°å…¶ä»–æ›´æœ‰ç”¨çš„å·¥å…·ï¼Œæ¬¢è¿ç•™è¨€ã€‚

</details>

## :flashlight: è®­ç»ƒæ‚¨è‡ªå·±çš„æ¨¡å‹
<details>


**æ³¨æ„**ï¼šæ‚¨å¿…é¡»å…ˆè®­ç»ƒ VQ-VAEï¼Œ**ç„¶å**æ‰èƒ½è®­ç»ƒæ©ç /æ®‹å·®å˜æ¢å™¨ã€‚åä¸¤è€…å¯ä»¥åŒæ—¶è®­ç»ƒã€‚

### è®­ç»ƒ VQ-VAE
æ‚¨å¯èƒ½è¿˜éœ€è¦ä¸‹è½½è¯„ä¼°æ¨¡å‹ä»¥è¿è¡Œè„šæœ¬ã€‚


```
python train_vq.py --name vq_name --gpu_id 1 --dataset_name t2m --batch_size 256  --max_epoch 50 --quantize_dropout_prob 0.2 --gamma 0.05
```
<translate-content>
### è®­ç»ƒ LaMP</translate-content>
```
python train_lamp.py --name lamp_name --gpu_id 2 --dataset_name t2m --batch_size 64 --vq_name vq_name
```
### è®­ç»ƒæ©ç å˜æ¢å™¨

```
python train_t2m_transformer.py --name mtrans_name --gpu_id 2 --dataset_name t2m --batch_size 64 --vq_name vq_name
```
* `--dataset_name`ï¼šåŠ¨ä½œæ•°æ®é›†ï¼ŒHumanML3D ä½¿ç”¨ `t2m`ï¼ŒKIT-ML ä½¿ç”¨ `kit`ã€‚  
* `--name`ï¼šä¸ºä½ çš„æ¨¡å‹å‘½åã€‚æ¨¡å‹å°†ä¿å­˜åœ¨ `./checkpoints/<dataset_name>/<name>` ç›®å½•ä¸‹ã€‚
* `--gpu_id`ï¼šGPU ç¼–å·ã€‚
* `--batch_size`ï¼šæˆ‘ä»¬åœ¨ VQ è®­ç»ƒä¸­ä½¿ç”¨ `512`ã€‚å¯¹äºæ©ç /æ®‹å·®å˜æ¢å™¨ï¼Œåœ¨ HumanML3D ä¸Šä½¿ç”¨ `64`ï¼Œåœ¨ KIT-ML ä¸Šä½¿ç”¨ `16`ã€‚
* `--quantize_drop_prob`ï¼šé‡åŒ–ä¸¢å¼ƒæ¯”ä¾‹ï¼Œä½¿ç”¨å€¼ä¸º `0.2`ã€‚
* `--vq_name`ï¼šè®­ç»ƒæ©ç /æ®‹å·®å˜æ¢å™¨æ—¶ï¼Œéœ€è¦æŒ‡å®šç”¨äºåˆ†è¯çš„ VQ æ¨¡å‹åç§°ã€‚
* `--cond_drop_prob`ï¼šæ¡ä»¶ä¸¢å¼ƒæ¯”ä¾‹ï¼Œç”¨äºæ— åˆ†ç±»å™¨å¼•å¯¼ã€‚ä½¿ç”¨å€¼ä¸º `0.2`ã€‚

æ‰€æœ‰é¢„è®­ç»ƒæ¨¡å‹å’Œä¸­é—´ç»“æœå°†ä¿å­˜åœ¨è·¯å¾„ `./checkpoints/<dataset_name>/<name>` ä¸­ã€‚

### è®­ç»ƒ M2T

```
python train_m2t.py --exp-name M2T --num-layers 12 --batch-size 80 --embed-dim-gpt 1024 --nb-code 512 --n-head-gpt 16 --block-size 51 --ff-rate 4 --drop-out-rate 0.1 --resume-pth your_own_vqvae --vq-name VQVAE --out-dir ./output --total-iter 150000 --lr-scheduler 75000 --lr 0.00005 --dataname kit --down-t 2 --depth 3 --quantizer ema_reset --eval-iter 10000 --pkeep 0.5 --dilation-growth-rate 3 --vq-act relu
```
</details>

## :artist: è¯„ä¼°
<details>

### è¯„ä¼° VQ-VAE é‡å»ºï¼š
HumanML3D:</details>

```
python eval_t2m_vq.py --gpu_id 0 --name  --dataset_name t2m

```
KIT-ML:
```
python eval_t2m_vq.py --gpu_id 0 --name  --dataset_name kit
```
### è¯„ä¼° LaMP-T2Mï¼š  
HumanML3Dï¼š

```
python eval_t2m_trans_res.py --res_name mtrans_name --dataset_name t2m --name eval_name --gpu_id 1 --cond_scale 4 --time_steps 10 --ext evaluation
```
KIT-ML:
```
python eval_t2m_trans_res.py --res_name mtrans_name_k --dataset_name kit --name eval_name_k --gpu_id 0 --cond_scale 2 --time_steps 10 --ext evaluation
```
* `--res_name`ï¼š`æ®‹å·®å˜æ¢å™¨`çš„æ¨¡å‹åç§°ã€‚  
* `--name`ï¼š`æ©ç å˜æ¢å™¨`çš„æ¨¡å‹åç§°ã€‚  
* `--cond_scale`ï¼šæ— åˆ†ç±»å™¨å¼•å¯¼çš„æ¯”ä¾‹ã€‚  
* `--time_steps`ï¼šæ¨ç†çš„è¿­ä»£æ¬¡æ•°ã€‚  
* `--ext`ï¼šä¿å­˜è¯„ä¼°ç»“æœçš„æ–‡ä»¶åã€‚  
* `--which_epoch`ï¼š`æ©ç å˜æ¢å™¨`çš„æ£€æŸ¥ç‚¹åç§°ã€‚

æœ€ç»ˆè¯„ä¼°ç»“æœå°†ä¿å­˜åœ¨ `./checkpoints/<dataset_name>/<name>/eval/<ext>.log`

### è¯„ä¼° LaMP-M2Tï¼š

```
python M2T_eval.py --exp-name Test_M2T --num-layers 9 --batch-size 1 --embed-dim-gpt 1024 --nb-code 512 --n-head-gpt 16 --block-size 51 --ff-rate 4 --drop-out-rate 0.1 --resume-pth your_own_vqvae --vq-name VQVAE --out-dir ./output --total-iter 150000 --lr-scheduler 75000 --lr 0.0001 --dataname t2m --down-t 2 --depth 3 --quantizer ema_reset --eval-iter 10000 --pkeep 0.5 --dilation-growth-rate 3 --vq-act relu --resume-trans your_own_m2t
```
LaMP-BertScore æŒ‡æ ‡çš„è®¡ç®—é¦–å…ˆé€šè¿‡ LaMP-M2T ç”ŸæˆåˆæˆåŠ¨ä½œçš„æ–‡æœ¬æè¿°ï¼Œç„¶åè®¡ç®—ç”Ÿæˆçš„æè¿°ä¸çœŸå®æ–‡æœ¬ä¹‹é—´çš„ BertScoreã€‚

</details>

## è‡´è°¢

æˆ‘ä»¬è¡·å¿ƒæ„Ÿè°¢ä»¥ä¸‹å¼€æºé¡¹ç›®ï¼Œä»£ç åŸºäºè¿™äº›å·¥ä½œï¼š

[T2M-GPT](https://github.com/Mael-zys/T2M-GPT) å’Œ [MoMask](https://github.com/EricGuo5513/momask-codes/tree/main)ã€‚

## è®¸å¯åè®®
æœ¬ä»£ç éµå¾ª [MIT è®¸å¯åè®®](https://github.com/gentlefress/LaMP/blob/main/LICENSE.md) è¿›è¡Œåˆ†å‘ã€‚

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬çš„ä»£ç ä¾èµ–äºå…¶ä»–åº“ï¼ŒåŒ…æ‹¬ SMPLã€SMPL-Xã€PyTorch3Dï¼Œå¹¶ä½¿ç”¨å„è‡ªæ‹¥æœ‰ç‹¬ç«‹è®¸å¯åè®®çš„æ•°æ®é›†ï¼Œè¿™äº›åè®®ä¹Ÿå¿…é¡»éµå®ˆã€‚

### å…¶ä»–
å¦‚æœ‰è¿›ä¸€æ­¥é—®é¢˜ï¼Œè¯·è”ç³» keycharon0122@gmail.comã€‚



---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-19

---