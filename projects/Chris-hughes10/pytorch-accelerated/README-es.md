# pytorch-accelerated

`pytorch-accelerated` es una biblioteca ligera dise침ada para acelerar el proceso de entrenamiento de modelos PyTorch
proporcionando un bucle de entrenamiento m칤nimo pero extensible - encapsulado en un solo objeto `Trainer` -
que es lo suficientemente flexible para manejar la mayor칤a de los casos de uso, y capaz de utilizar diferentes opciones de hardware
sin necesidad de cambios en el c칩digo.

`pytorch-accelerated` ofrece un conjunto de caracter칤sticas simplificado, y pone un gran 칠nfasis en la **simplicidad** y **transparencia**,
para permitir a los usuarios entender exactamente qu칠 est치 ocurriendo bajo el cap칩, pero sin tener que escribir y mantener el c칩digo repetitivo ellos mismos.

Las caracter칤sticas clave son:
- Un bucle de entrenamiento simple y contenido, pero f치cilmente personalizable, que deber칤a funcionar desde el primer momento en casos sencillos;
 el comportamiento puede personalizarse usando herencia y/o callbacks.
- Maneja la asignaci칩n de dispositivos, precisi칩n mixta, integraci칩n con DeepSpeed, entrenamiento multi-GPU y distribuido sin cambios en el c칩digo.
- Utiliza componentes puros de PyTorch, sin modificaciones ni envoltorios adicionales, y se interoperabiliza f치cilmente
 con otras bibliotecas populares como [timm](https://github.com/rwightman/pytorch-image-models),
 [transformers](https://huggingface.co/transformers/) y [torchmetrics](https://torchmetrics.readthedocs.io/en/latest/).
- Una API peque침a y simplificada asegura que la curva de aprendizaje sea m칤nima para usuarios existentes de PyTorch.

Se ha realizado un esfuerzo significativo para asegurar que cada parte de la biblioteca - tanto componentes internos como externos - sea lo m치s clara y simple posible,
facilitando la personalizaci칩n, depuraci칩n y comprensi칩n exacta de lo que est치 pasando tras bambalinas en cada paso; la mayor parte del
comportamiento del entrenador est치 contenido en una sola clase.
En el esp칤ritu de Python, nada est치 oculto y todo es accesible.

`pytorch-accelerated` est치 orgullosamente y transparentemente construido sobre
[Hugging Face Accelerate](https://github.com/huggingface/accelerate), que es responsable del
movimiento de datos entre dispositivos y el lanzamiento de configuraciones de entrenamiento. Al personalizar el entrenador o iniciar
el entrenamiento, se anima a los usuarios a consultar la [documentaci칩n de Accelerate](https://huggingface.co/docs/accelerate/)
para entender todas las opciones disponibles; Accelerate provee funciones convenientes para operaciones como la recolecci칩n de tensores
y el recorte de gradientes, cuyo uso puede verse en la carpeta de
[ejemplos](https://github.com/Chris-hughes10/pytorch-accelerated/tree/main/examples) de `pytorch-accelerated`!

Para aprender m치s sobre las motivaciones detr치s de esta biblioteca, junto con una gu칤a detallada para comenzar, revisa [este art칤culo de blog](https://medium.com/@chris.p.hughes10/introducing-pytorch-accelerated-6ba99530608c?source=friends_link&sk=868c2d2ec5229fdea42877c0bf82b968).

## Instalaci칩n

`pytorch-accelerated` puede instalarse desde pip usando el siguiente comando:
```
pip install pytorch-accelerated
```
Para hacer el paquete lo m치s ligero posible, los paquetes necesarios para ejecutar los ejemplos no se incluyen por defecto. Para incluir estos paquetes, puede usar el siguiente comando:

```
pip install pytorch-accelerated[examples]
```

## Inicio r치pido

Para comenzar, simplemente importe y utilice el `Trainer` acelerado por pytorch, como se demuestra en el siguiente fragmento,
y luego inicie el entrenamiento utilizando el 
[CLI de accelerate](https://huggingface.co/docs/accelerate/quicktour.html#launching-your-distributed-script)
descrito a continuaci칩n.

```python
# examples/core/train_mnist.py
import os

from torch import nn, optim
from torch.utils.data import random_split
from torchvision import transforms
from torchvision.datasets import MNIST

from pytorch_accelerated import Trainer

class MNISTModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.main = nn.Sequential(
            nn.Linear(in_features=784, out_features=128),
            nn.ReLU(),
            nn.Linear(in_features=128, out_features=64),
            nn.ReLU(),
            nn.Linear(in_features=64, out_features=10),
        )

    def forward(self, input):
        return self.main(input.view(input.shape[0], -1))

def main():
    dataset = MNIST(os.getcwd(), download=True, transform=transforms.ToTensor())
    train_dataset, validation_dataset, test_dataset = random_split(dataset, [50000, 5000, 5000])
    model = MNISTModel()
    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
    loss_func = nn.CrossEntropyLoss()

    trainer = Trainer(
            model,
            loss_func=loss_func,
            optimizer=optimizer,
    )

    trainer.train(
        train_dataset=train_dataset,
        eval_dataset=validation_dataset,
        num_epochs=8,
        per_device_batch_size=32,
    )

    trainer.evaluate(
        dataset=test_dataset,
        per_device_batch_size=64,
    )
    
if __name__ == "__main__":
    main()
```
Para iniciar el entrenamiento usando la [CLI de accelerate](https://huggingface.co/docs/accelerate/quicktour.html#launching-your-distributed-script)
, en tu(s) m치quina(s), ejecuta:

` accelerate config --config_file accelerate_config.yaml`

y responde las preguntas que se te hagan. Esto generar치 un archivo de configuraci칩n que se utilizar치 para establecer correctamente las opciones predeterminadas al hacer

`accelerate launch --config_file accelerate_config.yaml train.py [--training-args]`

*Nota*: Usar la [CLI de accelerate](https://huggingface.co/docs/accelerate/quicktour.html#launching-your-distributed-script) es completamente opcional, el entrenamiento tambi칠n puede iniciarse de la manera habitual usando:

`python train.py` / `python -m torch.distributed ...`

dependiendo de la configuraci칩n de tu infraestructura, para usuarios que prefieran mantener un control m치s detallado 
sobre el comando de lanzamiento.

Ejemplos de entrenamientos m치s complejos pueden verse en la carpeta de ejemplos 
[aqu칤](https://github.com/Chris-hughes10/pytorch-accelerated/tree/main/examples). 

Alternativamente, si prefieres entender primero los conceptos b치sicos, esto puede encontrarse en la [documentaci칩n](https://pytorch-accelerated.readthedocs.io/en/latest/).

## Uso

### 쮸 qui칠n est치 dirigido pytorch-accelerated?

- Usuarios que est치n familiarizados con PyTorch pero desean evitar tener que escribir el c칩digo repetitivo com칰n del ciclo de entrenamiento
para centrarse en las partes interesantes del ciclo de entrenamiento.
- Usuarios que les gusta y se sienten c칩modos seleccionando y creando sus propios modelos, funciones de p칠rdida, optimizadores y conjuntos de datos.
- Usuarios que valoran un conjunto de caracter칤sticas simple y optimizado, donde el comportamiento es f치cil de depurar, entender y razonar.

### 쮺u치ndo no deber칤a usar pytorch-accelerated?

- Si buscas una soluci칩n completa, que abarque desde la carga de datos hasta la inferencia,
  que te ayude a seleccionar un modelo, optimizador o funci칩n de p칠rdida, probablemente te convenga m치s
  [fastai](https://github.com/fastai/fastai). `pytorch-accelerated` se enfoca 칰nicamente en el proceso de entrenamiento, dejando todas las dem치s
  responsabilidades al usuario.
- Si deseas escribir todo el ciclo de entrenamiento t칰 mismo, solo sin todos los problemas de gesti칩n de dispositivos, 
probablemente te convenga usar directamente [Accelerate](https://github.com/huggingface/accelerate). Aunque es
posible personalizar cada parte del `Trainer`, el ciclo de entrenamiento se divide fundamentalmente en una serie de 

diferentes m칠todos que tendr칤as que sobrescribir. Pero, antes de que te vayas, realmente es tan importante escribir esos bucles `for`  
como para justificar empezar desde cero *otra vez* 游땔.  
- Si est치s trabajando en un caso de uso personalizado, muy complejo, que no encaja con los patrones de los bucles de entrenamiento habituales  
y quieres exprimir hasta el 칰ltimo bit de rendimiento en tu hardware elegido, probablemente sea mejor que te quedes  
con PyTorch puro; cualquier API de alto nivel se convierte en una sobrecarga en casos altamente especializados!  


## Agradecimientos  

Muchos aspectos detr치s del dise침o y las caracter칤sticas de `pytorch-accelerated` fueron grandemente inspirados por una serie de excelentes  
bibliotecas y frameworks como [fastai](https://github.com/fastai/fastai), [timm](https://github.com/rwightman/pytorch-image-models),  
[PyTorch-lightning](https://github.com/PyTorchLightning/pytorch-lightning) y [Hugging Face Accelerate](https://github.com/huggingface/accelerate). Cada una de estas herramientas  
ha tenido un impacto enorme tanto en esta biblioteca como en la comunidad de aprendizaje autom치tico, 춰y su influencia no puede ser  
subestimada!  

`pytorch-accelerated` solo ha tomado inspiraci칩n de estas herramientas, y toda la funcionalidad contenida ha sido implementada  
desde cero de una manera que beneficia a esta biblioteca. Las 칰nicas excepciones a esto son algunos de los scripts en la  
carpeta [examples](https://github.com/Chris-hughes10/pytorch-accelerated/tree/main/examples)  
en los que se tomaron y modificaron recursos existentes para mostrar las caracter칤sticas de `pytorch-accelerated`;  
estos casos est치n claramente marcados, dando reconocimiento a los autores originales.  



---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2026-02-23

---