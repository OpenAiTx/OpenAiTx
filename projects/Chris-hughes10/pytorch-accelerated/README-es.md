
<div align="right">
  <details>
    <summary >üåê Idioma</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=zh-TW">ÁπÅÈ´î‰∏≠Êñá</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=hi">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=th">‡πÑ‡∏ó‡∏¢</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=fr">Fran√ßais</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=es">Espa√±ol</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=it">Italiano</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=ru">–†—É—Å—Å–∫–∏–π</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=pt">Portugu√™s</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=ar">ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=fa">ŸÅÿßÿ±ÿ≥€å</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=tr">T√ºrk√ße</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=vi">Ti·∫øng Vi·ªát</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=id">Bahasa Indonesia</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=as">‡¶Ö‡¶∏‡¶Æ‡ßÄ‡¶Ø‡¶º‡¶æ</
      </div>
    </div>
  </details>
</div>

# pytorch-accelerated

`pytorch-accelerated` es una librer√≠a ligera dise√±ada para acelerar el proceso de entrenamiento de modelos en PyTorch
proporcionando un ciclo de entrenamiento m√≠nimo, pero extensible, encapsulado en un √∫nico objeto `Trainer`
que es lo suficientemente flexible para manejar la mayor√≠a de los casos de uso, y capaz de utilizar diferentes opciones de hardware
sin necesidad de realizar cambios en el c√≥digo.
 
`pytorch-accelerated` ofrece un conjunto de caracter√≠sticas optimizado y pone un gran √©nfasis en la **simplicidad** y la **transparencia**,
para permitir a los usuarios entender exactamente lo que sucede bajo el cap√≥, ¬°pero sin tener que escribir y mantener el c√≥digo repetitivo ellos mismos!
   
Las caracter√≠sticas clave son:
- Un bucle de entrenamiento simple y contenido, pero f√°cilmente personalizable, que deber√≠a funcionar directamente en casos sencillos;
  el comportamiento puede personalizarse usando herencia y/o callbacks.
- Maneja la asignaci√≥n de dispositivos, precisi√≥n mixta, integraci√≥n con DeepSpeed, entrenamiento multi-GPU y distribuido sin cambios en el c√≥digo.
- Utiliza componentes puros de PyTorch, sin modificaciones o envoltorios adicionales, e interoperan f√°cilmente
  con otras bibliotecas populares como [timm](https://github.com/rwightman/pytorch-image-models),
  [transformers](https://huggingface.co/transformers/) y [torchmetrics](https://torchmetrics.readthedocs.io/en/latest/).
- Una API peque√±a y simplificada asegura una curva de aprendizaje m√≠nima para usuarios existentes de PyTorch.

Se ha hecho un esfuerzo significativo para garantizar que cada parte de la biblioteca, tanto los componentes internos como externos, sean lo m√°s claros y simples posible,
lo que facilita la personalizaci√≥n, depuraci√≥n y comprensi√≥n exacta de lo que sucede tras bambalinas en cada paso; ¬°la mayor parte del
comportamiento del entrenador est√° contenido en una sola clase!
En el esp√≠ritu de Python, nada est√° oculto y todo es accesible.

`pytorch-accelerated` est√° orgullosa y transparentemente construida sobre
[Hugging Face Accelerate](https://github.com/huggingface/accelerate), que se encarga del
movimiento de datos entre dispositivos y el lanzamiento de configuraciones de entrenamiento. Al personalizar el entrenador o lanzar
el entrenamiento, se recomienda a los usuarios consultar la [documentaci√≥n de Accelerate](https://huggingface.co/docs/accelerate/)
para entender todas las opciones disponibles; Accelerate proporciona funciones convenientes para operaciones como recopilaci√≥n de tensores
y recorte de gradientes, cuyo uso puede verse en la carpeta de
[ejemplos](https://github.com/Chris-hughes10/pytorch-accelerated/tree/main/examples) de `pytorch-accelerated`!

Para saber m√°s sobre las motivaciones detr√°s de esta biblioteca, junto con una gu√≠a detallada de inicio, consulta [esta entrada de blog](https://medium.com/@chris.p.hughes10/introducing-pytorch-accelerated-6ba99530608c?source=friends_link&sk=868c2d2ec5229fdea42877c0bf82b968).

## Instalaci√≥n

`pytorch-accelerated` puede instalarse desde pip usando el siguiente comando:
```
pip install pytorch-accelerated
```
Para hacer el paquete lo m√°s ligero posible, los paquetes necesarios para ejecutar los ejemplos no se incluyen por defecto. Para incluir estos paquetes, puede usar el siguiente comando:

```
pip install pytorch-accelerated[examples]
```

## Inicio r√°pido

Para comenzar, simplemente importe y utilice el `Trainer` acelerado por pytorch, como se demuestra en el siguiente fragmento,
y luego inicie el entrenamiento utilizando el 
[CLI de accelerate](https://huggingface.co/docs/accelerate/quicktour.html#launching-your-distributed-script)
descrito a continuaci√≥n.

```python
# examples/core/train_mnist.py
import os

from torch import nn, optim
from torch.utils.data import random_split
from torchvision import transforms
from torchvision.datasets import MNIST

from pytorch_accelerated import Trainer

class MNISTModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.main = nn.Sequential(
            nn.Linear(in_features=784, out_features=128),
            nn.ReLU(),
            nn.Linear(in_features=128, out_features=64),
            nn.ReLU(),
            nn.Linear(in_features=64, out_features=10),
        )

    def forward(self, input):
        return self.main(input.view(input.shape[0], -1))

def main():
    dataset = MNIST(os.getcwd(), download=True, transform=transforms.ToTensor())
    train_dataset, validation_dataset, test_dataset = random_split(dataset, [50000, 5000, 5000])
    model = MNISTModel()
    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
    loss_func = nn.CrossEntropyLoss()

    trainer = Trainer(
            model,
            loss_func=loss_func,
            optimizer=optimizer,
    )

    trainer.train(
        train_dataset=train_dataset,
        eval_dataset=validation_dataset,
        num_epochs=8,
        per_device_batch_size=32,
    )

    trainer.evaluate(
        dataset=test_dataset,
        per_device_batch_size=64,
    )
    
if __name__ == "__main__":
    main()
```
Para iniciar el entrenamiento usando la [CLI de accelerate](https://huggingface.co/docs/accelerate/quicktour.html#launching-your-distributed-script)
, en tu(s) m√°quina(s), ejecuta:

` accelerate config --config_file accelerate_config.yaml`

y responde las preguntas que se te hagan. Esto generar√° un archivo de configuraci√≥n que se utilizar√° para establecer correctamente las opciones predeterminadas al hacer

`accelerate launch --config_file accelerate_config.yaml train.py [--training-args]`

*Nota*: Usar la [CLI de accelerate](https://huggingface.co/docs/accelerate/quicktour.html#launching-your-distributed-script) es completamente opcional, el entrenamiento tambi√©n puede iniciarse de la manera habitual usando:

`python train.py` / `python -m torch.distributed ...`

dependiendo de la configuraci√≥n de tu infraestructura, para usuarios que prefieran mantener un control m√°s detallado 
sobre el comando de lanzamiento.

Ejemplos de entrenamientos m√°s complejos pueden verse en la carpeta de ejemplos 
[aqu√≠](https://github.com/Chris-hughes10/pytorch-accelerated/tree/main/examples). 

Alternativamente, si prefieres entender primero los conceptos b√°sicos, esto puede encontrarse en la [documentaci√≥n](https://pytorch-accelerated.readthedocs.io/en/latest/).

## Uso

### ¬øA qui√©n est√° dirigido pytorch-accelerated?

- Usuarios que est√°n familiarizados con PyTorch pero desean evitar tener que escribir el c√≥digo repetitivo com√∫n del ciclo de entrenamiento
para centrarse en las partes interesantes del ciclo de entrenamiento.
- Usuarios que les gusta y se sienten c√≥modos seleccionando y creando sus propios modelos, funciones de p√©rdida, optimizadores y conjuntos de datos.
- Usuarios que valoran un conjunto de caracter√≠sticas simple y optimizado, donde el comportamiento es f√°cil de depurar, entender y razonar.

### ¬øCu√°ndo no deber√≠a usar pytorch-accelerated?

- Si buscas una soluci√≥n completa, que abarque desde la carga de datos hasta la inferencia,
  que te ayude a seleccionar un modelo, optimizador o funci√≥n de p√©rdida, probablemente te convenga m√°s
  [fastai](https://github.com/fastai/fastai). `pytorch-accelerated` se enfoca √∫nicamente en el proceso de entrenamiento, dejando todas las dem√°s
  responsabilidades al usuario.
- Si deseas escribir todo el ciclo de entrenamiento t√∫ mismo, solo sin todos los problemas de gesti√≥n de dispositivos, 
probablemente te convenga usar directamente [Accelerate](https://github.com/huggingface/accelerate). Aunque es
posible personalizar cada parte del `Trainer`, el ciclo de entrenamiento se divide fundamentalmente en una serie de 

diferentes m√©todos que tendr√≠as que sobrescribir. Pero, antes de que te vayas, ¬ørealmente es tan importante escribir esos bucles `for`  
como para justificar empezar desde cero *otra vez* üòâ.  
- Si est√°s trabajando en un caso de uso personalizado, muy complejo, que no encaja con los patrones de los bucles de entrenamiento habituales  
y quieres exprimir hasta el √∫ltimo bit de rendimiento en tu hardware elegido, probablemente sea mejor que te quedes  
con PyTorch puro; cualquier API de alto nivel se convierte en una sobrecarga en casos altamente especializados!  


## Agradecimientos  

Muchos aspectos detr√°s del dise√±o y las caracter√≠sticas de `pytorch-accelerated` fueron grandemente inspirados por una serie de excelentes  
bibliotecas y frameworks como [fastai](https://github.com/fastai/fastai), [timm](https://github.com/rwightman/pytorch-image-models),  
[PyTorch-lightning](https://github.com/PyTorchLightning/pytorch-lightning) y [Hugging Face Accelerate](https://github.com/huggingface/accelerate). Cada una de estas herramientas  
ha tenido un impacto enorme tanto en esta biblioteca como en la comunidad de aprendizaje autom√°tico, ¬°y su influencia no puede ser  
subestimada!  

`pytorch-accelerated` solo ha tomado inspiraci√≥n de estas herramientas, y toda la funcionalidad contenida ha sido implementada  
desde cero de una manera que beneficia a esta biblioteca. Las √∫nicas excepciones a esto son algunos de los scripts en la  
carpeta [examples](https://github.com/Chris-hughes10/pytorch-accelerated/tree/main/examples)  
en los que se tomaron y modificaron recursos existentes para mostrar las caracter√≠sticas de `pytorch-accelerated`;  
estos casos est√°n claramente marcados, dando reconocimiento a los autores originales.  



---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2026-02-28

---