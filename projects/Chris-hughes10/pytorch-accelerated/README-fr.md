
<div align="right">
  <details>
    <summary >üåê Langue</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=en">Anglais</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=zh-TW">ÁπÅÈ´î‰∏≠Êñá</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=hi">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=th">‡πÑ‡∏ó‡∏¢</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=fr">Fran√ßais</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=es">Espa√±ol</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=it">Italiano</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=ru">–†—É—Å—Å–∫–∏–π</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=pt">Portugu√™s</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=ar">ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=fa">ŸÅÿßÿ±ÿ≥€å</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=tr">T√ºrk√ße</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=vi">Ti·∫øng Vi·ªát</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=id">Bahasa Indonesia</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=as">‡¶Ö‡¶∏‡¶Æ‡ßÄ‡¶Ø‡¶º‡¶æ</
      </div>
    </div>
  </details>
</div>

# pytorch-accelerated

`pytorch-accelerated` est une biblioth√®que l√©g√®re con√ßue pour acc√©l√©rer le processus d'entra√Ænement des mod√®les PyTorch
 en fournissant une boucle d'entra√Ænement minimale mais extensible - encapsul√©e dans un objet unique `Trainer`
qui est suffisamment flexible pour g√©rer la majorit√© des cas d'utilisation, et capable d'utiliser diff√©rents mat√©riels
 sans n√©cessiter de modification du code.
 
`pytorch-accelerated` offre un ensemble de fonctionnalit√©s simplifi√©es, et accorde une grande importance √† la **simplicit√©** et √† la **transparence**,
pour permettre aux utilisateurs de comprendre exactement ce qui se passe sous le capot, mais sans avoir √† √©crire et maintenir eux-m√™mes le code standard !
   
Les principales fonctionnalit√©s sont :
- Une boucle d'entra√Ænement simple et contenue, mais facilement personnalisable, qui devrait fonctionner d√®s la sortie de la bo√Æte dans les cas simples ;
 le comportement peut √™tre personnalis√© via l'h√©ritage et/ou des callbacks.
- G√®re le placement des appareils, la pr√©cision mixte, l'int√©gration DeepSpeed, l'entra√Ænement multi-GPU et distribu√© sans modification du code.
- Utilise des composants PyTorch purs, sans modifications ou wrappers suppl√©mentaires, et s'interop√®re facilement
 avec d'autres biblioth√®ques populaires telles que [timm](https://github.com/rwightman/pytorch-image-models), 
 [transformers](https://huggingface.co/transformers/) et [torchmetrics](https://torchmetrics.readthedocs.io/en/latest/).
- Une API petite et √©pur√©e garantit une courbe d'apprentissage minimale pour les utilisateurs PyTorch existants.

Un effort important a √©t√© fait pour garantir que chaque partie de la biblioth√®que - composants internes et externes - soit aussi claire et simple que possible,
rendant facile la personnalisation, le d√©bogage et la compr√©hension exacte de ce qui se passe en coulisses √† chaque √©tape ; la plupart du
comportement de l'entra√Æneur est contenu dans une seule classe !
Dans l'esprit de Python, rien n'est cach√© et tout est accessible.

`pytorch-accelerated` est fi√®rement et transparent construit sur 
[Hugging Face Accelerate](https://github.com/huggingface/accelerate), qui est responsable du
transfert des donn√©es entre appareils et du lancement des configurations d'entra√Ænement. Lors de la personnalisation de l'entra√Æneur, ou du lancement
de l'entra√Ænement, il est conseill√© aux utilisateurs de consulter la [documentation Accelerate](https://huggingface.co/docs/accelerate/)
pour comprendre toutes les options disponibles ; Accelerate fournit des fonctions pratiques pour des op√©rations telles que la collecte de tenseurs
et la coupure de gradient, dont l'utilisation peut √™tre vue dans le dossier 
[exemples](https://github.com/Chris-hughes10/pytorch-accelerated/tree/main/examples) de `pytorch-accelerated` !

Pour en savoir plus sur les motivations derri√®re cette biblioth√®que, ainsi qu'un guide d√©taill√© pour bien d√©marrer, consultez [cet article de blog](https://medium.com/@chris.p.hughes10/introducing-pytorch-accelerated-6ba99530608c?source=friends_link&sk=868c2d2ec5229fdea42877c0bf82b968).

## Installation

`pytorch-accelerated` peut √™tre install√© via pip en utilisant la commande suivante :
```
pip install pytorch-accelerated
```

Pour rendre le package aussi l√©ger que possible, les packages n√©cessaires pour ex√©cuter les exemples ne sont pas inclus par d√©faut. Pour inclure ces packages, vous pouvez utiliser la commande suivante :
```
pip install pytorch-accelerated[examples]
```

## D√©marrage rapide

Pour commencer, importez simplement et utilisez le `Trainer` acc√©l√©r√© par pytorch, comme d√©montr√© dans l'extrait suivant,
puis lancez l'entra√Ænement en utilisant la 
[CLI accelerate](https://huggingface.co/docs/accelerate/quicktour.html#launching-your-distributed-script)
d√©crite ci-dessous.

```python
# examples/core/train_mnist.py
import os

from torch import nn, optim
from torch.utils.data import random_split
from torchvision import transforms
from torchvision.datasets import MNIST

from pytorch_accelerated import Trainer

class MNISTModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.main = nn.Sequential(
            nn.Linear(in_features=784, out_features=128),
            nn.ReLU(),
            nn.Linear(in_features=128, out_features=64),
            nn.ReLU(),
            nn.Linear(in_features=64, out_features=10),
        )

    def forward(self, input):
        return self.main(input.view(input.shape[0], -1))

def main():
    dataset = MNIST(os.getcwd(), download=True, transform=transforms.ToTensor())
    train_dataset, validation_dataset, test_dataset = random_split(dataset, [50000, 5000, 5000])
    model = MNISTModel()
    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
    loss_func = nn.CrossEntropyLoss()

    trainer = Trainer(
            model,
            loss_func=loss_func,
            optimizer=optimizer,
    )

    trainer.train(
        train_dataset=train_dataset,
        eval_dataset=validation_dataset,
        num_epochs=8,
        per_device_batch_size=32,
    )

    trainer.evaluate(
        dataset=test_dataset,
        per_device_batch_size=64,
    )
    
if __name__ == "__main__":
    main()
```
Pour lancer l'entra√Ænement en utilisant le [CLI accelerate](https://huggingface.co/docs/accelerate/quicktour.html#launching-your-distributed-script)
, sur votre(s) machine(s), ex√©cutez :

` accelerate config --config_file accelerate_config.yaml`

et r√©pondez aux questions pos√©es. Cela g√©n√©rera un fichier de configuration qui sera utilis√© pour d√©finir correctement les options par d√©faut lors de l'ex√©cution de

`accelerate launch --config_file accelerate_config.yaml train.py [--training-args]`

*Note* : L'utilisation du [CLI accelerate](https://huggingface.co/docs/accelerate/quicktour.html#launching-your-distributed-script) est totalement optionnelle, l'entra√Ænement peut √©galement √™tre lanc√© de la mani√®re habituelle via :

`python train.py` / `python -m torch.distributed ...`

selon la configuration de votre infrastructure, pour les utilisateurs qui souhaitent garder un contr√¥le plus pr√©cis
sur la commande de lancement.

Des exemples d'entra√Ænement plus complexes sont disponibles dans le dossier examples
[ici](https://github.com/Chris-hughes10/pytorch-accelerated/tree/main/examples).

Alternativement, si vous pr√©f√©rez comprendre d'abord les concepts de base, vous pouvez les trouver dans la [documentation](https://pytorch-accelerated.readthedocs.io/en/latest/).

## Utilisation

### √Ä qui s'adresse pytorch-accelerated ?

- Aux utilisateurs familiers avec PyTorch mais qui souhaitent √©viter d'√©crire la structure standard de la boucle d'entra√Ænement
pour se concentrer sur les parties int√©ressantes de la boucle d'entra√Ænement.
- Aux utilisateurs qui aiment, et sont √† l'aise avec, la s√©lection et la cr√©ation de leurs propres mod√®les, fonctions de perte, optimiseurs et ensembles de donn√©es.
- Aux utilisateurs qui appr√©cient un ensemble de fonctionnalit√©s simple et √©pur√©, o√π le comportement est facile √† d√©boguer, comprendre et analyser !

### Quand ne devrais-je pas utiliser pytorch-accelerated ?

- Si vous cherchez une solution compl√®te, couvrant tout depuis le chargement des donn√©es jusqu'√† l'inf√©rence,
  qui vous aide √† s√©lectionner un mod√®le, un optimiseur ou une fonction de perte, vous seriez probablement mieux servi par
  [fastai](https://github.com/fastai/fastai). `pytorch-accelerated` se concentre uniquement sur le processus d'entra√Ænement, toutes les autres
  pr√©occupations restant de la responsabilit√© de l'utilisateur.
- Si vous souhaitez √©crire vous-m√™me toute la boucle d'entra√Ænement, simplement sans toutes les complications li√©es √† la gestion des appareils,
vous seriez probablement mieux servi en utilisant directement [Accelerate](https://github.com/huggingface/accelerate) ! Bien qu'il
soit possible de personnaliser chaque partie du `Trainer`, la boucle d'entra√Ænement est fondamentalement d√©coup√©e en un certain nombre de

diff√©rentes m√©thodes que vous devrez surcharger. Mais, avant de partir, est-ce que l'√©criture de ces boucles `for` est vraiment assez importante  
pour justifier de repartir de z√©ro *encore* üòâ.  
- Si vous travaillez sur un cas d'utilisation personnalis√©, tr√®s complexe, qui ne correspond pas aux sch√©mas habituels des boucles d'entra√Ænement  
et que vous souhaitez exploiter chaque dernier bit de performance sur votre mat√©riel choisi, il est probablement pr√©f√©rable de rester  
avec PyTorch vanilla ; toute API de haut niveau devient une surcharge dans les cas hautement sp√©cialis√©s !  


## Remerciements  

De nombreux aspects derri√®re la conception et les fonctionnalit√©s de `pytorch-accelerated` ont √©t√© grandement inspir√©s par un certain nombre d'excellentes  
biblioth√®ques et frameworks tels que [fastai](https://github.com/fastai/fastai), [timm](https://github.com/rwightman/pytorch-image-models),  
[PyTorch-lightning](https://github.com/PyTorchLightning/pytorch-lightning) et [Hugging Face Accelerate](https://github.com/huggingface/accelerate). Chacun de ces outils  
a eu un impact √©norme √† la fois sur cette biblioth√®que et sur la communaut√© de l'apprentissage automatique, et leur influence ne peut  
√™tre assez soulign√©e !  

`pytorch-accelerated` s'est inspir√© uniquement de ces outils, et toute la fonctionnalit√© contenue a √©t√© impl√©ment√©e  
√† partir de z√©ro de mani√®re √† b√©n√©ficier √† cette biblioth√®que. Les seules exceptions sont certains scripts dans le  
[dossier examples](https://github.com/Chris-hughes10/pytorch-accelerated/tree/main/examples)  
dans lesquels des ressources existantes ont √©t√© prises et modifi√©es afin de pr√©senter les fonctionnalit√©s de `pytorch-accelerated` ;  
ces cas sont clairement indiqu√©s, avec une reconnaissance donn√©e aux auteurs originaux.  



---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2026-02-28

---