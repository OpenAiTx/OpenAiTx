
<div align="right">
  <details>
    <summary >üåê Idioma</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=zh-TW">ÁπÅÈ´î‰∏≠Êñá</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=hi">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=th">‡πÑ‡∏ó‡∏¢</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=fr">Fran√ßais</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=es">Espa√±ol</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=it">Italiano</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=ru">–†—É—Å—Å–∫–∏–π</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=pt">Portugu√™s</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=ar">ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=fa">ŸÅÿßÿ±ÿ≥€å</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=tr">T√ºrk√ße</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=vi">Ti·∫øng Vi·ªát</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=id">Bahasa Indonesia</a>
        | <a href="https://openaitx.github.io/view.html?user=Chris-hughes10&project=pytorch-accelerated&lang=as">‡¶Ö‡¶∏‡¶Æ‡ßÄ‡¶Ø‡¶º‡¶æ</
      </div>
    </div>
  </details>
</div>

# pytorch-accelerated

`pytorch-accelerated` √© uma biblioteca leve projetada para acelerar o processo de treinamento de modelos PyTorch
 fornecendo um loop de treinamento minimalista, mas extens√≠vel - encapsulado em um √∫nico objeto `Trainer`
- que √© flex√≠vel o suficiente para lidar com a maioria dos casos de uso e capaz de utilizar diferentes op√ß√µes de hardware
 sem necessidade de altera√ß√µes no c√≥digo.
 
`pytorch-accelerated` oferece um conjunto de recursos simplificado e d√° grande √™nfase √† **simplicidade** e **transpar√™ncia**,
para permitir que os usu√°rios compreendam exatamente o que est√° acontecendo nos bastidores, mas sem precisar escrever e manter o c√≥digo repetitivo eles mesmos!

Os principais recursos s√£o:
- Um loop de treinamento simples e contido, mas facilmente personaliz√°vel, que deve funcionar imediatamente em casos simples;
 o comportamento pode ser personalizado usando heran√ßa e/ou callbacks.
- Gerencia aloca√ß√£o de dispositivo, precis√£o mista, integra√ß√£o com DeepSpeed, treinamento multi-GPU e distribu√≠do sem necessidade de altera√ß√µes no c√≥digo.
- Usa componentes puros do PyTorch, sem modifica√ß√µes ou wrappers adicionais, e interopera facilmente
 com outras bibliotecas populares como [timm](https://github.com/rwightman/pytorch-image-models),
 [transformers](https://huggingface.co/transformers/) e [torchmetrics](https://torchmetrics.readthedocs.io/en/latest/).
- Uma API pequena e enxuta garante uma curva de aprendizado m√≠nima para usu√°rios j√° familiarizados com PyTorch.

Um esfor√ßo significativo foi feito para garantir que cada parte da biblioteca ‚Äì tanto componentes internos quanto externos ‚Äì seja o mais clara e simples poss√≠vel,
tornando f√°cil personalizar, depurar e entender exatamente o que est√° acontecendo nos bastidores em cada etapa; a maior parte do
comportamento do treinador est√° contido em uma √∫nica classe!
No esp√≠rito do Python, nada √© escondido e tudo √© acess√≠vel.

O `pytorch-accelerated` √© constru√≠do de forma orgulhosa e transparente sobre
[Hugging Face Accelerate](https://github.com/huggingface/accelerate), que √© respons√°vel pelo
movimento dos dados entre dispositivos e inicializa√ß√£o das configura√ß√µes de treinamento. Ao customizar o treinador ou iniciar
o treinamento, os usu√°rios s√£o incentivados a consultar a [documenta√ß√£o do Accelerate](https://huggingface.co/docs/accelerate/)
para entender todas as op√ß√µes dispon√≠veis; o Accelerate fornece fun√ß√µes convenientes para opera√ß√µes como agrega√ß√£o de tensores
e clipping de gradiente, cujo uso pode ser visto na pasta de
[exemplos](https://github.com/Chris-hughes10/pytorch-accelerated/tree/main/examples) do `pytorch-accelerated`!

Para saber mais sobre as motiva√ß√µes por tr√°s desta biblioteca, junto com um guia detalhado de introdu√ß√£o, confira [este post no blog](https://medium.com/@chris.p.hughes10/introducing-pytorch-accelerated-6ba99530608c?source=friends_link&sk=868c2d2ec5229fdea42877c0bf82b968).

## Instala√ß√£o

`pytorch-accelerated` pode ser instalado via pip usando o seguinte comando:
```
pip install pytorch-accelerated
```

Para tornar o pacote o mais enxuto poss√≠vel, os pacotes necess√°rios para executar os exemplos n√£o s√£o inclu√≠dos por padr√£o. Para incluir esses pacotes, voc√™ pode usar o seguinte comando:
```
pip install pytorch-accelerated[examples]
```

## In√≠cio R√°pido

Para come√ßar, basta importar e usar o `Trainer` do pytorch-accelerated, como demonstrado no trecho a seguir,
e ent√£o iniciar o treinamento usando o 
[accelerate CLI](https://huggingface.co/docs/accelerate/quicktour.html#launching-your-distributed-script)
descrito abaixo.

```python
# examples/core/train_mnist.py
import os

from torch import nn, optim
from torch.utils.data import random_split
from torchvision import transforms
from torchvision.datasets import MNIST

from pytorch_accelerated import Trainer

class MNISTModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.main = nn.Sequential(
            nn.Linear(in_features=784, out_features=128),
            nn.ReLU(),
            nn.Linear(in_features=128, out_features=64),
            nn.ReLU(),
            nn.Linear(in_features=64, out_features=10),
        )

    def forward(self, input):
        return self.main(input.view(input.shape[0], -1))

def main():
    dataset = MNIST(os.getcwd(), download=True, transform=transforms.ToTensor())
    train_dataset, validation_dataset, test_dataset = random_split(dataset, [50000, 5000, 5000])
    model = MNISTModel()
    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
    loss_func = nn.CrossEntropyLoss()

    trainer = Trainer(
            model,
            loss_func=loss_func,
            optimizer=optimizer,
    )

    trainer.train(
        train_dataset=train_dataset,
        eval_dataset=validation_dataset,
        num_epochs=8,
        per_device_batch_size=32,
    )

    trainer.evaluate(
        dataset=test_dataset,
        per_device_batch_size=64,
    )
    
if __name__ == "__main__":
    main()
```
Para iniciar o treinamento usando o [CLI do accelerate](https://huggingface.co/docs/accelerate/quicktour.html#launching-your-distributed-script)
, na(s) sua(s) m√°quina(s), execute:

` accelerate config --config_file accelerate_config.yaml`

e responda √†s perguntas feitas. Isso ir√° gerar um arquivo de configura√ß√£o que ser√° usado para definir corretamente as op√ß√µes padr√£o ao executar

`accelerate launch --config_file accelerate_config.yaml train.py [--training-args]`

*Nota*: Utilizar o [CLI do accelerate](https://huggingface.co/docs/accelerate/quicktour.html#launching-your-distributed-script) √© completamente opcional, o treinamento tamb√©m pode ser iniciado da maneira usual utilizando:

`python train.py` / `python -m torch.distributed ...`

dependendo da configura√ß√£o da sua infraestrutura, para usu√°rios que desejam manter um controle mais detalhado
sobre o comando de inicializa√ß√£o.

Exemplos de treinamento mais complexos podem ser vistos na pasta de exemplos
[aqui](https://github.com/Chris-hughes10/pytorch-accelerated/tree/main/examples).

Alternativamente, se preferir entender os conceitos principais primeiro, isso pode ser encontrado na [documenta√ß√£o](https://pytorch-accelerated.readthedocs.io/en/latest/).

## Uso

### Para quem o pytorch-accelerated √© indicado?

- Usu√°rios que est√£o familiarizados com PyTorch, mas gostariam de evitar ter que escrever o c√≥digo repetitivo comum do loop de treinamento
para se concentrar nas partes interessantes do loop de treinamento.
- Usu√°rios que gostam, e se sentem confort√°veis, em selecionar e criar seus pr√≥prios modelos, fun√ß√µes de perda, otimizadores e conjuntos de dados.
- Usu√°rios que valorizam um conjunto de funcionalidades simples e direto, onde o comportamento √© f√°cil de depurar, entender e raciocinar!

### Quando n√£o devo usar o pytorch-accelerated?

- Se voc√™ est√° procurando uma solu√ß√£o completa de ponta a ponta, abrangendo tudo desde o carregamento dos dados at√© a infer√™ncia,
  que te ajude a selecionar um modelo, otimizador ou fun√ß√£o de perda, provavelmente voc√™ se adaptaria melhor ao
  [fastai](https://github.com/fastai/fastai). O `pytorch-accelerated` foca apenas no processo de treinamento, deixando todas as outras
  responsabilidades ao usu√°rio.
- Se voc√™ gostaria de escrever todo o loop de treinamento voc√™ mesmo, mas sem todas as dores de cabe√ßa de gerenciamento de dispositivos,
provavelmente o melhor seria usar diretamente o [Accelerate](https://github.com/huggingface/accelerate)! Embora seja
poss√≠vel customizar cada parte do `Trainer`, o loop de treinamento √© fundamentalmente dividido em v√°rios

diferentes m√©todos que voc√™ teria que sobrescrever. Mas, antes de ir, ser√° que escrever aqueles loops `for` √© realmente importante 
o suficiente para justificar come√ßar tudo do zero *de novo* üòâ.
- Se voc√™ est√° trabalhando em um caso de uso personalizado, altamente complexo, que n√£o se encaixa nos padr√µes dos loops de treinamento usuais
e quer extrair cada gota de desempenho do seu hardware escolhido, provavelmente o melhor √© continuar
com o PyTorch puro; qualquer API de alto n√≠vel se torna um overhead em casos altamente especializados!


## Agradecimentos

Muitos aspectos por tr√°s do design e das funcionalidades do `pytorch-accelerated` foram grandemente inspirados por uma s√©rie de excelentes 
bibliotecas e frameworks como [fastai](https://github.com/fastai/fastai), [timm](https://github.com/rwightman/pytorch-image-models), 
[PyTorch-lightning](https://github.com/PyTorchLightning/pytorch-lightning) e [Hugging Face Accelerate](https://github.com/huggingface/accelerate). Cada uma dessas ferramentas 
teve um enorme impacto tanto nesta biblioteca quanto na comunidade de aprendizado de m√°quina, e sua influ√™ncia n√£o pode ser 
enfatizada o suficiente!

O `pytorch-accelerated` apenas tomou inspira√ß√£o dessas ferramentas, e toda a funcionalidade contida foi implementada
do zero de uma forma que beneficie esta biblioteca. As √∫nicas exce√ß√µes a isso s√£o alguns dos scripts na 
[pasta de exemplos](https://github.com/Chris-hughes10/pytorch-accelerated/tree/main/examples)
onde recursos existentes foram usados e modificados para mostrar as funcionalidades do `pytorch-accelerated`;
esses casos est√£o claramente marcados, com o devido reconhecimento aos autores originais.
 


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2026-02-28

---