
<div align="right">
  <details>
    <summary >üåê Idioma</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=zh-TW">ÁπÅÈ´î‰∏≠Êñá</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=hi">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=th">‡πÑ‡∏ó‡∏¢</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=fr">Fran√ßais</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=es">Espa√±ol</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=it">Itapano</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ru">–†—É—Å—Å–∫–∏–π</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=pt">Portugu√™s</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ar">ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=fa">ŸÅÿßÿ±ÿ≥€å</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=tr">T√ºrk√ße</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=vi">Ti·∫øng Vi·ªát</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=id">Bahasa Indonesia</a>
      </div>
    </div>
  </details>
</div>

<div align="center">

# ZipVoice‚ö°

## Texto a Voz R√°pido y de Alta Calidad Zero-Shot con Flow Matching
</div>

## Descripci√≥n General

ZipVoice es una serie de modelos TTS de cero-shot r√°pidos y de alta calidad basados en flow matching.

### 1. Caracter√≠sticas clave

- Peque√±o y r√°pido: solo 123M de par√°metros.

- Clonaci√≥n de voz de alta calidad: rendimiento de vanguardia en similitud de hablante, inteligibilidad y naturalidad.

- Multiling√ºe: soporta chino e ingl√©s.

- Multimodo: soporta generaci√≥n de habla tanto de un solo hablante como de di√°logos.

### 2. Variantes del modelo

<table>
  <thead>
    <tr>
      <th>Nombre del Modelo</th>
      <th>Descripci√≥n</th>
      <th>Art√≠culo</th>
      <th>Demo</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ZipVoice</td>
      <td>El modelo b√°sico que soporta TTS de un solo hablante en chino e ingl√©s con cero-shot.</td>
      <td rowspan="2"><a href="https://arxiv.org/abs/2506.13053"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg"></a></td>
      <td rowspan="2"><a href="https://zipvoice.github.io"><img src="https://img.shields.io/badge/GitHub.io-P√°gina_Demo-blue?logo=Github&style=flat-square"></a></td>
    </tr>
    <tr>
      <td>ZipVoice-Distill</td>
      <td>La versi√≥n destilada de ZipVoice, con velocidad mejorada y m√≠nima degradaci√≥n de rendimiento.</td>
    </tr>
    <tr>
      <td>ZipVoice-Dialog</td>
      <td>Un modelo de generaci√≥n de di√°logos basado en ZipVoice, capaz de generar di√°logos hablados de dos partes en un solo canal.</td>
      <td rowspan="2"><a href="https://arxiv.org/abs/2507.09318"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg"></a></td>
      <td rowspan="2"><a href="https://zipvoice-dialog.github.io"><img src="https://img.shields.io/badge/GitHub.io-P√°gina_Demo-blue?logo=Github&style=flat-square"></a></td>
    </tr>
    <tr>
      <td>ZipVoice-Dialog-Stereo</td>
      <td>La variante est√©reo de ZipVoice-Dialog, que permite la generaci√≥n de di√°logos en dos canales con cada hablante asignado a un canal distinto.</td>
    </tr>
  </tbody>
</table>

## Noticias

**2025/07/14**: Se lanzan **ZipVoice-Dialog** y **ZipVoice-Dialog-Stereo**, dos modelos para generaci√≥n de di√°logos hablados. [![arXiv](https://img.shields.io/badge/arXiv-Paper-COLOR.svg)](https://arxiv.org/abs/2507.09318) [![demo page](https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square)](https://zipvoice-dialog.github.io)

**2025/07/14**: Se publica el conjunto de datos **OpenDialog**, un dataset de di√°logos hablados de 6.8k horas. Desc√°rgalo en [![hf](https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Dataset-yellow)](https://huggingface.co/datasets/k2-fsa/OpenDialog), [![ms](https://img.shields.io/badge/ModelScope-Dataset-blue?logo=data)](https://www.modelscope.cn/datasets/k2-fsa/OpenDialog). Consulta detalles en [![arXiv](https://img.shields.io/badge/arXiv-Paper-COLOR.svg)](https://arxiv.org/abs/2507.09318).

**2025/06/16**: Se lanzan **ZipVoice** y **ZipVoice-Distill**. [![arXiv](https://img.shields.io/badge/arXiv-Paper-COLOR.svg)](https://arxiv.org/abs/2506.13053) [![demo page](https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square)](https://zipvoice.github.io)

## Instalaci√≥n

### 1. Clona el repositorio de ZipVoice

```bash
git clone https://github.com/k2-fsa/ZipVoice.git
```
### 2. (Opcional) Crear un entorno virtual de Python


```bash
python3 -m venv zipvoice
source zipvoice/bin/activate
```
### 3. Instale los paquetes requeridos


```bash
pip install -r requirements.txt
```
### 4. Instala k2 para entrenamiento o inferencia eficiente

**k2 es necesario para el entrenamiento** y puede acelerar la inferencia. Sin embargo, a√∫n puedes usar el modo de inferencia de ZipVoice sin instalar k2.

> **Nota:** Aseg√∫rate de instalar la versi√≥n de k2 que coincida con tu versi√≥n de PyTorch y CUDA. Por ejemplo, si est√°s usando pytorch 2.5.1 y CUDA 12.1, puedes instalar k2 de la siguiente manera:


```bash
pip install k2==1.24.4.dev20250208+cuda12.1.torch2.5.1 -f https://k2-fsa.github.io/k2/cuda.html
```
Por favor, consulte https://k2-fsa.org/get-started/k2/ para m√°s detalles.
Los usuarios en China continental pueden consultar https://k2-fsa.org/zh-CN/get-started/k2/.

- Para comprobar la instalaci√≥n de k2:


```bash
python3 -c "import k2; print(k2.__file__)"
```
## Uso

### 1. Generaci√≥n de voz de un solo hablante

Para generar voz de un solo hablante con nuestros modelos preentrenados ZipVoice o ZipVoice-Distill, utilice los siguientes comandos (los modelos necesarios se descargar√°n desde HuggingFace):

#### 1.1 Inferencia de una sola oraci√≥n


```bash
python3 -m zipvoice.bin.infer_zipvoice \
    --model-name zipvoice \
    --prompt-wav prompt.wav \
    --prompt-text "I am the transcription of the prompt wav." \
    --text "I am the text to be synthesized." \
    --res-wav-path result.wav
```
- `--model-name` puede ser `zipvoice` o `zipvoice_distill`, que son modelos antes y despu√©s de la destilaci√≥n, respectivamente.
- Si aparecen `<>` o `[]` en el texto, las cadenas encerradas por ellos se tratar√°n como tokens especiales. `<>` denota pinyin chino y `[]` denota otras etiquetas especiales.

#### 1.2 Inferencia de una lista de oraciones

```bash
python3 -m zipvoice.bin.infer_zipvoice \
    --model-name zipvoice \
    --test-list test.tsv \
    --res-dir results
```
- Cada l√≠nea de `test.tsv` tiene el formato `{wav_name}\t{prompt_transcription}\t{prompt_wav}\t{text}`.

### 2. Generaci√≥n de habla en di√°logo

#### 2.1 Comando de inferencia

Para generar di√°logos hablados de dos partes con nuestros modelos preentrenados ZipVoice-Dialogue o ZipVoice-Dialogue-Stereo, utilice los siguientes comandos (los modelos requeridos se descargar√°n desde HuggingFace):


```bash
python3 -m zipvoice.bin.infer_zipvoice_dialog \
    --model-name "zipvoice_dialog" \
    --test-list test.tsv \
    --res-dir results
```
- `--model-name` puede ser `zipvoice_dialog` o `zipvoice_dialog_stereo`,
    que generan di√°logos mono y est√©reo, respectivamente.

#### 2.2 Formatos de entrada

Cada l√≠nea de `test.tsv` est√° en uno de los siguientes formatos:

(1) **Formato de prompt combinado** donde los audios y transcripciones de los prompts de dos hablantes se combinan en un solo archivo wav de prompt:

```
{wav_name}\t{prompt_transcription}\t{prompt_wav}\t{text}
```

- `wav_name` es el nombre del archivo wav de salida.
- `prompt_transcription` es la transcripci√≥n del archivo wav del prompt conversacional, por ejemplo, "[S1] Hola. [S2] ¬øC√≥mo est√°s?"
- `prompt_wav` es la ruta al archivo wav del prompt.
- `text` es el texto a sintetizar, por ejemplo, "[S1] Estoy bien. [S2] ¬øC√≥mo te llamas? [S1] Soy Eric. [S2] Hola Eric."

(2) **Formato de prompt dividido** donde los audios y transcripciones de dos hablantes existen en archivos separados:

```
{wav_name}\t{spk1_prompt_transcription}\t{spk2_prompt_transcription}\t{spk1_prompt_wav}\t{spk2_prompt_wav}\t{text}
```
- `wav_name` es el nombre del archivo wav de salida.
- `spk1_prompt_transcription` es la transcripci√≥n del archivo wav de indicaci√≥n del primer hablante, por ejemplo, "Hola".
- `spk2_prompt_transcription` es la transcripci√≥n del archivo wav de indicaci√≥n del segundo hablante, por ejemplo, "¬øC√≥mo est√°s?"
- `spk1_prompt_wav` es la ruta al archivo wav de indicaci√≥n del primer hablante.
- `spk2_prompt_wav` es la ruta al archivo wav de indicaci√≥n del segundo hablante.
- `text` es el texto que se va a sintetizar, por ejemplo, "[S1] Estoy bien. [S2] ¬øCu√°l es tu nombre? [S1] Soy Eric. [S2] Hola Eric."

### 3 Gu√≠a para un mejor uso:

#### 3.1 Longitud de la indicaci√≥n

Recomendamos un archivo wav de indicaci√≥n corto (por ejemplo, menos de 3 segundos para generaci√≥n de habla de un solo hablante, menos de 10 segundos para generaci√≥n de di√°logos) para una velocidad de inferencia m√°s r√°pida. Una indicaci√≥n muy larga ralentizar√° la inferencia y degradar√° la calidad del habla.

#### 3.2 Optimizaci√≥n de velocidad

Si la velocidad de inferencia no es satisfactoria, puede acelerarla de la siguiente manera:

- **Modelo destilado y menos pasos**: Para el modelo de generaci√≥n de habla de un solo hablante, usamos el modelo `zipvoice` por defecto para mejor calidad de habla. Si la velocidad es una prioridad, puede cambiar a `zipvoice_distill` y reducir el par√°metro `--num-steps` hasta `4` (8 por defecto).

- **Aceleraci√≥n de CPU con multiproceso**: Al ejecutar en CPU, puede pasar el par√°metro `--num-thread` (por ejemplo, `--num-thread 4`) para aumentar el n√∫mero de hilos y acelerar la velocidad. Usamos 1 hilo por defecto.

- **Aceleraci√≥n de CPU con ONNX**: Al ejecutar en CPU, puede usar modelos ONNX con `zipvoice.bin.infer_zipvoice_onnx` para mayor velocidad (a√∫n no soporta ONNX para modelos de generaci√≥n de di√°logo). Para mayor velocidad, puede establecer `--onnx-int8 True` para usar un modelo ONNX cuantizado en INT8. Tenga en cuenta que el modelo cuantizado puede degradar la calidad del habla. **No use ONNX en GPU**, ya que es m√°s lento que PyTorch en GPU.

- **Aceleraci√≥n de GPU con NVIDIA TensorRT**: Para obtener un gran aumento de rendimiento en GPUs NVIDIA, primero exporte el modelo a un motor TensorRT usando zipvoice.bin.tensorrt_export. Luego, ejecute la inferencia en su conjunto de datos (por ejemplo, un conjunto de datos de Hugging Face) con zipvoice.bin.infer_zipvoice. Esto puede lograr aproximadamente el doble de rendimiento en comparaci√≥n con la implementaci√≥n est√°ndar de PyTorch en GPU.

#### 3.3 Control de memoria

El texto dado se dividir√° en fragmentos seg√∫n la puntuaci√≥n (para generaci√≥n de habla de un solo hablante) o el s√≠mbolo de cambio de hablante (para generaci√≥n de di√°logo). Luego, los textos fragmentados se procesar√°n en lotes. Por lo tanto, el modelo puede procesar textos arbitrariamente largos con uso de memoria casi constante. Puede controlar el uso de memoria ajustando el par√°metro `--max-duration`.

#### 3.4 Evaluaci√≥n "cruda"

Por defecto, preprocesamos las entradas (archivo wav de indicaci√≥n, transcripci√≥n de indicaci√≥n y texto) para una inferencia eficiente y mejor rendimiento. Si desea evaluar el rendimiento "crudo" del modelo usando exactamente las entradas proporcionadas (por ejemplo, para reproducir los resultados de nuestro art√≠culo), puede pasar `--raw-evaluation True`.

#### 3.5 Texto corto

Al generar habla para textos muy cortos (por ejemplo, una o dos palabras), el habla generada puede omitir ciertas pronunciaciones. Para resolver este problema, puede pasar `--speed 0.3` (donde 0.3 es un valor ajustable) para extender la duraci√≥n del habla generada.

#### 3.6 Correcci√≥n de caracteres polif√≥nicos chinos mal pronunciados


Utilizamos [pypinyin](https://github.com/mozillazg/python-pinyin) para convertir caracteres chinos a pinyin. Sin embargo, ocasionalmente puede pronunciar incorrectamente **caracteres polif√≥nicos** (Â§öÈü≥Â≠ó).

Para corregir manualmente estas malas pronunciaciones, encierre el **pinyin corregido** entre signos de menor y mayor `< >` e incluya la **marca de tono**.

**Ejemplo:**

- Texto original: `ËøôÊääÂâëÈïø‰∏âÂçÅÂÖ¨ÂàÜ`
- Corrija el pinyin de `Èïø`:  `ËøôÊääÂâë<chang2>‰∏âÂçÅÂÖ¨ÂàÜ`

> **Nota:** Si desea asignar manualmente varios pinyins, encierre cada pinyin con `<>`, por ejemplo, `ËøôÊää<jian4><chang2><san1>ÂçÅÂÖ¨ÂàÜ`

#### 3.7 Eliminar silencios largos del habla generada

El modelo determinar√° autom√°ticamente las posiciones y longitudes de los silencios en el habla generada. Ocasionalmente tiene un silencio largo en medio del discurso. Si no desea esto, puede pasar `--remove-long-sil` para eliminar silencios largos en medio de la voz generada (los silencios en los extremos se eliminar√°n por defecto).

#### 3.8 Descarga del modelo

Si tiene problemas para conectarse a HuggingFace al descargar los modelos preentrenados, intente cambiar el endpoint al sitio espejo: `export HF_ENDPOINT=https://hf-mirror.com`.

## Entrene su propio modelo

Consulte el directorio [egs](egs) para ejemplos de entrenamiento, ajuste fino y evaluaci√≥n.

## Despliegue en producci√≥n

### Tiempo de ejecuci√≥n NVIDIA Triton GPU

Para un despliegue listo para producci√≥n con alto rendimiento y escalabilidad, consulte la [integraci√≥n con Triton Inference Server](runtime/nvidia_triton/) que ofrece motores TensorRT optimizados, manejo de solicitudes concurrentes y APIs gRPC/HTTP para uso empresarial.

### Despliegue en CPU

Consulte [sherpa-onnx](https://github.com/k2-fsa/sherpa-onnx/pull/2487#issuecomment-3227884498) para la soluci√≥n de despliegue en C++ sobre CPU.

## Discusi√≥n y comunicaci√≥n

Puede discutir directamente en [Github Issues](https://github.com/k2-fsa/ZipVoice/issues).

Tambi√©n puede escanear el c√≥digo QR para unirse a nuestro grupo de wechat o seguir nuestra cuenta oficial de wechat.

| Grupo de Wechat | Cuenta Oficial de Wechat |
| ------------ | ----------------------- |
|![wechat](https://k2-fsa.org/zh-CN/assets/pic/wechat_group.jpg) |![wechat](https://k2-fsa.org/zh-CN/assets/pic/wechat_account.jpg) |

## Citaci√≥n

```bibtex
@article{zhu2025zipvoice,
      title={ZipVoice: Fast and High-Quality Zero-Shot Text-to-Speech with Flow Matching},
      author={Zhu, Han and Kang, Wei and Yao, Zengwei and Guo, Liyong and Kuang, Fangjun and Li, Zhaoqing and Zhuang, Weiji and Lin, Long and Povey, Daniel},
      journal={arXiv preprint arXiv:2506.13053},
      year={2025}
}

@article{zhu2025zipvoicedialog,
      title={ZipVoice-Dialog: Non-Autoregressive Spoken Dialogue Generation with Flow Matching},
      author={Zhu, Han and Kang, Wei and Guo, Liyong and Yao, Zengwei and Kuang, Fangjun and Zhuang, Weiji and Li, Zhaoqing and Han, Zhifeng and Zhang, Dong and Zhang, Xin and Song, Xingchen and Lin, Long and Povey, Daniel},
      journal={arXiv preprint arXiv:2507.09318},
      year={2025}
}
```




---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-12-30

---