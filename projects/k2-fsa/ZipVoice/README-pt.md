
<div align="right">
  <details>
    <summary >üåê Idioma</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=zh-TW">ÁπÅÈ´î‰∏≠Êñá</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=hi">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=th">‡πÑ‡∏ó‡∏¢</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=fr">Fran√ßais</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=es">Espa√±ol</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=it">Itapano</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ru">–†—É—Å—Å–∫–∏–π</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=pt">Portugu√™s</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ar">ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=fa">ŸÅÿßÿ±ÿ≥€å</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=tr">T√ºrk√ße</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=vi">Ti·∫øng Vi·ªát</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=id">Bahasa Indonesia</a>
      </div>
    </div>
  </details>
</div>

<div align="center">

# ZipVoice‚ö°

## Texto para Fala Zero-Shot R√°pido e de Alta Qualidade com Flow Matching
</div>

## Vis√£o Geral

ZipVoice √© uma s√©rie de modelos TTS de zero-shot r√°pidos e de alta qualidade baseados em flow matching.

### 1. Principais caracter√≠sticas

- Pequeno e r√°pido: apenas 123M de par√¢metros.

- Clonagem de voz de alta qualidade: desempenho de ponta em similaridade de locutor, inteligibilidade e naturalidade.

- Multi-idiomas: suporta chin√™s e ingl√™s.

- Multi-modo: suporta gera√ß√£o de fala de locutor √∫nico e de di√°logos.

### 2. Variantes do modelo

<table>
  <thead>
    <tr>
      <th>Nome do Modelo</th>
      <th>Descri√ß√£o</th>
      <th>Artigo</th>
      <th>Demo</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ZipVoice</td>
      <td>O modelo b√°sico que suporta TTS zero-shot de locutor √∫nico em chin√™s e ingl√™s.</td>
      <td rowspan="2"><a href="https://arxiv.org/abs/2506.13053"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg"></a></td>
      <td rowspan="2"><a href="https://zipvoice.github.io"><img src="https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square"></a></td>
    </tr>
    <tr>
      <td>ZipVoice-Distill</td>
      <td>Vers√£o destilada do ZipVoice, com velocidade aprimorada e m√≠nima degrada√ß√£o de desempenho.</td>
    </tr>
    <tr>
      <td>ZipVoice-Dialog</td>
      <td>Modelo de gera√ß√£o de di√°logos baseado no ZipVoice, capaz de gerar di√°logos falados de dois participantes em canal √∫nico.</td>
      <td rowspan="2"><a href="https://arxiv.org/abs/2507.09318"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg"></a></td>
      <td rowspan="2"><a href="https://zipvoice-dialog.github.io"><img src="https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square"></a></td>
    </tr>
    <tr>
      <td>ZipVoice-Dialog-Stereo</td>
      <td>A variante est√©reo do ZipVoice-Dialog, permitindo gera√ß√£o de di√°logos em dois canais com cada falante atribu√≠do a um canal distinto.</td>
    </tr>
  </tbody>
</table>

## Novidades

**2025/07/14**: **ZipVoice-Dialog** e **ZipVoice-Dialog-Stereo**, dois modelos de gera√ß√£o de di√°logos falados, foram lan√ßados. [![arXiv](https://img.shields.io/badge/arXiv-Paper-COLOR.svg)](https://arxiv.org/abs/2507.09318) [![demo page](https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square)](https://zipvoice-dialog.github.io)

**2025/07/14**: O conjunto de dados **OpenDialog**, um dataset de di√°logos falados de 6,8 mil horas, foi lan√ßado. Baixe em [![hf](https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Dataset-yellow)](https://huggingface.co/datasets/k2-fsa/OpenDialog), [![ms](https://img.shields.io/badge/ModelScope-Dataset-blue?logo=data)](https://www.modelscope.cn/datasets/k2-fsa/OpenDialog). Confira os detalhes em [![arXiv](https://img.shields.io/badge/arXiv-Paper-COLOR.svg)](https://arxiv.org/abs/2507.09318).

**2025/06/16**: **ZipVoice** e **ZipVoice-Distill** foram lan√ßados. [![arXiv](https://img.shields.io/badge/arXiv-Paper-COLOR.svg)](https://arxiv.org/abs/2506.13053) [![demo page](https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square)](https://zipvoice.github.io)

## Instala√ß√£o

### 1. Clone o reposit√≥rio ZipVoice

```bash
git clone https://github.com/k2-fsa/ZipVoice.git
```
### 2. (Opcional) Crie um ambiente virtual Python


```bash
python3 -m venv zipvoice
source zipvoice/bin/activate
```
### 3. Instale os pacotes necess√°rios


```bash
pip install -r requirements.txt
```
### 4. Instale o k2 para treinamento ou infer√™ncia eficiente

**k2 √© necess√°rio para o treinamento** e pode acelerar a infer√™ncia. No entanto, voc√™ ainda pode usar o modo de infer√™ncia do ZipVoice sem instalar o k2.

> **Nota:** Certifique-se de instalar a vers√£o do k2 que corresponde √† sua vers√£o do PyTorch e do CUDA. Por exemplo, se voc√™ estiver usando pytorch 2.5.1 e CUDA 12.1, voc√™ pode instalar o k2 da seguinte forma:


```bash
pip install k2==1.24.4.dev20250208+cuda12.1.torch2.5.1 -f https://k2-fsa.github.io/k2/cuda.html
```
Consulte https://k2-fsa.org/get-started/k2/ para mais detalhes.
Usu√°rios na China continental podem consultar https://k2-fsa.org/zh-CN/get-started/k2/.

- Para verificar a instala√ß√£o do k2:


```bash
python3 -c "import k2; print(k2.__file__)"
```
## Uso

### 1. Gera√ß√£o de fala de um √∫nico locutor

Para gerar fala de um √∫nico locutor com nossos modelos pr√©-treinados ZipVoice ou ZipVoice-Distill, use os seguintes comandos (Os modelos necess√°rios ser√£o baixados do HuggingFace):

#### 1.1 Infer√™ncia de uma √∫nica senten√ßa


```bash
python3 -m zipvoice.bin.infer_zipvoice \
    --model-name zipvoice \
    --prompt-wav prompt.wav \
    --prompt-text "I am the transcription of the prompt wav." \
    --text "I am the text to be synthesized." \
    --res-wav-path result.wav
```
- `--model-name` pode ser `zipvoice` ou `zipvoice_distill`, que s√£o modelos antes e depois da destila√ß√£o, respectivamente.
- Se `<>` ou `[]` aparecerem no texto, cadeias de caracteres entre eles ser√£o tratadas como tokens especiais. `<>` denota pinyin chin√™s e `[]` denota outras tags especiais.

#### 1.2 Infer√™ncia de uma lista de senten√ßas

```bash
python3 -m zipvoice.bin.infer_zipvoice \
    --model-name zipvoice \
    --test-list test.tsv \
    --res-dir results
```
- Cada linha de `test.tsv` est√° no formato `{wav_name}\t{prompt_transcription}\t{prompt_wav}\t{text}`.

### 2. Gera√ß√£o de fala em di√°logo

#### 2.1 Comando de infer√™ncia

Para gerar di√°logos falados entre duas pessoas com nossos modelos pr√©-treinados ZipVoice-Dialogue ou ZipVoice-Dialogue-Stereo, use os seguintes comandos (Os modelos necess√°rios ser√£o baixados do HuggingFace):


```bash
python3 -m zipvoice.bin.infer_zipvoice_dialog \
    --model-name "zipvoice_dialog" \
    --test-list test.tsv \
    --res-dir results
```
- `--model-name` pode ser `zipvoice_dialog` ou `zipvoice_dialog_stereo`,
    que geram di√°logos mono e est√©reo, respectivamente.

#### 2.2 Formatos de entrada

Cada linha do `test.tsv` est√° em um dos seguintes formatos:

(1) **Formato de prompt mesclado** onde os √°udios e transcri√ß√µes dos prompts de dois falantes s√£o mesclados em um √∫nico arquivo wav de prompt:

```
{wav_name}\t{prompt_transcription}\t{prompt_wav}\t{text}
```

- `wav_name` √© o nome do arquivo wav de sa√≠da.
- `prompt_transcription` √© a transcri√ß√£o do arquivo wav de prompt de conversa√ß√£o, por exemplo, "[S1] Ol√°. [S2] Como vai voc√™?"
- `prompt_wav` √© o caminho para o arquivo wav de prompt.
- `text` √© o texto a ser sintetizado, por exemplo, "[S1] Estou bem. [S2] Qual √© o seu nome? [S1] Sou Eric. [S2] Oi Eric."

(2) **Formato de prompt dividido** onde os √°udios e transcri√ß√µes de dois falantes existem em arquivos separados:

```
{wav_name}\t{spk1_prompt_transcription}\t{spk2_prompt_transcription}\t{spk1_prompt_wav}\t{spk2_prompt_wav}\t{text}
```
- `wav_name` √© o nome do arquivo wav de sa√≠da.
- `spk1_prompt_transcription` √© a transcri√ß√£o do prompt wav do primeiro locutor, por exemplo, "Ol√°"
- `spk2_prompt_transcription` √© a transcri√ß√£o do prompt wav do segundo locutor, por exemplo, "Como vai voc√™?"
- `spk1_prompt_wav` √© o caminho para o arquivo wav do prompt do primeiro locutor.
- `spk2_prompt_wav` √© o caminho para o arquivo wav do prompt do segundo locutor.
- `text` √© o texto a ser sintetizado, por exemplo, "[S1] Estou bem. [S2] Qual √© o seu nome? [S1] Eu sou Eric. [S2] Ol√° Eric."

### 3 Orienta√ß√µes para melhor uso:

#### 3.1 Comprimento do prompt

Recomendamos um arquivo wav de prompt curto (por exemplo, menos de 3 segundos para gera√ß√£o de fala de um √∫nico locutor, menos de 10 segundos para gera√ß√£o de fala em di√°logo) para maior velocidade de infer√™ncia. Um prompt muito longo ir√° desacelerar a infer√™ncia e degradar a qualidade da fala.

#### 3.2 Otimiza√ß√£o de velocidade

Se a velocidade de infer√™ncia n√£o for satisfat√≥ria, voc√™ pode aceler√°-la da seguinte forma:

- **Modelo destilado e menos passos**: Para o modelo de gera√ß√£o de fala de √∫nico locutor, usamos o modelo `zipvoice` por padr√£o para melhor qualidade de fala. Se a prioridade for velocidade, voc√™ pode trocar para o `zipvoice_distill` e reduzir o `--num-steps` para at√© `4` (8 por padr√£o).

- **Acelera√ß√£o da CPU com multithreading**: Ao rodar na CPU, voc√™ pode passar o par√¢metro `--num-thread` (por exemplo, `--num-thread 4`) para aumentar o n√∫mero de threads e obter maior velocidade. Usamos 1 thread por padr√£o.

- **Acelera√ß√£o da CPU com ONNX**: Ao rodar na CPU, voc√™ pode usar modelos ONNX com `zipvoice.bin.infer_zipvoice_onnx` para mais velocidade (ainda n√£o h√° suporte ONNX para modelos de gera√ß√£o de di√°logo). Para ainda mais velocidade, voc√™ pode definir `--onnx-int8 True` para usar um modelo ONNX quantizado em INT8. Note que o modelo quantizado pode apresentar certa degrada√ß√£o na qualidade da fala. **N√£o use ONNX na GPU**, pois √© mais lento do que o PyTorch na GPU.

- **Acelera√ß√£o de GPU com NVIDIA TensorRT**: Para um aumento significativo de desempenho em GPUs NVIDIA, primeiro exporte o modelo para um engine TensorRT usando zipvoice.bin.tensorrt_export. Em seguida, fa√ßa a infer√™ncia em seu dataset (por exemplo, um dataset Hugging Face) com zipvoice.bin.infer_zipvoice. Isso pode alcan√ßar aproximadamente 2x o throughput em rela√ß√£o √† implementa√ß√£o padr√£o PyTorch na GPU.

#### 3.3 Controle de mem√≥ria

O texto fornecido ser√° dividido em partes com base na pontua√ß√£o (para gera√ß√£o de fala de √∫nico locutor) ou s√≠mbolo de troca de locutor (para gera√ß√£o de di√°logo). Ent√£o, os textos fragmentados ser√£o processados em lotes. Portanto, o modelo pode processar textos arbitrariamente longos com uso de mem√≥ria quase constante. Voc√™ pode controlar o uso de mem√≥ria ajustando o par√¢metro `--max-duration`.

#### 3.4 Avalia√ß√£o "Bruta"

Por padr√£o, pr√©-processamos as entradas (prompt wav, transcri√ß√£o do prompt e texto) para infer√™ncia eficiente e melhor desempenho. Se voc√™ quiser avaliar o desempenho "bruto" do modelo usando exatamente as entradas fornecidas (por exemplo, para reproduzir os resultados do nosso artigo), pode passar `--raw-evaluation True`.

#### 3.5 Texto curto

Ao gerar fala para textos muito curtos (por exemplo, uma ou duas palavras), a fala gerada pode, √†s vezes, omitir certas pron√∫ncias. Para resolver esse problema, voc√™ pode passar `--speed 0.3` (onde 0.3 √© um valor ajust√°vel) para prolongar a dura√ß√£o da fala gerada.

#### 3.6 Corrigindo caracteres polif√¥nicos chineses pronunciados incorretamente


Usamos o [pypinyin](https://github.com/mozillazg/python-pinyin) para converter caracteres chineses em pinyin. No entanto, ele pode ocasionalmente pronunciar incorretamente **caracteres polif√¥nicos** (Â§öÈü≥Â≠ó).

Para corrigir manualmente essas pron√∫ncias incorretas, coloque o **pinyin corrigido** entre sinais de menor e maior `< >` e inclua o **acento tonal**.

**Exemplo:**

- Texto original: `ËøôÊääÂâëÈïø‰∏âÂçÅÂÖ¨ÂàÜ`
- Corrija o pinyin de `Èïø`:  `ËøôÊääÂâë<chang2>‰∏âÂçÅÂÖ¨ÂàÜ`

> **Nota:** Se voc√™ quiser atribuir manualmente m√∫ltiplos pinyins, coloque cada pinyin com `<>`, por exemplo, `ËøôÊää<jian4><chang2><san1>ÂçÅÂÖ¨ÂàÜ`

#### 3.7 Remover longos sil√™ncios da fala gerada

O modelo determinar√° automaticamente as posi√ß√µes e dura√ß√µes dos sil√™ncios na fala gerada. Ocasionalmente, ele insere longos sil√™ncios no meio da fala. Se n√£o desejar isso, voc√™ pode passar `--remove-long-sil` para remover longos sil√™ncios no meio da fala gerada (sil√™ncios nas bordas ser√£o removidos por padr√£o).

#### 3.8 Download do modelo

Se voc√™ tiver problemas para conectar ao HuggingFace ao baixar os modelos pr√©-treinados, tente mudar o endpoint para o site espelho: `export HF_ENDPOINT=https://hf-mirror.com`.

## Treine Seu Pr√≥prio Modelo

Veja o diret√≥rio [egs](egs) para exemplos de treinamento, ajuste fino e avalia√ß√£o.

## Implanta√ß√£o em Produ√ß√£o

### Runtime NVIDIA Triton GPU

Para uma implanta√ß√£o pronta para produ√ß√£o com alto desempenho e escalabilidade, confira a [integra√ß√£o com o Triton Inference Server](runtime/nvidia_triton/), que fornece engines TensorRT otimizadas, manipula√ß√£o de solicita√ß√µes concorrentes e APIs gRPC/HTTP para uso corporativo.

### Implanta√ß√£o em CPU

Confira [sherpa-onnx](https://github.com/k2-fsa/sherpa-onnx/pull/2487#issuecomment-3227884498) para a solu√ß√£o de implanta√ß√£o em CPU com C++.

## Discuss√£o & Comunica√ß√£o

Voc√™ pode discutir diretamente em [Github Issues](https://github.com/k2-fsa/ZipVoice/issues).

Voc√™ tamb√©m pode escanear o c√≥digo QR para entrar em nosso grupo do wechat ou seguir nossa conta oficial do wechat.

| Grupo Wechat | Conta Oficial do Wechat |
| ------------ | ----------------------- |
|![wechat](https://k2-fsa.org/zh-CN/assets/pic/wechat_group.jpg) |![wechat](https://k2-fsa.org/zh-CN/assets/pic/wechat_account.jpg) |

## Cita√ß√£o

```bibtex
@article{zhu2025zipvoice,
      title={ZipVoice: Fast and High-Quality Zero-Shot Text-to-Speech with Flow Matching},
      author={Zhu, Han and Kang, Wei and Yao, Zengwei and Guo, Liyong and Kuang, Fangjun and Li, Zhaoqing and Zhuang, Weiji and Lin, Long and Povey, Daniel},
      journal={arXiv preprint arXiv:2506.13053},
      year={2025}
}

@article{zhu2025zipvoicedialog,
      title={ZipVoice-Dialog: Non-Autoregressive Spoken Dialogue Generation with Flow Matching},
      author={Zhu, Han and Kang, Wei and Guo, Liyong and Yao, Zengwei and Kuang, Fangjun and Zhuang, Weiji and Li, Zhaoqing and Han, Zhifeng and Zhang, Dong and Zhang, Xin and Song, Xingchen and Lin, Long and Povey, Daniel},
      journal={arXiv preprint arXiv:2507.09318},
      year={2025}
}
```




---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-12-30

---