[
  {
    "Id": 1,
    "Content": "\n<div align=\"right\">\n  <details>\n    <summary >ЁЯМР Language</summary>\n    <div>\n      <div align=\"center\">\n        <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=en\">English</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=zh-CN\">чоАф╜Уф╕нцЦЗ</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=zh-TW\">ч╣БщлФф╕нцЦЗ</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ja\">цЧецЬмшкЮ</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ko\">эХЬъ╡ньЦ┤</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=hi\">рд╣рд┐рдиреНрджреА</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=th\">р╣Др╕Чр╕в</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=fr\">Fran├зais</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=de\">Deutsch</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=es\">Espa├▒ol</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=it\">Itapano</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ru\">╨а╤Г╤Б╤Б╨║╨╕╨╣</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=pt\">Portugu├кs</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=nl\">Nederlands</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=pl\">Polski</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ar\">╪з┘Д╪╣╪▒╪и┘К╪й</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=fa\">┘Б╪з╪▒╪│█М</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=tr\">T├╝rk├зe</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=vi\">Tiс║┐ng Viс╗Зt</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=id\">Bahasa Indonesia</a>\n      </div>\n    </div>\n  </details>\n</div>\n\n<div align=\"center\">\n\n# ZipVoiceтЪб\n\n## Fast and High-Quality Zero-Shot Text-to-Speech with Flow Matching\n</div>\n\n## Overview\n",
    "ContentSha": "4yvqxrt72y1T/XJlpvF5+w536K1FZVvNlGdllJQl3D8=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "\n<div align=\"right\">\n  <details>\n    <summary >ЁЯМР ржнрж╛рж╖рж╛</summary>\n    <div>\n      <div align=\"center\">\n        <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=en\">English</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=zh-CN\">чоАф╜Уф╕нцЦЗ</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=zh-TW\">ч╣БщлФф╕нцЦЗ</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ja\">цЧецЬмшкЮ</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ko\">эХЬъ╡ньЦ┤</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=hi\">рд╣рд┐рдиреНрджреА</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=th\">р╣Др╕Чр╕в</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=fr\">Fran├зais</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=de\">Deutsch</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=es\">Espa├▒ol</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=it\">Itapano</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ru\">╨а╤Г╤Б╤Б╨║╨╕╨╣</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=pt\">Portugu├кs</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=nl\">Nederlands</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=pl\">Polski</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ar\">╪з┘Д╪╣╪▒╪и┘К╪й</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=fa\">┘Б╪з╪▒╪│█М</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=tr\">T├╝rk├зe</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=vi\">Tiс║┐ng Viс╗Зt</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=id\">Bahasa Indonesia</a>\n      </div>\n    </div>\n  </details>\n</div>\n\n<div align=\"center\">\n\n# ZipVoiceтЪб\n\n## Flow Matching ржмрзНржпрз▒рж╣рж╛рз░ ржХрз░рж┐ ржжрзНрз░рзБржд ржЖрз░рзБ ржЙржЪрзНржЪ-ржЧрзБржгржорж╛ржирз░ рж╢рзВржирзНржп-рж╢рзНржмржЯ ржкрж╛ржа-ржЯрзБ-рж╕рзНржкрзАржЪ\n</div>\n\n## ржкрз░рж┐рж╕ржВржЦрзНржпрж╛ржи\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 2,
    "Content": "ZipVoice is a series of fast and high-quality zero-shot TTS models based on flow matching.\n\n### 1. Key features\n\n- Small and fast: only 123M parameters.\n\n- High-quality voice cloning: state-of-the-art performance in speaker similarity, intelligibility, and naturalness.\n\n- Multi-lingual: support Chinese and English.\n\n- Multi-mode: support both single-speaker and dialogue speech generation.\n\n### 2. Model variants\n\n<table>\n  <thead>\n    <tr>\n      <th>Model Name</th>\n      <th>Description</th>\n      <th>Paper</th>\n      <th>Demo</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>ZipVoice</td>\n      <td>The basic model supporting zero-shot single-speaker TTS in both Chinese and English.</td>\n      <td rowspan=\"2\"><a href=\"https://arxiv.org/abs/2506.13053\"><img src=\"https://img.shields.io/badge/arXiv-Paper-COLOR.svg\"></a></td>\n      <td rowspan=\"2\"><a href=\"https://zipvoice.github.io\"><img src=\"https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square\"></a></td>\n    </tr>\n    <tr>\n      <td>ZipVoice-Distill</td>\n      <td>The distilled version of ZipVoice, featuring improved speed with minimal performance degradation.</td>\n    </tr>\n    <tr>\n      <td>ZipVoice-Dialog</td>\n      <td>A dialogue generation model built on ZipVoice, capable of generating single-channel two-party spoken dialogues.</td>\n      <td rowspan=\"2\"><a href=\"https://arxiv.org/abs/2507.09318\"><img src=\"https://img.shields.io/badge/arXiv-Paper-COLOR.svg\"></a></td>\n      <td rowspan=\"2\"><a href=\"https://zipvoice-dialog.github.io\"><img src=\"https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square\"></a></td>\n    </tr>",
    "ContentSha": "qUZer10kkUVucN3aqovgCkJ059aWMO67vNYjNUfJcJw=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "ZipVoice рж╣рзИржЫрзЗ flow matching-рз░ ржЖржзрж╛рз░ржд ржирж┐рз░рзНржорж┐ржд, ржжрзНрз░рзБржд ржЖрз░рзБ ржЙржЪрзНржЪ-ржЧрзБржгржЧрждржорж╛ржирз░ zero-shot TTS ржоржбрзЗрж▓рз░ ржПржХ рж╢рзГржВржЦрж▓рж╛ред\n\n### рзз. ржорзБржЦрзНржп ржмрзИрж╢рж┐рж╖рзНржЯрзНржпрж╕ржорзВрж╣\n\n- рж╕рз░рзБ ржЖрз░рзБ ржжрзНрз░рзБржд: ржХрзЗрз▒рж▓ рззрзирзйM ржкрз░рж╛ржорж┐рждрзНрз░ред\n\n- ржЙржЪрзНржЪ-ржЧрзБржгржЧрждржорж╛ржирз░ ржнрзЯрзЗржЫ ржХрзНрж▓тАЩржирж┐ржВ: рж╕рзНржкрзАржХрж╛рз░рз░ рж╕рж╛ржжрзГрж╢рзНржп, ржмрзБржЬрж╛ржмрж▓рзИ рж╕ржХрзНрж╖ржорждрж╛, ржЖрз░рзБ ржкрзНрз░рж╛ржХрзГрждрж┐ржХрждрж╛рз░ ржХрзНрж╖рзЗрждрзНрз░ржд рж╢рзНрз░рзЗрж╖рзНржа ржкрз░рзНржпрж╛ржпрж╝рз░ ржХрж╛рз░рзНржпржХрзНрж╖ржорждрж╛ред\n\n- ржмрж╣рзБ-ржнрж╛рж╖рж┐ржХ: ржЪрзАржирж╛ ржЖрз░рзБ ржЗржВрз░рж╛ржЬрзА рж╕ржорз░рзНржерж┐рждред\n\n- ржмрж╣рзБ-ржорзЛржб: ржПржХржХ рж╕рзНржкрзАржХрж╛рз░ ржЖрз░рзБ рж╕ржВрж▓рж╛ржк ржЙрждрзНржкрж╛ржжржирз░ ржжрзБржпрж╝рзЛ рж╕ржорз░рзНржерж┐рждред\n\n### рзи. ржоржбрзЗрж▓ ржнрзЗрз░рж┐ржпрж╝рзЗржгрзНржЯрж╕ржорзВрж╣\n\n<table>\n  <thead>\n    <tr>\n      <th>ржоржбрзЗрж▓рз░ ржирж╛ржо</th>\n      <th>ржмрж┐рз▒рз░ржг</th>\n      <th>ржкрзЗржкрж╛рз░</th>\n      <th>ржбрзЗржорзЛ</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>ZipVoice</td>\n      <td>ржорзВрж▓ ржоржбрзЗрж▓, ржпрж┐ ржЪрзАржирж╛ ржЖрз░рзБ ржЗржВрз░рж╛ржЬрзАржд zero-shot ржПржХржХ-рж╕рзНржкрзАржХрж╛рз░ TTS рж╕ржорз░рзНржержи ржХрз░рзЗред</td>\n      <td rowspan=\"2\"><a href=\"https://arxiv.org/abs/2506.13053\"><img src=\"https://img.shields.io/badge/arXiv-ржкрзЗржкрж╛рз░-COLOR.svg\"></a></td>\n      <td rowspan=\"2\"><a href=\"https://zipvoice.github.io\"><img src=\"https://img.shields.io/badge/GitHub.io-ржбрзЗржорзЛ_ржкрзГрж╖рзНржарж╛-blue?logo=Github&style=flat-square\"></a></td>\n    </tr>\n    <tr>\n      <td>ZipVoice-Distill</td>\n      <td>ZipVoice-рз░ ржбрж┐рж╕рзНржЯрж┐рж▓рзНржб рж╕ржВрж╕рзНржХрз░ржг, ржЦрзБржм ржХржо ржХрж╛рз░рзНржпржХрзНрж╖ржорждрж╛ рж╣рзНрз░рж╛рж╕рз░ рж╕рзИрждрзЗ ржмрзЗржЫрж┐ ржжрзНрз░рзБрждред</td>\n    </tr>\n    <tr>\n      <td>ZipVoice-Dialog</td>\n      <td>ZipVoice-рз░ ржУржкрз░ржд ржирж┐рз░рзНржорж┐ржд рж╕ржВрж▓рж╛ржк ржЙрждрзНржкрж╛ржжржи ржоржбрзЗрж▓, ржпрж┐ ржПржХржХ-ржЪрзЗржирзЗрж▓ ржжрзБржЬржи ржмрзНржпржХрзНрждрж┐рз░ ржорзБржЦрз░ рж╕ржВрж▓рж╛ржк ржЙрждрзНржкрж╛ржжржи ржХрз░рж┐ржм ржкрж╛рз░рзЗред</td>\n      <td rowspan=\"2\"><a href=\"https://arxiv.org/abs/2507.09318\"><img src=\"https://img.shields.io/badge/arXiv-ржкрзЗржкрж╛рз░-COLOR.svg\"></a></td>\n      <td rowspan=\"2\"><a href=\"https://zipvoice-dialog.github.io\"><img src=\"https://img.shields.io/badge/GitHub.io-ржбрзЗржорзЛ_ржкрзГрж╖рзНржарж╛-blue?logo=Github&style=flat-square\"></a></td>\n    </tr>",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 3,
    "Content": "    <tr>\n      <td>ZipVoice-Dialog-Stereo</td>\n      <td>The stereo variant of ZipVoice-Dialog, enabling two-channel dialogue generation with each speaker assigned to a distinct channel.</td>\n    </tr>\n  </tbody>\n</table>\n\n## News\n\n**2025/07/14**: **ZipVoice-Dialog** and **ZipVoice-Dialog-Stereo**, two spoken dialogue generation models, are released. [![arXiv](https://img.shields.io/badge/arXiv-Paper-COLOR.svg)](https://arxiv.org/abs/2507.09318) [![demo page](https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square)](https://zipvoice-dialog.github.io)\n\n**2025/07/14**: **OpenDialog** dataset, a 6.8k-hour spoken dialogue dataset, is released. Download at [![hf](https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Dataset-yellow)](https://huggingface.co/datasets/k2-fsa/OpenDialog), [![ms](https://img.shields.io/badge/ModelScope-Dataset-blue?logo=data)](https://www.modelscope.cn/datasets/k2-fsa/OpenDialog). Check details at [![arXiv](https://img.shields.io/badge/arXiv-Paper-COLOR.svg)](https://arxiv.org/abs/2507.09318).\n\n**2025/06/16**: **ZipVoice** and **ZipVoice-Distill** are released. [![arXiv](https://img.shields.io/badge/arXiv-Paper-COLOR.svg)](https://arxiv.org/abs/2506.13053) [![demo page](https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square)](https://zipvoice.github.io)\n\n## Installation\n\n### 1. Clone the ZipVoice repository\n",
    "ContentSha": "qyykZIxy9KKTHUZalqgKJAhZ9ZS/CaAb/vbLISMH1+E=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "    <tr>\n      <td>ZipVoice-Dialog-Stereo</td>\n      <td>ZipVoice-Dialog рз░ рж╖рзНржЯрзЗрз░рж┐ржЕ' рз░рзВржк, ржпрж┐ ржжрзБржЯрж╛ ржЪрзЗржирзЗрж▓ржд рж╕ржВрж▓рж╛ржк рж╕рзГрж╖рзНржЯрж┐ ржХрз░рж┐ржмрж▓рзИ рж╕ржХрзНрж╖ржо, ржп'ржд ржкрзНрз░рждрж┐ржЯрзЛ ржмржХрзНрждрж╛ржХ ржкрзГржержХ ржЪрзЗржирзЗрж▓ржд ржирж┐ржпрзБржХрзНржд ржХрз░рж╛ рж╣рзЯред</td>\n    </tr>\n  </tbody>\n</table>\n\n## ржмрж╛рждрз░рж┐\n\n**рзирзжрзирзл/рзжрзн/рззрзк**: **ZipVoice-Dialog** ржЖрз░рзБ **ZipVoice-Dialog-Stereo**, ржжрзБржЯрж╛ ржорзМржЦрж┐ржХ рж╕ржВрж▓рж╛ржк рж╕рзГрж╖рзНржЯрж┐ ржХрз░рж╛рз░ ржоржбрзЗрж▓, ржорзБржХрзНржд ржХрз░рж╛ рж╣рзИржЫрзЗред [![arXiv](https://img.shields.io/badge/arXiv-Paper-COLOR.svg)](https://arxiv.org/abs/2507.09318) [![demo page](https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square)](https://zipvoice-dialog.github.io)\n\n**рзирзжрзирзл/рзжрзн/рззрзк**: **OpenDialog** ржбрж╛ржЯрж╛ржЫрзЗржЯ, рзм.рзо ржХрж┐ржШржирзНржЯрж╛рз░ ржорзМржЦрж┐ржХ рж╕ржВрж▓рж╛ржк ржбрж╛ржЯрж╛ржЫрзЗржЯ, ржорзБржХрзНржд ржХрз░рж╛ рж╣рзИржЫрзЗред ржбрж╛ржЙржирж▓'ржб ржХрз░ржХ [![hf](https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Dataset-yellow)](https://huggingface.co/datasets/k2-fsa/OpenDialog), [![ms](https://img.shields.io/badge/ModelScope-Dataset-blue?logo=data)](https://www.modelscope.cn/datasets/k2-fsa/OpenDialog)ред ржмрж┐рж╕рзНрждрж╛рж░рж┐ржд ржЪрж╛ржУржХ [![arXiv](https://img.shields.io/badge/arXiv-Paper-COLOR.svg)](https://arxiv.org/abs/2507.09318)ред\n\n**рзирзжрзирзл/рзжрзм/рззрзм**: **ZipVoice** ржЖрз░рзБ **ZipVoice-Distill** ржорзБржХрзНржд ржХрз░рж╛ рж╣рзИржЫрзЗред [![arXiv](https://img.shields.io/badge/arXiv-Paper-COLOR.svg)](https://arxiv.org/abs/2506.13053) [![demo page](https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square)](https://zipvoice.github.io)\n\n## рж╕ржВрж╕рзНржерж╛ржкржи\n\n### рзз. ZipVoice рз░рзЗржк'ржЬрж┐ржЯ'рз░рзА ржХрзНрж▓рзЛржи ржХрз░ржХ\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 4,
    "Content": "```bash\ngit clone https://github.com/k2-fsa/ZipVoice.git\n```",
    "ContentSha": "JJo1EP7bWO0BWMfBMdp5X937bp3+DWhl7nAOm71R7lA=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bash\ngit clone https://github.com/k2-fsa/ZipVoice.git\n```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 5,
    "Content": "\n### 2. (Optional) Create a Python virtual environment\n",
    "ContentSha": "SdfmTQw39ITwBrkeETcUaJ4CPFr6cQ+HCqumZ483/iY=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "\n### 2. (ржмрзИржХрж▓рзНржкрж┐ржХ) ржПржЯрж╛ Python ржнрж╛рз░рзНржЪрзБрз▒рзЗрж▓ ржкрз░рж┐рз▒рзЗрж╢ рж╕рзГрж╖рзНржЯрж┐ ржХрз░ржХ\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 6,
    "Content": "```bash\npython3 -m venv zipvoice\nsource zipvoice/bin/activate\n```",
    "ContentSha": "glR0Rdvd5rjEtSF3LycYEtxGB8VzT68abW/ywL40bxw=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bash\npython3 -m venv zipvoice\nsource zipvoice/bin/activate\n```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 7,
    "Content": "\n### 3. Install the required packages\n",
    "ContentSha": "97VTTWuamYvk4THPyO5Ex48XMRIdHtV0dBXABNHe3qQ=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "\n### рзй. ржкрзНрз░рзЯрзЛржЬржирзАрзЯ ржкрзЗржХрзЗржЬрж╕ржорзВрж╣ рж╕ржВрж╕рзНржерж╛ржкржи ржХрз░ржХ\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 8,
    "Content": "```bash\npip install -r requirements.txt\n```",
    "ContentSha": "TxMa9uJC0PmBOnm3/TRl4YDLNvSwCWaRNjyXpFhndHU=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bash\npip install -r requirements.txt\n```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 9,
    "Content": "\n### 4. Install k2 for training or efficient inference\n\n**k2 is necessary for training** and can speed up inference. Nevertheless, you can still use the inference mode of ZipVoice without installing k2.\n\n> **Note:**  Make sure to install the k2 version that matches your PyTorch and CUDA version. For example, if you are using pytorch 2.5.1 and CUDA 12.1, you can install k2 as follows:\n",
    "ContentSha": "it0kOYkTtKi6ipV/r+Px+BHDpullJx7ZxRNyR9UNc40=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "\n### 4. ржкрзНрз░рж╢рж┐ржХрзНрж╖ржг ржмрж╛ ржХрж╛рж░рзНржпржХрз░рзА ржЕржирзБржорж╛ржирз░ ржмрж╛ржмрзЗ k2 рж╕ржВрж╕рзНржерж╛ржкржи ржХрз░ржХ\n\n**k2 ржкрзНрз░рж╢рж┐ржХрзНрж╖ржгрз░ ржмрж╛ржмрзЗ ржкрзНрз░ржпрж╝рзЛржЬржирзАржпрж╝** ржЖрз░рзБ ржЕржирзБржорж╛ржирз░ ржЧрждрж┐ ржмрзГржжрзНржзрж┐ ржХрз░рж┐ржм ржкрж╛рз░рзЗред рждржерж╛ржкрж┐, k2 рж╕ржВрж╕рзНржерж╛ржкржи ржиржХрз░рж╛ржХрзИ ZipVoice рз░ ржЕржирзБржорж╛ржи ржорзЛржб ржмрзНржпрз▒рж╣рж╛рз░ ржХрз░рж┐ржм ржкрж╛рз░рзЗред\n\n> **ржЯрзЛржХрж╛:**  ржЖржкрзЛржирж╛рз░ PyTorch ржЖрз░рзБ CUDA рж╕ржВрж╕рзНржХрз░ржгрз░ рж╕рзИрждрзЗ ржорж┐рж▓ ржержХрж╛ k2 рж╕ржВрж╕рзНржХрз░ржг рж╕ржВрж╕рзНржерж╛ржкржи ржХрз░рж╛ ржирж┐рж╢рзНржЪрж┐ржд ржХрз░ржХред ржЙржжрж╛рж╣рз░ржгрж╕рзНржмрз░рзВржк, ржпржжрж┐ ржЖржкрзБржирж┐ pytorch 2.5.1 ржЖрз░рзБ CUDA 12.1 ржмрзНржпрз▒рж╣рж╛рз░ ржХрз░рж┐ржЫрзЗ, рждрзЗржирзНрждрзЗ k2 рждрж▓ржд ржжрж┐ржпрж╝рж╛ ржЕржирзБрж╕рз░рж┐ рж╕ржВрж╕рзНржерж╛ржкржи ржХрз░рж┐ржм ржкрж╛рз░рзЗ:\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 10,
    "Content": "```bash\npip install k2==1.24.4.dev20250208+cuda12.1.torch2.5.1 -f https://k2-fsa.github.io/k2/cuda.html\n```",
    "ContentSha": "ScZHk5YUmlVP9WL5ZgwHsvBVbwTCenfwKc3cJb6nrWo=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bash\npip install k2==1.24.4.dev20250208+cuda12.1.torch2.5.1 -f https://k2-fsa.github.io/k2/cuda.html\n```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 11,
    "Content": "\nPlease refer to https://k2-fsa.org/get-started/k2/ for details.\nUsers in China mainland can refer to https://k2-fsa.org/zh-CN/get-started/k2/.\n\n- To check the k2 installation:\n",
    "ContentSha": "rY1Zhq3aduWMw4XQSft+r2qBy7l8kqnHBs8cs4FGpZ4=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "\nржЕржирзБржЧрзНрз░рж╣ ржХрз░рж┐ https://k2-fsa.org/get-started/k2/ ржЪрж╛ржУржХ ржмрж┐рз▒рз░ржгрз░ ржмрж╛ржмрзЗред\nржЪрзАржи ржорзВрж▓ ржнрзВ-ржЦржгрзНржбрз░ ржмрзНржпрз▒рж╣рж╛рз░ржХрж╛рз░рзАрж╕ржХрж▓рзЗ https://k2-fsa.org/zh-CN/get-started/k2/ ржЪрж╛ржм ржкрж╛рз░рзЗред\n\n- k2 рж╕ржВрж╕рзНржерж╛ржкржи ржкрз░рзАржХрзНрж╖рж╛ ржХрз░рж┐ржмрж▓рзИ:\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 12,
    "Content": "```bash\npython3 -c \"import k2; print(k2.__file__)\"\n```",
    "ContentSha": "PH4E7oc9yWRIVi2n1L4BiFuChpCHjHuJ65gAwNBxvHE=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bash\npython3 -c \"import k2; print(k2.__file__)\"\n```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 13,
    "Content": "\n## Usage\n\n### 1. Single-speaker speech generation\n\nTo generate single-speaker speech with our pre-trained ZipVoice or ZipVoice-Distill models, use the following commands (Required models will be downloaded from HuggingFace):\n\n#### 1.1 Inference of a single sentence\n",
    "ContentSha": "B4h5YyIywYfr2neMM1PwmS6nY8/eY48N4jkDeP3wG+Y=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "\n## ржмрзНржпрз▒рж╣рж╛рз░\n\n### 1. ржПржХржХ ржмржХрзНрждрж╛рз░ ржмржХрзНрждржмрзНржп ржЙрждрзНржкрж╛ржжржи\n\nржЖржорж╛рж▓рзИ ржЙржкрж▓ржнрзНржп ZipVoice ржмрж╛ ZipVoice-Distill ржоржбрзЗрж▓рз░ рж╕рж╣рж╛ржпрж╝ржд ржПржХржХ ржмржХрзНрждрж╛рз░ ржмржХрзНрждржмрзНржп ржЙрждрзНржкрж╛ржжржи ржХрз░рж┐ржмрж▓рзИ, рждрж▓ржд ржжрж┐ржпрж╝рж╛ ржЖржжрзЗрж╢рж╕ржорзВрж╣ ржмрзНржпрз▒рж╣рж╛рз░ ржХрз░ржХ (ржкрзНрз░ржпрж╝рзЛржЬржирзАржпрж╝ ржоржбрзЗрж▓рж╕ржорзВрж╣ HuggingFace рз░ ржкрз░рж╛ ржбрж╛ржЙржирж▓рзЛржб ржХрз░рж╛ рж╣'ржм):\n\n#### 1.1 ржПржЯрж╛ ржмрж╛ржХрзНржпрз░ ржЗржиржлрж╛рз░рзЗржирзНрж╕\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 14,
    "Content": "```bash\npython3 -m zipvoice.bin.infer_zipvoice \\\n    --model-name zipvoice \\\n    --prompt-wav prompt.wav \\\n    --prompt-text \"I am the transcription of the prompt wav.\" \\\n    --text \"I am the text to be synthesized.\" \\\n    --res-wav-path result.wav\n```",
    "ContentSha": "s3L0IUGcm9ppsQesBx8AKaxAjpRhRDaqb8rE/HLRinc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bash\npython3 -m zipvoice.bin.infer_zipvoice \\\n    --model-name zipvoice \\\n    --prompt-wav prompt.wav \\\n    --prompt-text \"I am the transcription of the prompt wav.\" \\\n    --text \"I am the text to be synthesized.\" \\\n    --res-wav-path result.wav\n```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 15,
    "Content": "- `--model-name` can be `zipvoice` or `zipvoice_distill`, which are models before and after distillation, respectively.\n- If `<>` or `[]` appear in the text, strings enclosed by them will be treated as special tokens. `<>` denotes Chinese pinyin and `[]` denotes other special tags.\n\n#### 1.2 Inference of a list of sentences\n",
    "ContentSha": "9gu4tqRbp3LNcYk2S6twKEix9A9CPCJP4IQ8qVja+jw=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "- `--model-name` рж╣рж┐ржЪрж╛ржкрзЗ `zipvoice` ржмрж╛ `zipvoice_distill` ржмрзНржпрз▒рж╣рж╛рз░ ржХрз░рж┐ржм ржкрж╛рз░рж┐, ржпрж┐ржмрзЛрз░ ржпржерж╛ржХрзНрз░ржорзЗ distillationрз░ ржЖржЧрз░ ржЖрз░рзБ ржкрж╛ржЫрз░ ржоржбрзЗрж▓ред\n- ржпржжрж┐ ржкрж╛ржаржд `<>` ржмрж╛ `[]` ржжрзЗржЦрж╛ ржпрж╛рзЯ, рждрзЗржирзНрждрзЗ ржЗрзЯрж╛рз░рзЗ ржЖржмржжрзНржз strings ржмрж┐рж╢рзЗрж╖ ржЯрзЛржХрзЗржи рж╣рж┐ржЪрж╛ржкрзЗ ржЧржгрзНржп ржХрз░рж╛ рж╣тАЩржмред `<>` ржЪрж╛ржЗржирзАржЬ ржкрж┐ржиржЗржирз░ ржмрж╛ржмрзЗ ржЖрз░рзБ `[]` ржЖржи ржмрж┐рж╢рзЗрж╖ ржЯрзЗржЧрз░ ржмрж╛ржмрзЗ ржмрзНржпрз▒рж╣рж╛рз░ рж╣рзЯред\n\n#### 1.2 ржмрж╛ржХрзНржпрз░ рждрж╛рж▓рж┐ржХрж╛ inference\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 16,
    "Content": "```bash\npython3 -m zipvoice.bin.infer_zipvoice \\\n    --model-name zipvoice \\\n    --test-list test.tsv \\\n    --res-dir results\n```",
    "ContentSha": "wVKNQBx9Qf3wuIvCUTrQwDyZzDuqDpC7W9a1psJg5Ds=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bash\npython3 -m zipvoice.bin.infer_zipvoice \\\n    --model-name zipvoice \\\n    --test-list test.tsv \\\n    --res-dir results\n```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 17,
    "Content": "\n- Each line of `test.tsv` is in the format of `{wav_name}\\t{prompt_transcription}\\t{prompt_wav}\\t{text}`.\n\n### 2. Dialogue speech generation\n\n#### 2.1 Inference command\n\nTo generate two-party spoken dialogues with our pre-trained ZipVoice-Dialogue or ZipVoice-Dialogue-Stereo models, use the following commands (Required models will be downloaded from HuggingFace):\n",
    "ContentSha": "bkRixLiKF8JLzfAqriyk1UZSkb5qCPhJVvI3VACgZos=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "\n- `test.tsv`-рз░ ржкрзНрз░рждрзНржпрзЗржХржЯрзЛ рж╢рж╛рз░рзА `{wav_name}\\t{prompt_transcription}\\t{prompt_wav}\\t{text}` рз░рзВржкржд ржерж╛ржХрзЗред\n\n### 2. рж╕ржВрж▓рж╛ржк ржмржХрзНрждржмрзНржп ржЙрзОржкрж╛ржжржи\n\n#### 2.1 ржЕржирзБржорж╛ржи ржЖржжрзЗрж╢\n\nржЖржорж╛рз░ ржкрзВрз░рзНржм-ржкрзНрз░рж╢рж┐ржХрзНрж╖рж┐ржд ZipVoice-Dialogue ржЕржержмрж╛ ZipVoice-Dialogue-Stereo ржоржбрзЗрж▓рз░ рж╕рж╣рж╛ржпрж╝ржд ржжрзБржЬржи ржмрзНржпржХрзНрждрж┐рз░ рж╕ржВрж▓рж╛ржк ржмржХрзНрждржмрзНржп ржЙрзОржкрж╛ржжржи ржХрз░рж┐ржмрж▓рзИ, рждрж▓ржд ржжрж┐ржпрж╝рж╛ ржЖржжрзЗрж╢рж╕ржорзВрж╣ ржмрзНржпрз▒рж╣рж╛рз░ ржХрз░ржХ (ржкрзНрз░ржпрж╝рзЛржЬржирзАржпрж╝ ржоржбрзЗрж▓рж╕ржорзВрж╣ HuggingFace-рз░ ржкрз░рж╛ ржбрж╛ржЙржирж▓'ржб рж╣ржм):\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 18,
    "Content": "```bash\npython3 -m zipvoice.bin.infer_zipvoice_dialog \\\n    --model-name \"zipvoice_dialog\" \\\n    --test-list test.tsv \\\n    --res-dir results\n```",
    "ContentSha": "SmNrjO7IvCsVTs0ROGG3evCMgCtj54DYGkGCZbRdz8k=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bash\npython3 -m zipvoice.bin.infer_zipvoice_dialog \\\n    --model-name \"zipvoice_dialog\" \\\n    --test-list test.tsv \\\n    --res-dir results\n```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 19,
    "Content": "\n- `--model-name` can be `zipvoice_dialog` or `zipvoice_dialog_stereo`,\n    which generate mono and stereo dialogues, respectively.\n\n#### 2.2 Input formats\n\nEach line of `test.tsv` is in one of the following formats:\n\n(1) **Merged prompt format** where the audios and transcriptions of two speakers prompts are merged into one prompt wav file:",
    "ContentSha": "e336Qt1qFvFmNefniyPEWJue5A1mLBZUSlD6p1+H8To=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "\n- `--model-name` рж╣рж┐ржЪрж╛ржкрзЗ `zipvoice_dialog` ржЕржержмрж╛ `zipvoice_dialog_stereo` ржерж╛ржХрж┐ржм ржкрж╛рз░рзЗ,\n    ржпрж┐ржпрж╝рзЗ ржЕржирзБрж╕рз░рж┐ ржорзЛржирзЛ ржЖрз░рзБ рж╖рзНржЯрзЗрз░рж┐ржЕ' рж╕ржВрж▓рж╛ржк ржЙрждрзНржкрж╛ржжржи ржХрз░рзЗред\n\n#### 2.2 ржЗржиржкрзБржЯ ржлрз░рзНржорзЗржЯрж╕ржорзВрж╣\n\n`test.tsv`-рз░ ржкрзНрз░рждрзНржпрзЗржХржЯрж╛ рж╢рж╛рз░рзА рждрж▓ржд ржжрж┐ржпрж╝рж╛ ржлрз░рзНржорзЗржЯрж╕ржорзВрж╣рз░ ржЕржирзНржпрждржоржд ржерж╛ржХрзЗ:\n\n(1) **Merged prompt format** ржп'ржд ржжрзБржЬржи ржмржХрзНрждрж╛рз░ ржЕржбрж┐ржЕ' ржЖрз░рзБ ржкрзНрз░рждрж┐рж▓рж┐ржкрж┐ ржПржХрзЗрж▓ржЧ ржХрз░рж┐ ржПржЯрж╛ ржкрзНрз░ржорзНржкржЯ рз▒рзЗржн ржлрж╛ржЗрж▓ржд рж╕ржВржпрзБржХрзНржд ржХрз░рж╛ рж╣ржпрж╝:",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 20,
    "Content": "```\n{wav_name}\\t{prompt_transcription}\\t{prompt_wav}\\t{text}\n```",
    "ContentSha": "F8c2S4lpByZ5Nhd693ESYvOeDT7lT7vF2Txm3q64ync=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```\n{wav_name}\\t{prompt_transcription}\\t{prompt_wav}\\t{text}\n```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 21,
    "Content": "\n- `wav_name` is the name of the output wav file.\n- `prompt_transcription` is the transcription of the conversational prompt wav, e.g, \"[S1] Hello. [S2] How are you?\"\n- `prompt_wav` is the path to the prompt wav.\n- `text` is the text to be synthesized, e.g. \"[S1] I'm fine. [S2] What's your name? [S1] I'm Eric. [S2] Hi Eric.\"\n\n(2) **Splitted prompt format** where the audios and transciptions of two speakers exist in separate files:\n",
    "ContentSha": "Gj5W4GhLunSOhvyVf7uwdfnNL3DFgIeOvHB01tH9I/A=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "\n- `wav_name` рж╣рзИржЫрзЗ ржЖржЙржЯржкрзБржЯ wav ржлрж╛ржЗрж▓рз░ ржирж╛ржоред\n- `prompt_transcription` рж╣рзИржЫрзЗ ржХржерзЛржкржХржержирз░ ржкрзНрз░ржорзНржкржЯ wav-рз░ рж▓рж┐ржкрзНржпржирзНрждрз░ржг, ржпрзЗржирзЗ, \"[S1] Hello. [S2] How are you?\"\n- `prompt_wav` рж╣рзИржЫрзЗ ржкрзНрз░ржорзНржкржЯ wav-рз░ ржкржеред\n- `text` рж╣рзИржЫрзЗ рж╕ржВрж╢рзНрж▓рзЗрж╖ржг ржХрз░рж┐ржмрж▓ржЧрзАржпрж╝рж╛ ржкрж╛ржарзНржп, ржпрзЗржирзЗ \"[S1] I'm fine. [S2] What's your name? [S1] I'm Eric. [S2] Hi Eric.\"\n\n(2) **Splitted prompt format** ржп'ржд ржжрзБржЯрж╛ ржмржХрзНрждрж╛рз░ ржЕржбрж┐ржЕ' ржЖрз░рзБ рж▓рж┐ржкрзНржпржирзНрждрз░ржг ржкрзГржержХ ржлрж╛ржЗрж▓ржд ржерж╛ржХрзЗ:\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 22,
    "Content": "```\n{wav_name}\\t{spk1_prompt_transcription}\\t{spk2_prompt_transcription}\\t{spk1_prompt_wav}\\t{spk2_prompt_wav}\\t{text}\n```",
    "ContentSha": "zPaMLy5mnnAP5WeOve+uEMlDenRN6Anuru4V4waQX9w=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```\n{wav_name}\\t{spk1_prompt_transcription}\\t{spk2_prompt_transcription}\\t{spk1_prompt_wav}\\t{spk2_prompt_wav}\\t{text}\n```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 23,
    "Content": "\n- `wav_name` is the name of the output wav file.\n- `spk1_prompt_transcription` is the transcription of the first speaker's prompt wav, e.g, \"Hello\"\n- `spk2_prompt_transcription` is the transcription of the second speaker's prompt wav, e.g, \"How are you?\"\n- `spk1_prompt_wav` is the path to the first speaker's prompt wav file.\n- `spk2_prompt_wav` is the path to the second speaker's prompt wav file.\n- `text` is the text to be synthesized, e.g. \"[S1] I'm fine. [S2] What's your name? [S1] I'm Eric. [S2] Hi Eric.\"\n\n### 3 Guidance for better usage:\n\n#### 3.1 Prompt length\n\nWe recommand a short prompt wav file (e.g., less than 3 seconds for single-speaker speech generation, less than 10 seconds for dialogue speech generation) for faster inference speed. A very long prompt will slow down the inference and degenerate the speech quality.\n\n#### 3.2 Speed optimization\n\nIf the inference speed is unsatisfactory, you can speed it up as follows:\n\n- **Distill model and less steps**: For the single-speaker speech generation model, we use the `zipvoice` model by default for better speech quality. If faster speed is a priority, you can switch to the `zipvoice_distill` and can reduce the `--num-steps` to as low as `4` (8 by default).\n\n- **CPU speedup with multi-threading**: When running on CPU, you can pass the `--num-thread` parameter (e.g., `--num-thread 4`) to increase the number of threads for faster speed. We use 1 thread by default.\n\n- **CPU speedup with ONNX**: When running on CPU, you can use ONNX models with `zipvoice.bin.infer_zipvoice_onnx` for faster speed (haven't supported ONNX for dialogue generation models yet). For even faster speed, you can further set `--onnx-int8 True` to use an INT8-quantized ONNX model. Note that the quantized model will result in a certain degree of speech quality degradation. **Don't use ONNX on GPU**, as it is slower than PyTorch on GPU.\n\n- **GPU Acceleration with NVIDIA TensorRT**: For a significant performance boost on NVIDIA GPUs, first export the model to a TensorRT engine using zipvoice.bin.tensorrt_export. Then, run inference on your dataset (e.g., a Hugging Face dataset) with zipvoice.bin.infer_zipvoice. This can achieve approximately 2x the throughput compared to the standard PyTorch implementation on a GPU.\n\n#### 3.3 Memory control\n\nThe given text will be splitted into chunks based on punctuation (for single-speaker speech generation) or speaker-turn symbol (for dialogue speech generation). Then, the chunked texts will be processed in batches. Therefore, the model can process arbitrarily long text with almost constant memory usage. You can control memory usage by adjusting the `--max-duration` parameter.\n\n#### 3.4 \"Raw\" evaluation\n\nBy default, we preprocess inputs (prompt wav, prompt transcription, and text) for efficient inference and better performance. If you want to evaluate the modelтАЩs \"raw\" performance using exact provided inputs (e.g., to reproduce the results in our paper), you can pass `--raw-evaluation True`.\n\n#### 3.5 Short text\n\nWhen generating speech for very short texts (e.g., one or two words), the generated speech may sometimes omit certain pronunciations. To resolve this issue, you can pass `--speed 0.3` (where 0.3 is a tunable value) to extend the duration of the generated speech.\n\n#### 3.6 Correcting mispronounced chinese polyphone characters\n",
    "ContentSha": "6AwuUDJOteKl74OkYkXg6kAf+AJPVnWbItdRfk762Xs=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "- `wav_name` рж╣рзИржЫрзЗ ржЖржЙржЯржкрзБржЯ рз▒рзЗржн ржлрж╛ржЗрж▓рз░ ржирж╛ржоред\n- `spk1_prompt_transcription` рж╣рзИржЫрзЗ ржкрзНрз░ржержо ржмржХрзНрждрж╛рз░ ржкрзНрз░ржорзНржкрзНржЯ рз▒рзЗржнрз░ рж▓рж┐ржЦржирж┐, ржЙржжрж╛рж╣рз░ржгрж╕рзНржмрз░рзВржк, \"Hello\"\n- `spk2_prompt_transcription` рж╣рзИржЫрзЗ ржжрзНржмрж┐рждрзАржпрж╝ ржмржХрзНрждрж╛рз░ ржкрзНрз░ржорзНржкрзНржЯ рз▒рзЗржнрз░ рж▓рж┐ржЦржирж┐, ржЙржжрж╛рж╣рз░ржгрж╕рзНржмрз░рзВржк, \"How are you?\"\n- `spk1_prompt_wav` рж╣рзИржЫрзЗ ржкрзНрз░ржержо ржмржХрзНрждрж╛рз░ ржкрзНрз░ржорзНржкрзНржЯ рз▒рзЗржн ржлрж╛ржЗрж▓рз░ ржкржеред\n- `spk2_prompt_wav` рж╣рзИржЫрзЗ ржжрзНржмрж┐рждрзАржпрж╝ ржмржХрзНрждрж╛рз░ ржкрзНрз░ржорзНржкрзНржЯ рз▒рзЗржн ржлрж╛ржЗрж▓рз░ ржкржеред\n- `text` рж╣рзИржЫрзЗ рж╕ржВрж╢рзНрж▓рж┐рж╖рзНржЯ ржЯрзЗржХрзНрж╕ржЯ ржпрж┐ ржЪрж┐ржирзНржерзЗржЯрж╛ржЗржЬ ржХрз░рж╛ рж╣'ржм, ржЙржжрж╛рж╣рз░ржгрж╕рзНржмрз░рзВржк, \"[S1] I'm fine. [S2] What's your name? [S1] I'm Eric. [S2] Hi Eric.\"\n\n### 3 ржЙржирзНржиржд ржмрзНржпрз▒рж╣рж╛рз░рз░ ржмрж╛ржмрзЗ ржирж┐рз░рзНржжрзЗрж╢ржирж╛:\n\n#### 3.1 ржкрзНрз░ржорзНржкрзНржЯрз░ ржжрзИрз░рзНржШрзНржп\n\nржЖржорж┐ ржПржЯрж╛ ржЪрзБржЯрж┐ ржкрзНрз░ржорзНржкрзНржЯ рз▒рзЗржн ржлрж╛ржЗрж▓ (ржЙржжрж╛рж╣рз░ржгрж╕рзНржмрз░рзВржк, ржПржХржмржХрзНрждрж╛ ржмржХрзНрждрзГрждрж╛ рж╕рзГрж╖рзНржЯрж┐ ржмрж╛ржмрзЗ рзй ржЫрзЗржХрзЗржгрзНржбрз░ ржХржо, рж╕ржВрж▓рж╛ржк ржмржХрзНрждрзГрждрж╛ рж╕рзГрж╖рзНржЯрж┐ ржмрж╛ржмрзЗ рззрзж ржЫрзЗржХрзЗржгрзНржбрз░ ржХржо) ржмрзНржпрз▒рж╣рж╛рз░рз░ ржкрз░рж╛ржорз░рзНрж╢ ржжрж┐ржУржБ ржЕржзрж┐ржХ ржжрзНрз░рзБржд inference рж╕рзНржкрж┐ржбрз░ ржмрж╛ржмрзЗред ржЕрждрзНржпржзрж┐ржХ ржжрзАржШрж▓ ржкрзНрз░ржорзНржкрзНржЯрзЗ inference ржоржирзНржерз░ ржХрз░рзЗ ржЖрз░рзБ ржмржХрзНрждрзГрждрж╛рз░ ржЧрзБржгржорж╛ржи ржХржорж╛ржЗ ржжрж┐ржпрж╝рзЗред\n\n#### 3.2 ржЧрждрж┐ ржЕржкрзНржЯрж┐ржорж╛ржЗржЬрзЗржЪржи\n\nржпржжрж┐ inference рж╕рзНржкрж┐ржб рж╕ржирзНрждрзЛрж╖ржЬржиржХ ржирж╣ржпрж╝, рждрзЗржирзНрждрзЗ рждрж▓рз░ржжрз░рзЗ ржЧрждрж┐ ржмржврж╝рж╛ржм ржкрж╛рз░рж┐:\n\n- **Distill ржоржбрзЗрж▓ ржЖрз░рзБ ржХржо рж╖рзНржЯрзЗржкржЫ**: ржПржХржмржХрзНрждрж╛ ржмржХрзНрждрзГрждрж╛ рж╕рзГрж╖рзНржЯрж┐ ржоржбрзЗрж▓рз░ ржмрж╛ржмрзЗ, ржЖржорж┐ speech ржЧрзБржгржорж╛ржи ржЙржирзНржиржд ржХрз░рж╛рз░ ржмрж╛ржмрзЗ `zipvoice` ржоржбрзЗрж▓ ржбрж┐ржлрж▓рзНржЯ рж╣рж┐ржЪрж╛ржкрзЗ ржмрзНржпрз▒рж╣рж╛рз░ ржХрз░рзЛред ржпржжрж┐ ржЧрждрж┐ ржЕржЧрзНрз░рж╛ржзрж┐ржХрж╛рз░, рждрзЗржирзНрждрзЗ ржЖржкрзБржирж┐ `zipvoice_distill` рж▓рзИ рж╕рж▓ржирж┐ ржХрз░рж┐ржм ржкрж╛рз░рзЗ ржЖрз░рзБ `--num-steps` ржХржорж╛ржЗ рзк (ржбрж┐ржлрж▓рзНржЯ рзо) рж▓рзИ ржЖржирж┐ржм ржкрж╛рз░рзЗред\n\n- **CPU рж╕рзНржкрж┐ржбржЖржк ржорж╛рж▓рзНржЯрж┐-ржерзНрз░рзЗржбрж┐ржЩрз░ рж╕рзИрждрзЗ**: CPU-ржд ржЪрж▓рж╛ржЗ ржерж╛ржХрж┐рж▓рзЗ, ржЖржкрзБржирж┐ `--num-thread` ржкрзЗрз░рж╛ржорж┐ржЯрж╛рз░ (ржЙржжрж╛рж╣рз░ржгрж╕рзНржмрз░рзВржк, `--num-thread 4`) ржжрж┐ ржерзНрз░рзЗржбрз░ рж╕ржВржЦрзНржпрж╛ ржмрзГржжрзНржзрж┐ ржХрз░рж┐ ржЧрждрж┐ ржмржврж╝рж╛ржм ржкрж╛рз░рзЗред ржЖржорж┐ ржбрж┐ржлрж▓рзНржЯржнрж╛рз▒рзЗ рзз ржерзНрз░рзЗржб ржмрзНржпрз▒рж╣рж╛рз░ ржХрз░рзЛред\n\n- **CPU рж╕рзНржкрж┐ржбржЖржк ONNXрз░ рж╕рзИрждрзЗ**: CPU-ржд ржЪрж▓рж╛ржЗ ржерж╛ржХрж┐рж▓рзЗ, ржЖржкрзБржирж┐ ONNX ржоржбрзЗрж▓рж╕ржорзВрж╣ `zipvoice.bin.infer_zipvoice_onnx`рз░ рж╕рзИрждрзЗ ржмрзНржпрз▒рж╣рж╛рз░ ржХрз░рж┐ржм ржкрж╛рз░рзЗ ржЕржзрж┐ржХ ржЧрждрж┐рз░ ржмрж╛ржмрзЗ (рж╕ржВрж▓рж╛ржк рж╕рзГрж╖рзНржЯрж┐ ржоржбрзЗрж▓рз░ ржмрж╛ржмрзЗ ONNX ржПрждрж┐ржпрж╝рж╛ржУ рж╕ржорз░рзНржерж┐ржд ржирж╣ржпрж╝)ред ржЕржзрж┐ржХ ржЧрждрж┐рз░ ржмрж╛ржмрзЗ, ржЖржкрзБржирж┐ `--onnx-int8 True` ржжрж┐ INT8-quantized ONNX ржоржбрзЗрж▓ ржмрзНржпрз▒рж╣рж╛рз░ ржХрз░рж┐ржм ржкрж╛рз░рзЗред ржоржиржд рз░рж╛ржЦрж┐ржм, quantized ржоржбрзЗрж▓рзЗ ржмржХрзНрждрзГрждрж╛рз░ ржЧрзБржгржорж╛ржи ржХрж┐ржЫрзБ ржкрз░рж┐ржорж╛ржгрзЗ рж╣рзНрз░рж╛рж╕ ржХрз░рж┐ржмред **GPU-ржд ONNX ржмрзНржпрз▒рж╣рж╛рз░ ржиржХрз░рж┐ржм**, ржХрж╛рз░ржг ржЗ PyTorch-рждржХрзИ ржоржирзНржерз░ред\n\n- **GPU Acceleration NVIDIA TensorRTрз░ рж╕рзИрждрзЗ**: NVIDIA GPU-ржд ржЙрж▓рзНрж▓рзЗржЦржпрзЛржЧрзНржп ржкрж╛рз░ржлрз░рзНржорзЗржирзНрж╕ ржмржврж╝рж╛ржмрз░ ржмрж╛ржмрзЗ, ржкрзНрз░ржержорзЗ ржоржбрзЗрж▓ржЯрзЛ zipvoice.bin.tensorrt_export ржмрзНржпрз▒рж╣рж╛рз░ ржХрз░рж┐ TensorRT ржЗржЮрзНржЬрж┐ржирж▓рзИ ржПржХрзНрж╕ржкрз░рзНржЯ ржХрз░ржХред рждрж╛рз░ ржкрж┐ржЫржд, ржЖржкрзЛржирж╛рз░ ржбрзЗржЯрж╛ржЫрзЗржЯржд (ржЙржжрж╛рж╣рз░ржгрж╕рзНржмрз░рзВржк, Hugging Face ржбрзЗржЯрж╛ржЫрзЗржЯ) zipvoice.bin.infer_zipvoice ржмрзНржпрз▒рж╣рж╛рз░ ржХрз░рж┐ inference ржЪрж▓рж╛ржУржХред ржЗржпрж╝рж╛рз░ ржлрж▓ржд рж╕рж╛ржзрж╛рз░ржг PyTorch ржЗржоржкрзНрж▓рж┐ржорзЗржгрзНржЯрзЗрж╢ржирз░ рждрзБрж▓ржирж╛ржд ржкрзНрз░рж╛ржпрж╝ рзи ржЧрзБржг throughput ржкрж╛ржмред\n\n#### 3.3 ржорзЗржо'рз░рж┐ ржирж┐ржпрж╝ржирзНрждрзНрз░ржг\n\nржжрж┐ржпрж╝рж╛ ржЯрзЗржХрзНрж╕ржЯржЯрзЛ punctuation (ржПржХржмржХрзНрждрж╛ ржмржХрзНрждрзГрждрж╛ рж╕рзГрж╖рзНржЯрж┐) ржЕржержмрж╛ speaker-turn ржЪрж┐рж╣рзНржи (рж╕ржВрж▓рж╛ржк ржмржХрзНрждрзГрждрж╛ рж╕рзГрж╖рзНржЯрж┐) ржЕржирзБрж╕рз░рж┐ ржнрж╛ржЧ ржХрз░рж╛ рж╣'ржмред рждрж╛рз░ ржкрж┐ржЫржд, ржнрж╛ржЧ ржХрз░рж╛ ржЯрзЗржХрзНрж╕ржЯрж╕ржорзВрж╣ ржмрзЗржЪржд ржкрзНрз░рж╕рзЗржЫ ржХрз░рж╛ рж╣'ржмред рж╕рзЗржЗржХрж╛рз░ржгрзЗ, ржоржбрзЗрж▓ржЯрзЛ ржкрзНрз░рж╛ржпрж╝ рж╕рзНржерж╛ржпрж╝рзА ржорзЗржо'рз░рж┐ ржмрзНржпрз▒рж╣рж╛рз░рз░рзЗ ржпрж┐ржХрзЛржирзЛ ржжрзАржШрж▓ ржЯрзЗржХрзНрж╕ржЯ ржкрзНрз░рж╕рзЗржЫ ржХрз░рж┐ржм ржкрж╛рз░рзЗред ржЖржкрзБржирж┐ `--max-duration` ржкрзЗрз░рж╛ржорж┐ржЯрж╛рз░ рж╕ржоржирзНржмржпрж╝ ржХрз░рж┐ ржорзЗржо'рз░рж┐ ржмрзНржпрз▒рж╣рж╛рз░ ржирж┐ржпрж╝ржирзНрждрзНрз░ржг ржХрз░рж┐ржм ржкрж╛рз░рзЗред\n\n#### 3.4 \"Raw\" ржорзВрж▓рзНржпрж╛ржпрж╝ржи\n\nржбрж┐ржлрж▓рзНржЯржнрж╛рз▒рзЗ, ржЖржорж┐ ржЗржиржлрж╛рз░рзЗржирзНрж╕ ржЕржзрж┐ржХ ржХрж╛рж░рзНржпржХрз░рзА ржЖрз░рзБ ржЙрзОржХрзГрж╖рзНржЯ ржкрж╛рж░ржлрз░рзНржорзЗржирзНрж╕рз░ ржмрж╛ржмрзЗ ржЗржиржкрзБржЯрж╕ржорзВрж╣ (ржкрзНрз░ржорзНржкрзНржЯ рз▒рзЗржн, ржкрзНрз░ржорзНржкрзНржЯ рж▓рж┐ржЦржирж┐, ржЖрз░рзБ ржЯрзЗржХрзНрж╕ржЯ) ржкрзНрз░рж┐ржкрзНрз░ржЪрзЗржЫ ржХрз░рзЛред ржпржжрж┐ ржЖржкрзБржирж┐ ржоржбрзЗрж▓ржЯрзЛрз░ \"raw\" ржкрж╛рж░ржлрз░рзНржорзЗржирзНрж╕ ржарж┐ржХ ржжрж┐ржпрж╝рж╛ ржЗржиржкрзБржЯрж╕ржорзВрж╣рз░рзЗ ржорзВрж▓рзНржпрж╛ржпрж╝ржи ржХрз░рж┐ржм ржмрж┐ржЪрж╛рз░рзЗ (ржЙржжрж╛рж╣рз░ржгрж╕рзНржмрз░рзВржк, ржЖржорж╛рз░ ржХрж╛ржЧржЬржд ржлрж▓рж╛ржлрж▓ ржкрзБржирз░рзБрждрзНржкрж╛ржжржи ржХрз░рж┐ржмрж▓рзИ), рждрзЗржирзНрждрзЗ `--raw-evaluation True` ржмрзНржпрз▒рж╣рж╛рз░ ржХрз░рж┐ржм ржкрж╛рз░рзЗред\n\n#### 3.5 ржЪрзБржЯрж┐ ржЯрзЗржХрзНрж╕ржЯ\n\nржЕрждрж┐ ржЪрзБржЯрж┐ ржЯрзЗржХрзНрж╕ржЯ (ржЙржжрж╛рж╣рз░ржгрж╕рзНржмрз░рзВржк, ржПржЯрж╛ ржмрж╛ ржжрзБржЯрж╛ рж╢ржмрзНржж) рз░ ржмрж╛ржмрзЗ speech рж╕рзГрж╖рзНржЯрж┐ ржХрз░рзЛржБрждрзЗ, ржХрзЗрждрж┐ржпрж╝рж╛ржмрж╛ ржХрж┐ржЫрзБ ржЙржЪрзНржЪрж╛рз░ржг ржмрж╛ржж ржкрз░рж┐ржм ржкрж╛рз░рзЗред ржПржЗ рж╕ржорж╕рзНржпрж╛ рж╕ржорж╛ржзрж╛ржи ржХрз░рж┐ржмрж▓рзИ, ржЖржкрзБржирж┐ `--speed 0.3` (ржп'ржд рзж.рзй ржПржЯрж╛ ржЯрж┐ржЙржи ржХрз░рж┐ржм ржкрз░рж╛ ржорж╛ржи) ржжрж┐ speechрз░ ржжрзИрз░рзНржШрзНржп ржмржврж╝рж╛ржм ржкрж╛рз░рзЗред\n\n#### 3.6 ржнрзБрж▓ ржЙржЪрзНржЪрж╛рз░рж┐ржд ржЪрж╛ржЗржирзАржЬ ржкрж▓рж┐ржлрзЛржи ржЕржХрзНрж╖рз░ рж╕ржВрж╢рзЛржзржи\n\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "- `wav_name` рж╣рзИржЫрзЗ ржЖржЙржЯржкрзБржЯ рз▒рзЗржн ржлрж╛ржЗрж▓рз░ ржирж╛ржоред"
      },
      {
        "row": 2,
        "rowsha": "6vgIR6WfUAFmULhn8LhxFttPwZEo/YwGhN1DAECwHCQ=",
        "originContent": "- `wav_name` is the name of the output wav file.",
        "translatedContent": "- `spk1_prompt_transcription` рж╣рзИржЫрзЗ ржкрзНрз░ржержо ржмржХрзНрждрж╛рз░ ржкрзНрз░ржорзНржкрзНржЯ рз▒рзЗржнрз░ рж▓рж┐ржЦржирж┐, ржЙржжрж╛рж╣рз░ржгрж╕рзНржмрз░рзВржк, \"Hello\""
      },
      {
        "row": 3,
        "rowsha": "7BNq8UaBvTut4Ow/oJBAgIDTn3EwEZXK7mlUaYdFwqw=",
        "originContent": "- `spk1_prompt_transcription` is the transcription of the first speaker's prompt wav, e.g, \"Hello\"",
        "translatedContent": "- `spk2_prompt_transcription` рж╣рзИржЫрзЗ ржжрзНржмрж┐рждрзАржпрж╝ ржмржХрзНрждрж╛рз░ ржкрзНрз░ржорзНржкрзНржЯ рз▒рзЗржнрз░ рж▓рж┐ржЦржирж┐, ржЙржжрж╛рж╣рз░ржгрж╕рзНржмрз░рзВржк, \"How are you?\""
      },
      {
        "row": 4,
        "rowsha": "CES8w9dqVdkdJyOJBUVP282aaKeevVWB3d/+59TEsuk=",
        "originContent": "- `spk2_prompt_transcription` is the transcription of the second speaker's prompt wav, e.g, \"How are you?\"",
        "translatedContent": "- `spk1_prompt_wav` рж╣рзИржЫрзЗ ржкрзНрз░ржержо ржмржХрзНрждрж╛рз░ ржкрзНрз░ржорзНржкрзНржЯ рз▒рзЗржн ржлрж╛ржЗрж▓рз░ ржкржеред"
      },
      {
        "row": 5,
        "rowsha": "gXLLRf4BR7Xko2q2l4nK04KIs/L8CjvZ/UBQaP1+vck=",
        "originContent": "- `spk1_prompt_wav` is the path to the first speaker's prompt wav file.",
        "translatedContent": "- `spk2_prompt_wav` рж╣рзИржЫрзЗ ржжрзНржмрж┐рждрзАржпрж╝ ржмржХрзНрждрж╛рз░ ржкрзНрз░ржорзНржкрзНржЯ рз▒рзЗржн ржлрж╛ржЗрж▓рз░ ржкржеред"
      },
      {
        "row": 6,
        "rowsha": "oS1+heJwBnnDtA57WYtG6LbzxK79DOIeb8hwhZQwcDg=",
        "originContent": "- `spk2_prompt_wav` is the path to the second speaker's prompt wav file.",
        "translatedContent": "- `text` рж╣рзИржЫрзЗ рж╕ржВрж╢рзНрж▓рж┐рж╖рзНржЯ ржЯрзЗржХрзНрж╕ржЯ ржпрж┐ ржЪрж┐ржирзНржерзЗржЯрж╛ржЗржЬ ржХрз░рж╛ рж╣'ржм, ржЙржжрж╛рж╣рз░ржгрж╕рзНржмрз░рзВржк, \"[S1] I'm fine. [S2] What's your name? [S1] I'm Eric. [S2] Hi Eric.\""
      },
      {
        "row": 7,
        "rowsha": "M4Z2DDajNBdyF/JosIaDZ44oyZnjNA7lzfGzEpuoako=",
        "originContent": "- `text` is the text to be synthesized, e.g. \"[S1] I'm fine. [S2] What's your name? [S1] I'm Eric. [S2] Hi Eric.\"",
        "translatedContent": ""
      },
      {
        "row": 8,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "### 3 ржЙржирзНржиржд ржмрзНржпрз▒рж╣рж╛рз░рз░ ржмрж╛ржмрзЗ ржирж┐рз░рзНржжрзЗрж╢ржирж╛:"
      },
      {
        "row": 9,
        "rowsha": "SdDI3h73wOzKSM3kbrbNrmpigHGer7kumuaZsQgAeao=",
        "originContent": "### 3 Guidance for better usage:",
        "translatedContent": ""
      },
      {
        "row": 10,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "#### 3.1 ржкрзНрз░ржорзНржкрзНржЯрз░ ржжрзИрз░рзНржШрзНржп"
      },
      {
        "row": 11,
        "rowsha": "cVxukE6jyFFOxlNKI5ecOTo/suYYJ8hnYyW2XA2wg+o=",
        "originContent": "#### 3.1 Prompt length",
        "translatedContent": ""
      },
      {
        "row": 12,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "ржЖржорж┐ ржПржЯрж╛ ржЪрзБржЯрж┐ ржкрзНрз░ржорзНржкрзНржЯ рз▒рзЗржн ржлрж╛ржЗрж▓ (ржЙржжрж╛рж╣рз░ржгрж╕рзНржмрз░рзВржк, ржПржХржмржХрзНрждрж╛ ржмржХрзНрждрзГрждрж╛ рж╕рзГрж╖рзНржЯрж┐ ржмрж╛ржмрзЗ рзй ржЫрзЗржХрзЗржгрзНржбрз░ ржХржо, рж╕ржВрж▓рж╛ржк ржмржХрзНрждрзГрждрж╛ рж╕рзГрж╖рзНржЯрж┐ ржмрж╛ржмрзЗ рззрзж ржЫрзЗржХрзЗржгрзНржбрз░ ржХржо) ржмрзНржпрз▒рж╣рж╛рз░рз░ ржкрз░рж╛ржорз░рзНрж╢ ржжрж┐ржУржБ ржЕржзрж┐ржХ ржжрзНрз░рзБржд inference рж╕рзНржкрж┐ржбрз░ ржмрж╛ржмрзЗред ржЕрждрзНржпржзрж┐ржХ ржжрзАржШрж▓ ржкрзНрз░ржорзНржкрзНржЯрзЗ inference ржоржирзНржерз░ ржХрз░рзЗ ржЖрз░рзБ ржмржХрзНрждрзГрждрж╛рз░ ржЧрзБржгржорж╛ржи ржХржорж╛ржЗ ржжрж┐ржпрж╝рзЗред"
      },
      {
        "row": 13,
        "rowsha": "f19zq78QrLul7wiVSlCSojGS7qNEvtef9GFg6AA8eMY=",
        "originContent": "We recommand a short prompt wav file (e.g., less than 3 seconds for single-speaker speech generation, less than 10 seconds for dialogue speech generation) for faster inference speed. A very long prompt will slow down the inference and degenerate the speech quality.",
        "translatedContent": ""
      },
      {
        "row": 14,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "#### 3.2 ржЧрждрж┐ ржЕржкрзНржЯрж┐ржорж╛ржЗржЬрзЗржЪржи"
      },
      {
        "row": 15,
        "rowsha": "lpgNpm20ulCcTiEU/xfEVVgMZhjiQjymkdljF8dD/vw=",
        "originContent": "#### 3.2 Speed optimization",
        "translatedContent": ""
      },
      {
        "row": 16,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "ржпржжрж┐ inference рж╕рзНржкрж┐ржб рж╕ржирзНрждрзЛрж╖ржЬржиржХ ржирж╣ржпрж╝, рждрзЗржирзНрждрзЗ рждрж▓рз░ржжрз░рзЗ ржЧрждрж┐ ржмржврж╝рж╛ржм ржкрж╛рз░рж┐:"
      },
      {
        "row": 17,
        "rowsha": "iBJxMfYOjV9HvSuRT3p/EsU/iATeDCDAk/wGWLXqQI8=",
        "originContent": "If the inference speed is unsatisfactory, you can speed it up as follows:",
        "translatedContent": ""
      },
      {
        "row": 18,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "- **Distill ржоржбрзЗрж▓ ржЖрз░рзБ ржХржо рж╖рзНржЯрзЗржкржЫ**: ржПржХржмржХрзНрждрж╛ ржмржХрзНрждрзГрждрж╛ рж╕рзГрж╖рзНржЯрж┐ ржоржбрзЗрж▓рз░ ржмрж╛ржмрзЗ, ржЖржорж┐ speech ржЧрзБржгржорж╛ржи ржЙржирзНржиржд ржХрз░рж╛рз░ ржмрж╛ржмрзЗ `zipvoice` ржоржбрзЗрж▓ ржбрж┐ржлрж▓рзНржЯ рж╣рж┐ржЪрж╛ржкрзЗ ржмрзНржпрз▒рж╣рж╛рз░ ржХрз░рзЛред ржпржжрж┐ ржЧрждрж┐ ржЕржЧрзНрз░рж╛ржзрж┐ржХрж╛рз░, рждрзЗржирзНрждрзЗ ржЖржкрзБржирж┐ `zipvoice_distill` рж▓рзИ рж╕рж▓ржирж┐ ржХрз░рж┐ржм ржкрж╛рз░рзЗ ржЖрз░рзБ `--num-steps` ржХржорж╛ржЗ рзк (ржбрж┐ржлрж▓рзНржЯ рзо) рж▓рзИ ржЖржирж┐ржм ржкрж╛рз░рзЗред"
      },
      {
        "row": 19,
        "rowsha": "IzTHxzS1e5yJRhHF5d8CsjfqjIzhNX1AeGR4FTjuUCA=",
        "originContent": "- **Distill model and less steps**: For the single-speaker speech generation model, we use the `zipvoice` model by default for better speech quality. If faster speed is a priority, you can switch to the `zipvoice_distill` and can reduce the `--num-steps` to as low as `4` (8 by default).",
        "translatedContent": ""
      },
      {
        "row": 20,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "- **CPU рж╕рзНржкрж┐ржбржЖржк ржорж╛рж▓рзНржЯрж┐-ржерзНрз░рзЗржбрж┐ржЩрз░ рж╕рзИрждрзЗ**: CPU-ржд ржЪрж▓рж╛ржЗ ржерж╛ржХрж┐рж▓рзЗ, ржЖржкрзБржирж┐ `--num-thread` ржкрзЗрз░рж╛ржорж┐ржЯрж╛рз░ (ржЙржжрж╛рж╣рз░ржгрж╕рзНржмрз░рзВржк, `--num-thread 4`) ржжрж┐ ржерзНрз░рзЗржбрз░ рж╕ржВржЦрзНржпрж╛ ржмрзГржжрзНржзрж┐ ржХрз░рж┐ ржЧрждрж┐ ржмржврж╝рж╛ржм ржкрж╛рз░рзЗред ржЖржорж┐ ржбрж┐ржлрж▓рзНржЯржнрж╛рз▒рзЗ рзз ржерзНрз░рзЗржб ржмрзНржпрз▒рж╣рж╛рз░ ржХрз░рзЛред"
      },
      {
        "row": 21,
        "rowsha": "mNiqyHjFr4rbx4boH0cix2peH8Q0+tDTOlzgZeLLDqM=",
        "originContent": "- **CPU speedup with multi-threading**: When running on CPU, you can pass the `--num-thread` parameter (e.g., `--num-thread 4`) to increase the number of threads for faster speed. We use 1 thread by default.",
        "translatedContent": ""
      },
      {
        "row": 22,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "- **CPU рж╕рзНржкрж┐ржбржЖржк ONNXрз░ рж╕рзИрждрзЗ**: CPU-ржд ржЪрж▓рж╛ржЗ ржерж╛ржХрж┐рж▓рзЗ, ржЖржкрзБржирж┐ ONNX ржоржбрзЗрж▓рж╕ржорзВрж╣ `zipvoice.bin.infer_zipvoice_onnx`рз░ рж╕рзИрждрзЗ ржмрзНржпрз▒рж╣рж╛рз░ ржХрз░рж┐ржм ржкрж╛рз░рзЗ ржЕржзрж┐ржХ ржЧрждрж┐рз░ ржмрж╛ржмрзЗ (рж╕ржВрж▓рж╛ржк рж╕рзГрж╖рзНржЯрж┐ ржоржбрзЗрж▓рз░ ржмрж╛ржмрзЗ ONNX ржПрждрж┐ржпрж╝рж╛ржУ рж╕ржорз░рзНржерж┐ржд ржирж╣ржпрж╝)ред ржЕржзрж┐ржХ ржЧрждрж┐рз░ ржмрж╛ржмрзЗ, ржЖржкрзБржирж┐ `--onnx-int8 True` ржжрж┐ INT8-quantized ONNX ржоржбрзЗрж▓ ржмрзНржпрз▒рж╣рж╛рз░ ржХрз░рж┐ржм ржкрж╛рз░рзЗред ржоржиржд рз░рж╛ржЦрж┐ржм, quantized ржоржбрзЗрж▓рзЗ ржмржХрзНрждрзГрждрж╛рз░ ржЧрзБржгржорж╛ржи ржХрж┐ржЫрзБ ржкрз░рж┐ржорж╛ржгрзЗ рж╣рзНрз░рж╛рж╕ ржХрз░рж┐ржмред **GPU-ржд ONNX ржмрзНржпрз▒рж╣рж╛рз░ ржиржХрз░рж┐ржм**, ржХрж╛рз░ржг ржЗ PyTorch-рждржХрзИ ржоржирзНржерз░ред"
      },
      {
        "row": 23,
        "rowsha": "YqtmXdPz7OfUIbrIkeeIwyKEPdEZUqn3m5EyTNd967s=",
        "originContent": "- **CPU speedup with ONNX**: When running on CPU, you can use ONNX models with `zipvoice.bin.infer_zipvoice_onnx` for faster speed (haven't supported ONNX for dialogue generation models yet). For even faster speed, you can further set `--onnx-int8 True` to use an INT8-quantized ONNX model. Note that the quantized model will result in a certain degree of speech quality degradation. **Don't use ONNX on GPU**, as it is slower than PyTorch on GPU.",
        "translatedContent": ""
      },
      {
        "row": 24,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "- **GPU Acceleration NVIDIA TensorRTрз░ рж╕рзИрждрзЗ**: NVIDIA GPU-ржд ржЙрж▓рзНрж▓рзЗржЦржпрзЛржЧрзНржп ржкрж╛рз░ржлрз░рзНржорзЗржирзНрж╕ ржмржврж╝рж╛ржмрз░ ржмрж╛ржмрзЗ, ржкрзНрз░ржержорзЗ ржоржбрзЗрж▓ржЯрзЛ zipvoice.bin.tensorrt_export ржмрзНржпрз▒рж╣рж╛рз░ ржХрз░рж┐ TensorRT ржЗржЮрзНржЬрж┐ржирж▓рзИ ржПржХрзНрж╕ржкрз░рзНржЯ ржХрз░ржХред рждрж╛рз░ ржкрж┐ржЫржд, ржЖржкрзЛржирж╛рз░ ржбрзЗржЯрж╛ржЫрзЗржЯржд (ржЙржжрж╛рж╣рз░ржгрж╕рзНржмрз░рзВржк, Hugging Face ржбрзЗржЯрж╛ржЫрзЗржЯ) zipvoice.bin.infer_zipvoice ржмрзНржпрз▒рж╣рж╛рз░ ржХрз░рж┐ inference ржЪрж▓рж╛ржУржХред ржЗржпрж╝рж╛рз░ ржлрж▓ржд рж╕рж╛ржзрж╛рз░ржг PyTorch ржЗржоржкрзНрж▓рж┐ржорзЗржгрзНржЯрзЗрж╢ржирз░ рждрзБрж▓ржирж╛ржд ржкрзНрз░рж╛ржпрж╝ рзи ржЧрзБржг throughput ржкрж╛ржмред"
      },
      {
        "row": 25,
        "rowsha": "kj1A4DWWe02Utusq07KI3xRRH55QdxQWRCzFeimIzww=",
        "originContent": "- **GPU Acceleration with NVIDIA TensorRT**: For a significant performance boost on NVIDIA GPUs, first export the model to a TensorRT engine using zipvoice.bin.tensorrt_export. Then, run inference on your dataset (e.g., a Hugging Face dataset) with zipvoice.bin.infer_zipvoice. This can achieve approximately 2x the throughput compared to the standard PyTorch implementation on a GPU.",
        "translatedContent": ""
      },
      {
        "row": 26,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "#### 3.3 ржорзЗржо'рз░рж┐ ржирж┐ржпрж╝ржирзНрждрзНрз░ржг"
      },
      {
        "row": 27,
        "rowsha": "fze8iMUXPcPsZgNFyWFzWSuCffZnzh7SpzLs21tQLtE=",
        "originContent": "#### 3.3 Memory control",
        "translatedContent": ""
      },
      {
        "row": 28,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "ржжрж┐ржпрж╝рж╛ ржЯрзЗржХрзНрж╕ржЯржЯрзЛ punctuation (ржПржХржмржХрзНрждрж╛ ржмржХрзНрждрзГрждрж╛ рж╕рзГрж╖рзНржЯрж┐) ржЕржержмрж╛ speaker-turn ржЪрж┐рж╣рзНржи (рж╕ржВрж▓рж╛ржк ржмржХрзНрждрзГрждрж╛ рж╕рзГрж╖рзНржЯрж┐) ржЕржирзБрж╕рз░рж┐ ржнрж╛ржЧ ржХрз░рж╛ рж╣'ржмред рждрж╛рз░ ржкрж┐ржЫржд, ржнрж╛ржЧ ржХрз░рж╛ ржЯрзЗржХрзНрж╕ржЯрж╕ржорзВрж╣ ржмрзЗржЪржд ржкрзНрз░рж╕рзЗржЫ ржХрз░рж╛ рж╣'ржмред рж╕рзЗржЗржХрж╛рз░ржгрзЗ, ржоржбрзЗрж▓ржЯрзЛ ржкрзНрз░рж╛ржпрж╝ рж╕рзНржерж╛ржпрж╝рзА ржорзЗржо'рз░рж┐ ржмрзНржпрз▒рж╣рж╛рз░рз░рзЗ ржпрж┐ржХрзЛржирзЛ ржжрзАржШрж▓ ржЯрзЗржХрзНрж╕ржЯ ржкрзНрз░рж╕рзЗржЫ ржХрз░рж┐ржм ржкрж╛рз░рзЗред ржЖржкрзБржирж┐ `--max-duration` ржкрзЗрз░рж╛ржорж┐ржЯрж╛рз░ рж╕ржоржирзНржмржпрж╝ ржХрз░рж┐ ржорзЗржо'рз░рж┐ ржмрзНржпрз▒рж╣рж╛рз░ ржирж┐ржпрж╝ржирзНрждрзНрз░ржг ржХрз░рж┐ржм ржкрж╛рз░рзЗред"
      },
      {
        "row": 29,
        "rowsha": "uM67TExtHYq7ALHmglqtjLRqv0Xu0OOSx2aFJquZPmw=",
        "originContent": "The given text will be splitted into chunks based on punctuation (for single-speaker speech generation) or speaker-turn symbol (for dialogue speech generation). Then, the chunked texts will be processed in batches. Therefore, the model can process arbitrarily long text with almost constant memory usage. You can control memory usage by adjusting the `--max-duration` parameter.",
        "translatedContent": ""
      },
      {
        "row": 30,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "#### 3.4 \"Raw\" ржорзВрж▓рзНржпрж╛ржпрж╝ржи"
      },
      {
        "row": 31,
        "rowsha": "foa86E9JcH+Sc/k2OCmyfIKHwggsFBXhSUfHDcmJQA0=",
        "originContent": "#### 3.4 \"Raw\" evaluation",
        "translatedContent": ""
      },
      {
        "row": 32,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "ржбрж┐ржлрж▓рзНржЯржнрж╛рз▒рзЗ, ржЖржорж┐ ржЗржиржлрж╛рз░рзЗржирзНрж╕ ржЕржзрж┐ржХ ржХрж╛рж░рзНржпржХрз░рзА ржЖрз░рзБ ржЙрзОржХрзГрж╖рзНржЯ ржкрж╛рж░ржлрз░рзНржорзЗржирзНрж╕рз░ ржмрж╛ржмрзЗ ржЗржиржкрзБржЯрж╕ржорзВрж╣ (ржкрзНрз░ржорзНржкрзНржЯ рз▒рзЗржн, ржкрзНрз░ржорзНржкрзНржЯ рж▓рж┐ржЦржирж┐, ржЖрз░рзБ ржЯрзЗржХрзНрж╕ржЯ) ржкрзНрз░рж┐ржкрзНрз░ржЪрзЗржЫ ржХрз░рзЛред ржпржжрж┐ ржЖржкрзБржирж┐ ржоржбрзЗрж▓ржЯрзЛрз░ \"raw\" ржкрж╛рж░ржлрз░рзНржорзЗржирзНрж╕ ржарж┐ржХ ржжрж┐ржпрж╝рж╛ ржЗржиржкрзБржЯрж╕ржорзВрж╣рз░рзЗ ржорзВрж▓рзНржпрж╛ржпрж╝ржи ржХрз░рж┐ржм ржмрж┐ржЪрж╛рз░рзЗ (ржЙржжрж╛рж╣рз░ржгрж╕рзНржмрз░рзВржк, ржЖржорж╛рз░ ржХрж╛ржЧржЬржд ржлрж▓рж╛ржлрж▓ ржкрзБржирз░рзБрждрзНржкрж╛ржжржи ржХрз░рж┐ржмрж▓рзИ), рждрзЗржирзНрждрзЗ `--raw-evaluation True` ржмрзНржпрз▒рж╣рж╛рз░ ржХрз░рж┐ржм ржкрж╛рз░рзЗред"
      },
      {
        "row": 33,
        "rowsha": "+2nxKNvXmuxUQpf2Z+hw0Rxydt00FpmK4y4rlK5/8og=",
        "originContent": "By default, we preprocess inputs (prompt wav, prompt transcription, and text) for efficient inference and better performance. If you want to evaluate the modelтАЩs \"raw\" performance using exact provided inputs (e.g., to reproduce the results in our paper), you can pass `--raw-evaluation True`.",
        "translatedContent": ""
      },
      {
        "row": 34,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "#### 3.5 ржЪрзБржЯрж┐ ржЯрзЗржХрзНрж╕ржЯ"
      },
      {
        "row": 35,
        "rowsha": "g658opDssPKmJCvr7Jw9N130Xud1IbMHTwMK+S89WO0=",
        "originContent": "#### 3.5 Short text",
        "translatedContent": ""
      },
      {
        "row": 36,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "ржЕрждрж┐ ржЪрзБржЯрж┐ ржЯрзЗржХрзНрж╕ржЯ (ржЙржжрж╛рж╣рз░ржгрж╕рзНржмрз░рзВржк, ржПржЯрж╛ ржмрж╛ ржжрзБржЯрж╛ рж╢ржмрзНржж) рз░ ржмрж╛ржмрзЗ speech рж╕рзГрж╖рзНржЯрж┐ ржХрз░рзЛржБрждрзЗ, ржХрзЗрждрж┐ржпрж╝рж╛ржмрж╛ ржХрж┐ржЫрзБ ржЙржЪрзНржЪрж╛рз░ржг ржмрж╛ржж ржкрз░рж┐ржм ржкрж╛рз░рзЗред ржПржЗ рж╕ржорж╕рзНржпрж╛ рж╕ржорж╛ржзрж╛ржи ржХрз░рж┐ржмрж▓рзИ, ржЖржкрзБржирж┐ `--speed 0.3` (ржп'ржд рзж.рзй ржПржЯрж╛ ржЯрж┐ржЙржи ржХрз░рж┐ржм ржкрз░рж╛ ржорж╛ржи) ржжрж┐ speechрз░ ржжрзИрз░рзНржШрзНржп ржмржврж╝рж╛ржм ржкрж╛рз░рзЗред"
      },
      {
        "row": 37,
        "rowsha": "/IVkHehTtKgVQNwGOgQO/BoRh95RFHVJPH3e0W6Gixs=",
        "originContent": "When generating speech for very short texts (e.g., one or two words), the generated speech may sometimes omit certain pronunciations. To resolve this issue, you can pass `--speed 0.3` (where 0.3 is a tunable value) to extend the duration of the generated speech.",
        "translatedContent": ""
      },
      {
        "row": 38,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "#### 3.6 ржнрзБрж▓ ржЙржЪрзНржЪрж╛рз░рж┐ржд ржЪрж╛ржЗржирзАржЬ ржкрж▓рж┐ржлрзЛржи ржЕржХрзНрж╖рз░ рж╕ржВрж╢рзЛржзржи"
      },
      {
        "row": 39,
        "rowsha": "PAPz1JYDhpLF6dsiNH/BVipH4SufvLcqzLiPLACOcK4=",
        "originContent": "#### 3.6 Correcting mispronounced chinese polyphone characters",
        "translatedContent": ""
      },
      {
        "row": 40,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 24,
    "Content": "We use [pypinyin](https://github.com/mozillazg/python-pinyin) to convert Chinese characters to pinyin. However, it can occasionally mispronounce **polyphone characters** (хдЪщЯ│хнЧ).\n\nTo manually correct these mispronunciations, enclose the **corrected pinyin** in angle brackets `< >` and include the **tone mark**.\n\n**Example:**\n\n- Original text: `ш┐ЩцККхЙСщХ┐ф╕ЙхНБхЕмхИЖ`\n- Correct the pinyin of `щХ┐`:  `ш┐ЩцККхЙС<chang2>ф╕ЙхНБхЕмхИЖ`\n\n> **Note:** If you want to manually assign multiple pinyins, enclose each pinyin with `<>`, e.g., `ш┐ЩцКК<jian4><chang2><san1>хНБхЕмхИЖ`\n\n#### 3.7 Remove long silences from the generated speech\n\nModel will automatically determine the positions and lengths of silences in the generated speech. It occasionally has long silence in the middle of the speech. If you don't want this, you can pass `--remove-long-sil` to remove long silences in the middle of the generated speech (edge silences will be removed by default).\n\n#### 3.8 Model downloading\n\nIf you have trouble connecting to HuggingFace when downloading the pre-trained models, try switching endpoint to the mirror site: `export HF_ENDPOINT=https://hf-mirror.com`.\n\n## Train Your Own Model\n\nSee the [egs](egs) directory for training, fine-tuning and evaluation examples.\n\n## Production Deployment\n\n### NVIDIA Triton GPU Runtime\n\nFor production-ready deployment with high performance and scalability, check out the [Triton Inference Server integration](runtime/nvidia_triton/) that provides optimized TensorRT engines, concurrent request handling, and both gRPC/HTTP APIs for enterprise use.\n\n### CPU Deployment\n\nCheck [sherpa-onnx](https://github.com/k2-fsa/sherpa-onnx/pull/2487#issuecomment-3227884498) for the C++ deployment solution on CPU.\n\n## Discussion & Communication\n\nYou can directly discuss on [Github Issues](https://github.com/k2-fsa/ZipVoice/issues).\n\nYou can also scan the QR code to join our wechat group or follow our wechat official account.\n\n| Wechat Group | Wechat Official Account |",
    "ContentSha": "nAAjO+GVPZsjYiLFM/o02EX48i9vuDX4qL4j6+7om6U=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "ржЖржорж╛рз░ ржкрж┐ржиржЗржирж▓рзИ ржЪрж┐ржирж╛ ржЖржЦрз░ рз░рзВржкрж╛ржирзНрждрз░ ржХрз░рж┐ржмрж▓рзИ [pypinyin](https://github.com/mozillazg/python-pinyin) ржмрзНржпрз▒рж╣рж╛рз░ ржХрз░рж╛ рж╣ржпрж╝ред ржпржжрж┐ржУ, ржХрзЗрждрж┐ржпрж╝рж╛ржмрж╛ ржЗ **ржмрж╣рзБ-ржЙржЪрзНржЪрж╛рз░рж┐ржд ржЖржЦрз░** (хдЪщЯ│хнЧ) ржнрзБрж▓ржХрзИ ржЙржЪрзНржЪрж╛рз░ржг ржХрз░рж┐ржм ржкрж╛рз░рзЗред\n\nржПржЗ ржнрзБрж▓ ржЙржЪрзНржЪрж╛рз░ржгрж╕ржорзВрж╣ рж╣рж╛рждрзЗржжрж┐ рж╢рзБржжрзНржз ржХрз░рж┐ржмрж▓рзИ, **рж╢рзБржжрзНржз ржХрз░рж╛ ржкрж┐ржиржЗржи** ржХрзМржгрж┐ржХ ржмржирзНржзржирзА `< >`-ржд рз░рж╛ржЦржХ ржЖрз░рзБ **рж╕рзНржмрз░ ржЪрж┐рж╣рзНржи** рж╕ржВржпрзЛржЬржи ржХрз░ржХред\n\n**ржЙржжрж╛рж╣рз░ржг:**\n\n- ржорзВрж▓ ржкрж╛ржа: `ш┐ЩцККхЙСщХ┐ф╕ЙхНБхЕмхИЖ`\n- `щХ┐`-рз░ ржкрж┐ржиржЗржи рж╢рзБржжрзНржз ржХрз░ржХ:  `ш┐ЩцККхЙС<chang2>ф╕ЙхНБхЕмхИЖ`\n\n> **ржЯрзЛржХрж╛:** ржпржжрж┐ ржЖржкрзБржирж┐ ржмрж╣рзБ ржкрж┐ржиржЗржи ржирж┐ржЬрзЗ ржирж┐ржпрзБржХрзНржд ржХрз░рж┐ржм ржмрж┐ржЪрж╛рз░рзЗ, ржкрзНрз░рждрж┐ржЯрзЛ ржкрж┐ржиржЗржи `<>`-ржд рж╕ржВрж▓ржЧрзНржи ржХрз░ржХ, ржпрзЗржирзЗ, `ш┐ЩцКК<jian4><chang2><san1>хНБхЕмхИЖ`\n\n#### 3.7 рж╕рзГрж╖рзНржЯрж┐ ржХрз░рж╛ ржмржХрзНрждрзГрждрж╛рз░ ржкрз░рж╛ ржжрзАржШрж▓ ржирзАрз░рз▒рждрж╛ ржЖржБрждрз░рж╛ржУржХ\n\nржоржбрзЗрж▓рзЗ рж╕рзГрж╖рзНржЯрж┐ ржХрз░рж╛ ржмржХрзНрждрзГрждрж╛ржд ржирзАрз░рз▒рждрж╛рз░ рж╕рзНржерж╛ржи ржЖрз░рзБ ржжрзИрз░рзНржШрзНржп ржЖржкрзБржирж┐ ржирж┐рз░рзНржзрж╛рз░ржг ржХрз░рж┐ржмред ржХрзЗрждрж┐ржпрж╝рж╛ржмрж╛ ржмржХрзНрждрзГрждрж╛рз░ ржорж╛ржЬржд ржжрзАржШрж▓ ржирзАрз░рз▒рждрж╛ ржерж╛ржХрж┐ржм ржкрж╛рз░рзЗред ржЖржкрзБржирж┐ ржПржЗржЯрзЛ ржиржЪрж╛рж╣рж┐рж▓рзЗ, ржоржзрзНржпржнрж╛ржЧржд ржжрзАржШрж▓ ржирзАрз░рз▒рждрж╛ ржЖржБрждрз░рж╛ржмрж▓рзИ `--remove-long-sil` ржкрзНрз░ржжрж╛ржи ржХрз░рж┐ржм ржкрж╛рз░рзЗ (ржХрж╛рж╖рз░ ржирзАрз░рз▒рждрж╛ рж╕рзНржмрж╛ржнрж╛рз▒рж┐ржХржнрж╛рз▒рзЗ ржЖржБрждрз░ ржХрз░рж╛ рж╣тАЩржм)ред\n\n#### 3.8 ржоржбрзЗрж▓ ржбрж╛ржЙржирж▓рзЛржб ржХрз░рж╛\n\nржкрзНрз░рж┐-ржЯрзНрз░рзЗржЗржи ржХрз░рж╛ ржоржбрзЗрж▓рж╕ржорзВрж╣ ржбрж╛ржЙржирж▓рзЛржб ржХрз░рзЛржБрждрзЗ HuggingFace-рж▓рзИ рж╕ржВржпрзЛржЧржд рж╕ржорж╕рзНржпрж╛ рж╣тАЩрж▓рзЗ, endpoint-ржЯрзЛ ржорж┐рз░рз░ ржЫрж╛ржЗржЯрж▓рзИ рж╕рж▓ржирж┐ ржХрз░ржХ: `export HF_ENDPOINT=https://hf-mirror.com`ред\n\n## ржирж┐ржЬрз░ ржоржбрзЗрж▓ ржЯрзНрз░рзЗржЗржи ржХрз░ржХ\n\nржЯрзНрз░рзЗржЗржи, ржлрж╛ржЗржи-ржЯрж┐ржЙржи ржЖрз░рзБ ржорзВрж▓рзНржпрж╛рзЯржирз░ ржЙржжрж╛рж╣рз░ржгрж╕ржорзВрж╣рз░ ржмрж╛ржмрзЗ [egs](egs) ржбрж╛ржЗрз░рзЗржХрзНржЯрз░рзА ржЪрж╛ржУржХред\n\n## ржЙрзОржкрж╛ржжржи рж╕рзНржерж╛ржкржи\n\n### NVIDIA Triton GPU рз░рж╛ржиржЯрж╛ржЗржо\n\nржЙрзОржкрж╛ржжржи-ржкрзНрз░рж╕рзНрждрзБржд, ржЙржЪрзНржЪ ржХрж╛рз░рзНржпржХрзНрж╖ржорждрж╛ ржЖрз░рзБ рж╕рзНржХрзЗрж▓рзЗржмрж▓ рж╕рзНржерж╛ржкржирз░ ржмрж╛ржмрзЗ, [Triton Inference Server integration](runtime/nvidia_triton/) ржЪрж╛ржУржХ ржпрж┐ ржЕржкрзНржЯрж┐ржорж╛ржЗржЬ ржХрз░рж╛ TensorRT ржЗржЮрзНржЬрж┐ржи, ржПржХрж╛ржзрж┐ржХ ржЕржирзБрз░рзЛржз рж╣рзЗржгрзНржбрж▓рж┐ржВ, ржЖрз░рзБ ржПржгрзНржЯрж╛рз░ржкрзНрз░рж╛ржЗржЬ ржмрзНржпрз▒рж╣рж╛рз░рз░ ржмрж╛ржмрзЗ gRPC/HTTP API ржкрзНрз░ржжрж╛ржи ржХрз░рзЗред\n\n### CPU рж╕рзНржерж╛ржкржи\n\nCPU-ржд C++ рж╕рзНржерж╛ржкржи рж╕ржорж╛ржзрж╛ржирз░ ржмрж╛ржмрзЗ [sherpa-onnx](https://github.com/k2-fsa/sherpa-onnx/pull/2487#issuecomment-3227884498) ржЪрж╛ржУржХред\n\n## ржЖрж▓рзЛржЪржирж╛ ржЖрз░рзБ ржпрзЛржЧрж╛ржпрзЛржЧ\n\nржЖржкрзБржирж┐ рж╕рз░рзНржмрж╛рж╕рж╛ржзрж╛рз░ржгрждрзЗ [Github Issues](https://github.com/k2-fsa/ZipVoice/issues)-ржд ржЖрж▓рзЛржЪржирж╛ ржХрз░рж┐ржм ржкрж╛рз░рзЗред\n\nржЖржкрзБржирж┐ ржЖржорж╛рз░ wechat ржЧрзЛржЯржд ржпрзЛржЧ ржжрж┐рзЯрж╛рз░ ржмрж╛ржмрзЗ QR ржХтАЩржб рж╕рзНржХрзЗржи ржХрз░рж┐ржм ржкрж╛рз░рзЗ ржмрж╛ ржЖржорж╛рз░ wechat ржЪрз░ржХрж╛рз░рзА ржПржХрж╛ржЙржгрзНржЯржЯрзЛ ржЕржирзБрж╕рз░ржг ржХрз░рж┐ржм ржкрж╛рз░рзЗред\n\n| Wechat ржЧрзЛржЯ | Wechat ржЪрз░ржХрж╛рз░рзА ржПржХрж╛ржЙржгрзНржЯ |",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "hF52KZEnGKLuaot2w0AmXt52eB6Y3adYIo2qMJSMx5o=",
        "originContent": "We use [pypinyin](https://github.com/mozillazg/python-pinyin) to convert Chinese characters to pinyin. However, it can occasionally mispronounce **polyphone characters** (хдЪщЯ│хнЧ).",
        "translatedContent": "ржЖржорж╛рз░ ржкрж┐ржиржЗржирж▓рзИ ржЪрж┐ржирж╛ ржЖржЦрз░ рз░рзВржкрж╛ржирзНрждрз░ ржХрз░рж┐ржмрж▓рзИ [pypinyin](https://github.com/mozillazg/python-pinyin) ржмрзНржпрз▒рж╣рж╛рз░ ржХрз░рж╛ рж╣ржпрж╝ред ржпржжрж┐ржУ, ржХрзЗрждрж┐ржпрж╝рж╛ржмрж╛ ржЗ **ржмрж╣рзБ-ржЙржЪрзНржЪрж╛рз░рж┐ржд ржЖржЦрз░** (хдЪщЯ│хнЧ) ржнрзБрж▓ржХрзИ ржЙржЪрзНржЪрж╛рз░ржг ржХрз░рж┐ржм ржкрж╛рз░рзЗред"
      },
      {
        "row": 2,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 3,
        "rowsha": "h6Qq8AvUPYme90k2BWG054cVE6RHNflr0OwdnKA4BEE=",
        "originContent": "To manually correct these mispronunciations, enclose the **corrected pinyin** in angle brackets `< >` and include the **tone mark**.",
        "translatedContent": "ржПржЗ ржнрзБрж▓ ржЙржЪрзНржЪрж╛рз░ржгрж╕ржорзВрж╣ рж╣рж╛рждрзЗржжрж┐ рж╢рзБржжрзНржз ржХрз░рж┐ржмрж▓рзИ, **рж╢рзБржжрзНржз ржХрз░рж╛ ржкрж┐ржиржЗржи** ржХрзМржгрж┐ржХ ржмржирзНржзржирзА `< >`-ржд рз░рж╛ржЦржХ ржЖрз░рзБ **рж╕рзНржмрз░ ржЪрж┐рж╣рзНржи** рж╕ржВржпрзЛржЬржи ржХрз░ржХред"
      },
      {
        "row": 4,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 5,
        "rowsha": "fP4bnCe7+qhcgDDajGMIv4obksa4WSdUp3hExEbpci0=",
        "originContent": "**Example:**",
        "translatedContent": "**ржЙржжрж╛рж╣рз░ржг:**"
      },
      {
        "row": 6,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 7,
        "rowsha": "ntwz9/IQqGC1ThJXQn9D83h+54cDriyg2snkkrk0KoI=",
        "originContent": "- Original text: `ш┐ЩцККхЙСщХ┐ф╕ЙхНБхЕмхИЖ`",
        "translatedContent": "- ржорзВрж▓ ржкрж╛ржа: `ш┐ЩцККхЙСщХ┐ф╕ЙхНБхЕмхИЖ`"
      },
      {
        "row": 8,
        "rowsha": "sfnMRvscnvdKs1fvbVePwH0RpAikXkFIi9i7HZK7D9w=",
        "originContent": "- Correct the pinyin of `щХ┐`:  `ш┐ЩцККхЙС<chang2>ф╕ЙхНБхЕмхИЖ`",
        "translatedContent": "- `щХ┐`-рз░ ржкрж┐ржиржЗржи рж╢рзБржжрзНржз ржХрз░ржХ:  `ш┐ЩцККхЙС<chang2>ф╕ЙхНБхЕмхИЖ`"
      },
      {
        "row": 9,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 10,
        "rowsha": "7f45Y23fyK7AQrUO7HdTtPZZzoyRkW6WwoznauYmQew=",
        "originContent": "> **Note:** If you want to manually assign multiple pinyins, enclose each pinyin with `<>`, e.g., `ш┐ЩцКК<jian4><chang2><san1>хНБхЕмхИЖ`",
        "translatedContent": "> **ржЯрзЛржХрж╛:** ржпржжрж┐ ржЖржкрзБржирж┐ ржмрж╣рзБ ржкрж┐ржиржЗржи ржирж┐ржЬрзЗ ржирж┐ржпрзБржХрзНржд ржХрз░рж┐ржм ржмрж┐ржЪрж╛рз░рзЗ, ржкрзНрз░рждрж┐ржЯрзЛ ржкрж┐ржиржЗржи `<>`-ржд рж╕ржВрж▓ржЧрзНржи ржХрз░ржХ, ржпрзЗржирзЗ, `ш┐ЩцКК<jian4><chang2><san1>хНБхЕмхИЖ`"
      },
      {
        "row": 11,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 12,
        "rowsha": "FJBRXZnczlB/CZyp0UgEFMr440NrcuTPBheyQJ9lxZI=",
        "originContent": "#### 3.7 Remove long silences from the generated speech",
        "translatedContent": "#### 3.7 рж╕рзГрж╖рзНржЯрж┐ ржХрз░рж╛ ржмржХрзНрждрзГрждрж╛рз░ ржкрз░рж╛ ржжрзАржШрж▓ ржирзАрз░рз▒рждрж╛ ржЖржБрждрз░рж╛ржУржХ"
      },
      {
        "row": 13,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 14,
        "rowsha": "9UF5wHdPZ48OWeIPfCnOHPEWzgW4Z6I0e+GaTEZ1GXI=",
        "originContent": "Model will automatically determine the positions and lengths of silences in the generated speech. It occasionally has long silence in the middle of the speech. If you don't want this, you can pass `--remove-long-sil` to remove long silences in the middle of the generated speech (edge silences will be removed by default).",
        "translatedContent": "ржоржбрзЗрж▓рзЗ рж╕рзГрж╖рзНржЯрж┐ ржХрз░рж╛ ржмржХрзНрждрзГрждрж╛ржд ржирзАрз░рз▒рждрж╛рз░ рж╕рзНржерж╛ржи ржЖрз░рзБ ржжрзИрз░рзНржШрзНржп ржЖржкрзБржирж┐ ржирж┐рз░рзНржзрж╛рз░ржг ржХрз░рж┐ржмред ржХрзЗрждрж┐ржпрж╝рж╛ржмрж╛ ржмржХрзНрждрзГрждрж╛рз░ ржорж╛ржЬржд ржжрзАржШрж▓ ржирзАрз░рз▒рждрж╛ ржерж╛ржХрж┐ржм ржкрж╛рз░рзЗред ржЖржкрзБржирж┐ ржПржЗржЯрзЛ ржиржЪрж╛рж╣рж┐рж▓рзЗ, ржоржзрзНржпржнрж╛ржЧржд ржжрзАржШрж▓ ржирзАрз░рз▒рждрж╛ ржЖржБрждрз░рж╛ржмрж▓рзИ `--remove-long-sil` ржкрзНрз░ржжрж╛ржи ржХрз░рж┐ржм ржкрж╛рз░рзЗ (ржХрж╛рж╖рз░ ржирзАрз░рз▒рждрж╛ рж╕рзНржмрж╛ржнрж╛рз▒рж┐ржХржнрж╛рз▒рзЗ ржЖржБрждрз░ ржХрз░рж╛ рж╣тАЩржм)ред"
      },
      {
        "row": 15,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 16,
        "rowsha": "t3GDCbkkN4PM6Y7xA/ZDAXgu4WdMFfXJ+5E/xKU9AKo=",
        "originContent": "#### 3.8 Model downloading",
        "translatedContent": "#### 3.8 ржоржбрзЗрж▓ ржбрж╛ржЙржирж▓рзЛржб ржХрз░рж╛"
      },
      {
        "row": 17,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 18,
        "rowsha": "I2UvthbxkflPXxq8Yy6W9nJCtzb40mZPJbfmDIWQPmA=",
        "originContent": "If you have trouble connecting to HuggingFace when downloading the pre-trained models, try switching endpoint to the mirror site: `export HF_ENDPOINT=https://hf-mirror.com`.",
        "translatedContent": "ржкрзНрз░рж┐-ржЯрзНрз░рзЗржЗржи ржХрз░рж╛ ржоржбрзЗрж▓рж╕ржорзВрж╣ ржбрж╛ржЙржирж▓рзЛржб ржХрз░рзЛржБрждрзЗ HuggingFace-рж▓рзИ рж╕ржВржпрзЛржЧржд рж╕ржорж╕рзНржпрж╛ рж╣тАЩрж▓рзЗ, endpoint-ржЯрзЛ ржорж┐рз░рз░ ржЫрж╛ржЗржЯрж▓рзИ рж╕рж▓ржирж┐ ржХрз░ржХ: `export HF_ENDPOINT=https://hf-mirror.com`ред"
      },
      {
        "row": 19,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 20,
        "rowsha": "SEsrfyGZhBYqHMMdMldgN+tSz6ynJT5BVJeLrTV5lHw=",
        "originContent": "## Train Your Own Model",
        "translatedContent": "## ржирж┐ржЬрз░ ржоржбрзЗрж▓ ржЯрзНрз░рзЗржЗржи ржХрз░ржХ"
      },
      {
        "row": 21,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 22,
        "rowsha": "XaSJNyFaxQRx1Xc0mphwUGAxovKELo/54WkMCnFDLyE=",
        "originContent": "See the [egs](egs) directory for training, fine-tuning and evaluation examples.",
        "translatedContent": "ржЯрзНрз░рзЗржЗржи, ржлрж╛ржЗржи-ржЯрж┐ржЙржи ржЖрз░рзБ ржорзВрж▓рзНржпрж╛рзЯржирз░ ржЙржжрж╛рж╣рз░ржгрж╕ржорзВрж╣рз░ ржмрж╛ржмрзЗ [egs](egs) ржбрж╛ржЗрз░рзЗржХрзНржЯрз░рзА ржЪрж╛ржУржХред"
      },
      {
        "row": 23,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 24,
        "rowsha": "tfXaiJ7qvaTkZXp5azeYhmiU88iPPhEvLfYayUjE5+g=",
        "originContent": "## Production Deployment",
        "translatedContent": "## ржЙрзОржкрж╛ржжржи рж╕рзНржерж╛ржкржи"
      },
      {
        "row": 25,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 26,
        "rowsha": "Tv94iSjTAB/OzGKZj2XftoBUCOzD2x+C0/xHRwRVo1c=",
        "originContent": "### NVIDIA Triton GPU Runtime",
        "translatedContent": "### NVIDIA Triton GPU рз░рж╛ржиржЯрж╛ржЗржо"
      },
      {
        "row": 27,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 28,
        "rowsha": "cGPOVgf7Mpi/ILRt8hu96EmjQo85nhgnSfBGLfBqMeM=",
        "originContent": "For production-ready deployment with high performance and scalability, check out the [Triton Inference Server integration](runtime/nvidia_triton/) that provides optimized TensorRT engines, concurrent request handling, and both gRPC/HTTP APIs for enterprise use.",
        "translatedContent": "ржЙрзОржкрж╛ржжржи-ржкрзНрз░рж╕рзНрждрзБржд, ржЙржЪрзНржЪ ржХрж╛рз░рзНржпржХрзНрж╖ржорждрж╛ ржЖрз░рзБ рж╕рзНржХрзЗрж▓рзЗржмрж▓ рж╕рзНржерж╛ржкржирз░ ржмрж╛ржмрзЗ, [Triton Inference Server integration](runtime/nvidia_triton/) ржЪрж╛ржУржХ ржпрж┐ ржЕржкрзНржЯрж┐ржорж╛ржЗржЬ ржХрз░рж╛ TensorRT ржЗржЮрзНржЬрж┐ржи, ржПржХрж╛ржзрж┐ржХ ржЕржирзБрз░рзЛржз рж╣рзЗржгрзНржбрж▓рж┐ржВ, ржЖрз░рзБ ржПржгрзНржЯрж╛рз░ржкрзНрз░рж╛ржЗржЬ ржмрзНржпрз▒рж╣рж╛рз░рз░ ржмрж╛ржмрзЗ gRPC/HTTP API ржкрзНрз░ржжрж╛ржи ржХрз░рзЗред"
      },
      {
        "row": 29,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 30,
        "rowsha": "qR/CeuOSoGV5ipBKRWpnI+ohlytt878WhTEjtxZenks=",
        "originContent": "### CPU Deployment",
        "translatedContent": "### CPU рж╕рзНржерж╛ржкржи"
      },
      {
        "row": 31,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 32,
        "rowsha": "B8jnXFyKu8XRyVw/Pu0Xuj1ted9/BVoBfwJ1WW9LrcE=",
        "originContent": "Check [sherpa-onnx](https://github.com/k2-fsa/sherpa-onnx/pull/2487#issuecomment-3227884498) for the C++ deployment solution on CPU.",
        "translatedContent": "CPU-ржд C++ рж╕рзНржерж╛ржкржи рж╕ржорж╛ржзрж╛ржирз░ ржмрж╛ржмрзЗ [sherpa-onnx](https://github.com/k2-fsa/sherpa-onnx/pull/2487#issuecomment-3227884498) ржЪрж╛ржУржХред"
      },
      {
        "row": 33,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 34,
        "rowsha": "dEqRbPItUt3FEp1iC+8Ww+A6L57yd6oGeXfxSn5BYzs=",
        "originContent": "## Discussion & Communication",
        "translatedContent": "## ржЖрж▓рзЛржЪржирж╛ ржЖрз░рзБ ржпрзЛржЧрж╛ржпрзЛржЧ"
      },
      {
        "row": 35,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 36,
        "rowsha": "wjQUwaDgSP1a6ggLgGx8TWt44Dxu5IlJytNwCAlzZKg=",
        "originContent": "You can directly discuss on [Github Issues](https://github.com/k2-fsa/ZipVoice/issues).",
        "translatedContent": "ржЖржкрзБржирж┐ рж╕рз░рзНржмрж╛рж╕рж╛ржзрж╛рз░ржгрждрзЗ [Github Issues](https://github.com/k2-fsa/ZipVoice/issues)-ржд ржЖрж▓рзЛржЪржирж╛ ржХрз░рж┐ржм ржкрж╛рз░рзЗред"
      },
      {
        "row": 37,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 38,
        "rowsha": "WLbsT+slCE72T0wpzVIN8KFxhP+RAw29VxFhJcBEEIo=",
        "originContent": "You can also scan the QR code to join our wechat group or follow our wechat official account.",
        "translatedContent": "ржЖржкрзБржирж┐ ржЖржорж╛рз░ wechat ржЧрзЛржЯржд ржпрзЛржЧ ржжрж┐рзЯрж╛рз░ ржмрж╛ржмрзЗ QR ржХтАЩржб рж╕рзНржХрзЗржи ржХрз░рж┐ржм ржкрж╛рз░рзЗ ржмрж╛ ржЖржорж╛рз░ wechat ржЪрз░ржХрж╛рз░рзА ржПржХрж╛ржЙржгрзНржЯржЯрзЛ ржЕржирзБрж╕рз░ржг ржХрз░рж┐ржм ржкрж╛рз░рзЗред"
      },
      {
        "row": 39,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 40,
        "rowsha": "XMNeg//PyHlCkY1XlL7caNd2vlVOKKoslrNeasADjMY=",
        "originContent": "| Wechat Group | Wechat Official Account |",
        "translatedContent": "| Wechat ржЧрзЛржЯ | Wechat ржЪрз░ржХрж╛рз░рзА ржПржХрж╛ржЙржгрзНржЯ |"
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 25,
    "Content": "| ------------ | ----------------------- |\n|![wechat](https://k2-fsa.org/zh-CN/assets/pic/wechat_group.jpg) |![wechat](https://k2-fsa.org/zh-CN/assets/pic/wechat_account.jpg) |\n\n## Citation\n",
    "ContentSha": "z5P7Ai9AO6w/XhHPT5bFJ00FeUxhB51crq68OHJeIus=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "| ------------ | ----------------------- |\n|![wechat](https://k2-fsa.org/zh-CN/assets/pic/wechat_group.jpg) |![wechat](https://k2-fsa.org/zh-CN/assets/pic/wechat_account.jpg) |\n\n## ржЙржжрзНржзрзГрждрж┐\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "jdP52Pdk9hJ4eEQC1YzC887/bGdD6V25zHK1FxUbFjM=",
        "originContent": "| ------------ | ----------------------- |",
        "translatedContent": "| ------------ | ----------------------- |"
      },
      {
        "row": 2,
        "rowsha": "Q6eYrtLPPuG0fiZxZqhYquTYNk0vlyIOh+CRuwGZVk4=",
        "originContent": "|![wechat](https://k2-fsa.org/zh-CN/assets/pic/wechat_group.jpg) |![wechat](https://k2-fsa.org/zh-CN/assets/pic/wechat_account.jpg) |",
        "translatedContent": "|![wechat](https://k2-fsa.org/zh-CN/assets/pic/wechat_group.jpg) |![wechat](https://k2-fsa.org/zh-CN/assets/pic/wechat_account.jpg) |"
      },
      {
        "row": 3,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 4,
        "rowsha": "ZwTp5ajUmpHTJefyHhIKzXcG2wnB1jv8iv8cvmdcb/g=",
        "originContent": "## Citation",
        "translatedContent": "## ржЙржжрзНржзрзГрждрж┐"
      },
      {
        "row": 5,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 26,
    "Content": "```bibtex\n@article{zhu2025zipvoice,\n      title={ZipVoice: Fast and High-Quality Zero-Shot Text-to-Speech with Flow Matching},\n      author={Zhu, Han and Kang, Wei and Yao, Zengwei and Guo, Liyong and Kuang, Fangjun and Li, Zhaoqing and Zhuang, Weiji and Lin, Long and Povey, Daniel},\n      journal={arXiv preprint arXiv:2506.13053},\n      year={2025}\n}\n\n@article{zhu2025zipvoicedialog,\n      title={ZipVoice-Dialog: Non-Autoregressive Spoken Dialogue Generation with Flow Matching},\n      author={Zhu, Han and Kang, Wei and Guo, Liyong and Yao, Zengwei and Kuang, Fangjun and Zhuang, Weiji and Li, Zhaoqing and Han, Zhifeng and Zhang, Dong and Zhang, Xin and Song, Xingchen and Lin, Long and Povey, Daniel},\n      journal={arXiv preprint arXiv:2507.09318},\n      year={2025}\n}\n```",
    "ContentSha": "4y5htVtgE8qDxiQNpfNmGGVhWO4hKo26DrPCI9N/e9E=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bibtex\n@article{zhu2025zipvoice,\n      title={ZipVoice: Fast and High-Quality Zero-Shot Text-to-Speech with Flow Matching},\n      author={Zhu, Han and Kang, Wei and Yao, Zengwei and Guo, Liyong and Kuang, Fangjun and Li, Zhaoqing and Zhuang, Weiji and Lin, Long and Povey, Daniel},\n      journal={arXiv preprint arXiv:2506.13053},\n      year={2025}\n}\n\n@article{zhu2025zipvoicedialog,\n      title={ZipVoice-Dialog: Non-Autoregressive Spoken Dialogue Generation with Flow Matching},\n      author={Zhu, Han and Kang, Wei and Guo, Liyong and Yao, Zengwei and Kuang, Fangjun and Zhuang, Weiji and Li, Zhaoqing and Han, Zhifeng and Zhang, Dong and Zhang, Xin and Song, Xingchen and Lin, Long and Povey, Daniel},\n      journal={arXiv preprint arXiv:2507.09318},\n      year={2025}\n}\n```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 27,
    "Content": "",
    "ContentSha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  }
]