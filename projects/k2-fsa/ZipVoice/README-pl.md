
<div align="right">
  <details>
    <summary >ğŸŒ JÄ™zyk</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=zh-CN">ç®€ä½“ä¸­æ–‡</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=zh-TW">ç¹é«”ä¸­æ–‡</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ja">æ—¥æœ¬èª</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ko">í•œêµ­ì–´</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=hi">à¤¹à¤¿à¤¨à¥à¤¦à¥€</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=th">à¹„à¸—à¸¢</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=fr">FranÃ§ais</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=es">EspaÃ±ol</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=it">Itapano</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ru">Ğ ÑƒÑÑĞºĞ¸Ğ¹</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=pt">PortuguÃªs</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ar">Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=fa">ÙØ§Ø±Ø³ÛŒ</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=tr">TÃ¼rkÃ§e</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=vi">Tiáº¿ng Viá»‡t</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=id">Bahasa Indonesia</a>
      </div>
    </div>
  </details>
</div>

<div align="center">

# ZipVoiceâš¡

## Szybka i wysokiej jakoÅ›ci synteza mowy z tekstu bez uczenia gÅ‚osu z dopasowaniem przepÅ‚ywu
</div>

## PrzeglÄ…d

ZipVoice to seria szybkich i wysokiej jakoÅ›ci modeli TTS typu zero-shot opartych na flow matching.

### 1. Kluczowe cechy

- MaÅ‚y i szybki: tylko 123M parametrÃ³w.

- Wysokiej jakoÅ›ci klonowanie gÅ‚osu: najnowoczeÅ›niejsza wydajnoÅ›Ä‡ w zakresie podobieÅ„stwa mÃ³wcy, zrozumiaÅ‚oÅ›ci i naturalnoÅ›ci.

- WielojÄ™zyczny: obsÅ‚uguje jÄ™zyk chiÅ„ski i angielski.

- Wielomodowy: obsÅ‚uguje generowanie mowy pojedynczego mÃ³wcy oraz dialogÃ³w.

### 2. Warianty modeli

<table>
  <thead>
    <tr>
      <th>Nazwa modelu</th>
      <th>Opis</th>
      <th>Publikacja</th>
      <th>Demo</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ZipVoice</td>
      <td>Podstawowy model obsÅ‚ugujÄ…cy zero-shot TTS dla pojedynczego mÃ³wcy w jÄ™zyku chiÅ„skim i angielskim.</td>
      <td rowspan="2"><a href="https://arxiv.org/abs/2506.13053"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg"></a></td>
      <td rowspan="2"><a href="https://zipvoice.github.io"><img src="https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square"></a></td>
    </tr>
    <tr>
      <td>ZipVoice-Distill</td>
      <td>Wersja destylowana ZipVoice, charakteryzujÄ…ca siÄ™ zwiÄ™kszonÄ… szybkoÅ›ciÄ… przy minimalnej utracie wydajnoÅ›ci.</td>
    </tr>
    <tr>
      <td>ZipVoice-Dialog</td>
      <td>Model generowania dialogÃ³w oparty na ZipVoice, zdolny do generowania jednopasmowych, dwustronnych dialogÃ³w mÃ³wionych.</td>
      <td rowspan="2"><a href="https://arxiv.org/abs/2507.09318"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg"></a></td>
      <td rowspan="2"><a href="https://zipvoice-dialog.github.io"><img src="https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square"></a></td>
    </tr>
    <tr>
      <td>ZipVoice-Dialog-Stereo</td>
      <td>Wariant stereo ZipVoice-Dialog, umoÅ¼liwiajÄ…cy generowanie dwukanaÅ‚owych dialogÃ³w z przypisaniem kaÅ¼dego rozmÃ³wcy do oddzielnego kanaÅ‚u.</td>
    </tr>
  </tbody>
</table>

## AktualnoÅ›ci

**2025/07/14**: Modele generowania mowy dialogowej **ZipVoice-Dialog** oraz **ZipVoice-Dialog-Stereo** zostaÅ‚y wydane. [![arXiv](https://img.shields.io/badge/arXiv-Paper-COLOR.svg)](https://arxiv.org/abs/2507.09318) [![demo page](https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square)](https://zipvoice-dialog.github.io)

**2025/07/14**: ZbiÃ³r danych **OpenDialog**, obejmujÄ…cy 6,8 tysiÄ…ca godzin dialogÃ³w mÃ³wionych, zostaÅ‚ wydany. Pobierz z [![hf](https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Dataset-yellow)](https://huggingface.co/datasets/k2-fsa/OpenDialog), [![ms](https://img.shields.io/badge/ModelScope-Dataset-blue?logo=data)](https://www.modelscope.cn/datasets/k2-fsa/OpenDialog). SzczegÃ³Å‚y na [![arXiv](https://img.shields.io/badge/arXiv-Paper-COLOR.svg)](https://arxiv.org/abs/2507.09318).

**2025/06/16**: **ZipVoice** oraz **ZipVoice-Distill** zostaÅ‚y wydane. [![arXiv](https://img.shields.io/badge/arXiv-Paper-COLOR.svg)](https://arxiv.org/abs/2506.13053) [![demo page](https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square)](https://zipvoice.github.io)

## Instalacja

### 1. Sklonuj repozytorium ZipVoice

```bash
git clone https://github.com/k2-fsa/ZipVoice.git
```
### 2. (Opcjonalnie) UtwÃ³rz wirtualne Å›rodowisko Pythona


```bash
python3 -m venv zipvoice
source zipvoice/bin/activate
```
### 3. Zainstaluj wymagane pakiety


```bash
pip install -r requirements.txt
```
### 4. Zainstaluj k2 do treningu lub wydajnego wnioskowania

**k2 jest niezbÄ™dne do treningu** i moÅ¼e przyspieszyÄ‡ wnioskowanie. Niemniej jednak, moÅ¼esz korzystaÄ‡ z trybu wnioskowania ZipVoice bez instalowania k2.

> **Uwaga:** Upewnij siÄ™, Å¼e instalujesz wersjÄ™ k2 pasujÄ…cÄ… do Twojej wersji PyTorch i CUDA. Na przykÅ‚ad, jeÅ›li uÅ¼ywasz pytorch 2.5.1 i CUDA 12.1, moÅ¼esz zainstalowaÄ‡ k2 w nastÄ™pujÄ…cy sposÃ³b:


```bash
pip install k2==1.24.4.dev20250208+cuda12.1.torch2.5.1 -f https://k2-fsa.github.io/k2/cuda.html
```
ProszÄ™ zapoznaÄ‡ siÄ™ ze stronÄ… https://k2-fsa.org/get-started/k2/ po szczegÃ³Å‚y.
UÅ¼ytkownicy z Chin kontynentalnych mogÄ… zapoznaÄ‡ siÄ™ ze stronÄ… https://k2-fsa.org/zh-CN/get-started/k2/.

- Aby sprawdziÄ‡ instalacjÄ™ k2:


```bash
python3 -c "import k2; print(k2.__file__)"
```
## UÅ¼ytkowanie

### 1. Generowanie mowy jednego mÃ³wcy

Aby wygenerowaÄ‡ mowÄ™ jednego mÃ³wcy przy uÅ¼yciu naszych wytrenowanych modeli ZipVoice lub ZipVoice-Distill, uÅ¼yj nastÄ™pujÄ…cych poleceÅ„ (wymagane modele zostanÄ… pobrane z HuggingFace):

#### 1.1 Wnioskowanie dla pojedynczego zdania


```bash
python3 -m zipvoice.bin.infer_zipvoice \
    --model-name zipvoice \
    --prompt-wav prompt.wav \
    --prompt-text "I am the transcription of the prompt wav." \
    --text "I am the text to be synthesized." \
    --res-wav-path result.wav
```
- `--model-name` moÅ¼e byÄ‡ `zipvoice` lub `zipvoice_distill`, ktÃ³re oznaczajÄ… modele przed i po destylacji, odpowiednio.
- JeÅ›li w tekÅ›cie pojawiÄ… siÄ™ `<>` lub `[]`, ciÄ…gi znakÃ³w otoczone nimi bÄ™dÄ… traktowane jako specjalne tokeny. `<>` oznacza chiÅ„skÄ… transkrypcjÄ™ pinyin, a `[]` oznacza inne specjalne tagi.

#### 1.2 Wnioskowanie listy zdaÅ„

```bash
python3 -m zipvoice.bin.infer_zipvoice \
    --model-name zipvoice \
    --test-list test.tsv \
    --res-dir results
```
- KaÅ¼da linia pliku `test.tsv` ma format `{wav_name}\t{prompt_transcription}\t{prompt_wav}\t{text}`.

### 2. Generowanie mowy dialogowej

#### 2.1 Polecenie inferencji

Aby wygenerowaÄ‡ dwuosobowe dialogi mÃ³wione za pomocÄ… naszych wytrenowanych modeli ZipVoice-Dialogue lub ZipVoice-Dialogue-Stereo, uÅ¼yj nastÄ™pujÄ…cych poleceÅ„ (Wymagane modele zostanÄ… pobrane z HuggingFace):


```bash
python3 -m zipvoice.bin.infer_zipvoice_dialog \
    --model-name "zipvoice_dialog" \
    --test-list test.tsv \
    --res-dir results
```
- `--model-name` moÅ¼e przyjmowaÄ‡ wartoÅ›ci `zipvoice_dialog` lub `zipvoice_dialog_stereo`,
    ktÃ³re generujÄ… odpowiednio dialogi mono i stereo.

#### 2.2 Format wejÅ›ciowy

KaÅ¼da linia w pliku `test.tsv` ma jeden z poniÅ¼szych formatÃ³w:

(1) **Format scalonej podpowiedzi**, gdzie nagrania audio i transkrypcje dwÃ³ch mÃ³wcÃ³w sÄ… scalone w jeden plik wav z podpowiedziÄ…:

```
{wav_name}\t{prompt_transcription}\t{prompt_wav}\t{text}
```

- `wav_name` to nazwa wyjÅ›ciowego pliku wav.
- `prompt_transcription` to transkrypcja konwersacyjnego pliku wav, np. "[S1] CzeÅ›Ä‡. [S2] Jak siÄ™ masz?"
- `prompt_wav` to Å›cieÅ¼ka do pliku wav z promptem.
- `text` to tekst do syntezy, np. "[S1] Wszystko w porzÄ…dku. [S2] Jak masz na imiÄ™? [S1] Jestem Eric. [S2] CzeÅ›Ä‡ Eric."

(2) **Format podzielonego promptu**, gdzie nagrania i transkrypcje dwÃ³ch rozmÃ³wcÃ³w znajdujÄ… siÄ™ w osobnych plikach:

```
{wav_name}\t{spk1_prompt_transcription}\t{spk2_prompt_transcription}\t{spk1_prompt_wav}\t{spk2_prompt_wav}\t{text}
```
- `wav_name` to nazwa wyjÅ›ciowego pliku wav.
- `spk1_prompt_transcription` to transkrypcja promptu pierwszego mÃ³wcy, np. "Hello"
- `spk2_prompt_transcription` to transkrypcja promptu drugiego mÃ³wcy, np. "How are you?"
- `spk1_prompt_wav` to Å›cieÅ¼ka do pliku wav promptu pierwszego mÃ³wcy.
- `spk2_prompt_wav` to Å›cieÅ¼ka do pliku wav promptu drugiego mÃ³wcy.
- `text` to tekst do syntezy, np. "[S1] I'm fine. [S2] What's your name? [S1] I'm Eric. [S2] Hi Eric."

### 3 WskazÃ³wki dla lepszego uÅ¼ytkowania:

#### 3.1 DÅ‚ugoÅ›Ä‡ promptu

Zalecamy krÃ³tki plik promptu wav (np. krÃ³tszy niÅ¼ 3 sekundy dla syntezy pojedynczego mÃ³wcy, krÃ³tszy niÅ¼ 10 sekund dla syntezy dialogowej) dla szybszego dziaÅ‚ania. Bardzo dÅ‚ugi prompt spowolni inferencjÄ™ i pogorszy jakoÅ›Ä‡ mowy.

#### 3.2 Optymalizacja szybkoÅ›ci

JeÅ›li prÄ™dkoÅ›Ä‡ inferencji jest niezadowalajÄ…ca, moÅ¼esz jÄ… zwiÄ™kszyÄ‡ w nastÄ™pujÄ…cy sposÃ³b:

- **Model destylowany i mniej krokÃ³w**: Dla modelu syntezy mowy pojedynczego mÃ³wcy domyÅ›lnie uÅ¼ywamy modelu `zipvoice` dla lepszej jakoÅ›ci mowy. JeÅ›li priorytetem jest szybkoÅ›Ä‡, moÅ¼na przeÅ‚Ä…czyÄ‡ siÄ™ na `zipvoice_distill` i zmniejszyÄ‡ `--num-steps` nawet do `4` (domyÅ›lnie 8).

- **Przyspieszenie CPU przez wielowÄ…tkowoÅ›Ä‡**: Podczas pracy na CPU moÅ¼na uÅ¼yÄ‡ parametru `--num-thread` (np. `--num-thread 4`), aby zwiÄ™kszyÄ‡ liczbÄ™ wÄ…tkÃ³w dla szybszego dziaÅ‚ania. DomyÅ›lnie uÅ¼ywamy 1 wÄ…tku.

- **Przyspieszenie CPU przez ONNX**: Na CPU moÅ¼na uÅ¼yÄ‡ modeli ONNX z `zipvoice.bin.infer_zipvoice_onnx` dla szybszego dziaÅ‚ania (modele ONNX dla generowania dialogÃ³w nie sÄ… jeszcze obsÅ‚ugiwane). Dla jeszcze wiÄ™kszej szybkoÅ›ci moÅ¼na ustawiÄ‡ `--onnx-int8 True`, aby uÅ¼yÄ‡ modelu ONNX skwantowanego do INT8. NaleÅ¼y pamiÄ™taÄ‡, Å¼e model skwantowany obniÅ¼y jakoÅ›Ä‡ mowy. **Nie uÅ¼ywaj ONNX na GPU**, poniewaÅ¼ jest wolniejszy niÅ¼ PyTorch na GPU.

- **Przyspieszenie GPU przez NVIDIA TensorRT**: W celu znacznego zwiÄ™kszenia wydajnoÅ›ci na kartach NVIDIA GPU najpierw wyeksportuj model do silnika TensorRT za pomocÄ… zipvoice.bin.tensorrt_export. NastÄ™pnie wykonaj inferencjÄ™ na swoim zbiorze danych (np. Hugging Face) z uÅ¼yciem zipvoice.bin.infer_zipvoice. To pozwala uzyskaÄ‡ okoÅ‚o 2x wiÄ™kszÄ… przepustowoÅ›Ä‡ niÅ¼ standardowa implementacja PyTorch na GPU.

#### 3.3 Kontrola pamiÄ™ci

Podany tekst zostanie podzielony na fragmenty na podstawie interpunkcji (dla syntezy pojedynczego mÃ³wcy) lub symbolu zmiany mÃ³wcy (dla syntezy dialogowej). NastÄ™pnie fragmenty tekstu bÄ™dÄ… przetwarzane w partiach. DziÄ™ki temu model moÅ¼e przetwarzaÄ‡ dowolnie dÅ‚ugie teksty przy niemal staÅ‚ym zuÅ¼yciu pamiÄ™ci. MoÅ¼esz kontrolowaÄ‡ uÅ¼ycie pamiÄ™ci przez regulacjÄ™ parametru `--max-duration`.

#### 3.4 Ocena "Raw"

DomyÅ›lnie przetwarzamy wejÅ›cia (prompt wav, transkrypcjÄ™ promptu i tekst) dla wydajnej inferencji i lepszych wynikÃ³w. JeÅ›li chcesz oceniÄ‡ "surowe" dziaÅ‚anie modelu na dokÅ‚adnie podanych danych (np. by odtworzyÄ‡ wyniki z naszej publikacji), moÅ¼esz podaÄ‡ `--raw-evaluation True`.

#### 3.5 KrÃ³tki tekst

Podczas generowania mowy dla bardzo krÃ³tkich tekstÃ³w (np. jedno lub dwa sÅ‚owa), wygenerowana mowa moÅ¼e czasem pomijaÄ‡ pewne wymowy. Aby rozwiÄ…zaÄ‡ ten problem, moÅ¼na podaÄ‡ `--speed 0.3` (gdzie 0.3 to wartoÅ›Ä‡ do dostrojenia), by wydÅ‚uÅ¼yÄ‡ czas trwania wygenerowanej mowy.

#### 3.6 Korekta bÅ‚Ä™dnej wymowy chiÅ„skich znakÃ³w polifonicznych


UÅ¼ywamy [pypinyin](https://github.com/mozillazg/python-pinyin) do konwersji znakÃ³w chiÅ„skich na pinyin. Jednak czasami moÅ¼e bÅ‚Ä™dnie wymawiaÄ‡ **znaki wielogÅ‚osowe** (å¤šéŸ³å­—).

Aby rÄ™cznie poprawiÄ‡ te bÅ‚Ä™dne wymowy, umieÅ›Ä‡ **poprawiony pinyin** w nawiasach ostrych `< >` i dodaj **znak tonu**.

**PrzykÅ‚ad:**

- Oryginalny tekst: `è¿™æŠŠå‰‘é•¿ä¸‰åå…¬åˆ†`
- Popraw pinyin dla `é•¿`:  `è¿™æŠŠå‰‘<chang2>ä¸‰åå…¬åˆ†`

> **Uwaga:** JeÅ›li chcesz rÄ™cznie przypisaÄ‡ kilka pinyinÃ³w, umieÅ›Ä‡ kaÅ¼dy pinyin w `<>`, np. `è¿™æŠŠ<jian4><chang2><san1>åå…¬åˆ†`

#### 3.7 Usuwanie dÅ‚ugich pauz z wygenerowanej mowy

Model automatycznie okreÅ›la pozycje i dÅ‚ugoÅ›ci pauz w wygenerowanej mowie. Czasami pojawia siÄ™ dÅ‚uga pauza w Å›rodku wypowiedzi. JeÅ›li tego nie chcesz, moÅ¼esz uÅ¼yÄ‡ `--remove-long-sil`, aby usunÄ…Ä‡ dÅ‚ugie pauzy w Å›rodku wygenerowanej mowy (pauzy na poczÄ…tku i koÅ„cu sÄ… usuwane domyÅ›lnie).

#### 3.8 Pobieranie modelu

JeÅ›li masz problemy z poÅ‚Ä…czeniem z HuggingFace podczas pobierania wstÄ™pnie wytrenowanych modeli, sprÃ³buj zmieniÄ‡ endpoint na mirror: `export HF_ENDPOINT=https://hf-mirror.com`.

## Trenuj wÅ‚asny model

Zobacz katalog [egs](egs) po przykÅ‚ady treningu, fine-tuningu i ewaluacji.

## WdroÅ¼enie produkcyjne

### NVIDIA Triton GPU Runtime

Aby wdroÅ¼yÄ‡ produkcyjnie z wysokÄ… wydajnoÅ›ciÄ… i skalowalnoÅ›ciÄ…, zobacz [integracjÄ™ z Triton Inference Server](runtime/nvidia_triton/), ktÃ³ra zapewnia zoptymalizowane silniki TensorRT, obsÅ‚ugÄ™ wspÃ³Å‚bieÅ¼nych Å¼Ä…daÅ„ oraz oba API gRPC/HTTP dla zastosowaÅ„ firmowych.

### WdroÅ¼enie na CPU

SprawdÅº [sherpa-onnx](https://github.com/k2-fsa/sherpa-onnx/pull/2487#issuecomment-3227884498) jako rozwiÄ…zanie do wdroÅ¼enia w C++ na CPU.

## Dyskusja i komunikacja

MoÅ¼esz dyskutowaÄ‡ bezpoÅ›rednio na [Github Issues](https://github.com/k2-fsa/ZipVoice/issues).

MoÅ¼esz takÅ¼e zeskanowaÄ‡ kod QR, aby doÅ‚Ä…czyÄ‡ do naszej grupy na Wechat lub obserwowaÄ‡ nasze oficjalne konto Wechat.

| Grupa Wechat | Oficjalne konto Wechat |
| ------------ | ----------------------- |
|![wechat](https://k2-fsa.org/zh-CN/assets/pic/wechat_group.jpg) |![wechat](https://k2-fsa.org/zh-CN/assets/pic/wechat_account.jpg) |

## Cytowanie

```bibtex
@article{zhu2025zipvoice,
      title={ZipVoice: Fast and High-Quality Zero-Shot Text-to-Speech with Flow Matching},
      author={Zhu, Han and Kang, Wei and Yao, Zengwei and Guo, Liyong and Kuang, Fangjun and Li, Zhaoqing and Zhuang, Weiji and Lin, Long and Povey, Daniel},
      journal={arXiv preprint arXiv:2506.13053},
      year={2025}
}

@article{zhu2025zipvoicedialog,
      title={ZipVoice-Dialog: Non-Autoregressive Spoken Dialogue Generation with Flow Matching},
      author={Zhu, Han and Kang, Wei and Guo, Liyong and Yao, Zengwei and Kuang, Fangjun and Zhuang, Weiji and Li, Zhaoqing and Han, Zhifeng and Zhang, Dong and Zhang, Xin and Song, Xingchen and Lin, Long and Povey, Daniel},
      journal={arXiv preprint arXiv:2507.09318},
      year={2025}
}
```




---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-12-30

---