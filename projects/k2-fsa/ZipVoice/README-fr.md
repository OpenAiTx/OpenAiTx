
<div align="right">
  <details>
    <summary >üåê Langue</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=zh-TW">ÁπÅÈ´î‰∏≠Êñá</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=hi">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=th">‡πÑ‡∏ó‡∏¢</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=fr">Fran√ßais</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=es">Espa√±ol</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=it">Itapano</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ru">–†—É—Å—Å–∫–∏–π</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=pt">Portugu√™s</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ar">ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=fa">ŸÅÿßÿ±ÿ≥€å</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=tr">T√ºrk√ße</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=vi">Ti·∫øng Vi·ªát</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=id">Bahasa Indonesia</a>
      </div>
    </div>
  </details>
</div>

<div align="center">

# ZipVoice‚ö°

## Synth√®se vocale rapide et de haute qualit√© en z√©ro-shot gr√¢ce au Flow Matching
</div>

## Vue d'ensemble

ZipVoice est une s√©rie de mod√®les TTS zero-shot rapides et de haute qualit√©, bas√©s sur le flow matching.

### 1. Caract√©ristiques principales

- Petit et rapide : seulement 123M de param√®tres.

- Clonage vocal de haute qualit√© : performance √† l‚Äô√©tat de l‚Äôart en similarit√© de locuteur, intelligibilit√© et naturel.

- Multilingue : support du chinois et de l‚Äôanglais.

- Multi-mode : support de la g√©n√©ration vocale mono-locuteur et du dialogue.

### 2. Variantes du mod√®le

<table>
  <thead>
    <tr>
      <th>Nom du mod√®le</th>
      <th>Description</th>
      <th>Article</th>
      <th>D√©mo</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ZipVoice</td>
      <td>Le mod√®le de base supportant le TTS zero-shot mono-locuteur en chinois et anglais.</td>
      <td rowspan="2"><a href="https://arxiv.org/abs/2506.13053"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg"></a></td>
      <td rowspan="2"><a href="https://zipvoice.github.io"><img src="https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square"></a></td>
    </tr>
    <tr>
      <td>ZipVoice-Distill</td>
      <td>La version distill√©e de ZipVoice, offrant une vitesse am√©lior√©e avec une d√©gradation minimale des performances.</td>
    </tr>
    <tr>
      <td>ZipVoice-Dialog</td>
      <td>Un mod√®le de g√©n√©ration de dialogue bas√© sur ZipVoice, capable de g√©n√©rer des dialogues parl√©s √† deux voix sur un seul canal.</td>
      <td rowspan="2"><a href="https://arxiv.org/abs/2507.09318"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg"></a></td>
      <td rowspan="2"><a href="https://zipvoice-dialog.github.io"><img src="https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square"></a></td>
    </tr>
    <tr>
      <td>ZipVoice-Dialog-Stereo</td>
      <td>La variante st√©r√©o de ZipVoice-Dialog, permettant la g√©n√©ration de dialogues √† deux canaux avec chaque interlocuteur assign√© √† un canal distinct.</td>
    </tr>
  </tbody>
</table>

## Actualit√©s

**2025/07/14** : **ZipVoice-Dialog** et **ZipVoice-Dialog-Stereo**, deux mod√®les de g√©n√©ration de dialogues parl√©s, sont publi√©s. [![arXiv](https://img.shields.io/badge/arXiv-Paper-COLOR.svg)](https://arxiv.org/abs/2507.09318) [![demo page](https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square)](https://zipvoice-dialog.github.io)

**2025/07/14** : Le jeu de donn√©es **OpenDialog**, un corpus de dialogues parl√©s de 6,8k heures, est publi√©. T√©l√©chargez-le sur [![hf](https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Dataset-yellow)](https://huggingface.co/datasets/k2-fsa/OpenDialog), [![ms](https://img.shields.io/badge/ModelScope-Dataset-blue?logo=data)](https://www.modelscope.cn/datasets/k2-fsa/OpenDialog). Consultez les d√©tails sur [![arXiv](https://img.shields.io/badge/arXiv-Paper-COLOR.svg)](https://arxiv.org/abs/2507.09318).

**2025/06/16** : **ZipVoice** et **ZipVoice-Distill** sont publi√©s. [![arXiv](https://img.shields.io/badge/arXiv-Paper-COLOR.svg)](https://arxiv.org/abs/2506.13053) [![demo page](https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square)](https://zipvoice.github.io)

## Installation

### 1. Cloner le d√©p√¥t ZipVoice

```bash
git clone https://github.com/k2-fsa/ZipVoice.git
```
### 2. (Optionnel) Cr√©ez un environnement virtuel Python


```bash
python3 -m venv zipvoice
source zipvoice/bin/activate
```
### 3. Installez les paquets requis


```bash
pip install -r requirements.txt
```
### 4. Installer k2 pour l'entra√Ænement ou l'inf√©rence efficace

**k2 est n√©cessaire pour l'entra√Ænement** et peut acc√©l√©rer l'inf√©rence. N√©anmoins, vous pouvez toujours utiliser le mode inf√©rence de ZipVoice sans installer k2.

> **Remarque :** Assurez-vous d'installer la version de k2 qui correspond √† votre version de PyTorch et CUDA. Par exemple, si vous utilisez pytorch 2.5.1 et CUDA 12.1, vous pouvez installer k2 comme suit :


```bash
pip install k2==1.24.4.dev20250208+cuda12.1.torch2.5.1 -f https://k2-fsa.github.io/k2/cuda.html
```
Veuillez consulter https://k2-fsa.org/get-started/k2/ pour plus de d√©tails.
Les utilisateurs en Chine continentale peuvent consulter https://k2-fsa.org/zh-CN/get-started/k2/.

- Pour v√©rifier l'installation de k2 :


```bash
python3 -c "import k2; print(k2.__file__)"
```
## Utilisation

### 1. G√©n√©ration de parole √† un seul locuteur

Pour g√©n√©rer de la parole √† un seul locuteur avec nos mod√®les ZipVoice ou ZipVoice-Distill pr√©-entra√Æn√©s, utilisez les commandes suivantes (Les mod√®les requis seront t√©l√©charg√©s depuis HuggingFace) :

#### 1.1 Inf√©rence d'une seule phrase


```bash
python3 -m zipvoice.bin.infer_zipvoice \
    --model-name zipvoice \
    --prompt-wav prompt.wav \
    --prompt-text "I am the transcription of the prompt wav." \
    --text "I am the text to be synthesized." \
    --res-wav-path result.wav
```
- `--model-name` peut √™tre `zipvoice` ou `zipvoice_distill`, qui sont respectivement les mod√®les avant et apr√®s distillation.
- Si `<>` ou `[]` apparaissent dans le texte, les cha√Ænes entour√©es par celles-ci seront trait√©es comme des jetons sp√©ciaux. `<>` indique le pinyin chinois et `[]` indique d'autres balises sp√©ciales.

#### 1.2 Inf√©rence d'une liste de phrases

```bash
python3 -m zipvoice.bin.infer_zipvoice \
    --model-name zipvoice \
    --test-list test.tsv \
    --res-dir results
```
- Chaque ligne de `test.tsv` est au format `{wav_name}\t{prompt_transcription}\t{prompt_wav}\t{text}`.

### 2. G√©n√©ration de la parole en dialogue

#### 2.1 Commande d'inf√©rence

Pour g√©n√©rer des dialogues parl√©s √† deux voix avec nos mod√®les pr√©-entra√Æn√©s ZipVoice-Dialogue ou ZipVoice-Dialogue-Stereo, utilisez les commandes suivantes (les mod√®les requis seront t√©l√©charg√©s depuis HuggingFace) :


```bash
python3 -m zipvoice.bin.infer_zipvoice_dialog \
    --model-name "zipvoice_dialog" \
    --test-list test.tsv \
    --res-dir results
```
- `--model-name` peut √™tre `zipvoice_dialog` ou `zipvoice_dialog_stereo`,
    qui g√©n√®rent respectivement des dialogues mono et st√©r√©o.

#### 2.2 Formats d'entr√©e

Chaque ligne de `test.tsv` est dans l'un des formats suivants :

(1) **Format de prompt fusionn√©** o√π les audios et transcriptions des prompts des deux locuteurs sont fusionn√©s en un seul fichier wav de prompt :

```
{wav_name}\t{prompt_transcription}\t{prompt_wav}\t{text}
```

- `wav_name` est le nom du fichier wav de sortie.
- `prompt_transcription` est la transcription du fichier wav du prompt conversationnel, par exemple, "[S1] Bonjour. [S2] Comment √ßa va ?"
- `prompt_wav` est le chemin vers le fichier wav du prompt.
- `text` est le texte √† synth√©tiser, par exemple "[S1] Je vais bien. [S2] Comment tu t'appelles ? [S1] Je m'appelle Eric. [S2] Salut Eric."

(2) **Format de prompt s√©par√©** o√π les audios et les transcriptions des deux interlocuteurs existent dans des fichiers distincts :

```
{wav_name}\t{spk1_prompt_transcription}\t{spk2_prompt_transcription}\t{spk1_prompt_wav}\t{spk2_prompt_wav}\t{text}
```

- `wav_name` est le nom du fichier wav de sortie.
- `spk1_prompt_transcription` est la transcription du fichier wav de prompt du premier locuteur, par exemple "Bonjour"
- `spk2_prompt_transcription` est la transcription du fichier wav de prompt du second locuteur, par exemple "Comment √ßa va ?"
- `spk1_prompt_wav` est le chemin du fichier wav de prompt du premier locuteur.
- `spk2_prompt_wav` est le chemin du fichier wav de prompt du second locuteur.
- `text` est le texte √† synth√©tiser, par exemple "[S1] Je vais bien. [S2] Comment tu t'appelles ? [S1] Je m'appelle Eric. [S2] Salut Eric."

### 3 Conseils pour une meilleure utilisation :

#### 3.1 Longueur du prompt

Nous recommandons un fichier wav de prompt court (par exemple, moins de 3 secondes pour la g√©n√©ration de parole √† un seul locuteur, moins de 10 secondes pour la g√©n√©ration de dialogue) pour une vitesse d'inf√©rence plus rapide. Un prompt tr√®s long ralentira l'inf√©rence et d√©gradera la qualit√© de la parole.

#### 3.2 Optimisation de la vitesse

Si la vitesse d'inf√©rence n'est pas satisfaisante, vous pouvez l'acc√©l√©rer comme suit :

- **Mod√®le distill√© et moins d'√©tapes** : Pour le mod√®le de g√©n√©ration de parole √† un seul locuteur, nous utilisons le mod√®le `zipvoice` par d√©faut pour une meilleure qualit√© de parole. Si la rapidit√© est prioritaire, vous pouvez passer √† `zipvoice_distill` et r√©duire le param√®tre `--num-steps` jusqu'√† `4` (8 par d√©faut).

- **Acc√©l√©ration CPU avec multi-threading** : Lors de l'ex√©cution sur CPU, vous pouvez utiliser le param√®tre `--num-thread` (par exemple, `--num-thread 4`) pour augmenter le nombre de threads et acc√©l√©rer la vitesse. Nous utilisons 1 thread par d√©faut.

- **Acc√©l√©ration CPU avec ONNX** : Lors de l'ex√©cution sur CPU, vous pouvez utiliser les mod√®les ONNX avec `zipvoice.bin.infer_zipvoice_onnx` pour une vitesse sup√©rieure (ONNX n'est pas encore support√© pour les mod√®les de g√©n√©ration de dialogue). Pour encore plus de rapidit√©, vous pouvez d√©finir `--onnx-int8 True` pour utiliser un mod√®le ONNX quantifi√© INT8. Notez que le mod√®le quantifi√© d√©gradera la qualit√© de la parole dans une certaine mesure. **N'utilisez pas ONNX sur GPU**, car il est plus lent que PyTorch sur GPU.

#### 3.3 Contr√¥le de la m√©moire

Le texte fourni sera d√©coup√© en morceaux selon la ponctuation (pour la g√©n√©ration de parole √† un seul locuteur) ou le symbole de changement de locuteur (pour la g√©n√©ration de dialogue). Ensuite, les morceaux seront trait√©s en lots. Ainsi, le mod√®le peut traiter des textes arbitrairement longs avec une utilisation m√©moire quasiment constante. Vous pouvez contr√¥ler l'utilisation m√©moire en ajustant le param√®tre `--max-duration`.

#### 3.4 √âvaluation "brute"

Par d√©faut, nous pr√©traitons les entr√©es (prompt wav, transcription du prompt et texte) pour une inf√©rence efficace et de meilleures performances. Si vous souhaitez √©valuer la performance "brute" du mod√®le avec les entr√©es exactes fournies (par exemple, pour reproduire les r√©sultats de notre article), vous pouvez utiliser `--raw-evaluation True`.

#### 3.5 Texte court

Lors de la g√©n√©ration de parole pour des textes tr√®s courts (par exemple, un ou deux mots), la parole g√©n√©r√©e peut parfois omettre certaines prononciations. Pour r√©soudre ce probl√®me, vous pouvez utiliser `--speed 0.3` (o√π 0.3 est une valeur ajustable) pour prolonger la dur√©e de la parole g√©n√©r√©e.

#### 3.6 Correction de la prononciation incorrecte des caract√®res chinois polyphoniques

Nous utilisons [pypinyin](https://github.com/mozillazg/python-pinyin) pour convertir les caract√®res chinois en pinyin. Cependant, il peut parfois mal prononcer les **caract√®res polyphoniques** (Â§öÈü≥Â≠ó).

Pour corriger manuellement ces erreurs de prononciation, encadrez le **pinyin corrig√©** entre chevrons `< >` et incluez l‚Äô**accent tonique**.

**Exemple :**

- Texte original : `ËøôÊääÂâëÈïø‰∏âÂçÅÂÖ¨ÂàÜ`
- Corrigez le pinyin de `Èïø` :  `ËøôÊääÂâë<chang2>‰∏âÂçÅÂÖ¨ÂàÜ`

> **Remarque :** Si vous souhaitez attribuer plusieurs pinyins manuellement, encadrez chaque pinyin avec `<>`, par exemple : `ËøôÊää<jian4><chang2><san1>ÂçÅÂÖ¨ÂàÜ`

#### 3.7 Suppression des silences longs dans la parole g√©n√©r√©e

Le mod√®le d√©termine automatiquement les positions et la dur√©e des silences dans la parole g√©n√©r√©e. Il arrive qu‚Äôil y ait de longs silences au milieu de la parole. Si vous ne souhaitez pas cela, vous pouvez passer l‚Äôoption `--remove-long-sil` pour supprimer les longs silences au milieu de la parole g√©n√©r√©e (les silences en d√©but et fin seront supprim√©s par d√©faut).

#### 3.8 T√©l√©chargement du mod√®le

Si vous rencontrez des difficult√©s pour vous connecter √† HuggingFace lors du t√©l√©chargement des mod√®les pr√©-entra√Æn√©s, essayez de changer l‚Äôendpoint vers le site miroir : `export HF_ENDPOINT=https://hf-mirror.com`.

## Entra√Ænez votre propre mod√®le

Consultez le r√©pertoire [egs](egs) pour des exemples d‚Äôentra√Ænement, de fine-tuning et d‚Äô√©valuation.

## D√©ploiement C++

Consultez [sherpa-onnx](https://github.com/k2-fsa/sherpa-onnx/pull/2487#issuecomment-3227884498) pour la solution de d√©ploiement C++ sur CPU.

## Discussion & Communication

Vous pouvez discuter directement sur [Github Issues](https://github.com/k2-fsa/ZipVoice/issues).

Vous pouvez √©galement scanner le code QR pour rejoindre notre groupe WeChat ou suivre notre compte officiel WeChat.

| Groupe WeChat | Compte Officiel WeChat |
| ------------- | ---------------------- |
|![wechat](https://k2-fsa.org/zh-CN/assets/pic/wechat_group.jpg) |![wechat](https://k2-fsa.org/zh-CN/assets/pic/wechat_account.jpg) |

## Citation

```bibtex
@article{zhu2025zipvoice,
      title={ZipVoice: Fast and High-Quality Zero-Shot Text-to-Speech with Flow Matching},
      author={Zhu, Han and Kang, Wei and Yao, Zengwei and Guo, Liyong and Kuang, Fangjun and Li, Zhaoqing and Zhuang, Weiji and Lin, Long and Povey, Daniel},
      journal={arXiv preprint arXiv:2506.13053},
      year={2025}
}

@article{zhu2025zipvoicedialog,
      title={ZipVoice-Dialog: Non-Autoregressive Spoken Dialogue Generation with Flow Matching},
      author={Zhu, Han and Kang, Wei and Guo, Liyong and Yao, Zengwei and Kuang, Fangjun and Zhuang, Weiji and Li, Zhaoqing and Han, Zhifeng and Zhang, Dong and Zhang, Xin and Song, Xingchen and Lin, Long and Povey, Daniel},
      journal={arXiv preprint arXiv:2507.09318},
      year={2025}
}
```




---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-10-06

---