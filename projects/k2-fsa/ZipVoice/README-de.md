
<div align="right">
  <details>
    <summary >üåê Sprache</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=zh-TW">ÁπÅÈ´î‰∏≠Êñá</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=hi">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=th">‡πÑ‡∏ó‡∏¢</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=fr">Fran√ßais</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=es">Espa√±ol</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=it">Itapano</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ru">–†—É—Å—Å–∫–∏–π</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=pt">Portugu√™s</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ar">ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=fa">ŸÅÿßÿ±ÿ≥€å</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=tr">T√ºrk√ße</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=vi">Ti·∫øng Vi·ªát</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=id">Bahasa Indonesia</a>
      </div>
    </div>
  </details>
</div>

<div align="center">

# ZipVoice‚ö°

## Schnelle und hochwertige Zero-Shot-Text-zu-Sprache mit Flow Matching
</div>

## √úbersicht

ZipVoice ist eine Serie von schnellen und hochwertigen Zero-Shot-TTS-Modellen, die auf Flow Matching basieren.

### 1. Hauptmerkmale

- Klein und schnell: nur 123M Parameter.

- Hochwertiges Voice Cloning: branchenf√ºhrende Leistung bei Sprecher√§hnlichkeit, Verst√§ndlichkeit und Nat√ºrlichkeit.

- Mehrsprachig: unterst√ºtzt Chinesisch und Englisch.

- Multi-Mode: unterst√ºtzt sowohl Einzelsprecher- als auch Dialog-Sprachgenerierung.

### 2. Modellvarianten

<table>
  <thead>
    <tr>
      <th>Modellname</th>
      <th>Beschreibung</th>
      <th>Paper</th>
      <th>Demo</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ZipVoice</td>
      <td>Das Basismodell, das Zero-Shot-Einzelsprecher-TTS in Chinesisch und Englisch unterst√ºtzt.</td>
      <td rowspan="2"><a href="https://arxiv.org/abs/2506.13053"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg"></a></td>
      <td rowspan="2"><a href="https://zipvoice.github.io"><img src="https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square"></a></td>
    </tr>
    <tr>
      <td>ZipVoice-Distill</td>
      <td>Die destillierte Version von ZipVoice mit verbesserter Geschwindigkeit und minimalem Leistungsverlust.</td>
    </tr>
    <tr>
      <td>ZipVoice-Dialog</td>
      <td>Ein Dialoggenerierungsmodell, das auf ZipVoice basiert und einsprachige Zwei-Parteien-Gespr√§che erzeugen kann.</td>
      <td rowspan="2"><a href="https://arxiv.org/abs/2507.09318"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg"></a></td>
      <td rowspan="2"><a href="https://zipvoice-dialog.github.io"><img src="https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square"></a></td>
    </tr>
    <tr>
      <td>ZipVoice-Dialog-Stereo</td>
      <td>Die Stereo-Variante von ZipVoice-Dialog, erm√∂glicht zweikanalige Dialoggenerierung mit jedem Sprecher auf einem eigenen Kanal.</td>
    </tr>
  </tbody>
</table>

## Neuigkeiten

**2025/07/14**: **ZipVoice-Dialog** und **ZipVoice-Dialog-Stereo**, zwei Modelle zur gesprochenen Dialoggenerierung, sind ver√∂ffentlicht. [![arXiv](https://img.shields.io/badge/arXiv-Paper-COLOR.svg)](https://arxiv.org/abs/2507.09318) [![Demo-Seite](https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square)](https://zipvoice-dialog.github.io)

**2025/07/14**: **OpenDialog** Datensatz, ein 6,8k-Stunden-Datensatz f√ºr gesprochene Dialoge, ist ver√∂ffentlicht. Download unter [![hf](https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Dataset-yellow)](https://huggingface.co/datasets/k2-fsa/OpenDialog), [![ms](https://img.shields.io/badge/ModelScope-Dataset-blue?logo=data)](https://www.modelscope.cn/datasets/k2-fsa/OpenDialog). Details unter [![arXiv](https://img.shields.io/badge/arXiv-Paper-COLOR.svg)](https://arxiv.org/abs/2507.09318).

**2025/06/16**: **ZipVoice** und **ZipVoice-Distill** sind ver√∂ffentlicht. [![arXiv](https://img.shields.io/badge/arXiv-Paper-COLOR.svg)](https://arxiv.org/abs/2506.13053) [![Demo-Seite](https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square)](https://zipvoice.github.io)

## Installation

### 1. Klone das ZipVoice-Repository

```bash
git clone https://github.com/k2-fsa/ZipVoice.git
```
### 2. (Optional) Erstellen Sie eine Python-virtuelle Umgebung


```bash
python3 -m venv zipvoice
source zipvoice/bin/activate
```
### 3. Installieren Sie die erforderlichen Pakete


```bash
pip install -r requirements.txt
```
### 4. Installieren Sie k2 f√ºr das Training oder effizientes Inferenzieren

**k2 ist f√ºr das Training notwendig** und kann die Inferenz beschleunigen. Dennoch k√∂nnen Sie den Inferenzmodus von ZipVoice auch ohne die Installation von k2 verwenden.

> **Hinweis:** Stellen Sie sicher, dass Sie die k2-Version installieren, die zu Ihrer PyTorch- und CUDA-Version passt. Wenn Sie beispielsweise pytorch 2.5.1 und CUDA 12.1 verwenden, k√∂nnen Sie k2 wie folgt installieren:


```bash
pip install k2==1.24.4.dev20250208+cuda12.1.torch2.5.1 -f https://k2-fsa.github.io/k2/cuda.html
```
Bitte beachten Sie https://k2-fsa.org/get-started/k2/ f√ºr weitere Details.
Nutzer in Festlandchina k√∂nnen https://k2-fsa.org/zh-CN/get-started/k2/ nutzen.

- Um die k2-Installation zu √ºberpr√ºfen:


```bash
python3 -c "import k2; print(k2.__file__)"
```
## Verwendung

### 1. Sprachgenerierung mit einem Sprecher

Um Sprachaufnahmen mit nur einem Sprecher mithilfe unserer vortrainierten ZipVoice- oder ZipVoice-Distill-Modelle zu erzeugen, verwenden Sie die folgenden Befehle (Erforderliche Modelle werden von HuggingFace heruntergeladen):

#### 1.1 Inferenz eines einzelnen Satzes


```bash
python3 -m zipvoice.bin.infer_zipvoice \
    --model-name zipvoice \
    --prompt-wav prompt.wav \
    --prompt-text "I am the transcription of the prompt wav." \
    --text "I am the text to be synthesized." \
    --res-wav-path result.wav
```
- `--model-name` kann `zipvoice` oder `zipvoice_distill` sein, was jeweils die Modelle vor und nach der Destillation bezeichnet.
- Wenn `<>` oder `[]` im Text erscheinen, werden von ihnen eingeschlossene Zeichenfolgen als spezielle Tokens behandelt. `<>` steht f√ºr chinesische Pinyin und `[]` f√ºr andere spezielle Tags.

#### 1.2 Inferenz einer Liste von S√§tzen

```bash
python3 -m zipvoice.bin.infer_zipvoice \
    --model-name zipvoice \
    --test-list test.tsv \
    --res-dir results
```
- Jede Zeile von `test.tsv` hat das Format `{wav_name}\t{prompt_transcription}\t{prompt_wav}\t{text}`.

### 2. Dialog-Sprachgenerierung

#### 2.1 Inferenzbefehl

Um Zwei-Parteien-Dialoge mit unseren vortrainierten ZipVoice-Dialogue oder ZipVoice-Dialogue-Stereo Modellen zu generieren, verwenden Sie die folgenden Befehle (Die ben√∂tigten Modelle werden von HuggingFace heruntergeladen):


```bash
python3 -m zipvoice.bin.infer_zipvoice_dialog \
    --model-name "zipvoice_dialog" \
    --test-list test.tsv \
    --res-dir results
```
- `--model-name` kann entweder `zipvoice_dialog` oder `zipvoice_dialog_stereo` sein,
    wobei jeweils Mono- bzw. Stereo-Dialoge erzeugt werden.

#### 2.2 Eingabeformate

Jede Zeile in `test.tsv` hat eines der folgenden Formate:

(1) **Zusammengef√ºhrtes Prompt-Format**, bei dem die Audiodateien und Transkriptionen der Prompts beider Sprecher in einer Prompt-WAV-Datei zusammengef√ºhrt werden:

```
{wav_name}\t{prompt_transcription}\t{prompt_wav}\t{text}
```

- `wav_name` ist der Name der Ausgabedatei im wav-Format.
- `prompt_transcription` ist die Transkription der Konversationsaufforderung (Prompt-wav), z.B. "[S1] Hallo. [S2] Wie geht es dir?"
- `prompt_wav` ist der Pfad zur Prompt-wav-Datei.
- `text` ist der zu synthetisierende Text, z.B. "[S1] Mir geht es gut. [S2] Wie hei√üt du? [S1] Ich bin Eric. [S2] Hallo Eric."

(2) **Geteiltes Prompt-Format**, bei dem die Audiodateien und Transkriptionen der beiden Sprecher in separaten Dateien vorliegen:

```
{wav_name}\t{spk1_prompt_transcription}\t{spk2_prompt_transcription}\t{spk1_prompt_wav}\t{spk2_prompt_wav}\t{text}
```
- `wav_name` ist der Name der Ausgabedatei im WAV-Format.
- `spk1_prompt_transcription` ist die Transkription der Prompt-WAV-Datei des ersten Sprechers, z. B. ‚ÄûHallo‚Äú.
- `spk2_prompt_transcription` ist die Transkription der Prompt-WAV-Datei des zweiten Sprechers, z. B. ‚ÄûWie geht es dir?‚Äú
- `spk1_prompt_wav` ist der Pfad zur Prompt-WAV-Datei des ersten Sprechers.
- `spk2_prompt_wav` ist der Pfad zur Prompt-WAV-Datei des zweiten Sprechers.
- `text` ist der zu synthetisierende Text, z. B. ‚Äû[S1] Mir geht's gut. [S2] Wie hei√üt du? [S1] Ich bin Eric. [S2] Hallo Eric.‚Äú

### 3 Hinweise f√ºr bessere Nutzung:

#### 3.1 Promptl√§nge

Wir empfehlen eine kurze Prompt-WAV-Datei (z. B. weniger als 3 Sekunden f√ºr die Sprachsynthese mit einem Sprecher, weniger als 10 Sekunden f√ºr die Dialog-Sprachsynthese) f√ºr eine schnellere Inferenzgeschwindigkeit. Eine sehr lange Prompt-Datei verlangsamt die Inferenz und verschlechtert die Sprachqualit√§t.

#### 3.2 Geschwindigkeitsoptimierung

Falls die Inferenzgeschwindigkeit unzureichend ist, k√∂nnen Sie diese wie folgt erh√∂hen:

- **Distill-Modell und weniger Schritte**: F√ºr das Einzelsprecher-Sprachgenerierungsmodell verwenden wir standardm√§√üig das `zipvoice`-Modell f√ºr bessere Sprachqualit√§t. Wenn schnellere Geschwindigkeit Priorit√§t hat, k√∂nnen Sie auf `zipvoice_distill` umschalten und die Anzahl der `--num-steps` auf bis zu `4` reduzieren (Standard ist 8).

- **CPU-Beschleunigung durch Multithreading**: Beim Ausf√ºhren auf der CPU k√∂nnen Sie den Parameter `--num-thread` (z. B. `--num-thread 4`) verwenden, um die Anzahl der Threads f√ºr h√∂here Geschwindigkeit zu erh√∂hen. Standardm√§√üig wird 1 Thread verwendet.

- **CPU-Beschleunigung mit ONNX**: Beim Ausf√ºhren auf der CPU k√∂nnen Sie ONNX-Modelle mit `zipvoice.bin.infer_zipvoice_onnx` f√ºr h√∂here Geschwindigkeit verwenden (ONNX wird f√ºr Dialoggenerierungsmodelle bisher nicht unterst√ºtzt). F√ºr noch h√∂here Geschwindigkeit k√∂nnen Sie zus√§tzlich `--onnx-int8 True` setzen, um ein INT8-quantisiertes ONNX-Modell zu nutzen. Beachten Sie, dass das quantisierte Modell zu einer gewissen Verschlechterung der Sprachqualit√§t f√ºhrt. **Verwenden Sie ONNX nicht auf der GPU**, da es langsamer ist als PyTorch auf der GPU.

#### 3.3 Speichersteuerung

Der angegebene Text wird anhand von Satzzeichen (f√ºr Einzelsprecher-Sprachsynthese) oder Sprecherwechsel-Symbolen (f√ºr Dialog-Sprachsynthese) in Abschnitte unterteilt. Anschlie√üend werden die Abschnitte stapelweise verarbeitet. Das Modell kann daher beliebig lange Texte mit nahezu konstantem Speicherverbrauch verarbeiten. Sie k√∂nnen den Speicherverbrauch durch Anpassung des Parameters `--max-duration` steuern.

#### 3.4 ‚ÄûRaw‚Äú-Bewertung

Standardm√§√üig werden die Eingaben (Prompt-WAV, Prompt-Transkription und Text) vorverarbeitet, um eine effiziente Inferenz und bessere Leistung zu erzielen. Wenn Sie die ‚Äûrohe‚Äú Leistung des Modells mit den exakt vorgegebenen Eingaben bewerten m√∂chten (z. B. um die Ergebnisse aus unserer Publikation zu reproduzieren), k√∂nnen Sie `--raw-evaluation True` verwenden.

#### 3.5 Kurzer Text

Bei der Spracherzeugung f√ºr sehr kurze Texte (z. B. ein oder zwei W√∂rter) kann es vorkommen, dass die erzeugte Sprache bestimmte Aussprachen ausl√§sst. Um dieses Problem zu beheben, k√∂nnen Sie `--speed 0.3` verwenden (wobei 0.3 ein anpassbarer Wert ist), um die Dauer der erzeugten Sprache zu verl√§ngern.

#### 3.6 Korrektur falsch ausgesprochener chinesischer Polyphon-Zeichen

Wir verwenden [pypinyin](https://github.com/mozillazg/python-pinyin), um chinesische Schriftzeichen in Pinyin umzuwandeln. Allerdings kann es gelegentlich zu falscher Aussprache von **Polyphon-Zeichen** (Â§öÈü≥Â≠ó) kommen.


Um diese Fehl-Aussprache manuell zu korrigieren, setze das **korrigierte Pinyin** in spitze Klammern `< >` und f√ºge das **Tonzeichen** hinzu.

**Beispiel:**

- Originaltext: `ËøôÊääÂâëÈïø‰∏âÂçÅÂÖ¨ÂàÜ`
- Korrigiere das Pinyin von `Èïø`:  `ËøôÊääÂâë<chang2>‰∏âÂçÅÂÖ¨ÂàÜ`

> **Hinweis:** Wenn du mehreren Zeichen manuell Pinyin zuweisen m√∂chtest, setze jedes Pinyin in `< >`, z.B.: `ËøôÊää<jian4><chang2><san1>ÂçÅÂÖ¨ÂàÜ`

#### 3.7 Entfernen von langen Pausen aus der generierten Sprache

Das Modell bestimmt automatisch die Positionen und L√§ngen der Pausen in der generierten Sprache. Gelegentlich gibt es eine lange Pause mitten in der Sprache. Wenn du das nicht m√∂chtest, kannst du `--remove-long-sil` verwenden, um lange Pausen in der Mitte der generierten Sprache zu entfernen (Randpausen werden standardm√§√üig entfernt).

#### 3.8 Modell-Download

Wenn du beim Herunterladen der vortrainierten Modelle Probleme hast, dich mit HuggingFace zu verbinden, versuche, den Endpunkt auf die Mirror-Seite zu wechseln: `export HF_ENDPOINT=https://hf-mirror.com`.

## Trainiere dein eigenes Modell

Siehe das [egs](egs) Verzeichnis f√ºr Beispiele zum Training, Fine-Tuning und zur Auswertung.

## C++-Bereitstellung

Siehe [sherpa-onnx](https://github.com/k2-fsa/sherpa-onnx/pull/2487#issuecomment-3227884498) f√ºr die C++-Bereitstellungsl√∂sung auf der CPU.

## Diskussion & Kommunikation

Du kannst direkt in [Github Issues](https://github.com/k2-fsa/ZipVoice/issues) diskutieren.

Du kannst auch den QR-Code scannen, um unserer Wechat-Gruppe beizutreten oder unserem offiziellen Wechat-Account zu folgen.

| Wechat-Gruppe | Offizieller Wechat-Account |
| ------------- | -------------------------- |
|![wechat](https://k2-fsa.org/zh-CN/assets/pic/wechat_group.jpg) |![wechat](https://k2-fsa.org/zh-CN/assets/pic/wechat_account.jpg) |

## Zitation

```bibtex
@article{zhu2025zipvoice,
      title={ZipVoice: Fast and High-Quality Zero-Shot Text-to-Speech with Flow Matching},
      author={Zhu, Han and Kang, Wei and Yao, Zengwei and Guo, Liyong and Kuang, Fangjun and Li, Zhaoqing and Zhuang, Weiji and Lin, Long and Povey, Daniel},
      journal={arXiv preprint arXiv:2506.13053},
      year={2025}
}

@article{zhu2025zipvoicedialog,
      title={ZipVoice-Dialog: Non-Autoregressive Spoken Dialogue Generation with Flow Matching},
      author={Zhu, Han and Kang, Wei and Guo, Liyong and Yao, Zengwei and Kuang, Fangjun and Zhuang, Weiji and Li, Zhaoqing and Han, Zhifeng and Zhang, Dong and Zhang, Xin and Song, Xingchen and Lin, Long and Povey, Daniel},
      journal={arXiv preprint arXiv:2507.09318},
      year={2025}
}
```




---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-10-06

---