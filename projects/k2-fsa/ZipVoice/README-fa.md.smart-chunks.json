[
  {
    "Id": 1,
    "Content": "\n<div align=\"right\">\n  <details>\n    <summary >ğŸŒ Language</summary>\n    <div>\n      <div align=\"center\">\n        <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=en\">English</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=zh-CN\">ç®€ä½“ä¸­æ–‡</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=zh-TW\">ç¹é«”ä¸­æ–‡</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ja\">æ—¥æœ¬èª</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ko\">í•œêµ­ì–´</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=hi\">à¤¹à¤¿à¤¨à¥à¤¦à¥€</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=th\">à¹„à¸—à¸¢</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=fr\">FranÃ§ais</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=de\">Deutsch</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=es\">EspaÃ±ol</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=it\">Itapano</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ru\">Ğ ÑƒÑÑĞºĞ¸Ğ¹</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=pt\">PortuguÃªs</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=nl\">Nederlands</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=pl\">Polski</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ar\">Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=fa\">ÙØ§Ø±Ø³ÛŒ</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=tr\">TÃ¼rkÃ§e</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=vi\">Tiáº¿ng Viá»‡t</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=id\">Bahasa Indonesia</a>\n      </div>\n    </div>\n  </details>\n</div>\n\n<div align=\"center\">\n\n# ZipVoiceâš¡\n\n## Fast and High-Quality Zero-Shot Text-to-Speech with Flow Matching\n</div>\n\n## Overview\n",
    "ContentSha": "4yvqxrt72y1T/XJlpvF5+w536K1FZVvNlGdllJQl3D8=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "\n<div align=\"right\">\n  <details>\n    <summary >ğŸŒ Ø²Ø¨Ø§Ù†</summary>\n    <div>\n      <div align=\"center\">\n        <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=en\">Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=zh-CN\">ç®€ä½“ä¸­æ–‡</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=zh-TW\">ç¹é«”ä¸­æ–‡</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ja\">Ú˜Ø§Ù¾Ù†ÛŒ</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ko\">Ú©Ø±Ù‡â€ŒØ§ÛŒ</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=hi\">Ù‡Ù†Ø¯ÛŒ</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=th\">ØªØ§ÛŒÙ„Ù†Ø¯ÛŒ</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=fr\">ÙØ±Ø§Ù†Ø³ÙˆÛŒ</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=de\">Ø¢Ù„Ù…Ø§Ù†ÛŒ</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=es\">Ø§Ø³Ù¾Ø§Ù†ÛŒØ§ÛŒÛŒ</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=it\">Ø§ÛŒØªØ§Ù„ÛŒØ§ÛŒÛŒ</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ru\">Ø±ÙˆØ³ÛŒ</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=pt\">Ù¾Ø±ØªØºØ§Ù„ÛŒ</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=nl\">Ù‡Ù„Ù†Ø¯ÛŒ</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=pl\">Ù„Ù‡Ø³ØªØ§Ù†ÛŒ</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ar\">Ø¹Ø±Ø¨ÛŒ</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=fa\">ÙØ§Ø±Ø³ÛŒ</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=tr\">ØªØ±Ú©ÛŒ</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=vi\">ÙˆÛŒØªÙ†Ø§Ù…ÛŒ</a>\n        | <a href=\"https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=id\">Ø§Ù†Ø¯ÙˆÙ†Ø²ÛŒØ§ÛŒÛŒ</a>\n      </div>\n    </div>\n  </details>\n</div>\n\n<div align=\"center\">\n\n# ZipVoiceâš¡\n\n## ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ Ú¯ÙØªØ§Ø± Ø³Ø±ÛŒØ¹ Ùˆ Ø¨Ø§Ú©ÛŒÙÛŒØª Ø¨Ø¯ÙˆÙ† Ù†Ù…ÙˆÙ†Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ø¨Ø§ ØªØ·Ø¨ÛŒÙ‚ Ø¬Ø±ÛŒØ§Ù†\n</div>\n\n## Ù…Ø±ÙˆØ± Ú©Ù„ÛŒ\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 2,
    "Content": "ZipVoice is a series of fast and high-quality zero-shot TTS models based on flow matching.\n\n### 1. Key features\n\n- Small and fast: only 123M parameters.\n\n- High-quality voice cloning: state-of-the-art performance in speaker similarity, intelligibility, and naturalness.\n\n- Multi-lingual: support Chinese and English.\n\n- Multi-mode: support both single-speaker and dialogue speech generation.\n\n### 2. Model variants\n\n<table>\n  <thead>\n    <tr>\n      <th>Model Name</th>\n      <th>Description</th>\n      <th>Paper</th>\n      <th>Demo</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>ZipVoice</td>\n      <td>The basic model supporting zero-shot single-speaker TTS in both Chinese and English.</td>\n      <td rowspan=\"2\"><a href=\"https://arxiv.org/abs/2506.13053\"><img src=\"https://img.shields.io/badge/arXiv-Paper-COLOR.svg\"></a></td>\n      <td rowspan=\"2\"><a href=\"https://zipvoice.github.io\"><img src=\"https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square\"></a></td>\n    </tr>\n    <tr>\n      <td>ZipVoice-Distill</td>\n      <td>The distilled version of ZipVoice, featuring improved speed with minimal performance degradation.</td>\n    </tr>\n    <tr>\n      <td>ZipVoice-Dialog</td>\n      <td>A dialogue generation model built on ZipVoice, capable of generating single-channel two-party spoken dialogues.</td>\n      <td rowspan=\"2\"><a href=\"https://arxiv.org/abs/2507.09318\"><img src=\"https://img.shields.io/badge/arXiv-Paper-COLOR.svg\"></a></td>\n      <td rowspan=\"2\"><a href=\"https://zipvoice-dialog.github.io\"><img src=\"https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square\"></a></td>\n    </tr>",
    "ContentSha": "qUZer10kkUVucN3aqovgCkJ059aWMO67vNYjNUfJcJw=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "ZipVoice Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ§ÛŒ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ TTS Ø³Ø±ÛŒØ¹ Ùˆ Ø¨Ø§ Ú©ÛŒÙÛŒØª Ø¨Ø§Ù„Ø§ Ø¨Ù‡ ØµÙˆØ±Øª zero-shot Ø§Ø³Øª Ú©Ù‡ Ø¨Ø± Ù¾Ø§ÛŒÙ‡ flow matching ØªÙˆØ³Ø¹Ù‡ ÛŒØ§ÙØªÙ‡â€ŒØ§Ù†Ø¯.\n\n### Û±. ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ\n\n- Ú©ÙˆÚ†Ú© Ùˆ Ø³Ø±ÛŒØ¹: ØªÙ†Ù‡Ø§ Û±Û²Û³ Ù…ÛŒÙ„ÛŒÙˆÙ† Ù¾Ø§Ø±Ø§Ù…ØªØ± Ø¯Ø§Ø±Ø¯.\n\n- Ú©Ù„ÙˆÙ†ÛŒÙ†Ú¯ ØµØ¯Ø§ÛŒ Ø¨Ø§ Ú©ÛŒÙÛŒØª Ø¨Ø§Ù„Ø§: Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¯Ø± Ø´Ø¨Ø§Ù‡Øª Ú¯ÙˆÛŒÙ†Ø¯Ù‡ØŒ ÙˆØ¶ÙˆØ­ Ùˆ Ø·Ø¨ÛŒØ¹ÛŒ Ø¨ÙˆØ¯Ù†.\n\n- Ú†Ù†Ø¯Ø²Ø¨Ø§Ù†Ù‡: Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² Ø²Ø¨Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ú†ÛŒÙ†ÛŒ Ùˆ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ.\n\n- Ú†Ù†Ø¯Ø­Ø§Ù„ØªÙ‡: Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² ØªÙˆÙ„ÛŒØ¯ Ú¯ÙØªØ§Ø± ØªÚ©â€ŒÚ¯ÙˆÛŒÙ†Ø¯Ù‡ Ùˆ Ø¯ÛŒØ§Ù„ÙˆÚ¯.\n\n### Û². Ø§Ù†ÙˆØ§Ø¹ Ù…Ø¯Ù„\n\n<table>\n  <thead>\n    <tr>\n      <th>Ù†Ø§Ù… Ù…Ø¯Ù„</th>\n      <th>ØªÙˆØ¶ÛŒØ­Ø§Øª</th>\n      <th>Ù…Ù‚Ø§Ù„Ù‡</th>\n      <th>Ø¯Ù…Ùˆ</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>ZipVoice</td>\n      <td>Ù…Ø¯Ù„ Ù¾Ø§ÛŒÙ‡ Ú©Ù‡ Ø§Ø² TTS zero-shot ØªÚ©â€ŒÚ¯ÙˆÛŒÙ†Ø¯Ù‡ Ø¨Ù‡ Ø²Ø¨Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ú†ÛŒÙ†ÛŒ Ùˆ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.</td>\n      <td rowspan=\"2\"><a href=\"https://arxiv.org/abs/2506.13053\"><img src=\"https://img.shields.io/badge/arXiv-Paper-COLOR.svg\"></a></td>\n      <td rowspan=\"2\"><a href=\"https://zipvoice.github.io\"><img src=\"https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square\"></a></td>\n    </tr>\n    <tr>\n      <td>ZipVoice-Distill</td>\n      <td>Ù†Ø³Ø®Ù‡ ØªÙ‚Ø·ÛŒØ± Ø´Ø¯Ù‡ ZipVoice Ú©Ù‡ Ø³Ø±Ø¹Øª Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡ Ø¨Ø§ Ø§ÙØª Ø­Ø¯Ø§Ù‚Ù„ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø±Ø§ Ø§Ø±Ø§Ø¦Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.</td>\n    </tr>\n    <tr>\n      <td>ZipVoice-Dialog</td>\n      <td>Ù…Ø¯Ù„ ØªÙˆÙ„ÛŒØ¯ Ø¯ÛŒØ§Ù„ÙˆÚ¯ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± ZipVoice Ú©Ù‡ Ù‚Ø§Ø¯Ø± Ø¨Ù‡ ØªÙˆÙ„ÛŒØ¯ Ø¯ÛŒØ§Ù„ÙˆÚ¯â€ŒÙ‡Ø§ÛŒ Ú¯ÙØªØ§Ø±ÛŒ Ø¯ÙˆØ·Ø±ÙÙ‡ Ø¯Ø± ÛŒÚ© Ú©Ø§Ù†Ø§Ù„ Ø§Ø³Øª.</td>\n      <td rowspan=\"2\"><a href=\"https://arxiv.org/abs/2507.09318\"><img src=\"https://img.shields.io/badge/arXiv-Paper-COLOR.svg\"></a></td>\n      <td rowspan=\"2\"><a href=\"https://zipvoice-dialog.github.io\"><img src=\"https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square\"></a></td>\n    </tr>",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 3,
    "Content": "    <tr>\n      <td>ZipVoice-Dialog-Stereo</td>\n      <td>The stereo variant of ZipVoice-Dialog, enabling two-channel dialogue generation with each speaker assigned to a distinct channel.</td>\n    </tr>\n  </tbody>\n</table>\n\n## News\n\n**2025/07/14**: **ZipVoice-Dialog** and **ZipVoice-Dialog-Stereo**, two spoken dialogue generation models, are released. [![arXiv](https://img.shields.io/badge/arXiv-Paper-COLOR.svg)](https://arxiv.org/abs/2507.09318) [![demo page](https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square)](https://zipvoice-dialog.github.io)\n\n**2025/07/14**: **OpenDialog** dataset, a 6.8k-hour spoken dialogue dataset, is released. Download at [![hf](https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Dataset-yellow)](https://huggingface.co/datasets/k2-fsa/OpenDialog), [![ms](https://img.shields.io/badge/ModelScope-Dataset-blue?logo=data)](https://www.modelscope.cn/datasets/k2-fsa/OpenDialog). Check details at [![arXiv](https://img.shields.io/badge/arXiv-Paper-COLOR.svg)](https://arxiv.org/abs/2507.09318).\n\n**2025/06/16**: **ZipVoice** and **ZipVoice-Distill** are released. [![arXiv](https://img.shields.io/badge/arXiv-Paper-COLOR.svg)](https://arxiv.org/abs/2506.13053) [![demo page](https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square)](https://zipvoice.github.io)\n\n## Installation\n\n### 1. Clone the ZipVoice repository\n",
    "ContentSha": "qyykZIxy9KKTHUZalqgKJAhZ9ZS/CaAb/vbLISMH1+E=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "    <tr>\n      <td>ZipVoice-Dialog-Stereo</td>\n      <td>Ù†ÙˆØ¹ Ø§Ø³ØªØ±ÛŒÙˆÛŒ ZipVoice-Dialog Ú©Ù‡ Ø§Ù…Ú©Ø§Ù† ØªÙˆÙ„ÛŒØ¯ Ú¯ÙØªÚ¯ÙˆÛŒ Ø¯Ùˆ Ú©Ø§Ù†Ø§Ù„Ù‡ Ø±Ø§ ÙØ±Ø§Ù‡Ù… Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŒ Ø¨Ù‡ Ø·ÙˆØ±ÛŒ Ú©Ù‡ Ù‡Ø± Ø³Ø®Ù†Ú¯Ùˆ Ø¯Ø± ÛŒÚ© Ú©Ø§Ù†Ø§Ù„ Ù…Ø¬Ø²Ø§ Ù‚Ø±Ø§Ø± Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯.</td>\n    </tr>\n  </tbody>\n</table>\n\n## Ø§Ø®Ø¨Ø§Ø±\n\n**Û±Û´Û°Û´/Û°Û´/Û²Û³**: **ZipVoice-Dialog** Ùˆ **ZipVoice-Dialog-Stereo**ØŒ Ø¯Ùˆ Ù…Ø¯Ù„ ØªÙˆÙ„ÛŒØ¯ Ú¯ÙØªÚ¯ÙˆÛŒ Ú¯ÙØªØ§Ø±ÛŒ Ù…Ù†ØªØ´Ø± Ø´Ø¯Ù†Ø¯. [![arXiv](https://img.shields.io/badge/arXiv-Paper-COLOR.svg)](https://arxiv.org/abs/2507.09318) [![demo page](https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square)](https://zipvoice-dialog.github.io)\n\n**Û±Û´Û°Û´/Û°Û´/Û²Û³**: Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ **OpenDialog**ØŒ ÛŒÚ© Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ú¯ÙØªÚ¯ÙˆÛŒ Ú¯ÙØªØ§Ø±ÛŒ Ø¨Ø§ Ù…Ø¯Øª Ø²Ù…Ø§Ù† Û¶.Û¸ Ù‡Ø²Ø§Ø± Ø³Ø§Ø¹Øª Ù…Ù†ØªØ´Ø± Ø´Ø¯. Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø§Ø² [![hf](https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Dataset-yellow)](https://huggingface.co/datasets/k2-fsa/OpenDialog)ØŒ [![ms](https://img.shields.io/badge/ModelScope-Dataset-blue?logo=data)](https://www.modelscope.cn/datasets/k2-fsa/OpenDialog). Ø¬Ø²Ø¦ÛŒØ§Øª Ø¯Ø± [![arXiv](https://img.shields.io/badge/arXiv-Paper-COLOR.svg)](https://arxiv.org/abs/2507.09318).\n\n**Û±Û´Û°Û´/Û°Û³/Û²Û¶**: **ZipVoice** Ùˆ **ZipVoice-Distill** Ù…Ù†ØªØ´Ø± Ø´Ø¯Ù†Ø¯. [![arXiv](https://img.shields.io/badge/arXiv-Paper-COLOR.svg)](https://arxiv.org/abs/2506.13053) [![demo page](https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square)](https://zipvoice.github.io)\n\n## Ù†ØµØ¨\n\n### Û±. Ù…Ø®Ø²Ù† ZipVoice Ø±Ø§ Ú©Ù„ÙˆÙ† Ú©Ù†ÛŒØ¯\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 4,
    "Content": "```bash\ngit clone https://github.com/k2-fsa/ZipVoice.git\n```",
    "ContentSha": "JJo1EP7bWO0BWMfBMdp5X937bp3+DWhl7nAOm71R7lA=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bash\ngit clone https://github.com/k2-fsa/ZipVoice.git\n```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 5,
    "Content": "\n### 2. (Optional) Create a Python virtual environment\n",
    "ContentSha": "SdfmTQw39ITwBrkeETcUaJ4CPFr6cQ+HCqumZ483/iY=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### Û². (Ø§Ø®ØªÛŒØ§Ø±ÛŒ) Ø§ÛŒØ¬Ø§Ø¯ ÛŒÚ© Ù…Ø­ÛŒØ· Ù…Ø¬Ø§Ø²ÛŒ Ù¾Ø§ÛŒØªÙˆÙ†\n\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 6,
    "Content": "```bash\npython3 -m venv zipvoice\nsource zipvoice/bin/activate\n```",
    "ContentSha": "glR0Rdvd5rjEtSF3LycYEtxGB8VzT68abW/ywL40bxw=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bash\npython3 -m venv zipvoice\nsource zipvoice/bin/activate\n```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 7,
    "Content": "\n### 3. Install the required packages\n",
    "ContentSha": "97VTTWuamYvk4THPyO5Ex48XMRIdHtV0dBXABNHe3qQ=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### Û³. Ø¨Ø³ØªÙ‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø±Ø§ Ù†ØµØ¨ Ú©Ù†ÛŒØ¯\n\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 8,
    "Content": "```bash\npip install -r requirements.txt\n```",
    "ContentSha": "TxMa9uJC0PmBOnm3/TRl4YDLNvSwCWaRNjyXpFhndHU=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bash\npip install -r requirements.txt\n```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 9,
    "Content": "\n### 4. Install k2 for training or efficient inference\n\n**k2 is necessary for training** and can speed up inference. Nevertheless, you can still use the inference mode of ZipVoice without installing k2.\n\n> **Note:**  Make sure to install the k2 version that matches your PyTorch and CUDA version. For example, if you are using pytorch 2.5.1 and CUDA 12.1, you can install k2 as follows:\n",
    "ContentSha": "it0kOYkTtKi6ipV/r+Px+BHDpullJx7ZxRNyR9UNc40=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "### Û´. Ù†ØµØ¨ k2 Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ ÛŒØ§ Ø§Ø³ØªÙ†ØªØ§Ø¬ Ø¨Ù‡ÛŒÙ†Ù‡\n\n**k2 Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ù„Ø§Ø²Ù… Ø§Ø³Øª** Ùˆ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø³Ø±Ø¹Øª Ø§Ø³ØªÙ†ØªØ§Ø¬ Ø±Ø§ Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ù‡Ø¯. Ø¨Ø§ Ø§ÛŒÙ† Ø­Ø§Ù„ØŒ Ù‡Ù…Ú†Ù†Ø§Ù† Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø² Ø­Ø§Ù„Øª Ø§Ø³ØªÙ†ØªØ§Ø¬ ZipVoice Ø¨Ø¯ÙˆÙ† Ù†ØµØ¨ k2 Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯.\n\n> **ØªÙˆØ¬Ù‡:** Ù…Ø·Ù…Ø¦Ù† Ø´ÙˆÛŒØ¯ Ú©Ù‡ Ù†Ø³Ø®Ù‡ k2 Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø§ Ù†Ø³Ø®Ù‡ PyTorch Ùˆ CUDA Ø®ÙˆØ¯ Ø±Ø§ Ù†ØµØ¨ Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯. Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ù…Ø«Ø§Ù„ØŒ Ø§Ú¯Ø± Ø§Ø² pytorch 2.5.1 Ùˆ CUDA 12.1 Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯ØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ k2 Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø²ÛŒØ± Ù†ØµØ¨ Ú©Ù†ÛŒØ¯:\n\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 10,
    "Content": "```bash\npip install k2==1.24.4.dev20250208+cuda12.1.torch2.5.1 -f https://k2-fsa.github.io/k2/cuda.html\n```",
    "ContentSha": "ScZHk5YUmlVP9WL5ZgwHsvBVbwTCenfwKc3cJb6nrWo=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bash\npip install k2==1.24.4.dev20250208+cuda12.1.torch2.5.1 -f https://k2-fsa.github.io/k2/cuda.html\n```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 11,
    "Content": "\nPlease refer to https://k2-fsa.org/get-started/k2/ for details.\nUsers in China mainland can refer to https://k2-fsa.org/zh-CN/get-started/k2/.\n\n- To check the k2 installation:\n",
    "ContentSha": "rY1Zhq3aduWMw4XQSft+r2qBy7l8kqnHBs8cs4FGpZ4=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "Ù„Ø·ÙØ§Ù‹ Ø¨Ø±Ø§ÛŒ Ø¬Ø²Ø¦ÛŒØ§Øª Ø¨Ù‡ https://k2-fsa.org/get-started/k2/ Ù…Ø±Ø§Ø¬Ø¹Ù‡ Ú©Ù†ÛŒØ¯.\nÚ©Ø§Ø±Ø¨Ø±Ø§Ù† Ø³Ø±Ø²Ù…ÛŒÙ† Ø§ØµÙ„ÛŒ Ú†ÛŒÙ† Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù†Ø¯ Ø¨Ù‡ https://k2-fsa.org/zh-CN/get-started/k2/ Ù…Ø±Ø§Ø¬Ø¹Ù‡ Ú©Ù†Ù†Ø¯.\n\n- Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ù†ØµØ¨ k2:\n\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 12,
    "Content": "```bash\npython3 -c \"import k2; print(k2.__file__)\"\n```",
    "ContentSha": "PH4E7oc9yWRIVi2n1L4BiFuChpCHjHuJ65gAwNBxvHE=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bash\npython3 -c \"import k2; print(k2.__file__)\"\n```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 13,
    "Content": "\n## Usage\n\n### 1. Single-speaker speech generation\n\nTo generate single-speaker speech with our pre-trained ZipVoice or ZipVoice-Distill models, use the following commands (Required models will be downloaded from HuggingFace):\n\n#### 1.1 Inference of a single sentence\n",
    "ContentSha": "B4h5YyIywYfr2neMM1PwmS6nY8/eY48N4jkDeP3wG+Y=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "## Ù†Ø­ÙˆÙ‡ Ø§Ø³ØªÙØ§Ø¯Ù‡\n\n### Û±. ØªÙˆÙ„ÛŒØ¯ Ú¯ÙØªØ§Ø± ØªÚ©â€ŒÚ¯ÙˆÛŒÙ†Ø¯Ù‡\n\nØ¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ú¯ÙØªØ§Ø± ØªÚ©â€ŒÚ¯ÙˆÛŒÙ†Ø¯Ù‡ Ø¨Ø§ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ZipVoice ÛŒØ§ ZipVoice-Distill Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡ Ù…Ø§ØŒ Ø§Ø² Ø¯Ø³ØªÙˆØ±Ø§Øª Ø²ÛŒØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ (Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø§Ø² HuggingFace Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø®ÙˆØ§Ù‡Ù†Ø¯ Ø´Ø¯):\n\n#### Û±.Û± Ø§Ø³ØªÙ†ØªØ§Ø¬ ÛŒÚ© Ø¬Ù…Ù„Ù‡ Ù…Ù†ÙØ±Ø¯\n\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 14,
    "Content": "```bash\npython3 -m zipvoice.bin.infer_zipvoice \\\n    --model-name zipvoice \\\n    --prompt-wav prompt.wav \\\n    --prompt-text \"I am the transcription of the prompt wav.\" \\\n    --text \"I am the text to be synthesized.\" \\\n    --res-wav-path result.wav\n```",
    "ContentSha": "s3L0IUGcm9ppsQesBx8AKaxAjpRhRDaqb8rE/HLRinc=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bash\npython3 -m zipvoice.bin.infer_zipvoice \\\n    --model-name zipvoice \\\n    --prompt-wav prompt.wav \\\n    --prompt-text \"I am the transcription of the prompt wav.\" \\\n    --text \"I am the text to be synthesized.\" \\\n    --res-wav-path result.wav\n```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 15,
    "Content": "- `--model-name` can be `zipvoice` or `zipvoice_distill`, which are models before and after distillation, respectively.\n- If `<>` or `[]` appear in the text, strings enclosed by them will be treated as special tokens. `<>` denotes Chinese pinyin and `[]` denotes other special tags.\n\n#### 1.2 Inference of a list of sentences\n",
    "ContentSha": "9gu4tqRbp3LNcYk2S6twKEix9A9CPCJP4IQ8qVja+jw=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "- `--model-name` Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ `zipvoice` ÛŒØ§ `zipvoice_distill` Ø¨Ø§Ø´Ø¯ØŒ Ú©Ù‡ Ø¨Ù‡ ØªØ±ØªÛŒØ¨ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù‚Ø¨Ù„ Ùˆ Ø¨Ø¹Ø¯ Ø§Ø² ØªÙ‚Ø·ÛŒØ± Ù‡Ø³ØªÙ†Ø¯.\n- Ø§Ú¯Ø± `<>` ÛŒØ§ `[]` Ø¯Ø± Ù…ØªÙ† Ø¸Ø§Ù‡Ø± Ø´ÙˆÙ†Ø¯ØŒ Ø±Ø´ØªÙ‡â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ ØªÙˆØ³Ø· Ø¢Ù†â€ŒÙ‡Ø§ Ø§Ø­Ø§Ø·Ù‡ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ ÙˆÛŒÚ˜Ù‡ Ø¯Ø± Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ‡ Ø®ÙˆØ§Ù‡Ù†Ø¯ Ø´Ø¯. `<>` Ù†Ø´Ø§Ù†â€ŒØ¯Ù‡Ù†Ø¯Ù‡ Ù¾ÛŒÙ†â€ŒÛŒÛŒÙ† Ú†ÛŒÙ†ÛŒ Ùˆ `[]` Ù†Ø´Ø§Ù†â€ŒØ¯Ù‡Ù†Ø¯Ù‡ Ø³Ø§ÛŒØ± Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ ÙˆÛŒÚ˜Ù‡ Ø§Ø³Øª.\n\n#### 1.2 Ø§Ø³ØªÙ†ØªØ§Ø¬ ÛŒÚ© Ù„ÛŒØ³Øª Ø§Ø² Ø¬Ù…Ù„Ø§Øª\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 16,
    "Content": "```bash\npython3 -m zipvoice.bin.infer_zipvoice \\\n    --model-name zipvoice \\\n    --test-list test.tsv \\\n    --res-dir results\n```",
    "ContentSha": "wVKNQBx9Qf3wuIvCUTrQwDyZzDuqDpC7W9a1psJg5Ds=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bash\npython3 -m zipvoice.bin.infer_zipvoice \\\n    --model-name zipvoice \\\n    --test-list test.tsv \\\n    --res-dir results\n```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 17,
    "Content": "\n- Each line of `test.tsv` is in the format of `{wav_name}\\t{prompt_transcription}\\t{prompt_wav}\\t{text}`.\n\n### 2. Dialogue speech generation\n\n#### 2.1 Inference command\n\nTo generate two-party spoken dialogues with our pre-trained ZipVoice-Dialogue or ZipVoice-Dialogue-Stereo models, use the following commands (Required models will be downloaded from HuggingFace):\n",
    "ContentSha": "bkRixLiKF8JLzfAqriyk1UZSkb5qCPhJVvI3VACgZos=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "- Ù‡Ø± Ø®Ø· Ø§Ø² ÙØ§ÛŒÙ„ `test.tsv` Ø¨Ù‡ ØµÙˆØ±Øª `{wav_name}\\t{prompt_transcription}\\t{prompt_wav}\\t{text}` Ø§Ø³Øª.\n\n### 2. ØªÙˆÙ„ÛŒØ¯ Ú¯ÙØªØ§Ø± Ú¯ÙØªÚ¯Ùˆ\n\n#### 2.1 ÙØ±Ù…Ø§Ù† Ø§Ø³ØªÙ†ØªØ§Ø¬\n\nØ¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ú¯ÙØªÚ¯ÙˆÙ‡Ø§ÛŒ Ú¯ÙØªØ§Ø±ÛŒ Ø¯Ùˆ Ù†ÙØ±Ù‡ Ø¨Ø§ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ZipVoice-Dialogue ÛŒØ§ ZipVoice-Dialogue-Stereo Ú©Ù‡ Ø§Ø² Ù¾ÛŒØ´ Ø¢Ù…ÙˆØ²Ø´ Ø¯ÛŒØ¯Ù‡â€ŒØ§Ù†Ø¯ØŒ Ø§Ø² Ø¯Ø³ØªÙˆØ±Ø§Øª Ø²ÛŒØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ (Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø§Ø² HuggingFace Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø®ÙˆØ§Ù‡Ù†Ø¯ Ø´Ø¯):\n\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 18,
    "Content": "```bash\npython3 -m zipvoice.bin.infer_zipvoice_dialog \\\n    --model-name \"zipvoice_dialog\" \\\n    --test-list test.tsv \\\n    --res-dir results\n```",
    "ContentSha": "SmNrjO7IvCsVTs0ROGG3evCMgCtj54DYGkGCZbRdz8k=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bash\npython3 -m zipvoice.bin.infer_zipvoice_dialog \\\n    --model-name \"zipvoice_dialog\" \\\n    --test-list test.tsv \\\n    --res-dir results\n```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 19,
    "Content": "\n- `--model-name` can be `zipvoice_dialog` or `zipvoice_dialog_stereo`,\n    which generate mono and stereo dialogues, respectively.\n\n#### 2.2 Input formats\n\nEach line of `test.tsv` is in one of the following formats:\n\n(1) **Merged prompt format** where the audios and transcriptions of two speakers prompts are merged into one prompt wav file:",
    "ContentSha": "e336Qt1qFvFmNefniyPEWJue5A1mLBZUSlD6p1+H8To=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "- `--model-name` Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ `zipvoice_dialog` ÛŒØ§ `zipvoice_dialog_stereo` Ø¨Ø§Ø´Ø¯ØŒ\n    Ú©Ù‡ Ø¨Ù‡ ØªØ±ØªÛŒØ¨ Ø¯ÛŒØ§Ù„ÙˆÚ¯â€ŒÙ‡Ø§ÛŒ Ù…ÙˆÙ†Ùˆ Ùˆ Ø§Ø³ØªØ±ÛŒÙˆ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯.\n\n#### 2.2 ÙØ±Ù…Øªâ€ŒÙ‡Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒ\n\nÙ‡Ø± Ø®Ø· Ø§Ø² ÙØ§ÛŒÙ„ `test.tsv` ÛŒÚ©ÛŒ Ø§Ø² ÙØ±Ù…Øªâ€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ± Ø±Ø§ Ø¯Ø§Ø±Ø¯:\n\n(1) **ÙØ±Ù…Øª Ù¾Ø±Ø§Ù…Ù¾Øª Ø§Ø¯ØºØ§Ù…â€ŒØ´Ø¯Ù‡** Ú©Ù‡ Ø¯Ø± Ø¢Ù† ØµØ¯Ø§Ù‡Ø§ Ùˆ Ø±ÙˆÙ†ÙˆØ´Øªâ€ŒÙ‡Ø§ÛŒ Ø¯Ùˆ Ú¯ÙˆÛŒÙ†Ø¯Ù‡ Ø¯Ø± ÛŒÚ© ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ Ù¾Ø±Ø§Ù…Ù¾Øª ØªØ±Ú©ÛŒØ¨ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯:\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 20,
    "Content": "```\n{wav_name}\\t{prompt_transcription}\\t{prompt_wav}\\t{text}\n```",
    "ContentSha": "F8c2S4lpByZ5Nhd693ESYvOeDT7lT7vF2Txm3q64ync=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```\n{wav_name}\\t{prompt_transcription}\\t{prompt_wav}\\t{text}\n```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 21,
    "Content": "\n- `wav_name` is the name of the output wav file.\n- `prompt_transcription` is the transcription of the conversational prompt wav, e.g, \"[S1] Hello. [S2] How are you?\"\n- `prompt_wav` is the path to the prompt wav.\n- `text` is the text to be synthesized, e.g. \"[S1] I'm fine. [S2] What's your name? [S1] I'm Eric. [S2] Hi Eric.\"\n\n(2) **Splitted prompt format** where the audios and transciptions of two speakers exist in separate files:\n",
    "ContentSha": "Gj5W4GhLunSOhvyVf7uwdfnNL3DFgIeOvHB01tH9I/A=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "\n- `wav_name` Ù†Ø§Ù… ÙØ§ÛŒÙ„ Ø®Ø±ÙˆØ¬ÛŒ wav Ø§Ø³Øª.\n- `prompt_transcription` Ù…ØªÙ† Ø±ÙˆÙ†ÙˆÛŒØ³ÛŒ Ø´Ø¯Ù‡ ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ Ù…Ú©Ø§Ù„Ù…Ù‡â€ŒÛŒ ÙˆØ±ÙˆØ¯ÛŒ Ø§Ø³ØªØŒ Ù…Ø«Ù„Ø§Ù‹ \"[S1] Ø³Ù„Ø§Ù…. [S2] Ø­Ø§Ù„Øª Ú†Ø·ÙˆØ±Ù‡ØŸ\"\n- `prompt_wav` Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ Ù…Ú©Ø§Ù„Ù…Ù‡â€ŒÛŒ ÙˆØ±ÙˆØ¯ÛŒ Ø§Ø³Øª.\n- `text` Ù…ØªÙ†ÛŒ Ø§Ø³Øª Ú©Ù‡ Ù‚Ø±Ø§Ø± Ø§Ø³Øª Ø³Ù†ØªØ² Ø´ÙˆØ¯ØŒ Ù…Ø«Ù„Ø§Ù‹ \"[S1] Ù…Ù† Ø®ÙˆØ¨Ù…. [S2] Ø§Ø³Ù… Ø´Ù…Ø§ Ú†ÛŒÙ‡ØŸ [S1] Ù…Ù† Ø§Ø±ÛŒÚ© Ù‡Ø³ØªÙ…. [S2] Ø³Ù„Ø§Ù… Ø§Ø±ÛŒÚ©.\"\n\n(2) **ÙØ±Ù…Øª Ù…Ú©Ø§Ù„Ù…Ù‡â€ŒÛŒ Ø¬Ø¯Ø§Ø´Ø¯Ù‡** Ú©Ù‡ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ØµÙˆØªÛŒ Ùˆ Ø±ÙˆÙ†ÙˆÛŒØ³ÛŒ Ø¯Ùˆ Ú¯ÙˆÛŒÙ†Ø¯Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ù†Ø¯:\n",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  },
  {
    "Id": 22,
    "Content": "```\n{wav_name}\\t{spk1_prompt_transcription}\\t{spk2_prompt_transcription}\\t{spk1_prompt_wav}\\t{spk2_prompt_wav}\\t{text}\n```",
    "ContentSha": "zPaMLy5mnnAP5WeOve+uEMlDenRN6Anuru4V4waQX9w=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```\n{wav_name}\\t{spk1_prompt_transcription}\\t{spk2_prompt_transcription}\\t{spk1_prompt_wav}\\t{spk2_prompt_wav}\\t{text}\n```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 23,
    "Content": "\n- `wav_name` is the name of the output wav file.\n- `spk1_prompt_transcription` is the transcription of the first speaker's prompt wav, e.g, \"Hello\"\n- `spk2_prompt_transcription` is the transcription of the second speaker's prompt wav, e.g, \"How are you?\"\n- `spk1_prompt_wav` is the path to the first speaker's prompt wav file.\n- `spk2_prompt_wav` is the path to the second speaker's prompt wav file.\n- `text` is the text to be synthesized, e.g. \"[S1] I'm fine. [S2] What's your name? [S1] I'm Eric. [S2] Hi Eric.\"\n\n### 3 Guidance for better usage:\n\n#### 3.1 Prompt length\n\nWe recommand a short prompt wav file (e.g., less than 3 seconds for single-speaker speech generation, less than 10 seconds for dialogue speech generation) for faster inference speed. A very long prompt will slow down the inference and degenerate the speech quality.\n\n#### 3.2 Speed optimization\n\nIf the inference speed is unsatisfactory, you can speed it up as follows:\n\n- **Distill model and less steps**: For the single-speaker speech generation model, we use the `zipvoice` model by default for better speech quality. If faster speed is a priority, you can switch to the `zipvoice_distill` and can reduce the `--num-steps` to as low as `4` (8 by default).\n\n- **CPU speedup with multi-threading**: When running on CPU, you can pass the `--num-thread` parameter (e.g., `--num-thread 4`) to increase the number of threads for faster speed. We use 1 thread by default.\n\n- **CPU speedup with ONNX**: When running on CPU, you can use ONNX models with `zipvoice.bin.infer_zipvoice_onnx` for faster speed (haven't supported ONNX for dialogue generation models yet). For even faster speed, you can further set `--onnx-int8 True` to use an INT8-quantized ONNX model. Note that the quantized model will result in a certain degree of speech quality degradation. **Don't use ONNX on GPU**, as it is slower than PyTorch on GPU.\n\n- **GPU Acceleration with NVIDIA TensorRT**: For a significant performance boost on NVIDIA GPUs, first export the model to a TensorRT engine using zipvoice.bin.tensorrt_export. Then, run inference on your dataset (e.g., a Hugging Face dataset) with zipvoice.bin.infer_zipvoice. This can achieve approximately 2x the throughput compared to the standard PyTorch implementation on a GPU.\n\n#### 3.3 Memory control\n\nThe given text will be splitted into chunks based on punctuation (for single-speaker speech generation) or speaker-turn symbol (for dialogue speech generation). Then, the chunked texts will be processed in batches. Therefore, the model can process arbitrarily long text with almost constant memory usage. You can control memory usage by adjusting the `--max-duration` parameter.\n\n#### 3.4 \"Raw\" evaluation\n\nBy default, we preprocess inputs (prompt wav, prompt transcription, and text) for efficient inference and better performance. If you want to evaluate the modelâ€™s \"raw\" performance using exact provided inputs (e.g., to reproduce the results in our paper), you can pass `--raw-evaluation True`.\n\n#### 3.5 Short text\n\nWhen generating speech for very short texts (e.g., one or two words), the generated speech may sometimes omit certain pronunciations. To resolve this issue, you can pass `--speed 0.3` (where 0.3 is a tunable value) to extend the duration of the generated speech.\n\n#### 3.6 Correcting mispronounced chinese polyphone characters\n",
    "ContentSha": "6AwuUDJOteKl74OkYkXg6kAf+AJPVnWbItdRfk762Xs=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "- `wav_name` Ù†Ø§Ù… ÙØ§ÛŒÙ„ Ø®Ø±ÙˆØ¬ÛŒ wav Ø§Ø³Øª.\n- `spk1_prompt_transcription` Ø±ÙˆÙ†ÙˆÛŒØ³ÛŒ ÙØ§ÛŒÙ„ wav Ù†Ù…ÙˆÙ†Ù‡ Ú¯ÙˆÛŒÙ†Ø¯Ù‡ Ø§ÙˆÙ„ Ø§Ø³ØªØŒ Ù…Ø«Ù„Ø§ \"Ø³Ù„Ø§Ù…\"\n- `spk2_prompt_transcription` Ø±ÙˆÙ†ÙˆÛŒØ³ÛŒ ÙØ§ÛŒÙ„ wav Ù†Ù…ÙˆÙ†Ù‡ Ú¯ÙˆÛŒÙ†Ø¯Ù‡ Ø¯ÙˆÙ… Ø§Ø³ØªØŒ Ù…Ø«Ù„Ø§ \"Ø­Ø§Ù„Øª Ú†Ø·ÙˆØ±Ù‡ØŸ\"\n- `spk1_prompt_wav` Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ wav Ù†Ù…ÙˆÙ†Ù‡ Ú¯ÙˆÛŒÙ†Ø¯Ù‡ Ø§ÙˆÙ„ Ø§Ø³Øª.\n- `spk2_prompt_wav` Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ wav Ù†Ù…ÙˆÙ†Ù‡ Ú¯ÙˆÛŒÙ†Ø¯Ù‡ Ø¯ÙˆÙ… Ø§Ø³Øª.\n- `text` Ù…ØªÙ†ÛŒ Ø§Ø³Øª Ú©Ù‡ Ø¨Ø§ÛŒØ¯ Ø³Ù†ØªØ² Ø´ÙˆØ¯ØŒ Ù…Ø«Ù„Ø§ \"[S1] Ù…Ù† Ø®ÙˆØ¨Ù…. [S2] Ø§Ø³Ù… Ø´Ù…Ø§ Ú†ÛŒØ³ØªØŸ [S1] Ù…Ù† Ø§Ø±ÛŒÚ© Ù‡Ø³ØªÙ…. [S2] Ø³Ù„Ø§Ù… Ø§Ø±ÛŒÚ©.\"\n\n### 3 Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¨Ù‡ØªØ±:\n\n#### 3.1 Ø·ÙˆÙ„ Ù†Ù…ÙˆÙ†Ù‡\n\nÙ…Ø§ ÛŒÚ© ÙØ§ÛŒÙ„ Ù†Ù…ÙˆÙ†Ù‡ wav Ú©ÙˆØªØ§Ù‡ (Ù…Ø«Ù„Ø§Ù‹ Ú©Ù…ØªØ± Ø§Ø² Û³ Ø«Ø§Ù†ÛŒÙ‡ Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ú¯ÙØªØ§Ø± ØªÚ©â€ŒÚ¯ÙˆÛŒÙ†Ø¯Ù‡ØŒ Ú©Ù…ØªØ± Ø§Ø² Û±Û° Ø«Ø§Ù†ÛŒÙ‡ Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ú¯ÙØªØ§Ø± Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒ) Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø§ÙØ²Ø§ÛŒØ´ Ø³Ø±Ø¹Øª Ø§Ø³ØªÙ†ØªØ§Ø¬ ØªÙˆØµÛŒÙ‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…. ÛŒÚ© Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ø³ÛŒØ§Ø± Ø·ÙˆÙ„Ø§Ù†ÛŒ Ø¨Ø§Ø¹Ø« Ú©Ø§Ù‡Ø´ Ø³Ø±Ø¹Øª Ø§Ø³ØªÙ†ØªØ§Ø¬ Ùˆ Ø§ÙØª Ú©ÛŒÙÛŒØª Ú¯ÙØªØ§Ø± Ù…ÛŒâ€ŒØ´ÙˆØ¯.\n\n#### 3.2 Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³Ø±Ø¹Øª\n\nØ§Ú¯Ø± Ø³Ø±Ø¹Øª Ø§Ø³ØªÙ†ØªØ§Ø¬ Ù…Ø·Ù„ÙˆØ¨ Ù†ÛŒØ³ØªØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø¨Ø§ Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ± Ø¢Ù† Ø±Ø§ Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ù‡ÛŒØ¯:\n\n- **Ù…Ø¯Ù„ ØªÙ‚Ø·ÛŒØ±Ø´Ø¯Ù‡ Ùˆ Ù…Ø±Ø§Ø­Ù„ Ú©Ù…ØªØ±**: Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ ØªÙˆÙ„ÛŒØ¯ Ú¯ÙØªØ§Ø± ØªÚ©â€ŒÚ¯ÙˆÛŒÙ†Ø¯Ù‡ØŒ Ø¨Ù‡ Ø·ÙˆØ± Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø§Ø² Ù…Ø¯Ù„ `zipvoice` Ø¨Ø±Ø§ÛŒ Ú©ÛŒÙÛŒØª Ú¯ÙØªØ§Ø± Ø¨Ù‡ØªØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…. Ø§Ú¯Ø± Ø³Ø±Ø¹Øª Ø¨Ø§Ù„Ø§ØªØ± Ø§ÙˆÙ„ÙˆÛŒØª Ø¯Ø§Ø±Ø¯ØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø¨Ù‡ `zipvoice_distill` Ø³ÙˆØ¦ÛŒÚ† Ú©Ù†ÛŒØ¯ Ùˆ Ù…Ù‚Ø¯Ø§Ø± `--num-steps` Ø±Ø§ ØªØ§ Ø­Ø¯Ø§Ù‚Ù„ `4` (Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Û¸) Ú©Ø§Ù‡Ø´ Ø¯Ù‡ÛŒØ¯.\n\n- **Ø§ÙØ²Ø§ÛŒØ´ Ø³Ø±Ø¹Øª CPU Ø¨Ø§ Ú†Ù†Ø¯Ø±ÛŒØ³Ù…Ø§Ù†ÛŒ**: Ù‡Ù†Ú¯Ø§Ù… Ø§Ø¬Ø±Ø§ Ø±ÙˆÛŒ CPUØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø¨Ø§ Ù¾Ø§Ø±Ø§Ù…ØªØ± `--num-thread` (Ù…Ø«Ù„Ø§Ù‹ `--num-thread 4`) ØªØ¹Ø¯Ø§Ø¯ Ø±ÛŒØ³Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹Øª Ø¨ÛŒØ´ØªØ± Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ù‡ÛŒØ¯. Ø¨Ù‡ Ø·ÙˆØ± Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø§Ø² Û± Ø±ÛŒØ³Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….\n\n- **Ø§ÙØ²Ø§ÛŒØ´ Ø³Ø±Ø¹Øª CPU Ø¨Ø§ ONNX**: Ù‡Ù†Ú¯Ø§Ù… Ø§Ø¬Ø±Ø§ Ø±ÙˆÛŒ CPUØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ONNX Ø¨Ø§ `zipvoice.bin.infer_zipvoice_onnx` Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹Øª Ø¨ÛŒØ´ØªØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ (Ù‡Ù†ÙˆØ² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ú¯ÙØªØ§Ø± Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒ Ø§Ø² ONNX Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù†Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯). Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹Øª Ø¨Ø§Ù„Ø§ØªØ±ØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ù…Ù‚Ø¯Ø§Ø± `--onnx-int8 True` Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ ONNX Ø¨Ø§ Ú©Ù…ÛŒØª INT8 ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯. ØªÙˆØ¬Ù‡ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯ Ú©Ù‡ Ù…Ø¯Ù„ Ú©Ù…ÛŒØª Ø´Ø¯Ù‡ Ø¨Ø§Ø¹Ø« Ú©Ø§Ù‡Ø´ Ú©ÛŒÙÛŒØª Ú¯ÙØªØ§Ø± Ù…ÛŒâ€ŒØ´ÙˆØ¯. **Ø§Ø² ONNX Ø±ÙˆÛŒ GPU Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ú©Ù†ÛŒØ¯**ØŒ Ú†ÙˆÙ† Ù†Ø³Ø¨Øª Ø¨Ù‡ PyTorch Ø±ÙˆÛŒ GPU Ú©Ù†Ø¯ØªØ± Ø§Ø³Øª.\n\n- **Ø´ØªØ§Ø¨ GPU Ø¨Ø§ NVIDIA TensorRT**: Ø¨Ø±Ø§ÛŒ Ø§ÙØ²Ø§ÛŒØ´ Ù‚Ø§Ø¨Ù„ ØªÙˆØ¬Ù‡ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø±ÙˆÛŒ Ú©Ø§Ø±Øªâ€ŒÙ‡Ø§ÛŒ NVIDIAØŒ Ø§Ø¨ØªØ¯Ø§ Ù…Ø¯Ù„ Ø±Ø§ Ø¨Ø§ zipvoice.bin.tensorrt_export Ø¨Ù‡ Ù…ÙˆØªÙˆØ± TensorRT ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†ÛŒØ¯. Ø³Ù¾Ø³ Ø§Ø³ØªÙ†ØªØ§Ø¬ Ø±Ø§ Ø±ÙˆÛŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ø®ÙˆØ¯ (Ù…Ø«Ù„Ø§Ù‹ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Hugging Face) Ø¨Ø§ zipvoice.bin.infer_zipvoice Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒØ¯. Ø§ÛŒÙ† Ú©Ø§Ø± Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹ Ø¯Ùˆ Ø¨Ø±Ø§Ø¨Ø± Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ PyTorch Ø±ÙˆÛŒ GPU Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯.\n\n#### 3.3 Ú©Ù†ØªØ±Ù„ Ø­Ø§ÙØ¸Ù‡\n\nÙ…ØªÙ† Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¹Ù„Ø§Ø¦Ù… Ù†Ú¯Ø§Ø±Ø´ÛŒ (Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ú¯ÙØªØ§Ø± ØªÚ©â€ŒÚ¯ÙˆÛŒÙ†Ø¯Ù‡) ÛŒØ§ Ù†Ù…Ø§Ø¯ ØªØºÛŒÛŒØ± Ú¯ÙˆÛŒÙ†Ø¯Ù‡ (Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ú¯ÙØªØ§Ø± Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒ) Ø¨Ù‡ Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒÛŒ ØªÙ‚Ø³ÛŒÙ… Ù…ÛŒâ€ŒØ´ÙˆØ¯. Ø³Ù¾Ø³ØŒ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø¨Ø®Ø´â€ŒØ¨Ù†Ø¯ÛŒ Ø´Ø¯Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯. Ø¨Ù†Ø§Ø¨Ø±Ø§ÛŒÙ†ØŒ Ù…Ø¯Ù„ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø¨Ø³ÛŒØ§Ø± Ø·ÙˆÙ„Ø§Ù†ÛŒ Ø±Ø§ Ø¨Ø§ ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹ Ø­Ø§ÙØ¸Ù‡ Ø«Ø§Ø¨Øª Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú©Ù†Ø¯. Ø´Ù…Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø¨Ø§ ØªÙ†Ø¸ÛŒÙ… Ù¾Ø§Ø±Ø§Ù…ØªØ± `--max-duration` Ù…ÛŒØ²Ø§Ù† Ù…ØµØ±Ù Ø­Ø§ÙØ¸Ù‡ Ø±Ø§ Ú©Ù†ØªØ±Ù„ Ú©Ù†ÛŒØ¯.\n\n#### 3.4 Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ \"Ø®Ø§Ù…\"\n\nØ¨Ù‡ Ø·ÙˆØ± Ù¾ÛŒØ´â€ŒÙØ±Ø¶ØŒ ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ (ÙØ§ÛŒÙ„ Ù†Ù…ÙˆÙ†Ù‡ wavØŒ Ø±ÙˆÙ†ÙˆÛŒØ³ÛŒ Ù†Ù…ÙˆÙ†Ù‡ØŒ Ùˆ Ù…ØªÙ†) Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙ†ØªØ§Ø¬ Ø¨Ù‡ÛŒÙ†Ù‡ Ùˆ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¨Ù‡ØªØ± Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…. Ø§Ú¯Ø± Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒØ¯ Ø¹Ù…Ù„Ú©Ø±Ø¯ \"Ø®Ø§Ù…\" Ù…Ø¯Ù„ Ø±Ø§ Ø¨Ø§ ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©Ù†ÛŒØ¯ (Ù…Ø«Ù„Ø§Ù‹ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø²ØªÙˆÙ„ÛŒØ¯ Ù†ØªØ§ÛŒØ¬ Ù…Ù‚Ø§Ù„Ù‡ Ù…Ø§)ØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ù…Ù‚Ø¯Ø§Ø± `--raw-evaluation True` Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯.\n\n#### 3.5 Ù…ØªÙ† Ú©ÙˆØªØ§Ù‡\n\nÙ‡Ù†Ú¯Ø§Ù… ØªÙˆÙ„ÛŒØ¯ Ú¯ÙØªØ§Ø± Ø¨Ø±Ø§ÛŒ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø¨Ø³ÛŒØ§Ø± Ú©ÙˆØªØ§Ù‡ (Ù…Ø«Ù„Ø§Ù‹ ÛŒÚ© ÛŒØ§ Ø¯Ùˆ Ú©Ù„Ù…Ù‡)ØŒ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ú¯ÙØªØ§Ø± ØªÙˆÙ„ÛŒØ¯Ø´Ø¯Ù‡ Ø¨Ø±Ø®ÛŒ ØªÙ„ÙØ¸â€ŒÙ‡Ø§ Ø±Ø§ Ø­Ø°Ù Ú©Ù†Ø¯. Ø¨Ø±Ø§ÛŒ Ø±ÙØ¹ Ø§ÛŒÙ† Ù…Ø´Ú©Ù„ØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ù…Ù‚Ø¯Ø§Ø± `--speed 0.3` (Ú©Ù‡ Ø¹Ø¯Ø¯ Û°.Û³ Ù‚Ø§Ø¨Ù„ ØªÙ†Ø¸ÛŒÙ… Ø§Ø³Øª) Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯ ØªØ§ Ù…Ø¯Øª Ø²Ù…Ø§Ù† Ú¯ÙØªØ§Ø± ØªÙˆÙ„ÛŒØ¯Ø´Ø¯Ù‡ Ø§ÙØ²Ø§ÛŒØ´ ÛŒØ§Ø¨Ø¯.\n\n#### 3.6 Ø§ØµÙ„Ø§Ø­ ØªÙ„ÙØ¸ Ø§Ø´ØªØ¨Ø§Ù‡ Ù†ÙˆÛŒØ³Ù‡â€ŒÙ‡Ø§ÛŒ Ú†Ù†Ø¯Ø¢ÙˆØ§ÛŒÛŒ Ú†ÛŒÙ†ÛŒ\n\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "- `wav_name` Ù†Ø§Ù… ÙØ§ÛŒÙ„ Ø®Ø±ÙˆØ¬ÛŒ wav Ø§Ø³Øª."
      },
      {
        "row": 2,
        "rowsha": "6vgIR6WfUAFmULhn8LhxFttPwZEo/YwGhN1DAECwHCQ=",
        "originContent": "- `wav_name` is the name of the output wav file.",
        "translatedContent": "- `spk1_prompt_transcription` Ø±ÙˆÙ†ÙˆÛŒØ³ÛŒ ÙØ§ÛŒÙ„ wav Ù†Ù…ÙˆÙ†Ù‡ Ú¯ÙˆÛŒÙ†Ø¯Ù‡ Ø§ÙˆÙ„ Ø§Ø³ØªØŒ Ù…Ø«Ù„Ø§ \"Ø³Ù„Ø§Ù…\""
      },
      {
        "row": 3,
        "rowsha": "7BNq8UaBvTut4Ow/oJBAgIDTn3EwEZXK7mlUaYdFwqw=",
        "originContent": "- `spk1_prompt_transcription` is the transcription of the first speaker's prompt wav, e.g, \"Hello\"",
        "translatedContent": "- `spk2_prompt_transcription` Ø±ÙˆÙ†ÙˆÛŒØ³ÛŒ ÙØ§ÛŒÙ„ wav Ù†Ù…ÙˆÙ†Ù‡ Ú¯ÙˆÛŒÙ†Ø¯Ù‡ Ø¯ÙˆÙ… Ø§Ø³ØªØŒ Ù…Ø«Ù„Ø§ \"Ø­Ø§Ù„Øª Ú†Ø·ÙˆØ±Ù‡ØŸ\""
      },
      {
        "row": 4,
        "rowsha": "CES8w9dqVdkdJyOJBUVP282aaKeevVWB3d/+59TEsuk=",
        "originContent": "- `spk2_prompt_transcription` is the transcription of the second speaker's prompt wav, e.g, \"How are you?\"",
        "translatedContent": "- `spk1_prompt_wav` Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ wav Ù†Ù…ÙˆÙ†Ù‡ Ú¯ÙˆÛŒÙ†Ø¯Ù‡ Ø§ÙˆÙ„ Ø§Ø³Øª."
      },
      {
        "row": 5,
        "rowsha": "gXLLRf4BR7Xko2q2l4nK04KIs/L8CjvZ/UBQaP1+vck=",
        "originContent": "- `spk1_prompt_wav` is the path to the first speaker's prompt wav file.",
        "translatedContent": "- `spk2_prompt_wav` Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ wav Ù†Ù…ÙˆÙ†Ù‡ Ú¯ÙˆÛŒÙ†Ø¯Ù‡ Ø¯ÙˆÙ… Ø§Ø³Øª."
      },
      {
        "row": 6,
        "rowsha": "oS1+heJwBnnDtA57WYtG6LbzxK79DOIeb8hwhZQwcDg=",
        "originContent": "- `spk2_prompt_wav` is the path to the second speaker's prompt wav file.",
        "translatedContent": "- `text` Ù…ØªÙ†ÛŒ Ø§Ø³Øª Ú©Ù‡ Ø¨Ø§ÛŒØ¯ Ø³Ù†ØªØ² Ø´ÙˆØ¯ØŒ Ù…Ø«Ù„Ø§ \"[S1] Ù…Ù† Ø®ÙˆØ¨Ù…. [S2] Ø§Ø³Ù… Ø´Ù…Ø§ Ú†ÛŒØ³ØªØŸ [S1] Ù…Ù† Ø§Ø±ÛŒÚ© Ù‡Ø³ØªÙ…. [S2] Ø³Ù„Ø§Ù… Ø§Ø±ÛŒÚ©.\""
      },
      {
        "row": 7,
        "rowsha": "M4Z2DDajNBdyF/JosIaDZ44oyZnjNA7lzfGzEpuoako=",
        "originContent": "- `text` is the text to be synthesized, e.g. \"[S1] I'm fine. [S2] What's your name? [S1] I'm Eric. [S2] Hi Eric.\"",
        "translatedContent": ""
      },
      {
        "row": 8,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "### 3 Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¨Ù‡ØªØ±:"
      },
      {
        "row": 9,
        "rowsha": "SdDI3h73wOzKSM3kbrbNrmpigHGer7kumuaZsQgAeao=",
        "originContent": "### 3 Guidance for better usage:",
        "translatedContent": ""
      },
      {
        "row": 10,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "#### 3.1 Ø·ÙˆÙ„ Ù†Ù…ÙˆÙ†Ù‡"
      },
      {
        "row": 11,
        "rowsha": "cVxukE6jyFFOxlNKI5ecOTo/suYYJ8hnYyW2XA2wg+o=",
        "originContent": "#### 3.1 Prompt length",
        "translatedContent": ""
      },
      {
        "row": 12,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "Ù…Ø§ ÛŒÚ© ÙØ§ÛŒÙ„ Ù†Ù…ÙˆÙ†Ù‡ wav Ú©ÙˆØªØ§Ù‡ (Ù…Ø«Ù„Ø§Ù‹ Ú©Ù…ØªØ± Ø§Ø² Û³ Ø«Ø§Ù†ÛŒÙ‡ Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ú¯ÙØªØ§Ø± ØªÚ©â€ŒÚ¯ÙˆÛŒÙ†Ø¯Ù‡ØŒ Ú©Ù…ØªØ± Ø§Ø² Û±Û° Ø«Ø§Ù†ÛŒÙ‡ Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ú¯ÙØªØ§Ø± Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒ) Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø§ÙØ²Ø§ÛŒØ´ Ø³Ø±Ø¹Øª Ø§Ø³ØªÙ†ØªØ§Ø¬ ØªÙˆØµÛŒÙ‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…. ÛŒÚ© Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ø³ÛŒØ§Ø± Ø·ÙˆÙ„Ø§Ù†ÛŒ Ø¨Ø§Ø¹Ø« Ú©Ø§Ù‡Ø´ Ø³Ø±Ø¹Øª Ø§Ø³ØªÙ†ØªØ§Ø¬ Ùˆ Ø§ÙØª Ú©ÛŒÙÛŒØª Ú¯ÙØªØ§Ø± Ù…ÛŒâ€ŒØ´ÙˆØ¯."
      },
      {
        "row": 13,
        "rowsha": "f19zq78QrLul7wiVSlCSojGS7qNEvtef9GFg6AA8eMY=",
        "originContent": "We recommand a short prompt wav file (e.g., less than 3 seconds for single-speaker speech generation, less than 10 seconds for dialogue speech generation) for faster inference speed. A very long prompt will slow down the inference and degenerate the speech quality.",
        "translatedContent": ""
      },
      {
        "row": 14,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "#### 3.2 Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³Ø±Ø¹Øª"
      },
      {
        "row": 15,
        "rowsha": "lpgNpm20ulCcTiEU/xfEVVgMZhjiQjymkdljF8dD/vw=",
        "originContent": "#### 3.2 Speed optimization",
        "translatedContent": ""
      },
      {
        "row": 16,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "Ø§Ú¯Ø± Ø³Ø±Ø¹Øª Ø§Ø³ØªÙ†ØªØ§Ø¬ Ù…Ø·Ù„ÙˆØ¨ Ù†ÛŒØ³ØªØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø¨Ø§ Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ± Ø¢Ù† Ø±Ø§ Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ù‡ÛŒØ¯:"
      },
      {
        "row": 17,
        "rowsha": "iBJxMfYOjV9HvSuRT3p/EsU/iATeDCDAk/wGWLXqQI8=",
        "originContent": "If the inference speed is unsatisfactory, you can speed it up as follows:",
        "translatedContent": ""
      },
      {
        "row": 18,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "- **Ù…Ø¯Ù„ ØªÙ‚Ø·ÛŒØ±Ø´Ø¯Ù‡ Ùˆ Ù…Ø±Ø§Ø­Ù„ Ú©Ù…ØªØ±**: Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ ØªÙˆÙ„ÛŒØ¯ Ú¯ÙØªØ§Ø± ØªÚ©â€ŒÚ¯ÙˆÛŒÙ†Ø¯Ù‡ØŒ Ø¨Ù‡ Ø·ÙˆØ± Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø§Ø² Ù…Ø¯Ù„ `zipvoice` Ø¨Ø±Ø§ÛŒ Ú©ÛŒÙÛŒØª Ú¯ÙØªØ§Ø± Ø¨Ù‡ØªØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…. Ø§Ú¯Ø± Ø³Ø±Ø¹Øª Ø¨Ø§Ù„Ø§ØªØ± Ø§ÙˆÙ„ÙˆÛŒØª Ø¯Ø§Ø±Ø¯ØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø¨Ù‡ `zipvoice_distill` Ø³ÙˆØ¦ÛŒÚ† Ú©Ù†ÛŒØ¯ Ùˆ Ù…Ù‚Ø¯Ø§Ø± `--num-steps` Ø±Ø§ ØªØ§ Ø­Ø¯Ø§Ù‚Ù„ `4` (Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Û¸) Ú©Ø§Ù‡Ø´ Ø¯Ù‡ÛŒØ¯."
      },
      {
        "row": 19,
        "rowsha": "IzTHxzS1e5yJRhHF5d8CsjfqjIzhNX1AeGR4FTjuUCA=",
        "originContent": "- **Distill model and less steps**: For the single-speaker speech generation model, we use the `zipvoice` model by default for better speech quality. If faster speed is a priority, you can switch to the `zipvoice_distill` and can reduce the `--num-steps` to as low as `4` (8 by default).",
        "translatedContent": ""
      },
      {
        "row": 20,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "- **Ø§ÙØ²Ø§ÛŒØ´ Ø³Ø±Ø¹Øª CPU Ø¨Ø§ Ú†Ù†Ø¯Ø±ÛŒØ³Ù…Ø§Ù†ÛŒ**: Ù‡Ù†Ú¯Ø§Ù… Ø§Ø¬Ø±Ø§ Ø±ÙˆÛŒ CPUØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø¨Ø§ Ù¾Ø§Ø±Ø§Ù…ØªØ± `--num-thread` (Ù…Ø«Ù„Ø§Ù‹ `--num-thread 4`) ØªØ¹Ø¯Ø§Ø¯ Ø±ÛŒØ³Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹Øª Ø¨ÛŒØ´ØªØ± Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ù‡ÛŒØ¯. Ø¨Ù‡ Ø·ÙˆØ± Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø§Ø² Û± Ø±ÛŒØ³Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…."
      },
      {
        "row": 21,
        "rowsha": "mNiqyHjFr4rbx4boH0cix2peH8Q0+tDTOlzgZeLLDqM=",
        "originContent": "- **CPU speedup with multi-threading**: When running on CPU, you can pass the `--num-thread` parameter (e.g., `--num-thread 4`) to increase the number of threads for faster speed. We use 1 thread by default.",
        "translatedContent": ""
      },
      {
        "row": 22,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "- **Ø§ÙØ²Ø§ÛŒØ´ Ø³Ø±Ø¹Øª CPU Ø¨Ø§ ONNX**: Ù‡Ù†Ú¯Ø§Ù… Ø§Ø¬Ø±Ø§ Ø±ÙˆÛŒ CPUØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ONNX Ø¨Ø§ `zipvoice.bin.infer_zipvoice_onnx` Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹Øª Ø¨ÛŒØ´ØªØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ (Ù‡Ù†ÙˆØ² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ú¯ÙØªØ§Ø± Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒ Ø§Ø² ONNX Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù†Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯). Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹Øª Ø¨Ø§Ù„Ø§ØªØ±ØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ù…Ù‚Ø¯Ø§Ø± `--onnx-int8 True` Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ ONNX Ø¨Ø§ Ú©Ù…ÛŒØª INT8 ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯. ØªÙˆØ¬Ù‡ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯ Ú©Ù‡ Ù…Ø¯Ù„ Ú©Ù…ÛŒØª Ø´Ø¯Ù‡ Ø¨Ø§Ø¹Ø« Ú©Ø§Ù‡Ø´ Ú©ÛŒÙÛŒØª Ú¯ÙØªØ§Ø± Ù…ÛŒâ€ŒØ´ÙˆØ¯. **Ø§Ø² ONNX Ø±ÙˆÛŒ GPU Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ú©Ù†ÛŒØ¯**ØŒ Ú†ÙˆÙ† Ù†Ø³Ø¨Øª Ø¨Ù‡ PyTorch Ø±ÙˆÛŒ GPU Ú©Ù†Ø¯ØªØ± Ø§Ø³Øª."
      },
      {
        "row": 23,
        "rowsha": "YqtmXdPz7OfUIbrIkeeIwyKEPdEZUqn3m5EyTNd967s=",
        "originContent": "- **CPU speedup with ONNX**: When running on CPU, you can use ONNX models with `zipvoice.bin.infer_zipvoice_onnx` for faster speed (haven't supported ONNX for dialogue generation models yet). For even faster speed, you can further set `--onnx-int8 True` to use an INT8-quantized ONNX model. Note that the quantized model will result in a certain degree of speech quality degradation. **Don't use ONNX on GPU**, as it is slower than PyTorch on GPU.",
        "translatedContent": ""
      },
      {
        "row": 24,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "- **Ø´ØªØ§Ø¨ GPU Ø¨Ø§ NVIDIA TensorRT**: Ø¨Ø±Ø§ÛŒ Ø§ÙØ²Ø§ÛŒØ´ Ù‚Ø§Ø¨Ù„ ØªÙˆØ¬Ù‡ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø±ÙˆÛŒ Ú©Ø§Ø±Øªâ€ŒÙ‡Ø§ÛŒ NVIDIAØŒ Ø§Ø¨ØªØ¯Ø§ Ù…Ø¯Ù„ Ø±Ø§ Ø¨Ø§ zipvoice.bin.tensorrt_export Ø¨Ù‡ Ù…ÙˆØªÙˆØ± TensorRT ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†ÛŒØ¯. Ø³Ù¾Ø³ Ø§Ø³ØªÙ†ØªØ§Ø¬ Ø±Ø§ Ø±ÙˆÛŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ø®ÙˆØ¯ (Ù…Ø«Ù„Ø§Ù‹ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Hugging Face) Ø¨Ø§ zipvoice.bin.infer_zipvoice Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒØ¯. Ø§ÛŒÙ† Ú©Ø§Ø± Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹ Ø¯Ùˆ Ø¨Ø±Ø§Ø¨Ø± Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ PyTorch Ø±ÙˆÛŒ GPU Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯."
      },
      {
        "row": 25,
        "rowsha": "kj1A4DWWe02Utusq07KI3xRRH55QdxQWRCzFeimIzww=",
        "originContent": "- **GPU Acceleration with NVIDIA TensorRT**: For a significant performance boost on NVIDIA GPUs, first export the model to a TensorRT engine using zipvoice.bin.tensorrt_export. Then, run inference on your dataset (e.g., a Hugging Face dataset) with zipvoice.bin.infer_zipvoice. This can achieve approximately 2x the throughput compared to the standard PyTorch implementation on a GPU.",
        "translatedContent": ""
      },
      {
        "row": 26,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "#### 3.3 Ú©Ù†ØªØ±Ù„ Ø­Ø§ÙØ¸Ù‡"
      },
      {
        "row": 27,
        "rowsha": "fze8iMUXPcPsZgNFyWFzWSuCffZnzh7SpzLs21tQLtE=",
        "originContent": "#### 3.3 Memory control",
        "translatedContent": ""
      },
      {
        "row": 28,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "Ù…ØªÙ† Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¹Ù„Ø§Ø¦Ù… Ù†Ú¯Ø§Ø±Ø´ÛŒ (Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ú¯ÙØªØ§Ø± ØªÚ©â€ŒÚ¯ÙˆÛŒÙ†Ø¯Ù‡) ÛŒØ§ Ù†Ù…Ø§Ø¯ ØªØºÛŒÛŒØ± Ú¯ÙˆÛŒÙ†Ø¯Ù‡ (Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ú¯ÙØªØ§Ø± Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒ) Ø¨Ù‡ Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒÛŒ ØªÙ‚Ø³ÛŒÙ… Ù…ÛŒâ€ŒØ´ÙˆØ¯. Ø³Ù¾Ø³ØŒ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø¨Ø®Ø´â€ŒØ¨Ù†Ø¯ÛŒ Ø´Ø¯Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯. Ø¨Ù†Ø§Ø¨Ø±Ø§ÛŒÙ†ØŒ Ù…Ø¯Ù„ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø¨Ø³ÛŒØ§Ø± Ø·ÙˆÙ„Ø§Ù†ÛŒ Ø±Ø§ Ø¨Ø§ ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹ Ø­Ø§ÙØ¸Ù‡ Ø«Ø§Ø¨Øª Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú©Ù†Ø¯. Ø´Ù…Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø¨Ø§ ØªÙ†Ø¸ÛŒÙ… Ù¾Ø§Ø±Ø§Ù…ØªØ± `--max-duration` Ù…ÛŒØ²Ø§Ù† Ù…ØµØ±Ù Ø­Ø§ÙØ¸Ù‡ Ø±Ø§ Ú©Ù†ØªØ±Ù„ Ú©Ù†ÛŒØ¯."
      },
      {
        "row": 29,
        "rowsha": "uM67TExtHYq7ALHmglqtjLRqv0Xu0OOSx2aFJquZPmw=",
        "originContent": "The given text will be splitted into chunks based on punctuation (for single-speaker speech generation) or speaker-turn symbol (for dialogue speech generation). Then, the chunked texts will be processed in batches. Therefore, the model can process arbitrarily long text with almost constant memory usage. You can control memory usage by adjusting the `--max-duration` parameter.",
        "translatedContent": ""
      },
      {
        "row": 30,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "#### 3.4 Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ \"Ø®Ø§Ù…\""
      },
      {
        "row": 31,
        "rowsha": "foa86E9JcH+Sc/k2OCmyfIKHwggsFBXhSUfHDcmJQA0=",
        "originContent": "#### 3.4 \"Raw\" evaluation",
        "translatedContent": ""
      },
      {
        "row": 32,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "Ø¨Ù‡ Ø·ÙˆØ± Ù¾ÛŒØ´â€ŒÙØ±Ø¶ØŒ ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ (ÙØ§ÛŒÙ„ Ù†Ù…ÙˆÙ†Ù‡ wavØŒ Ø±ÙˆÙ†ÙˆÛŒØ³ÛŒ Ù†Ù…ÙˆÙ†Ù‡ØŒ Ùˆ Ù…ØªÙ†) Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙ†ØªØ§Ø¬ Ø¨Ù‡ÛŒÙ†Ù‡ Ùˆ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¨Ù‡ØªØ± Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…. Ø§Ú¯Ø± Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒØ¯ Ø¹Ù…Ù„Ú©Ø±Ø¯ \"Ø®Ø§Ù…\" Ù…Ø¯Ù„ Ø±Ø§ Ø¨Ø§ ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©Ù†ÛŒØ¯ (Ù…Ø«Ù„Ø§Ù‹ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø²ØªÙˆÙ„ÛŒØ¯ Ù†ØªØ§ÛŒØ¬ Ù…Ù‚Ø§Ù„Ù‡ Ù…Ø§)ØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ù…Ù‚Ø¯Ø§Ø± `--raw-evaluation True` Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯."
      },
      {
        "row": 33,
        "rowsha": "+2nxKNvXmuxUQpf2Z+hw0Rxydt00FpmK4y4rlK5/8og=",
        "originContent": "By default, we preprocess inputs (prompt wav, prompt transcription, and text) for efficient inference and better performance. If you want to evaluate the modelâ€™s \"raw\" performance using exact provided inputs (e.g., to reproduce the results in our paper), you can pass `--raw-evaluation True`.",
        "translatedContent": ""
      },
      {
        "row": 34,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "#### 3.5 Ù…ØªÙ† Ú©ÙˆØªØ§Ù‡"
      },
      {
        "row": 35,
        "rowsha": "g658opDssPKmJCvr7Jw9N130Xud1IbMHTwMK+S89WO0=",
        "originContent": "#### 3.5 Short text",
        "translatedContent": ""
      },
      {
        "row": 36,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "Ù‡Ù†Ú¯Ø§Ù… ØªÙˆÙ„ÛŒØ¯ Ú¯ÙØªØ§Ø± Ø¨Ø±Ø§ÛŒ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø¨Ø³ÛŒØ§Ø± Ú©ÙˆØªØ§Ù‡ (Ù…Ø«Ù„Ø§Ù‹ ÛŒÚ© ÛŒØ§ Ø¯Ùˆ Ú©Ù„Ù…Ù‡)ØŒ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ú¯ÙØªØ§Ø± ØªÙˆÙ„ÛŒØ¯Ø´Ø¯Ù‡ Ø¨Ø±Ø®ÛŒ ØªÙ„ÙØ¸â€ŒÙ‡Ø§ Ø±Ø§ Ø­Ø°Ù Ú©Ù†Ø¯. Ø¨Ø±Ø§ÛŒ Ø±ÙØ¹ Ø§ÛŒÙ† Ù…Ø´Ú©Ù„ØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ù…Ù‚Ø¯Ø§Ø± `--speed 0.3` (Ú©Ù‡ Ø¹Ø¯Ø¯ Û°.Û³ Ù‚Ø§Ø¨Ù„ ØªÙ†Ø¸ÛŒÙ… Ø§Ø³Øª) Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯ ØªØ§ Ù…Ø¯Øª Ø²Ù…Ø§Ù† Ú¯ÙØªØ§Ø± ØªÙˆÙ„ÛŒØ¯Ø´Ø¯Ù‡ Ø§ÙØ²Ø§ÛŒØ´ ÛŒØ§Ø¨Ø¯."
      },
      {
        "row": 37,
        "rowsha": "/IVkHehTtKgVQNwGOgQO/BoRh95RFHVJPH3e0W6Gixs=",
        "originContent": "When generating speech for very short texts (e.g., one or two words), the generated speech may sometimes omit certain pronunciations. To resolve this issue, you can pass `--speed 0.3` (where 0.3 is a tunable value) to extend the duration of the generated speech.",
        "translatedContent": ""
      },
      {
        "row": 38,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "#### 3.6 Ø§ØµÙ„Ø§Ø­ ØªÙ„ÙØ¸ Ø§Ø´ØªØ¨Ø§Ù‡ Ù†ÙˆÛŒØ³Ù‡â€ŒÙ‡Ø§ÛŒ Ú†Ù†Ø¯Ø¢ÙˆØ§ÛŒÛŒ Ú†ÛŒÙ†ÛŒ"
      },
      {
        "row": 39,
        "rowsha": "PAPz1JYDhpLF6dsiNH/BVipH4SufvLcqzLiPLACOcK4=",
        "originContent": "#### 3.6 Correcting mispronounced chinese polyphone characters",
        "translatedContent": ""
      },
      {
        "row": 40,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 24,
    "Content": "We use [pypinyin](https://github.com/mozillazg/python-pinyin) to convert Chinese characters to pinyin. However, it can occasionally mispronounce **polyphone characters** (å¤šéŸ³å­—).\n\nTo manually correct these mispronunciations, enclose the **corrected pinyin** in angle brackets `< >` and include the **tone mark**.\n\n**Example:**\n\n- Original text: `è¿™æŠŠå‰‘é•¿ä¸‰åå…¬åˆ†`\n- Correct the pinyin of `é•¿`:  `è¿™æŠŠå‰‘<chang2>ä¸‰åå…¬åˆ†`\n\n> **Note:** If you want to manually assign multiple pinyins, enclose each pinyin with `<>`, e.g., `è¿™æŠŠ<jian4><chang2><san1>åå…¬åˆ†`\n\n#### 3.7 Remove long silences from the generated speech\n\nModel will automatically determine the positions and lengths of silences in the generated speech. It occasionally has long silence in the middle of the speech. If you don't want this, you can pass `--remove-long-sil` to remove long silences in the middle of the generated speech (edge silences will be removed by default).\n\n#### 3.8 Model downloading\n\nIf you have trouble connecting to HuggingFace when downloading the pre-trained models, try switching endpoint to the mirror site: `export HF_ENDPOINT=https://hf-mirror.com`.\n\n## Train Your Own Model\n\nSee the [egs](egs) directory for training, fine-tuning and evaluation examples.\n\n## Production Deployment\n\n### NVIDIA Triton GPU Runtime\n\nFor production-ready deployment with high performance and scalability, check out the [Triton Inference Server integration](runtime/nvidia_triton/) that provides optimized TensorRT engines, concurrent request handling, and both gRPC/HTTP APIs for enterprise use.\n\n### CPU Deployment\n\nCheck [sherpa-onnx](https://github.com/k2-fsa/sherpa-onnx/pull/2487#issuecomment-3227884498) for the C++ deployment solution on CPU.\n\n## Discussion & Communication\n\nYou can directly discuss on [Github Issues](https://github.com/k2-fsa/ZipVoice/issues).\n\nYou can also scan the QR code to join our wechat group or follow our wechat official account.\n\n| Wechat Group | Wechat Official Account |",
    "ContentSha": "nAAjO+GVPZsjYiLFM/o02EX48i9vuDX4qL4j6+7om6U=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "Ù…Ø§ Ø§Ø² [pypinyin](https://github.com/mozillazg/python-pinyin) Ø¨Ø±Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ø­Ø±ÙˆÙ Ú†ÛŒÙ†ÛŒ Ø¨Ù‡ Ù¾ÛŒÙ†â€ŒÛŒÛŒÙ† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…. Ø¨Ø§ Ø§ÛŒÙ† Ø­Ø§Ù„ØŒ Ú¯Ø§Ù‡ÛŒ Ø§ÙˆÙ‚Ø§Øª Ù…Ù…Ú©Ù† Ø§Ø³Øª **Ø­Ø±ÙˆÙ Ú†Ù†Ø¯Ø¢ÙˆØ§ÛŒÛŒ** (å¤šéŸ³å­—) Ø±Ø§ Ø§Ø´ØªØ¨Ø§Ù‡ ØªÙ„ÙØ¸ Ú©Ù†Ø¯.\n\nØ¨Ø±Ø§ÛŒ Ø§ØµÙ„Ø§Ø­ Ø¯Ø³ØªÛŒ Ø§ÛŒÙ† ØªÙ„ÙØ¸â€ŒÙ‡Ø§ÛŒ Ø§Ø´ØªØ¨Ø§Ù‡ØŒ Ù¾ÛŒÙ†â€ŒÛŒÛŒÙ† **Ø§ØµÙ„Ø§Ø­â€ŒØ´Ø¯Ù‡** Ø±Ø§ Ø¯Ø±ÙˆÙ† Ø¹Ù„Ø§Ù…Øª Ø²Ø§ÙˆÛŒÙ‡â€ŒØ§ÛŒ `< >` Ù‚Ø±Ø§Ø± Ø¯Ù‡ÛŒØ¯ Ùˆ **Ø¹Ù„Ø§Ù…Øª Ù†ØºÙ…Ù‡** Ø±Ø§ Ù†ÛŒØ² Ø¯Ø±Ø¬ Ú©Ù†ÛŒØ¯.\n\n**Ù…Ø«Ø§Ù„:**\n\n- Ù…ØªÙ† Ø§ØµÙ„ÛŒ: `è¿™æŠŠå‰‘é•¿ä¸‰åå…¬åˆ†`\n- Ø§ØµÙ„Ø§Ø­ Ù¾ÛŒÙ†â€ŒÛŒÛŒÙ† ÙˆØ§Ú˜Ù‡ `é•¿`:  `è¿™æŠŠå‰‘<chang2>ä¸‰åå…¬åˆ†`\n\n> **ØªÙˆØ¬Ù‡:** Ø§Ú¯Ø± Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒØ¯ Ú†Ù†Ø¯ Ù¾ÛŒÙ†â€ŒÛŒÛŒÙ† Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø¯Ø³ØªÛŒ ØªØ¹ÛŒÛŒÙ† Ú©Ù†ÛŒØ¯ØŒ Ù‡Ø± Ù¾ÛŒÙ†â€ŒÛŒÛŒÙ† Ø±Ø§ Ø¯Ø§Ø®Ù„ `< >` Ù‚Ø±Ø§Ø± Ø¯Ù‡ÛŒØ¯ØŒ Ù…Ø§Ù†Ù†Ø¯: `è¿™æŠŠ<jian4><chang2><san1>åå…¬åˆ†`\n\n#### Û³.Û· Ø­Ø°Ù Ø³Ú©ÙˆØªâ€ŒÙ‡Ø§ÛŒ Ø·ÙˆÙ„Ø§Ù†ÛŒ Ø§Ø² Ú¯ÙØªØ§Ø± ØªÙˆÙ„ÛŒØ¯Ø´Ø¯Ù‡\n\nÙ…Ø¯Ù„ Ø¨Ù‡ Ø·ÙˆØ± Ø®ÙˆØ¯Ú©Ø§Ø± Ù…ÙˆÙ‚Ø¹ÛŒØª Ùˆ Ø·ÙˆÙ„ Ø³Ú©ÙˆØªâ€ŒÙ‡Ø§ Ø¯Ø± Ú¯ÙØªØ§Ø± ØªÙˆÙ„ÛŒØ¯Ø´Ø¯Ù‡ Ø±Ø§ ØªØ´Ø®ÛŒØµ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯. Ú¯Ø§Ù‡ÛŒ Ø§ÙˆÙ‚Ø§Øª Ø³Ú©ÙˆØª Ø·ÙˆÙ„Ø§Ù†ÛŒ Ø¯Ø± ÙˆØ³Ø· Ú¯ÙØªØ§Ø± Ø§ÛŒØ¬Ø§Ø¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯. Ø§Ú¯Ø± Ø§ÛŒÙ† Ù…ÙˆØ±Ø¯ Ø±Ø§ Ù†Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒØ¯ØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø¨Ø§ Ù¾Ø§Ø±Ø§Ù…ØªØ± `--remove-long-sil` Ø³Ú©ÙˆØªâ€ŒÙ‡Ø§ÛŒ Ø·ÙˆÙ„Ø§Ù†ÛŒ Ø¯Ø± ÙˆØ³Ø· Ú¯ÙØªØ§Ø± ØªÙˆÙ„ÛŒØ¯Ø´Ø¯Ù‡ Ø±Ø§ Ø­Ø°Ù Ú©Ù†ÛŒØ¯ (Ø³Ú©ÙˆØªâ€ŒÙ‡Ø§ÛŒ Ù„Ø¨Ù‡â€ŒØ§ÛŒ Ø¨Ù‡ Ø·ÙˆØ± Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø­Ø°Ù Ø®ÙˆØ§Ù‡Ù†Ø¯ Ø´Ø¯).\n\n#### Û³.Û¸ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„\n\nØ§Ú¯Ø± Ù‡Ù†Ú¯Ø§Ù… Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø§Ø² Ù¾ÛŒØ´ Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡ Ø§Ø² HuggingFace Ø¨Ø§ Ù…Ø´Ú©Ù„ Ø§ØªØµØ§Ù„ Ù…ÙˆØ§Ø¬Ù‡ Ø´Ø¯ÛŒØ¯ØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ù†Ù‚Ø·Ù‡ Ø§Ù†ØªÙ‡Ø§ÛŒÛŒ Ø±Ø§ Ø¨Ù‡ Ø³Ø§ÛŒØª Ù…ÛŒØ±ÙˆØ± ØªØºÛŒÛŒØ± Ø¯Ù‡ÛŒØ¯: `export HF_ENDPOINT=https://hf-mirror.com`.\n\n## Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø§Ø®ØªØµØ§ØµÛŒ\n\nØ¨Ø±Ø§ÛŒ Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ØŒ ØªÙ†Ø¸ÛŒÙ… Ø¯Ù‚ÛŒÙ‚ Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¨Ù‡ Ø¯Ø§ÛŒØ±Ú©ØªÙˆØ±ÛŒ [egs](egs) Ù…Ø±Ø§Ø¬Ø¹Ù‡ Ú©Ù†ÛŒØ¯.\n\n## Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø¯Ø± Ù…Ø­ÛŒØ· ØªÙˆÙ„ÛŒØ¯\n\n### Ø§Ø¬Ø±Ø§ÛŒ GPU Ø¨Ø§ NVIDIA Triton\n\nØ¨Ø±Ø§ÛŒ Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø¢Ù…Ø§Ø¯Ù‡ ØªÙˆÙ„ÛŒØ¯ Ø¨Ø§ Ú©Ø§Ø±Ø§ÛŒÛŒ Ùˆ Ù…Ù‚ÛŒØ§Ø³â€ŒÙ¾Ø°ÛŒØ±ÛŒ Ø¨Ø§Ù„Ø§ØŒ Ø¨Ù‡ [Ø§Ø¯ØºØ§Ù… Ø³Ø±ÙˆØ± Ø§Ø³ØªÙ†ØªØ§Ø¬ Triton](runtime/nvidia_triton/) Ù…Ø±Ø§Ø¬Ø¹Ù‡ Ú©Ù†ÛŒØ¯ Ú©Ù‡ Ù…ÙˆØªÙˆØ±Ù‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ´Ø¯Ù‡ TensorRTØŒ Ù…Ø¯ÛŒØ±ÛŒØª Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ Ù‡Ù…Ø²Ù…Ø§Ù† Ùˆ Ù‡Ø± Ø¯Ùˆ API Ú¯Ø±Ø§Ù/HTTP Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø³Ø§Ø²Ù…Ø§Ù†ÛŒ Ø±Ø§ Ø§Ø±Ø§Ø¦Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.\n\n### Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø±ÙˆÛŒ CPU\n\nØ¨Ø±Ø§ÛŒ Ø±Ø§Ù‡â€ŒØ­Ù„ Ø§Ø³ØªÙ‚Ø±Ø§Ø± C++ Ø±ÙˆÛŒ CPU Ø¨Ù‡ [sherpa-onnx](https://github.com/k2-fsa/sherpa-onnx/pull/2487#issuecomment-3227884498) Ù…Ø±Ø§Ø¬Ø¹Ù‡ Ú©Ù†ÛŒØ¯.\n\n## Ø¨Ø­Ø« Ùˆ Ø§Ø±ØªØ¨Ø§Ø·\n\nÙ…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø¨Ù‡ Ø·ÙˆØ± Ù…Ø³ØªÙ‚ÛŒÙ… Ø¯Ø± [Github Issues](https://github.com/k2-fsa/ZipVoice/issues) Ø¨Ø­Ø« Ú©Ù†ÛŒØ¯.\n\nÙ‡Ù…Ú†Ù†ÛŒÙ† Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ú©Ø¯ QR Ø±Ø§ Ø§Ø³Ú©Ù† Ú©Ù†ÛŒØ¯ ØªØ§ Ø¨Ù‡ Ú¯Ø±ÙˆÙ‡ ÙˆÛŒâ€ŒÚ†Øª Ù…Ø§ Ø¨Ù¾ÛŒÙˆÙ†Ø¯ÛŒØ¯ ÛŒØ§ Ø­Ø³Ø§Ø¨ Ø±Ø³Ù…ÛŒ ÙˆÛŒâ€ŒÚ†Øª Ù…Ø§ Ø±Ø§ Ø¯Ù†Ø¨Ø§Ù„ Ú©Ù†ÛŒØ¯.\n\n| Ú¯Ø±ÙˆÙ‡ ÙˆÛŒâ€ŒÚ†Øª | Ø­Ø³Ø§Ø¨ Ø±Ø³Ù…ÛŒ ÙˆÛŒâ€ŒÚ†Øª |",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "hF52KZEnGKLuaot2w0AmXt52eB6Y3adYIo2qMJSMx5o=",
        "originContent": "We use [pypinyin](https://github.com/mozillazg/python-pinyin) to convert Chinese characters to pinyin. However, it can occasionally mispronounce **polyphone characters** (å¤šéŸ³å­—).",
        "translatedContent": "Ù…Ø§ Ø§Ø² [pypinyin](https://github.com/mozillazg/python-pinyin) Ø¨Ø±Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ø­Ø±ÙˆÙ Ú†ÛŒÙ†ÛŒ Ø¨Ù‡ Ù¾ÛŒÙ†â€ŒÛŒÛŒÙ† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…. Ø¨Ø§ Ø§ÛŒÙ† Ø­Ø§Ù„ØŒ Ú¯Ø§Ù‡ÛŒ Ø§ÙˆÙ‚Ø§Øª Ù…Ù…Ú©Ù† Ø§Ø³Øª **Ø­Ø±ÙˆÙ Ú†Ù†Ø¯Ø¢ÙˆØ§ÛŒÛŒ** (å¤šéŸ³å­—) Ø±Ø§ Ø§Ø´ØªØ¨Ø§Ù‡ ØªÙ„ÙØ¸ Ú©Ù†Ø¯."
      },
      {
        "row": 2,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 3,
        "rowsha": "h6Qq8AvUPYme90k2BWG054cVE6RHNflr0OwdnKA4BEE=",
        "originContent": "To manually correct these mispronunciations, enclose the **corrected pinyin** in angle brackets `< >` and include the **tone mark**.",
        "translatedContent": "Ø¨Ø±Ø§ÛŒ Ø§ØµÙ„Ø§Ø­ Ø¯Ø³ØªÛŒ Ø§ÛŒÙ† ØªÙ„ÙØ¸â€ŒÙ‡Ø§ÛŒ Ø§Ø´ØªØ¨Ø§Ù‡ØŒ Ù¾ÛŒÙ†â€ŒÛŒÛŒÙ† **Ø§ØµÙ„Ø§Ø­â€ŒØ´Ø¯Ù‡** Ø±Ø§ Ø¯Ø±ÙˆÙ† Ø¹Ù„Ø§Ù…Øª Ø²Ø§ÙˆÛŒÙ‡â€ŒØ§ÛŒ `< >` Ù‚Ø±Ø§Ø± Ø¯Ù‡ÛŒØ¯ Ùˆ **Ø¹Ù„Ø§Ù…Øª Ù†ØºÙ…Ù‡** Ø±Ø§ Ù†ÛŒØ² Ø¯Ø±Ø¬ Ú©Ù†ÛŒØ¯."
      },
      {
        "row": 4,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 5,
        "rowsha": "fP4bnCe7+qhcgDDajGMIv4obksa4WSdUp3hExEbpci0=",
        "originContent": "**Example:**",
        "translatedContent": "**Ù…Ø«Ø§Ù„:**"
      },
      {
        "row": 6,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 7,
        "rowsha": "ntwz9/IQqGC1ThJXQn9D83h+54cDriyg2snkkrk0KoI=",
        "originContent": "- Original text: `è¿™æŠŠå‰‘é•¿ä¸‰åå…¬åˆ†`",
        "translatedContent": "- Ù…ØªÙ† Ø§ØµÙ„ÛŒ: `è¿™æŠŠå‰‘é•¿ä¸‰åå…¬åˆ†`"
      },
      {
        "row": 8,
        "rowsha": "sfnMRvscnvdKs1fvbVePwH0RpAikXkFIi9i7HZK7D9w=",
        "originContent": "- Correct the pinyin of `é•¿`:  `è¿™æŠŠå‰‘<chang2>ä¸‰åå…¬åˆ†`",
        "translatedContent": "- Ø§ØµÙ„Ø§Ø­ Ù¾ÛŒÙ†â€ŒÛŒÛŒÙ† ÙˆØ§Ú˜Ù‡ `é•¿`:  `è¿™æŠŠå‰‘<chang2>ä¸‰åå…¬åˆ†`"
      },
      {
        "row": 9,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 10,
        "rowsha": "7f45Y23fyK7AQrUO7HdTtPZZzoyRkW6WwoznauYmQew=",
        "originContent": "> **Note:** If you want to manually assign multiple pinyins, enclose each pinyin with `<>`, e.g., `è¿™æŠŠ<jian4><chang2><san1>åå…¬åˆ†`",
        "translatedContent": "> **ØªÙˆØ¬Ù‡:** Ø§Ú¯Ø± Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒØ¯ Ú†Ù†Ø¯ Ù¾ÛŒÙ†â€ŒÛŒÛŒÙ† Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø¯Ø³ØªÛŒ ØªØ¹ÛŒÛŒÙ† Ú©Ù†ÛŒØ¯ØŒ Ù‡Ø± Ù¾ÛŒÙ†â€ŒÛŒÛŒÙ† Ø±Ø§ Ø¯Ø§Ø®Ù„ `< >` Ù‚Ø±Ø§Ø± Ø¯Ù‡ÛŒØ¯ØŒ Ù…Ø§Ù†Ù†Ø¯: `è¿™æŠŠ<jian4><chang2><san1>åå…¬åˆ†`"
      },
      {
        "row": 11,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 12,
        "rowsha": "FJBRXZnczlB/CZyp0UgEFMr440NrcuTPBheyQJ9lxZI=",
        "originContent": "#### 3.7 Remove long silences from the generated speech",
        "translatedContent": "#### Û³.Û· Ø­Ø°Ù Ø³Ú©ÙˆØªâ€ŒÙ‡Ø§ÛŒ Ø·ÙˆÙ„Ø§Ù†ÛŒ Ø§Ø² Ú¯ÙØªØ§Ø± ØªÙˆÙ„ÛŒØ¯Ø´Ø¯Ù‡"
      },
      {
        "row": 13,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 14,
        "rowsha": "9UF5wHdPZ48OWeIPfCnOHPEWzgW4Z6I0e+GaTEZ1GXI=",
        "originContent": "Model will automatically determine the positions and lengths of silences in the generated speech. It occasionally has long silence in the middle of the speech. If you don't want this, you can pass `--remove-long-sil` to remove long silences in the middle of the generated speech (edge silences will be removed by default).",
        "translatedContent": "Ù…Ø¯Ù„ Ø¨Ù‡ Ø·ÙˆØ± Ø®ÙˆØ¯Ú©Ø§Ø± Ù…ÙˆÙ‚Ø¹ÛŒØª Ùˆ Ø·ÙˆÙ„ Ø³Ú©ÙˆØªâ€ŒÙ‡Ø§ Ø¯Ø± Ú¯ÙØªØ§Ø± ØªÙˆÙ„ÛŒØ¯Ø´Ø¯Ù‡ Ø±Ø§ ØªØ´Ø®ÛŒØµ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯. Ú¯Ø§Ù‡ÛŒ Ø§ÙˆÙ‚Ø§Øª Ø³Ú©ÙˆØª Ø·ÙˆÙ„Ø§Ù†ÛŒ Ø¯Ø± ÙˆØ³Ø· Ú¯ÙØªØ§Ø± Ø§ÛŒØ¬Ø§Ø¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯. Ø§Ú¯Ø± Ø§ÛŒÙ† Ù…ÙˆØ±Ø¯ Ø±Ø§ Ù†Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒØ¯ØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø¨Ø§ Ù¾Ø§Ø±Ø§Ù…ØªØ± `--remove-long-sil` Ø³Ú©ÙˆØªâ€ŒÙ‡Ø§ÛŒ Ø·ÙˆÙ„Ø§Ù†ÛŒ Ø¯Ø± ÙˆØ³Ø· Ú¯ÙØªØ§Ø± ØªÙˆÙ„ÛŒØ¯Ø´Ø¯Ù‡ Ø±Ø§ Ø­Ø°Ù Ú©Ù†ÛŒØ¯ (Ø³Ú©ÙˆØªâ€ŒÙ‡Ø§ÛŒ Ù„Ø¨Ù‡â€ŒØ§ÛŒ Ø¨Ù‡ Ø·ÙˆØ± Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø­Ø°Ù Ø®ÙˆØ§Ù‡Ù†Ø¯ Ø´Ø¯)."
      },
      {
        "row": 15,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 16,
        "rowsha": "t3GDCbkkN4PM6Y7xA/ZDAXgu4WdMFfXJ+5E/xKU9AKo=",
        "originContent": "#### 3.8 Model downloading",
        "translatedContent": "#### Û³.Û¸ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„"
      },
      {
        "row": 17,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 18,
        "rowsha": "I2UvthbxkflPXxq8Yy6W9nJCtzb40mZPJbfmDIWQPmA=",
        "originContent": "If you have trouble connecting to HuggingFace when downloading the pre-trained models, try switching endpoint to the mirror site: `export HF_ENDPOINT=https://hf-mirror.com`.",
        "translatedContent": "Ø§Ú¯Ø± Ù‡Ù†Ú¯Ø§Ù… Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø§Ø² Ù¾ÛŒØ´ Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡ Ø§Ø² HuggingFace Ø¨Ø§ Ù…Ø´Ú©Ù„ Ø§ØªØµØ§Ù„ Ù…ÙˆØ§Ø¬Ù‡ Ø´Ø¯ÛŒØ¯ØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ù†Ù‚Ø·Ù‡ Ø§Ù†ØªÙ‡Ø§ÛŒÛŒ Ø±Ø§ Ø¨Ù‡ Ø³Ø§ÛŒØª Ù…ÛŒØ±ÙˆØ± ØªØºÛŒÛŒØ± Ø¯Ù‡ÛŒØ¯: `export HF_ENDPOINT=https://hf-mirror.com`."
      },
      {
        "row": 19,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 20,
        "rowsha": "SEsrfyGZhBYqHMMdMldgN+tSz6ynJT5BVJeLrTV5lHw=",
        "originContent": "## Train Your Own Model",
        "translatedContent": "## Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø§Ø®ØªØµØ§ØµÛŒ"
      },
      {
        "row": 21,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 22,
        "rowsha": "XaSJNyFaxQRx1Xc0mphwUGAxovKELo/54WkMCnFDLyE=",
        "originContent": "See the [egs](egs) directory for training, fine-tuning and evaluation examples.",
        "translatedContent": "Ø¨Ø±Ø§ÛŒ Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ØŒ ØªÙ†Ø¸ÛŒÙ… Ø¯Ù‚ÛŒÙ‚ Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¨Ù‡ Ø¯Ø§ÛŒØ±Ú©ØªÙˆØ±ÛŒ [egs](egs) Ù…Ø±Ø§Ø¬Ø¹Ù‡ Ú©Ù†ÛŒØ¯."
      },
      {
        "row": 23,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 24,
        "rowsha": "tfXaiJ7qvaTkZXp5azeYhmiU88iPPhEvLfYayUjE5+g=",
        "originContent": "## Production Deployment",
        "translatedContent": "## Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø¯Ø± Ù…Ø­ÛŒØ· ØªÙˆÙ„ÛŒØ¯"
      },
      {
        "row": 25,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 26,
        "rowsha": "Tv94iSjTAB/OzGKZj2XftoBUCOzD2x+C0/xHRwRVo1c=",
        "originContent": "### NVIDIA Triton GPU Runtime",
        "translatedContent": "### Ø§Ø¬Ø±Ø§ÛŒ GPU Ø¨Ø§ NVIDIA Triton"
      },
      {
        "row": 27,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 28,
        "rowsha": "cGPOVgf7Mpi/ILRt8hu96EmjQo85nhgnSfBGLfBqMeM=",
        "originContent": "For production-ready deployment with high performance and scalability, check out the [Triton Inference Server integration](runtime/nvidia_triton/) that provides optimized TensorRT engines, concurrent request handling, and both gRPC/HTTP APIs for enterprise use.",
        "translatedContent": "Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø¢Ù…Ø§Ø¯Ù‡ ØªÙˆÙ„ÛŒØ¯ Ø¨Ø§ Ú©Ø§Ø±Ø§ÛŒÛŒ Ùˆ Ù…Ù‚ÛŒØ§Ø³â€ŒÙ¾Ø°ÛŒØ±ÛŒ Ø¨Ø§Ù„Ø§ØŒ Ø¨Ù‡ [Ø§Ø¯ØºØ§Ù… Ø³Ø±ÙˆØ± Ø§Ø³ØªÙ†ØªØ§Ø¬ Triton](runtime/nvidia_triton/) Ù…Ø±Ø§Ø¬Ø¹Ù‡ Ú©Ù†ÛŒØ¯ Ú©Ù‡ Ù…ÙˆØªÙˆØ±Ù‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ´Ø¯Ù‡ TensorRTØŒ Ù…Ø¯ÛŒØ±ÛŒØª Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ Ù‡Ù…Ø²Ù…Ø§Ù† Ùˆ Ù‡Ø± Ø¯Ùˆ API Ú¯Ø±Ø§Ù/HTTP Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø³Ø§Ø²Ù…Ø§Ù†ÛŒ Ø±Ø§ Ø§Ø±Ø§Ø¦Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯."
      },
      {
        "row": 29,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 30,
        "rowsha": "qR/CeuOSoGV5ipBKRWpnI+ohlytt878WhTEjtxZenks=",
        "originContent": "### CPU Deployment",
        "translatedContent": "### Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø±ÙˆÛŒ CPU"
      },
      {
        "row": 31,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 32,
        "rowsha": "B8jnXFyKu8XRyVw/Pu0Xuj1ted9/BVoBfwJ1WW9LrcE=",
        "originContent": "Check [sherpa-onnx](https://github.com/k2-fsa/sherpa-onnx/pull/2487#issuecomment-3227884498) for the C++ deployment solution on CPU.",
        "translatedContent": "Ø¨Ø±Ø§ÛŒ Ø±Ø§Ù‡â€ŒØ­Ù„ Ø§Ø³ØªÙ‚Ø±Ø§Ø± C++ Ø±ÙˆÛŒ CPU Ø¨Ù‡ [sherpa-onnx](https://github.com/k2-fsa/sherpa-onnx/pull/2487#issuecomment-3227884498) Ù…Ø±Ø§Ø¬Ø¹Ù‡ Ú©Ù†ÛŒØ¯."
      },
      {
        "row": 33,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 34,
        "rowsha": "dEqRbPItUt3FEp1iC+8Ww+A6L57yd6oGeXfxSn5BYzs=",
        "originContent": "## Discussion & Communication",
        "translatedContent": "## Ø¨Ø­Ø« Ùˆ Ø§Ø±ØªØ¨Ø§Ø·"
      },
      {
        "row": 35,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 36,
        "rowsha": "wjQUwaDgSP1a6ggLgGx8TWt44Dxu5IlJytNwCAlzZKg=",
        "originContent": "You can directly discuss on [Github Issues](https://github.com/k2-fsa/ZipVoice/issues).",
        "translatedContent": "Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø¨Ù‡ Ø·ÙˆØ± Ù…Ø³ØªÙ‚ÛŒÙ… Ø¯Ø± [Github Issues](https://github.com/k2-fsa/ZipVoice/issues) Ø¨Ø­Ø« Ú©Ù†ÛŒØ¯."
      },
      {
        "row": 37,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 38,
        "rowsha": "WLbsT+slCE72T0wpzVIN8KFxhP+RAw29VxFhJcBEEIo=",
        "originContent": "You can also scan the QR code to join our wechat group or follow our wechat official account.",
        "translatedContent": "Ù‡Ù…Ú†Ù†ÛŒÙ† Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ú©Ø¯ QR Ø±Ø§ Ø§Ø³Ú©Ù† Ú©Ù†ÛŒØ¯ ØªØ§ Ø¨Ù‡ Ú¯Ø±ÙˆÙ‡ ÙˆÛŒâ€ŒÚ†Øª Ù…Ø§ Ø¨Ù¾ÛŒÙˆÙ†Ø¯ÛŒØ¯ ÛŒØ§ Ø­Ø³Ø§Ø¨ Ø±Ø³Ù…ÛŒ ÙˆÛŒâ€ŒÚ†Øª Ù…Ø§ Ø±Ø§ Ø¯Ù†Ø¨Ø§Ù„ Ú©Ù†ÛŒØ¯."
      },
      {
        "row": 39,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 40,
        "rowsha": "XMNeg//PyHlCkY1XlL7caNd2vlVOKKoslrNeasADjMY=",
        "originContent": "| Wechat Group | Wechat Official Account |",
        "translatedContent": "| Ú¯Ø±ÙˆÙ‡ ÙˆÛŒâ€ŒÚ†Øª | Ø­Ø³Ø§Ø¨ Ø±Ø³Ù…ÛŒ ÙˆÛŒâ€ŒÚ†Øª |"
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 25,
    "Content": "| ------------ | ----------------------- |\n|![wechat](https://k2-fsa.org/zh-CN/assets/pic/wechat_group.jpg) |![wechat](https://k2-fsa.org/zh-CN/assets/pic/wechat_account.jpg) |\n\n## Citation\n",
    "ContentSha": "z5P7Ai9AO6w/XhHPT5bFJ00FeUxhB51crq68OHJeIus=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "| ------------ | ----------------------- |\n|![wechat](https://k2-fsa.org/zh-CN/assets/pic/wechat_group.jpg) |![wechat](https://k2-fsa.org/zh-CN/assets/pic/wechat_account.jpg) |\n\n## Ø§Ø±Ø¬Ø§Ø¹\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "jdP52Pdk9hJ4eEQC1YzC887/bGdD6V25zHK1FxUbFjM=",
        "originContent": "| ------------ | ----------------------- |",
        "translatedContent": "| ------------ | ----------------------- |"
      },
      {
        "row": 2,
        "rowsha": "Q6eYrtLPPuG0fiZxZqhYquTYNk0vlyIOh+CRuwGZVk4=",
        "originContent": "|![wechat](https://k2-fsa.org/zh-CN/assets/pic/wechat_group.jpg) |![wechat](https://k2-fsa.org/zh-CN/assets/pic/wechat_account.jpg) |",
        "translatedContent": "|![wechat](https://k2-fsa.org/zh-CN/assets/pic/wechat_group.jpg) |![wechat](https://k2-fsa.org/zh-CN/assets/pic/wechat_account.jpg) |"
      },
      {
        "row": 3,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 4,
        "rowsha": "ZwTp5ajUmpHTJefyHhIKzXcG2wnB1jv8iv8cvmdcb/g=",
        "originContent": "## Citation",
        "translatedContent": "## Ø§Ø±Ø¬Ø§Ø¹"
      },
      {
        "row": 5,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 26,
    "Content": "```bibtex\n@article{zhu2025zipvoice,\n      title={ZipVoice: Fast and High-Quality Zero-Shot Text-to-Speech with Flow Matching},\n      author={Zhu, Han and Kang, Wei and Yao, Zengwei and Guo, Liyong and Kuang, Fangjun and Li, Zhaoqing and Zhuang, Weiji and Lin, Long and Povey, Daniel},\n      journal={arXiv preprint arXiv:2506.13053},\n      year={2025}\n}\n\n@article{zhu2025zipvoicedialog,\n      title={ZipVoice-Dialog: Non-Autoregressive Spoken Dialogue Generation with Flow Matching},\n      author={Zhu, Han and Kang, Wei and Guo, Liyong and Yao, Zengwei and Kuang, Fangjun and Zhuang, Weiji and Li, Zhaoqing and Han, Zhifeng and Zhang, Dong and Zhang, Xin and Song, Xingchen and Lin, Long and Povey, Daniel},\n      journal={arXiv preprint arXiv:2507.09318},\n      year={2025}\n}\n```",
    "ContentSha": "4y5htVtgE8qDxiQNpfNmGGVhWO4hKo26DrPCI9N/e9E=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bibtex\n@article{zhu2025zipvoice,\n      title={ZipVoice: Fast and High-Quality Zero-Shot Text-to-Speech with Flow Matching},\n      author={Zhu, Han and Kang, Wei and Yao, Zengwei and Guo, Liyong and Kuang, Fangjun and Li, Zhaoqing and Zhuang, Weiji and Lin, Long and Povey, Daniel},\n      journal={arXiv preprint arXiv:2506.13053},\n      year={2025}\n}\n\n@article{zhu2025zipvoicedialog,\n      title={ZipVoice-Dialog: Non-Autoregressive Spoken Dialogue Generation with Flow Matching},\n      author={Zhu, Han and Kang, Wei and Guo, Liyong and Yao, Zengwei and Kuang, Fangjun and Zhuang, Weiji and Li, Zhaoqing and Han, Zhifeng and Zhang, Dong and Zhang, Xin and Song, Xingchen and Lin, Long and Povey, Daniel},\n      journal={arXiv preprint arXiv:2507.09318},\n      year={2025}\n}\n```",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": true
  },
  {
    "Id": 27,
    "Content": "",
    "ContentSha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "<translate-content></translate-content>",
    "Status": "ok",
    "RowTranslations": [],
    "IsCodeBlock": false
  }
]