# 검증 가능한 보상을 갖춘 멋진 레이블 없는 강화 학습

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

대형 언어 모델(LLM)을 위한 검증 가능한 보상과 함께하는 레이블 없는 강화 학습(RLVR)에 관한 엄선된 논문 모음입니다.

> **[Qingyang Zhang](https://raw.githubusercontent.com/QingyangZhang/Label-Free-RLVR/main/qingyangzhang.github.io), [Haitao Wu](https://haitaowutju.github.io) 및 [Yi Ding](https://dripnowhy.github.io)이 작성했습니다. 누락된 논문이 있으면 알려주세요!**


## 목차
- [검증 가능한 보상을 갖춘 멋진 레이블 없는 강화 학습](#검증-가능한-보상을-갖춘-멋진-레이블-없는-강화-학습)
  - [목차](#목차)
  - [개요](#개요)
    - [DeepSeek-R1-Zero 이전](#deepseek-r1-zero-이전)
    - [외부 감독 없는 RLVR](#외부-감독-없는-rlvr)
    - [제한된 데이터가 있는 RLVR](#제한된-데이터가-있는-rlvr)
    - [기타](#기타)
  - [별 히스토리](#별-히스토리)

## 개요

### DeepSeek-R1-Zero 이전

[가상 피드백을 이용한 추론 선호 최적화](https://arxiv.org/abs/2411.16345), ArXiv, 2024-11, ICLR'25 스포트라이트

[자기 일관성 선호 최적화](https://arxiv.org/abs/2411.04109), ArXiv, 2024-11

### 외부 감독 없는 RLVR

[올바른 질문은 이미 답의 절반이다: 완전 비지도 LLM 추론 동기 부여](https://arxiv.org/abs/2504.05812), ArXiv, 2025-04-08

[Ttrl: 테스트 시 강화 학습](https://arxiv.org/abs/2504.16084), ArXiv, 2025-04-22

[절대 영점: 제로 데이터로 강화된 자기 대결 추론](https://arxiv.org/abs/2505.03335), ArXiv, 2025-05-06

[LLM 추론에서 엔트로피 최소화의 비합리적 효과](https://arxiv.org/abs/2505.15134), ArXiv, 2025-05-21

[SSR-Zero: 기계 번역을 위한 간단한 자기 보상 강화 학습](https://arxiv.org/abs/2505.16637), ArXiv, 2025-05-22

[SeRL: 제한된 데이터로 대형 언어 모델을 위한 자기 대결 강화 학습](https://arxiv.org/abs/2505.20347), ArXiv, 2025-05-25

[외부 보상 없이 추론 학습](https://arxiv.org/abs/2505.19590), ArXiv, 2025-05-26

[형식과 길이에서의 대리 신호: 정답 없이 수학 문제 해결을 위한 강화 학습](https://arxiv.org/abs/2505.19439), ArXiv, 2025-05-26

[허위 보상: RLVR에서 훈련 신호 재고](https://github.com/ruixin31/Rethink_RLVR/tree/main?tab=readme-ov-file), 블로그, 2025-05-27

[대형 추론 모델이 스스로 학습할 수 있을까?](https://arxiv.org/abs/2505.21444), ArXiv, 2025-05-27

[신뢰도 최대화만으로도 추론 향상](https://arxiv.org/abs/2505.22660), ArXiv, 2025-05-28

[GRPO를 통한 멀티모달 LLM 추론의 비지도 후학습](https://arxiv.org/abs/2505.22453v1), ArXiv, 2025-05-29

[ZeroGUI: 제로 인적 비용으로 온라인 GUI 학습 자동화](https://arxiv.org/abs/2505.23762), ArXiv, 2025-05-29

[일관된 경로가 진실로 이어진다: LLM 추론을 위한 자기 보상 강화 학습](https://arxiv.org/abs/2506.08745), ArXiv, 2025-06-02

[신뢰도만 있으면 충분하다: 언어 모델의 소수 샷 RL 미세 조정](https://arxiv.org/abs/2506.06395v1), ArXiv, 2025-06-05

[자기 적응 언어 모델](https://arxiv.org/abs/2506.10943), ArXiv, 2025-06-12

[공짜 점심은 없다: LLM 추론을 위한 내부 피드백 재고](https://arxiv.org/abs/2506.17219), ArXiv, 2025-06-20

### 제한된 데이터가 있는 RLVR

[수학적 추론을 위한 자기 보상 교정](https://arxiv.org/pdf/2502.19613), ArXiv, 2025-02-26

[하나의 학습 예제로 대형 언어 모델에서 추론을 위한 강화 학습](https://arxiv.org/abs/2504.20571), ArXiv, 2025-04-29

[반복 선호 최적화를 통한 LLM 자기 개선 능력 진화](https://arxiv.org/pdf/2502.05605), ArXiv, 2025-05-17

[Sherlock: 비전-언어 모델에서 자기 교정 추론](https://arxiv.org/pdf/2505.22651), ArXiv, 2025-05-28

[신뢰도만 있으면 충분하다: 언어 모델의 소수 샷 RL 미세 조정](https://arxiv.org/abs/2506.06395), Arxiv, 2025-07-05 

### 기타

[SLOT: 테스트 시 샘플 별 언어 모델 최적화](https://arxiv.org/abs/2505.12392), ArXiv, 2025-05-18

[원샷 엔트로피 최소화](https://arxiv.org/abs/2505.20282), ArXiv, 2025-05-26

[검증자 없이 일반 추론 강화](https://arxiv.org/abs/2505.21493), ArXiv, 2025-05-27

[부정확한 기준 평가가 최근 LLM-RL 주장에 의문 제기](https://safe-lip-9a8.notion.site/Incorrect-Baseline-Evaluations-Call-into-Question-Recent-LLM-RL-Claims-2012f1fbf0ee8094ab8ded1953c15a37#2022f1fbf0ee80cb9b18f7eac460410a), 블로그, 2025-05-29
> RLVR 평가 설정에 대한 비판적 검토.

## 별 히스토리

[![별 히스토리 차트](https://api.star-history.com/svg?repos=QingyangZhang/Label-Free-RLVR&Date&type=Date)](https://www.star-history.com/#QingyangZhang/Label-Free-RLVR&Date&Date)


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-14

---