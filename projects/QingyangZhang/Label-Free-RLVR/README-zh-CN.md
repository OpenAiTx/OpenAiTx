# 令人惊叹的无标签强化学习与可验证奖励

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

一个针对大型语言模型（LLMs）无标签强化学习与可验证奖励（RLVR）论文的精选合集。

> **由 [Qingyang Zhang](https://raw.githubusercontent.com/QingyangZhang/Label-Free-RLVR/main/qingyangzhang.github.io)、[Haitao Wu](https://haitaowutju.github.io) 和 [Yi Ding](https://dripnowhy.github.io) 编写。如有遗漏论文，欢迎告知！**


## 目录
- [令人惊叹的无标签强化学习与可验证奖励](#令人惊叹的无标签强化学习与可验证奖励)
  - [目录](#目录)
  - [概览](#概览)
    - [DeepSeek-R1-Zero 之前](#deepseek-r1-zero-之前)
    - [无外部监督的 RLVR](#无外部监督的-rlvr)
    - [有限数据下的 RLVR](#有限数据下的-rlvr)
    - [其他](#其他)
  - [Star 历史](#star-历史)

## 概览

### DeepSeek-R1-Zero 之前

[基于伪反馈的推理偏好优化](https://arxiv.org/abs/2411.16345)，ArXiv，2024-11，ICLR'25 焦点论文

[自洽偏好优化](https://arxiv.org/abs/2411.04109)，ArXiv，2024-11

### 无外部监督的 RLVR

[正确的问题已是答案的一半：完全无监督的 LLM 推理激励](https://arxiv.org/abs/2504.05812)，ArXiv，2025-04-08

[Ttrl：测试时强化学习](https://arxiv.org/abs/2504.16084)，ArXiv，2025-04-22

[绝对零：零数据强化自我对弈推理](https://arxiv.org/abs/2505.03335)，ArXiv，2025-05-06

[熵最小化在 LLM 推理中的非理性有效性](https://arxiv.org/abs/2505.15134)，ArXiv，2025-05-21

[SSR-Zero：用于机器翻译的简单自我奖励强化学习](https://arxiv.org/abs/2505.16637)，ArXiv，2025-05-22

[SeRL：有限数据下的大型语言模型自我对弈强化学习](https://arxiv.org/abs/2505.20347)，ArXiv，2025-05-25

[无外部奖励的学习推理](https://arxiv.org/abs/2505.19590)，ArXiv，2025-05-26

[来自格式和长度的替代信号：无真实答案的数学问题强化学习](https://arxiv.org/abs/2505.19439)，ArXiv，2025-05-26

[虚假奖励：重新思考 RLVR 训练信号](https://github.com/ruixin31/Rethink_RLVR/tree/main?tab=readme-ov-file)，博客，2025-05-27

[大型推理模型能自我训练吗？](https://arxiv.org/abs/2505.21444)，ArXiv，2025-05-27

[仅最大化置信度即可提升推理能力](https://arxiv.org/abs/2505.22660)，ArXiv，2025-05-28

[多模态 LLM 推理的无监督后训练通过 GRPO](https://arxiv.org/abs/2505.22453v1)，ArXiv，2025-05-29

[ZeroGUI：零人工成本的在线 GUI 学习自动化](https://arxiv.org/abs/2505.23762)，ArXiv，2025-05-29

[一致路径通向真理：LLM 推理的自我奖励强化学习](https://arxiv.org/abs/2506.08745)，ArXiv，2025-06-02

[置信度就是一切：语言模型的少样本 RL 微调](https://arxiv.org/abs/2506.06395v1)，ArXiv，2025-06-05

[自适应语言模型](https://arxiv.org/abs/2506.10943)，ArXiv，2025-06-12

[没有免费的午餐：重新思考 LLM 推理的内部反馈](https://arxiv.org/abs/2506.17219)，ArXiv，2025-06-20

### 有限数据下的 RLVR

[数学推理的自我奖励纠正](https://arxiv.org/pdf/2502.19613)，ArXiv，2025-02-26

[有限训练示例下的大型语言模型推理强化学习](https://arxiv.org/abs/2504.20571)，ArXiv，2025-04-29

[通过迭代偏好优化演化 LLM 的自我改进能力](https://arxiv.org/pdf/2502.05605)，ArXiv，2025-05-17

[Sherlock：视觉-语言模型中的自我纠正推理](https://arxiv.org/pdf/2505.22651)，ArXiv，2025-05-28

[置信度就是一切：语言模型的少样本 RL 微调](https://arxiv.org/abs/2506.06395)，Arxiv，2025-07-05

### 其他

[SLOT：测试时的样本特定语言模型优化](https://arxiv.org/abs/2505.12392)，ArXiv，2025-05-18

[一次性熵最小化](https://arxiv.org/abs/2505.20282)，ArXiv，2025-05-26

[无验证者的强化一般推理](https://arxiv.org/abs/2505.21493)，ArXiv，2025-05-27

[错误的基线评估质疑近期的 LLM-RL 声明](https://safe-lip-9a8.notion.site/Incorrect-Baseline-Evaluations-Call-into-Question-Recent-LLM-RL-Claims-2012f1fbf0ee8094ab8ded1953c15a37#2022f1fbf0ee80cb9b18f7eac460410a)，博客，2025-05-29
> 关于 RLVR 评估设置的关键评论。

## Star 历史

[![Star 历史图表](https://api.star-history.com/svg?repos=QingyangZhang/Label-Free-RLVR&Date&type=Date)](https://www.star-history.com/#QingyangZhang/Label-Free-RLVR&Date&Date)


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-14

---