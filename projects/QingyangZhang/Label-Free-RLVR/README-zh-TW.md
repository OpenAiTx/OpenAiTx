# 超棒的無標籤強化學習與可驗證獎勵

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

一份針對大型語言模型（LLMs）之無標籤強化學習與可驗證獎勵（RLVR）論文的精選彙整。

> **由 [Qingyang Zhang](https://raw.githubusercontent.com/QingyangZhang/Label-Free-RLVR/main/qingyangzhang.github.io)、[Haitao Wu](https://haitaowutju.github.io) 與 [Yi Ding](https://dripnowhy.github.io) 共同整理。若有遺漏的論文，請告訴我們！**


## 目錄
- [超棒的無標籤強化學習與可驗證獎勵](#超棒的無標籤強化學習與可驗證獎勵)
  - [目錄](#目錄)
  - [概述](#概述)
    - [DeepSeek-R1-Zero 之前](#deepseek-r1-zero-之前)
    - [無外部監督的 RLVR](#無外部監督的-rlvr)
    - [有限資料下的 RLVR](#有限資料下的-rlvr)
    - [其他](#其他)
  - [星標歷史](#星標歷史)

## 概述

### DeepSeek-R1-Zero 之前

[用偽反饋進行推理偏好優化](https://arxiv.org/abs/2411.16345)，ArXiv，2024-11，ICLR'25 spotlight

[自洽偏好優化](https://arxiv.org/abs/2411.04109)，ArXiv，2024-11

### 無外部監督的 RLVR

[正確問題即答案的一半：完全無監督的 LLM 推理激勵](https://arxiv.org/abs/2504.05812)，ArXiv，2025-04-08

[Ttrl：測試時強化學習](https://arxiv.org/abs/2504.16084)，ArXiv，2025-04-22

[Absolute Zero：零資料下的強化自我對弈推理](https://arxiv.org/abs/2505.03335)，ArXiv，2025-05-06

[熵最小化在 LLM 推理中的不合理有效性](https://arxiv.org/abs/2505.15134)，ArXiv，2025-05-21

[SSR-Zero：用於機器翻譯的簡單自我獎勵強化學習](https://arxiv.org/abs/2505.16637)，ArXiv，2025-05-22

[SeRL：有限資料下大型語言模型的自我對弈強化學習](https://arxiv.org/abs/2505.20347)，ArXiv，2025-05-25

[學習在無外部獎勵下推理](https://arxiv.org/abs/2505.19590)，ArXiv，2025-05-26

[格式與長度的代理信號：無需真實答案的數學問題強化學習](https://arxiv.org/abs/2505.19439)，ArXiv，2025-05-26

[偽獎勵：重新思考 RLVR 的訓練信號](https://github.com/ruixin31/Rethink_RLVR/tree/main?tab=readme-ov-file)，部落格，2025-05-27

[大型推理模型能自我訓練嗎？](https://arxiv.org/abs/2505.21444)，ArXiv，2025-05-27

[僅最大化信心即可提升推理](https://arxiv.org/abs/2505.22660)，ArXiv，2025-05-28

[多模態 LLM 推理的無監督後訓練透過 GRPO](https://arxiv.org/abs/2505.22453v1)，ArXiv，2025-05-29

[ZeroGUI：零人工成本自動化線上 GUI 學習](https://arxiv.org/abs/2505.23762)，ArXiv，2025-05-29

[一致路徑導向真理：LLM 推理的自我獎勵強化學習](https://arxiv.org/abs/2506.08745)，ArXiv，2025-06-02

[信心即一切：語言模型的少量樣本 RL 微調](https://arxiv.org/abs/2506.06395v1)，ArXiv，2025-06-05

[自適應語言模型](https://arxiv.org/abs/2506.10943)，ArXiv，2025-06-12

[沒有免費午餐：重新思考 LLM 推理的內部反饋](https://arxiv.org/abs/2506.17219)，ArXiv，2025-06-20

### 有限資料下的 RLVR

[數學推理的自我獎勵修正](https://arxiv.org/pdf/2502.19613)，ArXiv，2025-02-26

[用一個訓練範例進行大型語言模型的推理強化學習](https://arxiv.org/abs/2504.20571)，ArXiv，2025-04-29

[透過迭代偏好優化演進 LLM 的自我精煉能力](https://arxiv.org/pdf/2502.05605)，ArXiv，2025-05-17

[Sherlock：視覺語言模型中的自我修正推理](https://arxiv.org/pdf/2505.22651)，ArXiv，2025-05-28

[信心即一切：語言模型的少量樣本 RL 微調](https://arxiv.org/abs/2506.06395)，Arxiv，2025-07-05 

### 其他

[SLOT：測試時的樣本特定語言模型優化](https://arxiv.org/abs/2505.12392)，ArXiv，2025-05-18

[一次性熵最小化](https://arxiv.org/abs/2505.20282)，ArXiv，2025-05-26

[無需驗證者的通用推理強化](https://arxiv.org/abs/2505.21493)，ArXiv，2025-05-27

[錯誤基線評估質疑近期 LLM-RL 聲明](https://safe-lip-9a8.notion.site/Incorrect-Baseline-Evaluations-Call-into-Question-Recent-LLM-RL-Claims-2012f1fbf0ee8094ab8ded1953c15a37#2022f1fbf0ee80cb9b18f7eac460410a)，部落格，2025-05-29
> RLVR 評估設置的批判性回顧。

## 星標歷史

[![Star History Chart](https://api.star-history.com/svg?repos=QingyangZhang/Label-Free-RLVR&Date&type=Date)](https://www.star-history.com/#QingyangZhang/Label-Free-RLVR&Date&Date)


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-14

---