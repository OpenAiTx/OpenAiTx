<div align="center">

![thinking-spongebob](https://raw.githubusercontent.com/ruixin31/Spurious_Rewards/main/figs/thinking-spongebob.png)

# ğŸ’­ å½å ±é…¬: RLVRã«ãŠã‘ã‚‹å­¦ç¿’ä¿¡å·ã®å†è€ƒ

[Rulin Shao*](https://rulinshao.github.io/), [Shuyue Stella Li*](https://stellalisy.com/), [Rui Xin*](https://ruixin31.github.io/), [Scott Geng*](https://www.scottgeng.com/), [Yiping Wang](https://ypwang61.github.io/), [Sewoong Oh](https://homes.cs.washington.edu/~sewoong/), [Simon Shaolei Du](https://simonshaoleidu.com/), [Nathan Lambert](https://www.natolambert.com/), [Sewon Min](https://www.sewonmin.com/), [Ranjay Krishna](https://www.ranjaykrishna.com/index.html), [Yulia Tsvetkov](https://homes.cs.washington.edu/~yuliats/), [Hannaneh Hajishirzi](https://homes.cs.washington.edu/~hannaneh/), [Pang Wei Koh](https://koh.pw/), [Luke Zettlemoyer](https://www.cs.washington.edu/people/faculty/luke-zettlemoyer/)
</div>

<div align="center">

[![Github](https://img.shields.io/badge/Github-000000?style=for-the-badge&logo=github&logoColor=000&logoColor=white)](https://github.com/ruixin31/Rethink_RLVR)
[![Website](https://img.shields.io/badge/Site-000000.svg?style=for-the-badge&logo=notion&logoColor=white)](https://rethink-rlvr.notion.site/Spurious-Rewards-Rethinking-Training-Signals-in-RLVR-1f4df34dac1880948858f95aeb88872f) 
[![Paper](https://img.shields.io/badge/Paper-000000.svg?style=for-the-badge&logo=arxiv&logoColor=white)](http://arxiv.org/abs/2506.10947) 
[![Twitter](https://img.shields.io/badge/Twitter-000000?style=for-the-badge&logo=x&logoColor=white)](https://x.com/StellaLisy/status/1927392717593526780)
[![Wandb](https://img.shields.io/badge/ğŸ“_reproduction_W&B-000000?style=for-the-badge&logo=wandb&logoColor=white)](https://wandb.ai/rx31/SpuriousRewardRLVR)
[![Models](https://img.shields.io/badge/Models-000000?style=for-the-badge&logo=huggingface&logoColor=white)](https://huggingface.co/collections/stellalisy/spurious-rewards-684a38b8eeb32273c287a4db)

</div>


## ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```sh
# Our codebase is based on TTRL (https://github.com/PRIME-RL/TTRL).
git clone git@github.com:ruixin31/Spurious_Rewards
cd code

conda create -n spurious-rewards python=3.10 
conda activate spurious-rewards

pip install -r requirements.txt
pip install flash_attn==2.7.0.post2
pip install -e .
```
## ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°

```sh
bash scripts/rlvr_deepscaler_grpo_qwen_ground_truth.sh
```
## è¨­å®š

### ãƒ‡ãƒ¼ã‚¿
æœ¬è«–æ–‡ã§ã¯ã€ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿ãŠã‚ˆã³å¤šæ•°æ±ºãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ã‚’å«ã‚ã¦ã„ã¾ã™ã€‚å®Œå…¨ãªãƒªã‚¹ãƒˆã¯ `code/data` ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚ã‚Šã¾ã™ã€‚ä¾‹ãˆã°ã€æ­£è§£ãƒ‡ãƒ¼ã‚¿ã¯ `DeepScaleR` ã¨å‘¼ã°ã‚Œã€Llama 3.2 3B instruct ã§ãƒ©ãƒ™ãƒ«ä»˜ã‘ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã®ã†ã¡ã€èª¤ã£ãŸãƒ©ãƒ™ãƒ«ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ã—ãŸã‚‚ã®ã¯ `DeepScaleR_mv_labeled_llama3.2_3b_instruct_incorrect` ãƒ•ã‚©ãƒ«ãƒ€ã«ã‚ã‚Šã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’å¤‰æ›´ã™ã‚‹ã«ã¯ã€`code/scripts/rlvr_deepscaler_grpo_qwen_ground_truth.sh` å†…ã® `TASK` å¤‰æ•°ã‚’å¤‰æ›´ã—ã¦ãã ã•ã„ã€‚

### å ±é…¬
æœ¬è«–æ–‡ã§ä½¿ç”¨ã—ãŸå ±é…¬ãƒªã‚¹ãƒˆã‚’ä»¥ä¸‹ã«ç¤ºã—ã¾ã™ã€‚ã•ã‚‰ã«ã€ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ãªã„ãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦ã¯ã€å¿…ãšã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ã¨ã—ã¦ `_r1_only` ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚å ±é…¬é–¢æ•°ã‚’å¤‰æ›´ã™ã‚‹ã«ã¯ã€`code/scripts/rlvr_deepscaler_grpo_qwen_ground_truth.sh` å†…ã® `REWARD` å¤‰æ•°ã‚’å¤‰æ›´ã—ã¦ãã ã•ã„ã€‚

- `math`: æ•°å­¦çš„ç­‰ä¾¡æ€§å ±é…¬ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
- `box_only_format`: ãƒœãƒƒã‚¯ã‚¹ã®ã¿ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå ±é…¬
- `contain_python_wo_backticks`: Python è¨€åŠå ±é…¬
- `random0.5`: 50% ã®ç¢ºç‡ã§ 1 ã‚’è¿”ã™ãƒ©ãƒ³ãƒ€ãƒ å ±é…¬


## è©•ä¾¡
è©•ä¾¡çµæœã‚’å†ç¾ã™ã‚‹ã«ã¯ã€ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„:


```sh
cd code

# For MATH-500 evaluation (requires NVIDIA A100 80GB PCIe for exact reproduction)
python scripts/eval_checkpoint.py --model_path Qwen/Qwen2.5-Math-7B --datasets MATH-500,AIME-2024,AIME-2025,AMC

# For MATH-500 evaluation matching our reported scores in wandb using checkpoints (requires NVIDIA H200 for exact reproduction)
python scripts/eval_checkpoint.py --model_path {} --datasets MATH-500,AIME-2024,AIME-2025,AMC --shards 2
```
æ³¨æ„: `temperature = 0` ã®çµæœã‚’æ­£ç¢ºã«å†ç¾ã™ã‚‹ã«ã¯ã€GPUã®ç¨®é¡ã¨ `--shards` ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä¸¡æ–¹ãŒå…ƒã®è©•ä¾¡è¨­å®šã¨ä¸€è‡´ã—ã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã“ã‚Œã¯ã€VLLM ã«æ¸¡ã•ã‚Œã‚‹ãƒãƒƒãƒã‚µã‚¤ã‚ºãŒç”Ÿæˆã®æºã‚‰ãã‚’å¼•ãèµ·ã“ã™å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã§ã™ã€‚

## è«–æ–‡

ã“ã¡ã‚‰ãŒç§ãŸã¡ã®è«–æ–‡ã®[ãƒªãƒ³ã‚¯](http://arxiv.org/abs/2506.10947)ã§ã™ã€‚

## å¼•ç”¨


```bibtex
@misc{shao2025spuriousrewardsrethinkingtraining,
      title={Spurious Rewards: Rethinking Training Signals in RLVR}, 
      author={Rulin Shao and Shuyue Stella Li and Rui Xin and Scott Geng and Yiping Wang and Sewoong Oh and Simon Shaolei Du and Nathan Lambert and Sewon Min and Ranjay Krishna and Yulia Tsvetkov and Hannaneh Hajishirzi and Pang Wei Koh and Luke Zettlemoyer},
      year={2025},
      eprint={2506.10947},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2506.10947}, 
}
```
## è¬è¾
ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯ [TTRL](https://github.com/PRIME-RL/TTRL) ã‚’åŸºã«æ§‹ç¯‰ã•ã‚Œã¦ãŠã‚Šã€TTRL ã¯ [OpenRLHF](https://github.com/OpenRLHF/OpenRLHF) ã®ä¸Šã«æ§‹ç¯‰ã•ã‚Œã¦ã„ã¾ã™ã€‚
æˆ‘ã€…ã¯ã€éåŒæœŸè©•ä¾¡ãªã©ã®ç‹¬è‡ªæ©Ÿèƒ½ã‚’ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã«è¿½åŠ ã—ã¾ã—ãŸã€‚


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-14

---