# LamRAï¼šä½œä¸ºæ‚¨çš„é«˜çº§æ£€ç´¢åŠ©æ‰‹çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹

æœ¬ä»“åº“æ˜¯ LamRA çš„å®˜æ–¹å®ç°ã€‚

[ğŸ¡ é¡¹ç›®ä¸»é¡µ](https://code-kunkun.github.io/LamRA/) |  [ğŸ“„ è®ºæ–‡](https://arxiv.org/pdf/2412.01720) | [ğŸ¤— LamRA-Ret-é¢„è®­ç»ƒæ¨¡å‹](https://huggingface.co/code-kunkun/LamRA-Ret-Pretrained) | [ğŸ¤— LamRA-Ret](https://huggingface.co/code-kunkun/LamRA-Ret) | [ğŸ¤— LamRA-Rank](https://huggingface.co/code-kunkun/LamRA-Rank) | [ğŸ¤— æ•°æ®é›†](https://huggingface.co/datasets/code-kunkun/LamRA_Eval)

## å®‰è£…

```bash 
conda create -n lamra python=3.10 -y
conda activate lamra 

pip install --upgrade pip  # enable PEP 660 support 
pip install -r requirements.txt

pip install ninja
pip install flash-attn --no-build-isolation
```
## æ–°ç‰ˆæœ¬
æˆ‘ä»¬å·²åœ¨ `qwen2.5vl` åˆ†æ”¯æ›´æ–°äº† Qwen2.5-VL çš„ç‰ˆæœ¬ã€‚

## å¿«é€Ÿå¼€å§‹
è¯·å‚è€ƒ `demo.py`

## æ•°æ®å‡†å¤‡

ä¸‹è½½ Qwen2-VL-7B å¹¶æ”¾ç½®åœ¨ `./checkpoints/hf_models/Qwen2-VL-7B-Instruct`

æœ‰å…³é¢„è®­ç»ƒæ•°æ®é›†ï¼Œè¯·å‚è€ƒ [é“¾æ¥](https://huggingface.co/datasets/princeton-nlp/datasets-for-simcse)

æœ‰å…³å¤šæ¨¡æ€æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†ï¼Œè¯·å‚è€ƒ [M-BEIR](https://huggingface.co/datasets/TIGER-Lab/M-BEIR)

æœ‰å…³ LamRA ç›¸å…³çš„è¯„ä¼°æ•°æ®ï¼Œè¯·å‚è€ƒ [LamRA_Eval](https://huggingface.co/datasets/code-kunkun/LamRA_Eval)

ä¸‹è½½å®Œæˆåï¼Œå°†æ•°æ®æŒ‰å¦‚ä¸‹æ–¹å¼ç»„ç»‡åˆ° `./data` ä¸­

```
â”œâ”€â”€ M-BEIR
â”œâ”€â”€ nli_for_simcse.csv
â”œâ”€â”€ rerank_data_for_training
â”œâ”€â”€ flickr
â”œâ”€â”€ coco
â”œâ”€â”€ sharegpt4v
â”œâ”€â”€ Urban1K
â”œâ”€â”€ circo
â”œâ”€â”€ genecis
â”œâ”€â”€ vist
â”œâ”€â”€ visdial
â”œâ”€â”€ ccneg
â”œâ”€â”€ sugar-crepe
â”œâ”€â”€ MSVD
â””â”€â”€ msrvtt
```

## LamRA-Ret çš„è®­ç»ƒä¸è¯„ä¼°

### é¢„è®­ç»ƒ

```bash 
sh scripts/lamra_ret/pretrain.sh
```

```bash 
# Evaluation 
sh scripts/eval/eval_pretrained.sh
```

```bash 
# Merge LoRA for multimodal instruction tuning stage
sh scripts/merge_lora.sh 
```

### å¤šæ¨¡æ€æŒ‡ä»¤è°ƒä¼˜

```bash
sh scripts/lamra_ret/finetune.sh
```

```bash 
# Evaluation 
sh scripts/eval/eval_mbeir.sh   # eval under local pool setting

sh scripts/eval/eval_mbeir_global.sh   # eval under global pool setting
```

## LamRA-Rank çš„è®­ç»ƒä¸è¯„ä¼°

æ‚¨å¯ä»¥ä½¿ç”¨æˆ‘ä»¬æä¾›çš„[æ•°æ®](https://huggingface.co/datasets/code-kunkun/LamRA_Eval/tree/main/rerank_data_for_training)ï¼Œæˆ–è€…è¿è¡Œä»¥ä¸‹å‘½ä»¤è·å–ç”¨äºé‡æ’åºè®­ç»ƒçš„æ•°æ®ã€‚

```bash
# Collecting data for reranking training
sh scripts/lamra_rank/get_train_data.sh

sh scripts/lamra_rank/merge_train_data.sh
```

```bash
# training for reranking
sh scripts/lamra_rank/train_rerank.sh
```

```bash 
# pointwise reranking
sh scripts/eval/eval_rerank_mbeir_pointwise.sh

# listwise reranking
sh scripts/eval/eval_rerank_mbeir_listwise.sh
```

```bash
# Get the reranking results on M-BEIR
sh scirpts/eval/get_rerank_results_mbeir.sh
```

## å…¶ä»–åŸºå‡†è¯„ä¼°

```bash
# evaluation results on zeroshot datasets
sh scirpts/eval/eval_zeroshot.sh

# reranking the results on zeroshot datasets
sh scripts/eval/eval_rerank_zeroshot.sh

# get the final results
sh scripts/eval/get_rerank_results_zeroshot.sh
```

## ğŸ«¡ è‡´è°¢

éå¸¸æ„Ÿè°¢æ¥è‡ª [lmms-finetune](https://github.com/zjysteven/lmms-finetune) å’Œ [E5-V](https://github.com/kongds/E5-V) çš„ä»£ç åº“ã€‚


## å¼•ç”¨
å¦‚æœæ‚¨åœ¨ç ”ç©¶æˆ–é¡¹ç›®ä¸­ä½¿ç”¨æ­¤ä»£ç ï¼Œè¯·å¼•ç”¨ï¼š

```latex
@inproceedings{liu2025lamra,
  title={Lamra: Large multimodal model as your advanced retrieval assistant},
  author={Liu, Yikun and Zhang, Yajie and Cai, Jiayin and Jiang, Xiaolong and Hu, Yao and Yao, Jiangchao and Wang, Yanfeng and Xie, Weidi},
  booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
  pages={4015--4025},
  year={2025}
}
```

---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-09-18

---