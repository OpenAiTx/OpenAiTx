# LamRA: é«˜åº¦ãªæ¤œç´¢ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã¨ã—ã¦ã®å¤§è¦æ¨¡ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«

ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯LamRAã®å…¬å¼å®Ÿè£…ã§ã™ã€‚

[ğŸ¡ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒšãƒ¼ã‚¸](https://code-kunkun.github.io/LamRA/) |  [ğŸ“„ è«–æ–‡](https://arxiv.org/pdf/2412.01720) | [ğŸ¤— LamRA-Ret-Pretrained](https://huggingface.co/code-kunkun/LamRA-Ret-Pretrained) | [ğŸ¤— LamRA-Ret](https://huggingface.co/code-kunkun/LamRA-Ret) | [ğŸ¤— LamRA-Rank](https://huggingface.co/code-kunkun/LamRA-Rank) | [ğŸ¤— ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ](https://huggingface.co/datasets/code-kunkun/LamRA_Eval)

## ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash 
conda create -n lamra python=3.10 -y
conda activate lamra 

pip install --upgrade pip  # enable PEP 660 support 
pip install -r requirements.txt

pip install ninja
pip install flash-attn --no-build-isolation
```

## æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³
`qwen2.5vl` ãƒ–ãƒ©ãƒ³ãƒã§ Qwen2.5-VL ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’æ›´æ–°ã—ã¾ã—ãŸã€‚

## ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ
`demo.py` ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

## ãƒ‡ãƒ¼ã‚¿æº–å‚™ 

Qwen2-VL-7B ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€`./checkpoints/hf_models/Qwen2-VL-7B-Instruct` ã«é…ç½®ã—ã¦ãã ã•ã„ã€‚

äº‹å‰å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¤ã„ã¦ã¯ã€[ã“ã¡ã‚‰](https://huggingface.co/datasets/princeton-nlp/datasets-for-simcse) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æŒ‡ç¤ºèª¿æ•´ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¤ã„ã¦ã¯ã€[M-BEIR](https://huggingface.co/datasets/TIGER-Lab/M-BEIR) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

LamRA é–¢é€£ã®è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã«ã¤ã„ã¦ã¯ã€[LamRA_Eval](https://huggingface.co/datasets/code-kunkun/LamRA_Eval) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

ã™ã¹ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¾Œã€`./data` ã«ä»¥ä¸‹ã®ã‚ˆã†ã«ãƒ‡ãƒ¼ã‚¿ã‚’æ•´ç†ã—ã¦ãã ã•ã„ã€‚
```
â”œâ”€â”€ M-BEIR
â”œâ”€â”€ nli_for_simcse.csv
â”œâ”€â”€ rerank_data_for_training
â”œâ”€â”€ flickr
â”œâ”€â”€ coco
â”œâ”€â”€ sharegpt4v
â”œâ”€â”€ Urban1K
â”œâ”€â”€ circo
â”œâ”€â”€ genecis
â”œâ”€â”€ vist
â”œâ”€â”€ visdial
â”œâ”€â”€ ccneg
â”œâ”€â”€ sugar-crepe
â”œâ”€â”€ MSVD
â””â”€â”€ msrvtt
```

## LamRA-Retã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨è©•ä¾¡

### äº‹å‰å­¦ç¿’

```bash 
sh scripts/lamra_ret/pretrain.sh
```

```bash 
# Evaluation 
sh scripts/eval/eval_pretrained.sh
```

```bash 
# Merge LoRA for multimodal instruction tuning stage
sh scripts/merge_lora.sh 
```

### ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æŒ‡ç¤ºèª¿æ•´

```bash
sh scripts/lamra_ret/finetune.sh
```

```bash 
# Evaluation 
sh scripts/eval/eval_mbeir.sh   # eval under local pool setting

sh scripts/eval/eval_mbeir_global.sh   # eval under global pool setting
```

## LamRA-Rank ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨è©•ä¾¡

æä¾›ã—ã¦ã„ã‚‹[data](https://huggingface.co/datasets/code-kunkun/LamRA_Eval/tree/main/rerank_data_for_training)ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã€ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã™ã€‚

```bash
# Collecting data for reranking training
sh scripts/lamra_rank/get_train_data.sh

sh scripts/lamra_rank/merge_train_data.sh
```

```bash
# training for reranking
sh scripts/lamra_rank/train_rerank.sh
```

```bash 
# pointwise reranking
sh scripts/eval/eval_rerank_mbeir_pointwise.sh

# listwise reranking
sh scripts/eval/eval_rerank_mbeir_listwise.sh
```

```bash
# Get the reranking results on M-BEIR
sh scirpts/eval/get_rerank_results_mbeir.sh
```

## ä»–ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã®è©•ä¾¡

```bash
# evaluation results on zeroshot datasets
sh scirpts/eval/eval_zeroshot.sh

# reranking the results on zeroshot datasets
sh scripts/eval/eval_rerank_zeroshot.sh

# get the final results
sh scripts/eval/get_rerank_results_zeroshot.sh
```


## ğŸ«¡ è¬è¾

[ lmms-finetune](https://github.com/zjysteven/lmms-finetune) ã¨ [E5-V](https://github.com/kongds/E5-V) ã®ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã«æ·±ãæ„Ÿè¬ã—ã¾ã™ã€‚


## å¼•ç”¨
ã“ã®ã‚³ãƒ¼ãƒ‰ã‚’ç ”ç©¶ã‚„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€æ¬¡ã®ã‚ˆã†ã«å¼•ç”¨ã—ã¦ãã ã•ã„ï¼š
```latex
@inproceedings{liu2025lamra,
  title={Lamra: Large multimodal model as your advanced retrieval assistant},
  author={Liu, Yikun and Zhang, Yajie and Cai, Jiayin and Jiang, Xiaolong and Hu, Yao and Yao, Jiangchao and Wang, Yanfeng and Xie, Weidi},
  booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
  pages={4015--4025},
  year={2025}
}
```

---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-09-18

---