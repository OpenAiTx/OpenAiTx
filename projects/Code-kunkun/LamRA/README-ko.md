# LamRA: ê³ ê¸‰ ê²€ìƒ‰ ë„ìš°ë¯¸ë¡œì„œì˜ ëŒ€í˜• ë©€í‹°ëª¨ë‹¬ ëª¨ë¸

ì´ ì €ì¥ì†ŒëŠ” LamRAì˜ ê³µì‹ êµ¬í˜„ì²´ì…ë‹ˆë‹¤.

[ğŸ¡ í”„ë¡œì íŠ¸ í˜ì´ì§€](https://code-kunkun.github.io/LamRA/) |  [ğŸ“„ ë…¼ë¬¸](https://arxiv.org/pdf/2412.01720) | [ğŸ¤— LamRA-Ret-Pretrained](https://huggingface.co/code-kunkun/LamRA-Ret-Pretrained) | [ğŸ¤— LamRA-Ret](https://huggingface.co/code-kunkun/LamRA-Ret) | [ğŸ¤— LamRA-Rank](https://huggingface.co/code-kunkun/LamRA-Rank) | [ğŸ¤— ë°ì´í„°ì…‹](https://huggingface.co/datasets/code-kunkun/LamRA_Eval)

## ì„¤ì¹˜

```bash 
conda create -n lamra python=3.10 -y
conda activate lamra 

pip install --upgrade pip  # enable PEP 660 support 
pip install -r requirements.txt

pip install ninja
pip install flash-attn --no-build-isolation
```
## New Version
`qwen2.5vl` ë¸Œëœì¹˜ì—ì„œ Qwen2.5-VL ë²„ì „ì„ ì—…ë°ì´íŠ¸í–ˆìŠµë‹ˆë‹¤.

## Quickstart
`demo.py`ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

## Data Preparation 

Qwen2-VL-7Bë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì—¬ `./checkpoints/hf_models/Qwen2-VL-7B-Instruct`ì— ë°°ì¹˜í•˜ì„¸ìš”.

ì‚¬ì „ í•™ìŠµ ë°ì´í„°ì…‹ì€ [ë§í¬](https://huggingface.co/datasets/princeton-nlp/datasets-for-simcse)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

ë©€í‹°ëª¨ë‹¬ ì§€ì¹¨ íŠœë‹ ë°ì´í„°ì…‹ì€ [M-BEIR](https://huggingface.co/datasets/TIGER-Lab/M-BEIR)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

LamRA ê´€ë ¨ í‰ê°€ ë°ì´í„°ëŠ” [LamRA_Eval](https://huggingface.co/datasets/code-kunkun/LamRA_Eval)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

ëª¨ë‘ ë‹¤ìš´ë¡œë“œí•œ í›„, ë°ì´í„°ë¥¼ `./data`ì— ë‹¤ìŒê³¼ ê°™ì´ ì •ë¦¬í•˜ì„¸ìš”.

```
â”œâ”€â”€ M-BEIR
â”œâ”€â”€ nli_for_simcse.csv
â”œâ”€â”€ rerank_data_for_training
â”œâ”€â”€ flickr
â”œâ”€â”€ coco
â”œâ”€â”€ sharegpt4v
â”œâ”€â”€ Urban1K
â”œâ”€â”€ circo
â”œâ”€â”€ genecis
â”œâ”€â”€ vist
â”œâ”€â”€ visdial
â”œâ”€â”€ ccneg
â”œâ”€â”€ sugar-crepe
â”œâ”€â”€ MSVD
â””â”€â”€ msrvtt
```

## LamRA-Retì˜ í•™ìŠµ ë° í‰ê°€

### ì‚¬ì „ í•™ìŠµ

```bash 
sh scripts/lamra_ret/pretrain.sh
```

```bash 
# Evaluation 
sh scripts/eval/eval_pretrained.sh
```

```bash 
# Merge LoRA for multimodal instruction tuning stage
sh scripts/merge_lora.sh 
```

###  ë‹¤ì¤‘ ëª¨ë‹¬ ì§€ì¹¨ ì¡°ì •

```bash
sh scripts/lamra_ret/finetune.sh
```

```bash 
# Evaluation 
sh scripts/eval/eval_mbeir.sh   # eval under local pool setting

sh scripts/eval/eval_mbeir_global.sh   # eval under global pool setting
```

## LamRA-Rankì˜ í•™ìŠµ ë° í‰ê°€

ì œê³µí•˜ëŠ” [ë°ì´í„°](https://huggingface.co/datasets/code-kunkun/LamRA_Eval/tree/main/rerank_data_for_training)ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì—¬ ì¬ìˆœìœ„ í•™ìŠµìš© ë°ì´í„°ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```bash
# Collecting data for reranking training
sh scripts/lamra_rank/get_train_data.sh

sh scripts/lamra_rank/merge_train_data.sh
```

```bash
# training for reranking
sh scripts/lamra_rank/train_rerank.sh
```

```bash 
# pointwise reranking
sh scripts/eval/eval_rerank_mbeir_pointwise.sh

# listwise reranking
sh scripts/eval/eval_rerank_mbeir_listwise.sh
```

```bash
# Get the reranking results on M-BEIR
sh scirpts/eval/get_rerank_results_mbeir.sh
```

## ë‹¤ë¥¸ ë²¤ì¹˜ë§ˆí¬ í‰ê°€

```bash
# evaluation results on zeroshot datasets
sh scirpts/eval/eval_zeroshot.sh

# reranking the results on zeroshot datasets
sh scripts/eval/eval_rerank_zeroshot.sh

# get the final results
sh scripts/eval/get_rerank_results_zeroshot.sh
```
## ğŸ«¡ ê°ì‚¬ì˜ ê¸€

[ lmms-finetune](https://github.com/zjysteven/lmms-finetune) ë° [E5-V](https://github.com/kongds/E5-V) ì½”ë“œë² ì´ìŠ¤ì— ê¹Šì€ ê°ì‚¬ë¥¼ ë“œë¦½ë‹ˆë‹¤.


## ì¸ìš©
ì´ ì½”ë“œë¥¼ ì—°êµ¬ë‚˜ í”„ë¡œì íŠ¸ì— ì‚¬ìš©í•˜ì‹œëŠ” ê²½ìš°, ë‹¤ìŒì„ ì¸ìš©í•´ ì£¼ì‹­ì‹œì˜¤:


```latex
@inproceedings{liu2025lamra,
  title={Lamra: Large multimodal model as your advanced retrieval assistant},
  author={Liu, Yikun and Zhang, Yajie and Cai, Jiayin and Jiang, Xiaolong and Hu, Yao and Yao, Jiangchao and Wang, Yanfeng and Xie, Weidi},
  booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
  pages={4015--4025},
  year={2025}
}
```

---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-09-18

---