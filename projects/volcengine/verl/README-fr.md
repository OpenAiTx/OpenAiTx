<div align="center">
 üëã Bonjour √† tous ! 
    verl est une biblioth√®que d'entra√Ænement RL initi√©e par l'<b>√©quipe ByteDance Seed</b> et maintenue par la communaut√© verl.
    <br>
    <br>
</div>

<div align="center">

[<img src="https://devin.ai/assets/deepwiki-badge.png" alt="Ask DeepWiki.com" height="20"/>](https://deepwiki.com/volcengine/verl)
[![GitHub Repo stars](https://img.shields.io/github/stars/volcengine/verl)](https://github.com/volcengine/verl/stargazers)
[![Twitter](https://img.shields.io/twitter/follow/verl_project)](https://twitter.com/verl_project)
<a href="https://join.slack.com/t/verlgroup/shared_invite/zt-2w5p9o4c3-yy0x2Q56s_VlGLsJ93A6vA"><img src="https://img.shields.io/badge/Slack-verl-blueviolet?logo=slack&amp"></a>
<a href="https://arxiv.org/pdf/2409.19256"><img src="https://img.shields.io/static/v1?label=EuroSys&message=Paper&color=red"></a>
[![Documentation](https://img.shields.io/badge/documentation-blue)](https://verl.readthedocs.io/en/latest/)
<a href="https://raw.githubusercontent.com/eric-haibin-lin/verl-community/refs/heads/main/WeChat.JPG"><img src="https://img.shields.io/badge/ÂæÆ‰ø°-green?logo=wechat&amp"></a>

</div>

![seed logo](https://github.com/user-attachments/assets/c42e675e-497c-4508-8bb9-093ad4d1f216)

<h1 style="text-align: center;">verl : Volcano Engine Reinforcement Learning pour LLMs</h1>

verl est une biblioth√®que d'entra√Ænement RL flexible, efficace et pr√™te pour la production pour les grands mod√®les de langage (LLMs).

verl est la version open source de l'article **[HybridFlow: A Flexible and Efficient RLHF Framework](https://arxiv.org/abs/2409.19256v2)**.

verl est flexible et facile √† utiliser avec :

- **Extension facile de divers algorithmes RL** : Le mod√®le de programmation hybrid-controller permet une repr√©sentation flexible et une ex√©cution efficace de flux de donn√©es complexes de post-entra√Ænement. Construisez des flux RL tels que GRPO, PPO en quelques lignes de code.

- **Int√©gration transparente de l'infrastructure LLM existante avec des API modulaires** : D√©couple les d√©pendances de calcul et de donn√©es, permettant une int√©gration transparente avec les frameworks LLM existants, tels que FSDP, Megatron-LM, vLLM, SGLang, etc.

- **Mapping flexible des dispositifs** : Prend en charge divers placements de mod√®les sur diff√©rents ensembles de GPU pour une utilisation efficace des ressources et une √©volutivit√© sur diff√©rentes tailles de clusters.

- Int√©gration pr√™te √† l'emploi avec les mod√®les populaires HuggingFace

verl est rapide avec :

- **D√©bit √† l'√©tat de l'art** : Int√©gration des moteurs d'entra√Ænement et d'inf√©rence SOTA pour LLM, ainsi qu'un d√©bit RL SOTA.

- **Resharding efficace des mod√®les acteurs avec 3D-HybridEngine** : √âlimine la redondance m√©moire et r√©duit consid√©rablement les surco√ªts de communication lors des transitions entre les phases d'entra√Ænement et de g√©n√©ration.

</p>

## Actualit√©s

- [2025/06] verl avec backend Megatron permet de supporter de grands mod√®les MoE tels que [DeepSeek-671b et Qwen3-236b](https://verl.readthedocs.io/en/latest/perf/dpsk.html).
- [2025/06] L'√©quipe verl pr√©sentera les derni√®res avanc√©es du projet lors du [PyTorch Day China](https://www.lfasiallc.com/pytorch-day-china/) le 7 juin. Rencontrez notre √©quipe de dev √† P√©kin !
- [2025/05] [PF-PPO](https://arxiv.org/abs/2409.06957), accept√© √† l'ICML 2025, est d√©sormais support√© dans verl ! PF-PPO am√©liore l'efficacit√© et la robustesse de l'apprentissage de la politique en filtrant les signaux de r√©compense potentiellement bruyants et en r√©utilisant les exp√©riences de haute qualit√© via un buffer de relecture.
- [2025/04] Nous donnerons un tutoriel sur les derni√®res techniques de post-entra√Ænement et le guide de programmation pour verl lors de [ICLR 2025 Expo](https://iclr.cc/virtual/2025/calendar?filter_events=Expo+Talk+Panel&filter_rooms=), [atelier SCI-FM](https://open-foundation-model.github.io/) et [LMSys afterparty](https://lu.ma/d23nyynm). Les supports de pr√©sentation sont disponibles [ici](https://github.com/eric-haibin-lin/verl-community/tree/main/iclr25).
- [2025/04] Le rapport technique [Seed-Thinking-v1.5](https://github.com/ByteDance-Seed/Seed-Thinking-v1.5/blob/main/seed-thinking-v1.5.pdf) est publi√© ! Entra√Æn√© avec verl, Seed-Thinking-v1.5 atteint 86.7 sur AIME 2024, 55.0 sur Codeforces et 77.3 sur GPQA, d√©montrant d'excellentes capacit√©s de raisonnement en STEM et codage. Au-del√† des t√¢ches de raisonnement, la m√©thode d√©montre une g√©n√©ralisation notable sur divers domaines.
- [2025/04] [VAPO](https://arxiv.org/pdf/2504.05118) (value-based augmented PPO) pr√©sente notre derni√®re m√©thode RL pour les mod√®les de raisonnement. Entra√Æn√© √† partir du mod√®le Qwen-32B-base, VAPO atteint 60.4 sur AIME 2024, surpassant DAPO-32B.
- [2025/03] verl v0.3.0.post1 est publi√© ! Voir la [note de version](https://github.com/volcengine/verl/releases/) pour plus de d√©tails. Il atteint [~1.4x d'acc√©l√©ration](https://tongyx361.github.io/blogs/posts/verl-intro/#/verl-flexible-and-efficient-rl-for-llms) par rapport aux versions pr√©c√©dentes.
- [2025/03] [DAPO](https://dapo-sia.github.io/) est l'algorithme RL SOTA open source qui atteint 50 points sur AIME 2024 bas√© sur le mod√®le pr√©-entra√Æn√© Qwen2.5-32B, surpassant le pr√©c√©dent SOTA de DeepSeek GRPO (DeepSeek-R1-Zero-Qwen-32B). L'entra√Ænement de DAPO est enti√®rement propuls√© par verl et le code de reproduction est disponible dans `recipe/dapo` maintenant.
<details><summary> plus... </summary>
<ul>
  <li>[2025/05] verl sera pr√©sent√© √† [A2M Shanghai](https://a2m.msup.com.cn/home/?aid=4488&city=shanghai) du 16 au 17 mai.</li>
  <li>[2025/05] verl sera pr√©sent√© √† [GOSIM x PyTorch Day 2025](https://paris2025.gosim.org/). √Ä bient√¥t √† Paris !</li>
  <li>[2025/03] Nous avons pr√©sent√© le mod√®le de programmation de verl lors du [vLLM Beijing Meetup](https://mp.weixin.qq.com/s/n77GibL2corAtQHtVEAzfg) et [pr√©sentation et mises √† jour de verl](https://github.com/eric-haibin-lin/verl-community/blob/main/slides/verl-lmsys-meetup.pdf) lors du [SGLang-LMSYS Org Meetup](https://lu.ma/ntjrr7ig) √† Sunnyvale mi-mars.</li>
  <li>[2025/03] Nous pr√©senterons verl (HybridFlow) √† EuroSys 2025. Rendez-vous √† Rotterdam !</li>
  <li>[2025/02] verl v0.2.0.post2 est publi√© !</li>
  <li>[2025/02] Nous avons pr√©sent√© verl lors du <a href="https://lu.ma/ji7atxux">Bytedance/NVIDIA/Anyscale Ray Meetup</a>. Rendez-vous √† San Jose !</li>
  <li>[2025/01] [Doubao-1.5-pro](https://team.doubao.com/zh/special/doubao_1_5_pro) est publi√© avec des performances SOTA sur LLM & VLM. Le mod√®le d'aper√ßu RL scaling a √©t√© entra√Æn√© avec verl, atteignant des performances de niveau OpenAI O1 sur les benchmarks de math√©matiques (70.0 pass@1 sur AIME).</li>
  <li>[2024/12] verl a √©t√© pr√©sent√© √† Ray Forward 2024. Slides disponibles <a href="https://github.com/eric-haibin-lin/verl-community/blob/main/slides/Ray_Forward_2024_%E5%B7%AB%E9%94%A1%E6%96%8C.pdf">ici</a></li>
  <li>[2024/12] L'√©quipe a pr√©sent√© <a href="https://neurips.cc/Expo/Conferences/2024/workshop/100677">Post-training LLMs: From Algorithms to Infrastructure</a> √† NeurIPS 2024. <a href="https://github.com/eric-haibin-lin/verl-data/tree/neurips">Slides</a> et <a href="https://neurips.cc/Expo/Conferences/2024/workshop/100677">vid√©o</a> disponibles.</li>
  <li>[2024/10] verl a √©t√© pr√©sent√© au Ray Summit. <a href="https://www.youtube.com/watch?v=MrhMcXkXvJU&list=PLzTswPQNepXntmT8jr9WaNfqQ60QwW7-U&index=37">Vid√©o Youtube</a> disponible.</li>
  <li>[2024/08] HybridFlow (verl) est accept√© √† EuroSys 2025.</li>
</ul>   
</details>

## Fonctionnalit√©s cl√©s

- **FSDP**, **FSDP2** et **Megatron-LM** pour l'entra√Ænement.
- **vLLM**, **SGLang** et **HF Transformers** pour la g√©n√©ration des rollouts.
- Compatible avec Hugging Face Transformers et Modelscope Hub : [Qwen-3](https://raw.githubusercontent.com/volcengine/verl/main/examples/grpo_trainer/run_qwen3-8b.sh), Qwen-2.5, Llama3.1, Gemma2, DeepSeek-LLM, etc.
- Fine-tuning supervis√©.
- Apprentissage par renforcement avec [PPO](https://raw.githubusercontent.com/volcengine/verl/main/examples/ppo_trainer/), [GRPO](https://raw.githubusercontent.com/volcengine/verl/main/examples/grpo_trainer/), [ReMax](https://raw.githubusercontent.com/volcengine/verl/main/examples/remax_trainer/), [REINFORCE++](https://verl.readthedocs.io/en/latest/examples/config.html#algorithm), [RLOO](https://raw.githubusercontent.com/volcengine/verl/main/examples/rloo_trainer/), [PRIME](https://raw.githubusercontent.com/volcengine/verl/main/recipe/prime/), [DAPO](https://raw.githubusercontent.com/volcengine/verl/main/recipe/dapo/), [DrGRPO](https://raw.githubusercontent.com/volcengine/verl/main/recipe/drgrpo), etc.
  - Supporte la r√©compense bas√©e sur un mod√®le et la r√©compense bas√©e sur une fonction (r√©compense v√©rifiable) pour les maths, [le codage](https://raw.githubusercontent.com/volcengine/verl/main/recipe/dapo), etc.
  - Supporte les mod√®les vision-langage (VLMs) et [RL multi-modal](https://raw.githubusercontent.com/volcengine/verl/main/examples/grpo_trainer/run_qwen2_5_vl-7b.sh) avec Qwen2.5-vl, Kimi-VL
  - [Multi-turn avec appel d‚Äôoutils](https://raw.githubusercontent.com/volcengine/verl/main/examples/sglang_multiturn)
- Recettes d'alignement LLM comme [Self-play preference optimization (SPPO)](https://raw.githubusercontent.com/volcengine/verl/main/recipe/sppo)
- Flash attention 2, [sequence packing](https://raw.githubusercontent.com/volcengine/verl/main/examples/ppo_trainer/run_qwen2-7b_seq_balance.sh), [sequence parallelism](https://raw.githubusercontent.com/volcengine/verl/main/examples/ppo_trainer/run_deepseek7b_llm_sp2.sh) via DeepSpeed Ulysses, [LoRA](https://raw.githubusercontent.com/volcengine/verl/main/examples/sft/gsm8k/run_qwen_05_peft.sh), [Liger-kernel](https://raw.githubusercontent.com/volcengine/verl/main/examples/sft/gsm8k/run_qwen_05_sp2_liger.sh).
- √âvolue jusqu‚Äô√† 671B de param√®tres et des centaines de GPU avec [expert parallelism](https://github.com/volcengine/verl/pull/1467)
- Support RL LoRA multi-gpu [LoRA RL](https://verl.readthedocs.io/en/latest/advance/ppo_lora.html) pour √©conomiser de la m√©moire.
- Suivi d'exp√©rience avec wandb, swanlab, mlflow et tensorboard.

## Fonctionnalit√©s et changements √† venir

- Roadmap https://github.com/volcengine/verl/issues/710
- Optimisations DeepSeek 671b avec Megatron v0.11 https://github.com/volcengine/verl/issues/708
- Multi-turn rollout et outils utilisant des optimisations https://github.com/volcengine/verl/issues/1882
- Interactions avec l'environnement https://github.com/volcengine/verl/issues/1172
- Liste des changements majeurs depuis v0.3 https://github.com/volcengine/verl/discussions/943, entropy_coeff par d√©faut √† 0
- Lora pour RL https://github.com/volcengine/verl/pull/1127 

## D√©marrage rapide

<a href="https://verl.readthedocs.io/en/latest/index.html"><b>Documentation</b></a>

**D√©marrage rapide :**

- [Installation](https://verl.readthedocs.io/en/latest/start/install.html)
- [Quickstart](https://verl.readthedocs.io/en/latest/start/quickstart.html)
- [Guide de programmation](https://verl.readthedocs.io/en/latest/hybrid_flow.html)
- [PPO dans verl](https://verl.readthedocs.io/en/latest/algo/ppo.html)
- [GRPO dans verl](https://verl.readthedocs.io/en/latest/algo/grpo.html)

**Ex√©cution pas √† pas d'un exemple PPO :**


- [Pr√©parer les donn√©es pour le post-entra√Ænement](https://verl.readthedocs.io/en/latest/preparation/prepare_data.html)
- [Impl√©menter une fonction de r√©compense pour le dataset](https://verl.readthedocs.io/en/latest/preparation/reward_function.html)
- [Architecture d‚Äôun exemple PPO](https://verl.readthedocs.io/en/latest/examples/ppo_code_architecture.html)
- [Explication des configs](https://verl.readthedocs.io/en/latest/examples/config.html)

**Algorithmes de r√©f√©rence reproductibles :**

- [Performances RL sur le codage, les maths](https://verl.readthedocs.io/en/latest/algo/baseline.html)

**Pour l'explication du code et l'utilisation avanc√©e (extension) :**

- PPO Trainer et Workers
  - [PPO Ray Trainer](https://verl.readthedocs.io/en/latest/workers/ray_trainer.html)
  - [Backend PyTorch FSDP](https://verl.readthedocs.io/en/latest/workers/fsdp_workers.html)
  - [Backend Megatron-LM](https://verl.readthedocs.io/en/latest/index.html)

- Utilisation avanc√©e et extension
  - [Ajouter des mod√®les avec le backend FSDP](https://verl.readthedocs.io/en/latest/advance/fsdp_extension.html)
  - [Ajouter des mod√®les avec le backend Megatron-LM](https://verl.readthedocs.io/en/latest/advance/megatron_extension.html)
  - [Support multi-turn rollout](https://verl.readthedocs.io/en/latest/sglang_multiturn/multiturn.html)
  - [Int√©gration d‚Äôoutils de recherche](https://verl.readthedocs.io/en/latest/sglang_multiturn/search_tool_example.html)
  - [Int√©gration Sandbox Fusion](https://verl.readthedocs.io/en/latest/examples/sandbox_fusion_example.html)
  - [D√©ploiement avec r√©partition GPU s√©par√©e](https://raw.githubusercontent.com/volcengine/verl/main/examples/split_placement)
  - [Extension √† d‚Äôautres algorithmes RL(HF)](https://verl.readthedocs.io/en/latest/advance/dpo_extension.html)
  - [Tutoriel sur la conception d‚ÄôAPI Ray](https://verl.readthedocs.io/en/latest/advance/placement.html)

**Blogs de la communaut√©**

- [SGLang, verl, OpenBMB et Universit√© Tsinghua : Pionniers du RLHF multi-turn de bout en bout](https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/multi-turn/verl-multiturn-rollout-Release.md)
- [Reinforcement Learning from Human Feedback sur GPU AMD avec int√©gration verl et ROCm](https://rocm.blogs.amd.com/artificial-intelligence/verl-large-scale/README.html)
- [veMLP x verl : Jouer avec l‚Äôentra√Ænement RL](https://mp.weixin.qq.com/s/7nbqxk4knMGd-hQE9ls2tA)
- [Meilleures pratiques pour l'entra√Ænement RL distribu√© GRPO avec verl](https://www.volcengine.com/docs/6459/1463942)
- [Analyse du texte original HybridFlow verl](https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/readme.md)
- [Jusqu‚Äô√† 20 fois plus de d√©bit ! L‚Äô√©quipe Doubao publie un nouveau framework RLHF, d√©sormais open source !](https://team.doubao.com/en/blog/%E6%9C%80%E9%AB%98%E6%8F%90%E5%8D%8720%E5%80%8D%E5%90%9E%E5%90%90%E9%87%8F-%E8%B1%86%E5%8C%85%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9B%A2%E9%98%9F%E5%8F%91%E5%B8%83%E5%85%A8%E6%96%B0-rlhf-%E6%A1%86%E6%9E%B6-%E7%8E%B0%E5%B7%B2%E5%BC%80%E6%BA%90)

## Guide d'optimisation des performances

La performance est essentielle pour les algorithmes RL on-policy. Nous avons r√©dig√© un [guide d√©taill√© d‚Äôoptimisation des performances](https://verl.readthedocs.io/en/latest/perf/perf_tuning.html) pour vous aider √† optimiser les performances.

## Mise √† niveau vers vLLM >= v0.8.2

verl supporte d√©sormais vLLM>=0.8.2 lorsque FSDP est utilis√© comme backend d'entra√Ænement. Veuillez consulter [ce document](https://raw.githubusercontent.com/volcengine/verl/main/docs/README_vllm0.8.md) pour le guide d'installation et plus d'informations. √âvitez vllm 0.7.x, qui contient des bugs pouvant entra√Æner des OOM et des erreurs inattendues.

## Utilisez la derni√®re version de SGLang

SGLang est enti√®rement pris en charge avec verl, et le groupe SGLang RL travaille activement sur le d√©veloppement de fonctionnalit√©s uniques, y compris RL agentique multi-turn, VLM RLHF, RL bas√© serveur et rollout partiel. Veuillez consulter [ce document](https://verl.readthedocs.io/en/latest/workers/sglang_worker.html) pour le guide d'installation et plus d'informations.

## Mise √† niveau vers FSDP2

verl adopte pleinement FSDP2 ! FSDP2 est recommand√© par l‚Äô√©quipe torch distributed, offrant un meilleur d√©bit et une meilleure gestion de la m√©moire, et il est composable avec d‚Äôautres fonctionnalit√©s (par exemple torch.compile). Pour activer FSDP2, utilisez simplement verl main et d√©finissez les options suivantes :
```
actor_rollout_ref.ref.strategy=fsdp2
actor_rollout_ref.actor.strategy=fsdp2
critic.strategy=fsdp2 
reward_model.strategy=fsdp2 
```
De plus, l‚Äôoffloading CPU FSDP2 est compatible avec l‚Äôaccumulation de gradients. Vous pouvez l‚Äôactiver pour √©conomiser de la m√©moire avec `actor_rollout_ref.actor.offload_policy=True`. Pour plus de d√©tails, voir https://github.com/volcengine/verl/pull/1026

## Support AMD (ROCm Kernel)

verl prend maintenant en charge FSDP comme moteur d‚Äôentra√Ænement (Megatron sera bient√¥t support√©) et s‚Äôint√®gre √† la fois √† vLLM et SGLang comme moteurs d‚Äôinf√©rence. Veuillez consulter [ce document](https://raw.githubusercontent.com/volcengine/verl/main/docs/amd_tutorial/amd_build_dockerfile_page.rst) pour le guide d'installation et plus d'informations, et [ce document](https://raw.githubusercontent.com/volcengine/verl/main/docs/amd_tutorial/amd_vllm_page.rst) pour l‚Äôoptimisation des performances vLLM pour ROCm.

## Citation et remerciements

Si ce projet vous a √©t√© utile, merci de citer :

- [HybridFlow: A Flexible and Efficient RLHF Framework](https://arxiv.org/abs/2409.19256v2)
- [A Framework for Training Large Language Models for Code Generation via Proximal Policy Optimization](https://i.cs.hku.hk/~cwu/papers/gmsheng-NL2Code24.pdf)

```bibtex
@article{sheng2024hybridflow,
  title   = {HybridFlow: A Flexible and Efficient RLHF Framework},
  author  = {Guangming Sheng and Chi Zhang and Zilingfeng Ye and Xibin Wu and Wang Zhang and Ru Zhang and Yanghua Peng and Haibin Lin and Chuan Wu},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2409.19256}
}
```

verl s‚Äôinspire du design de Nemo-Aligner, Deepspeed-chat et OpenRLHF. Le projet est adopt√© et contribu√© par Bytedance, Anyscale, LMSys.org, [√©quipe Alibaba Qwen](https://github.com/QwenLM/), Shanghai AI Lab, Universit√© Tsinghua, UC Berkeley, UCLA, UIUC, Universit√© de Hong Kong, ke.com, [All Hands AI](https://www.all-hands.dev/), [ModelBest](http://modelbest.cn/), OpenPipe, JD AI Lab, Microsoft Research, [StepFun](https://www.stepfun.com/), Amazon, Linkedin, Meituan, [Camel-AI](https://www.camel-ai.org/), [OpenManus](https://github.com/OpenManus), Xiaomi, Prime Intellect, NVIDIA research, [Baichuan](https://www.baichuan-ai.com/home), [RedNote](https://www.xiaohongshu.com/), [SwissAI](https://www.swiss-ai.org/), [Moonshot AI (Kimi)](https://www.moonshot-ai.com/), Baidu, Snowflake, et bien d‚Äôautres.

## Projets remarquables utilisant verl

- [TinyZero](https://github.com/Jiayi-Pan/TinyZero) : une reproduction de la recette **DeepSeek R1 Zero** pour les t√¢ches de raisonnement ![GitHub Repo stars](https://img.shields.io/github/stars/Jiayi-Pan/TinyZero)
- [SkyThought](https://github.com/NovaSky-AI/SkyThought) : entra√Ænement RL pour Sky-T1-7B par l‚Äô√©quipe NovaSky AI. ![GitHub Repo stars](https://img.shields.io/github/stars/NovaSky-AI/SkyThought)
- [simpleRL-reason](https://github.com/hkust-nlp/simpleRL-reason) : SimpleRL-Zoo : √©tude et adaptation du Zero RL pour les mod√®les open base dans la nature ![GitHub Repo stars](https://img.shields.io/github/stars/hkust-nlp/simpleRL-reason)
- [Easy-R1](https://github.com/hiyouga/EasyR1) : framework d‚Äôentra√Ænement RL **multi-modal** ![GitHub Repo stars](https://img.shields.io/github/stars/hiyouga/EasyR1)
- [OpenManus-RL](https://github.com/OpenManus/OpenManus-RL) : framework RL tuning d‚Äôagents LLM pour environnements multi-agents. ![GitHub Repo stars](https://img.shields.io/github/stars/OpenManus/OpenManus-RL)
- [rllm](https://github.com/agentica-project/rllm) : entra√Ænement RL asynchrone avec [verl-pipeline](https://github.com/agentica-project/verl-pipeline) ![GitHub Repo stars](https://img.shields.io/github/stars/agentica-project/rllm)
- [PRIME](https://github.com/PRIME-RL/PRIME) : renforcement de processus via r√©compenses implicites ![GitHub Repo stars](https://img.shields.io/github/stars/PRIME-RL/PRIME)
- [RAGEN](https://github.com/ZihanWang314/ragen) : framework d‚Äôentra√Ænement d‚Äô**agent** de raisonnement polyvalent ![GitHub Repo stars](https://img.shields.io/github/stars/ZihanWang314/ragen)
- [Search-R1](https://github.com/PeterGriffinJin/Search-R1) : RL avec raisonnement et **recherche (tool-call)** intercal√©s dans les LLMs ![GitHub Repo stars](https://img.shields.io/github/stars/PeterGriffinJin/Search-R1)
- [DeepRetrieval](https://github.com/pat-jj/DeepRetrieval) : entra√Ænement RL d‚Äô**agents de recherche** avec **r√©sultats de recherche/retrieval** ![GitHub Repo stars](https://img.shields.io/github/stars/pat-jj/DeepRetrieval)
- [ReSearch](https://github.com/Agent-RL/ReSearch) : apprentissage du raisonnement avec recherche pour LLMs via RL ![GitHub Repo stars](https://img.shields.io/github/stars/Agent-RL/ReSearch)
- [Code-R1](https://github.com/ganler/code-r1) : reproduction de R1 pour le **code** avec r√©compenses fiables ![GitHub Repo stars](https://img.shields.io/github/stars/ganler/code-r1)
- [Skywork-OR1](https://github.com/SkyworkAI/Skywork-OR1) : s√©rie open reasoner Skywork ![GitHub Repo stars](https://img.shields.io/github/stars/SkyworkAI/Skywork-OR1)
- [ToRL](https://github.com/GAIR-NLP/ToRL) : scaling RL int√©gr√© aux outils ![GitHub Repo stars](https://img.shields.io/github/stars/GAIR-NLP/ToRL)
- [verl-agent](https://github.com/langfengQ/verl-agent) : framework d‚Äôentra√Ænement √©volutif pour **agents LLM/VLM longue-horizon**, avec un nouvel algorithme **GiGPO** ![GitHub Repo stars](https://img.shields.io/github/stars/langfengQ/verl-agent)
- [PF-PPO](https://arxiv.org/abs/2409.06957) : Policy Filtration pour PPO bas√©e sur la fiabilit√© des signaux de r√©compense pour un RLHF plus efficace et robuste.
- [GUI-R1](https://github.com/ritzz-ai/GUI-R1) : **GUI-R1** : un mod√®le action Vision-Language R1 g√©n√©raliste pour **agents GUI** ![GitHub Repo stars](https://img.shields.io/github/stars/ritzz-ai/GUI-R1)
- [DeepResearcher](https://github.com/GAIR-NLP/DeepResearcher) : scaling deep research via RL dans des environnements r√©els ![GitHub Repo stars](https://img.shields.io/github/stars/GAIR-NLP/DeepResearcher)
- [VAGEN](https://github.com/RAGEN-AI/VAGEN) : entra√Ænement d‚Äôagents VLM avec RL multi-turn ![GitHub Repo stars](https://img.shields.io/github/stars/RAGEN-AI/VAGEN)
- [ReTool](https://retool-rl.github.io/) : ReTool : RL pour l‚Äôutilisation strat√©gique d‚Äôoutils dans les LLMs. Publication du code en cours...
- [RM-R1](https://arxiv.org/abs/2505.02387) : entra√Ænement RL de mod√®les de r√©compense de raisonnement ![GitHub Repo stars](https://img.shields.io/github/stars/RM-R1-UIUC/RM-R1)
- [Absolute Zero Reasoner](https://arxiv.org/abs/2505.03335) : framework self-play sans donn√©es humaines pour le raisonnement ![GitHub Repo stars](https://img.shields.io/github/stars/LeapLabTHU/Absolute-Zero-Reasoner)
- [LUFFY](https://arxiv.org/pdf/2504.14945) : apprendre √† raisonner sous guidance off-policy ![GitHub Repo stars](https://img.shields.io/github/stars/ElliottYan/LUFFY)
- [verl-tool](https://github.com/TIGER-AI-Lab/verl-tool) : un framework unifi√© et facile √† √©tendre pour l‚Äôentra√Ænement d‚Äôagents-outils bas√© sur verl ![GitHub Repo stars](https://img.shields.io/github/stars/TIGER-AI-Lab/verl-tool)
- [DeepMath](https://github.com/zwhe99/DeepMath) : donn√©es DeepMath-103K et mod√®les de la s√©rie pour le raisonnement math√©matique ![GitHub Repo stars](https://img.shields.io/github/stars/zwhe99/DeepMath)

et bien d‚Äôautres projets remarquables list√©s dans [recipe](https://raw.githubusercontent.com/volcengine/verl/main/recipe/README.md).
## Guide de contribution

Les contributions de la communaut√© sont les bienvenues ! Consultez notre [roadmap du projet](https://github.com/volcengine/verl/issues/710) et les [good first issues](https://github.com/volcengine/verl/issues?q=is%3Aissue%20state%3Aopen%20label%3A%22good%20first%20issue%22) pour savoir o√π contribuer.

### Linting et formatage du code

Nous utilisons pre-commit pour am√©liorer la qualit√© du code. Pour initialiser pre-commit, ex√©cutez :

```bash
pip install pre-commit
pre-commit install
```

Pour r√©soudre les erreurs CI localement, vous pouvez lancer pre-commit manuellement :

```bash
pre-commit run
```

### Ajout de tests CI

Si possible, merci d‚Äôajouter un ou plusieurs tests CI pour votre nouvelle fonctionnalit√© :

1. Trouvez le fichier workflow yml le plus pertinent, g√©n√©ralement associ√© √† une config hydra par d√©faut (ex : `ppo_trainer`, `ppo_megatron_trainer`, `sft_trainer`, etc).
2. Ajoutez les patterns de chemin li√©s dans la section `paths` si ce n‚Äôest pas d√©j√† fait.
3. Minimisez la charge de travail des scripts de test (voir les scripts existants pour des exemples).

## √Ä propos de l‚Äô[√©quipe ByteDance Seed](https://team.doubao.com/)

Fond√©e en 2023, l‚Äô√©quipe ByteDance Seed s‚Äôengage √† concevoir les mod√®les d‚ÄôIA fondamentaux les plus avanc√©s de l‚Äôindustrie. L‚Äô√©quipe aspire √† devenir une √©quipe de recherche de classe mondiale et √† contribuer de mani√®re significative √† l‚Äôavancement de la science et de la soci√©t√©. Vous pouvez d√©couvrir l‚Äô√©quipe ByteDance Seed via les canaux suivantsüëá
<div>
  <a href="https://team.doubao.com/">
    <img src="https://img.shields.io/badge/Website-%231e37ff?style=for-the-badge&logo=bytedance&logoColor=white"></a>
  <a href="https://github.com/user-attachments/assets/469535a8-42f2-4797-acdf-4f7a1d4a0c3e">
    <img src="https://img.shields.io/badge/WeChat-07C160?style=for-the-badge&logo=wechat&logoColor=white"></a>
 <a href="https://www.xiaohongshu.com/user/profile/668e7e15000000000303157d?xsec_token=ABl2-aqekpytY6A8TuxjrwnZskU-6BsMRE_ufQQaSAvjc%3D&xsec_source=pc_search">
    <img src="https://img.shields.io/badge/Xiaohongshu-%23FF2442?style=for-the-badge&logo=xiaohongshu&logoColor=white"></a>
  <a href="https://www.zhihu.com/org/dou-bao-da-mo-xing-tuan-dui/">
    <img src="https://img.shields.io/badge/zhihu-%230084FF?style=for-the-badge&logo=zhihu&logoColor=white"></a>

</div>
---

Nous RECRUTONS ! Envoyez-nous un [email](mailto:haibin.lin@bytedance.com) si vous √™tes int√©ress√©(e) par un stage ou un poste FTE en RL pour agents.

---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-06-07

---