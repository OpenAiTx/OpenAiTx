[
  {
    "Id": 1,
    "Content": "# Qwen3 MoE Fused\n\nThe Qwen3 MoE model (and all other MoE models) in HF Transformers is notoriously slow, because it uses a [for loop](https://github.com/huggingface/transformers/blob/bdf5fb70aa11782cce22027d76879f71f4e41c1e/src/transformers/models/qwen3_moe/modular_qwen3_moe.py#L103) to access the experts. The purpose of this repo is to fine-tune Qwen3-30B-A3B on a single GPU with 24GB VRAM and achieve high throughput. The implementation is compatible with the HF Transformers ecosystem, such as LoRA, bitsandbytes 4-bit quantization, and Unsloth. See [`example_train_30b_a3b_unsloth.py`](https://github.com/woct0rdho/transformers-qwen3-moe-fused/blob/master/example_train_30b_a3b_unsloth.py) for the usage.\n\n## Fused linear layer\n\nThe critical part is to implement the [`moe_fused_linear`](https://github.com/woct0rdho/transformers-qwen3-moe-fused/blob/master/qwen3_moe_fused/functional.py) function:",
    "ContentSha": "sJaLD083y384YDC/xALyD1sEEsqIMPoTlk5AFNKv9Ys=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "# Qwen3 MoE 融合版\n\nHF Transformers 中的 Qwen3 MoE 模型（以及所有其他 MoE 模型）以速度緩慢著稱，因為它使用了[for 迴圈](https://github.com/huggingface/transformers/blob/bdf5fb70aa11782cce22027d76879f71f4e41c1e/src/transformers/models/qwen3_moe/modular_qwen3_moe.py#L103)來存取專家模型。此倉庫的目標是使用單一擁有 24GB VRAM 的 GPU 微調 Qwen3-30B-A3B 並達成高吞吐量。此實作與 HF Transformers 生態系統相容，例如 LoRA、bitsandbytes 4-bit 量化及 Unsloth。使用說明請參考[`example_train_30b_a3b_unsloth.py`](https://github.com/woct0rdho/transformers-qwen3-moe-fused/blob/master/example_train_30b_a3b_unsloth.py)。\n\n## 融合線性層\n\n關鍵部分是實作 [`moe_fused_linear`](https://github.com/woct0rdho/transformers-qwen3-moe-fused/blob/master/qwen3_moe_fused/functional.py) 函數：",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "4fnB711iyDvWoa0sKnw+8nYcLwawM1UTLQBb3f+uY/I=",
        "originContent": "# Qwen3 MoE Fused",
        "translatedContent": "# Qwen3 MoE 融合版"
      },
      {
        "row": 2,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 3,
        "rowsha": "E8ZuWnU15dO/sgSrAS2zLXz2v6/wQQRlilP1EcTpZs8=",
        "originContent": "The Qwen3 MoE model (and all other MoE models) in HF Transformers is notoriously slow, because it uses a [for loop](https://github.com/huggingface/transformers/blob/bdf5fb70aa11782cce22027d76879f71f4e41c1e/src/transformers/models/qwen3_moe/modular_qwen3_moe.py#L103) to access the experts. The purpose of this repo is to fine-tune Qwen3-30B-A3B on a single GPU with 24GB VRAM and achieve high throughput. The implementation is compatible with the HF Transformers ecosystem, such as LoRA, bitsandbytes 4-bit quantization, and Unsloth. See [`example_train_30b_a3b_unsloth.py`](https://github.com/woct0rdho/transformers-qwen3-moe-fused/blob/master/example_train_30b_a3b_unsloth.py) for the usage.",
        "translatedContent": "HF Transformers 中的 Qwen3 MoE 模型（以及所有其他 MoE 模型）以速度緩慢著稱，因為它使用了[for 迴圈](https://github.com/huggingface/transformers/blob/bdf5fb70aa11782cce22027d76879f71f4e41c1e/src/transformers/models/qwen3_moe/modular_qwen3_moe.py#L103)來存取專家模型。此倉庫的目標是使用單一擁有 24GB VRAM 的 GPU 微調 Qwen3-30B-A3B 並達成高吞吐量。此實作與 HF Transformers 生態系統相容，例如 LoRA、bitsandbytes 4-bit 量化及 Unsloth。使用說明請參考[`example_train_30b_a3b_unsloth.py`](https://github.com/woct0rdho/transformers-qwen3-moe-fused/blob/master/example_train_30b_a3b_unsloth.py)。"
      },
      {
        "row": 4,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 5,
        "rowsha": "Pmbwz3LvqDEsoiXIcoWxD2/gbOhXynGxYe92Z2EgDPc=",
        "originContent": "## Fused linear layer",
        "translatedContent": "## 融合線性層"
      },
      {
        "row": 6,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 7,
        "rowsha": "047G9Ex30bXGjZUejfj91sdJaMGjkewJppnmN8SAWcU=",
        "originContent": "The critical part is to implement the [`moe_fused_linear`](https://github.com/woct0rdho/transformers-qwen3-moe-fused/blob/master/qwen3_moe_fused/functional.py) function:",
        "translatedContent": "關鍵部分是實作 [`moe_fused_linear`](https://github.com/woct0rdho/transformers-qwen3-moe-fused/blob/master/qwen3_moe_fused/functional.py) 函數："
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 2,
    "Content": "```\noutput[b, o] = sum_i weight[selected_experts[b], o, i] * input[b, i]\n```",
    "ContentSha": "quw4WKeS/Jhf211mDBWyop2/s6jP9zPRNfJSVy8BYRI=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```\noutput[b, o] = sum_i weight[selected_experts[b], o, i] * input[b, i]\n```",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "8bkBhHOQsO1+N058HkZOwXtGpCfEh6WtbL0pBkBQg9U=",
        "originContent": "```",
        "translatedContent": "```"
      },
      {
        "row": 2,
        "rowsha": "q87k1IVVbIctgBDM3CY56OuF+LdL8R4pQtnzu0fEfPo=",
        "originContent": "output[b, o] = sum_i weight[selected_experts[b], o, i] * input[b, i]",
        "translatedContent": "output[b, o] = sum_i weight[selected_experts[b], o, i] * input[b, i]"
      },
      {
        "row": 3,
        "rowsha": "8bkBhHOQsO1+N058HkZOwXtGpCfEh6WtbL0pBkBQg9U=",
        "originContent": "```",
        "translatedContent": "```"
      }
    ],
    "IsCodeBlock": true
  },
  {
    "Id": 3,
    "Content": "There are already several good implementations, such as [triton-kernels](https://github.com/triton-lang/triton/blob/dd1c3d429d1c24904722ac699ea5750bc694c4d6/python/triton_kernels/triton_kernels/matmul_ogs.py), [llama.cpp](https://github.com/ggml-org/llama.cpp/blob/a0535ffa0d35fccfec3e1a0a3bfc9dbb6054d7c0/ggml/src/ggml-cuda/ggml-cuda.cu#L2065), [vLLM](https://github.com/vllm-project/vllm/blob/015fab8c2fa4db8776f7e91abd50371911673d88/vllm/model_executor/layers/fused_moe/fused_moe.py). `torch._grouped_mm` is also being implemented. We need to sort `input` by the experts to improve the memory coalescence of `weight`.\n\nThe implementation in this repo is largely based on the MoE kernel in [Unsloth](https://github.com/unslothai/unsloth/blob/2bfc39b6387577457834059c59f83fcdb954c9bd/unsloth/kernels/moe), which is based on the Triton [grouped GEMM](https://triton-lang.org/main/getting-started/tutorials/08-grouped-gemm.html). I've added strides, masks, and autotune configs for small or 'thin' matrices, which are needed for LoRA.\n\nI aim to keep the code readable and easy to follow. I only used the most mature features of Triton, such as load and store, rather than things like TMA and swizzle.\n\n### LoRA\n\nThe LoRA for the fused linear layer is define by first creating a LoRA for the linear layer in each expert, then stack them along the experts dimension. For the weight tensor with shape `(num_experts, out_features, in_features)`, the two LoRA weights have shape `lora_A: (num_experts, lora_rank, in_features), lora_B: (num_experts, out_features, lora_rank)`. Therefore, a previously trained LoRA can be losslessly converted to the fused format.\n\nThe functions in [`qwen3_moe_fused/convert.py`](https://github.com/woct0rdho/transformers-qwen3-moe-fused/blob/master/qwen3_moe_fused/convert.py) can convert a model or a LoRA between the fused and the unfused formats. After you train a LoRA in the fused format, you can convert it to the unfused format, then convert it to other formats such as GGUF.\n\n### TODO\n\n* Fuse 4-bit dequant and MoE linear, see [`qwen3_moe_fused/quantize/layer.py`](https://github.com/woct0rdho/transformers-qwen3-moe-fused/blob/master/qwen3_moe_fused/quantize/layer.py). Currently I've written a kernel in [`qwen3_moe_fused/grouped_gemm/forward_4bit.py`](https://github.com/woct0rdho/transformers-qwen3-moe-fused/blob/master/qwen3_moe_fused/grouped_gemm/forward_4bit.py) but it's slower than the unfused version when the batch size is large.\n* Multi-GPU support. I don't have multiple GPUs at home so I'm not focusing on this. Maybe worth checking [OpenSloth](https://github.com/anhvth/opensloth).\n* Upstream to Transformers or Unsloth. If you have any idea how to do this, please open an issue. Transformers itself never includes Triton or CUDA kernels in the package, but they have a [HuggingFace Kernels](https://github.com/huggingface/kernels) project for them, and the [vLLM MoE kernels](https://huggingface.co/kernels-community/moe) are already there.\n\n### License\n\nThe files in `qwen3_moe_fused/grouped_gemm/` are modified from the Unsloth MoE kernels so they are AGPLv3 licensed, see the [explanation](https://github.com/unslothai/unsloth/discussions/2890#discussioncomment-13675890). For more robust and performant integration, it's possible to use the MIT licensed [triton-kernels](https://github.com/triton-lang/triton/tree/main/python/triton_kernels/triton_kernels) as an alternative.\n\nThe rest of this repo, including files modified from Transformers, PEFT, and bitsandbytes, are Apache-2.0 licensed.\n",
    "ContentSha": "SBcaoBeEv3C7+SccVQZHq90MuERAkARDS0F9szUhpnM=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "已有幾個不錯的實作，例如 [triton-kernels](https://github.com/triton-lang/triton/blob/dd1c3d429d1c24904722ac699ea5750bc694c4d6/python/triton_kernels/triton_kernels/matmul_ogs.py)、[llama.cpp](https://github.com/ggml-org/llama.cpp/blob/a0535ffa0d35fccfec3e1a0a3bfc9dbb6054d7c0/ggml/src/ggml-cuda/ggml-cuda.cu#L2065)、[vLLM](https://github.com/vllm-project/vllm/blob/015fab8c2fa4db8776f7e91abd50371911673d88/vllm/model_executor/layers/fused_moe/fused_moe.py)。`torch._grouped_mm` 也正在實作中。我們需要依據專家將 `input` 進行排序，以提升 `weight` 的記憶體連續性。\n\n本倉庫的實作主要基於 [Unsloth](https://github.com/unslothai/unsloth/blob/2bfc39b6387577457834059c59f83fcdb954c9bd/unsloth/kernels/moe) 中的 MoE 核心，其又基於 Triton 的 [grouped GEMM](https://triton-lang.org/main/getting-started/tutorials/08-grouped-gemm.html)。我加入了 strides、遮罩以及針對小型或「細長」矩陣的自動調參配置，這些對 LoRA 是必要的。\n\n我希望保持程式碼的可讀性與易於理解。我只使用了 Triton 最成熟的功能，如 load 與 store，而非 TMA 或 swizzle 等較複雜的功能。\n\n### LoRA\n\n融合線性層的 LoRA 是先在每個專家中的線性層建立 LoRA，然後沿著專家維度堆疊。對於形狀為 `(num_experts, out_features, in_features)` 的權重張量，兩個 LoRA 權重分別為 `lora_A: (num_experts, lora_rank, in_features), lora_B: (num_experts, out_features, lora_rank)`。因此，先前訓練好的 LoRA 可以無損地轉換成融合格式。\n\n[`qwen3_moe_fused/convert.py`](https://github.com/woct0rdho/transformers-qwen3-moe-fused/blob/master/qwen3_moe_fused/convert.py) 中的函式可以在融合與非融合格式間轉換模型或 LoRA。當你在融合格式下訓練好 LoRA 後，可以轉換回非融合格式，再轉換成其他格式，如 GGUF。\n\n### TODO\n\n* 融合 4-bit 反量化與 MoE 線性層，詳見 [`qwen3_moe_fused/quantize/layer.py`](https://github.com/woct0rdho/transformers-qwen3-moe-fused/blob/master/qwen3_moe_fused/quantize/layer.py)。目前我在 [`qwen3_moe_fused/grouped_gemm/forward_4bit.py`](https://github.com/woct0rdho/transformers-qwen3-moe-fused/blob/master/qwen3_moe_fused/grouped_gemm/forward_4bit.py) 寫了一個核心，但批次大小大時速度比非融合版慢。\n* 多 GPU 支援。我家沒有多 GPU，所以目前沒專注在這塊。或許值得參考 [OpenSloth](https://github.com/anhvth/opensloth)。\n* 提交至 Transformers 或 Unsloth。如果你有任何想法，請開 issue。Transformers 本身不會在套件中包含 Triton 或 CUDA 核心，但他們有一個 [HuggingFace Kernels](https://github.com/huggingface/kernels) 專案，且 [vLLM MoE 核心](https://huggingface.co/kernels-community/moe) 已經存在那裡。\n\n### 授權\n\n`qwen3_moe_fused/grouped_gemm/` 中的檔案是修改自 Unsloth MoE 核心，因此採用 AGPLv3 授權，詳見 [說明](https://github.com/unslothai/unsloth/discussions/2890#discussioncomment-13675890)。若要更穩健且高效的整合，可以使用 MIT 授權的 [triton-kernels](https://github.com/triton-lang/triton/tree/main/python/triton_kernels/triton_kernels) 作為替代。\n\n本倉庫其餘部分，包括修改自 Transformers、PEFT 與 bitsandbytes 的檔案，均採用 Apache-2.0 授權。\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "MGxH1pV1ODf3IykcFMW7nwdj0eKYz9zxqp/QSAB2fEY=",
        "originContent": "There are already several good implementations, such as [triton-kernels](https://github.com/triton-lang/triton/blob/dd1c3d429d1c24904722ac699ea5750bc694c4d6/python/triton_kernels/triton_kernels/matmul_ogs.py), [llama.cpp](https://github.com/ggml-org/llama.cpp/blob/a0535ffa0d35fccfec3e1a0a3bfc9dbb6054d7c0/ggml/src/ggml-cuda/ggml-cuda.cu#L2065), [vLLM](https://github.com/vllm-project/vllm/blob/015fab8c2fa4db8776f7e91abd50371911673d88/vllm/model_executor/layers/fused_moe/fused_moe.py). `torch._grouped_mm` is also being implemented. We need to sort `input` by the experts to improve the memory coalescence of `weight`.",
        "translatedContent": "已有幾個不錯的實作，例如 [triton-kernels](https://github.com/triton-lang/triton/blob/dd1c3d429d1c24904722ac699ea5750bc694c4d6/python/triton_kernels/triton_kernels/matmul_ogs.py)、[llama.cpp](https://github.com/ggml-org/llama.cpp/blob/a0535ffa0d35fccfec3e1a0a3bfc9dbb6054d7c0/ggml/src/ggml-cuda/ggml-cuda.cu#L2065)、[vLLM](https://github.com/vllm-project/vllm/blob/015fab8c2fa4db8776f7e91abd50371911673d88/vllm/model_executor/layers/fused_moe/fused_moe.py)。`torch._grouped_mm` 也正在實作中。我們需要依據專家將 `input` 進行排序，以提升 `weight` 的記憶體連續性。"
      },
      {
        "row": 2,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 3,
        "rowsha": "ZUmetfWCpNh4LWV3piJ21Oyu/2zS1wBvLO1OqURV6e8=",
        "originContent": "The implementation in this repo is largely based on the MoE kernel in [Unsloth](https://github.com/unslothai/unsloth/blob/2bfc39b6387577457834059c59f83fcdb954c9bd/unsloth/kernels/moe), which is based on the Triton [grouped GEMM](https://triton-lang.org/main/getting-started/tutorials/08-grouped-gemm.html). I've added strides, masks, and autotune configs for small or 'thin' matrices, which are needed for LoRA.",
        "translatedContent": "本倉庫的實作主要基於 [Unsloth](https://github.com/unslothai/unsloth/blob/2bfc39b6387577457834059c59f83fcdb954c9bd/unsloth/kernels/moe) 中的 MoE 核心，其又基於 Triton 的 [grouped GEMM](https://triton-lang.org/main/getting-started/tutorials/08-grouped-gemm.html)。我加入了 strides、遮罩以及針對小型或「細長」矩陣的自動調參配置，這些對 LoRA 是必要的。"
      },
      {
        "row": 4,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 5,
        "rowsha": "YvkpS78wp3XNDaUQX5/fRMuSxzcGUKIe6j8TmHssxQw=",
        "originContent": "I aim to keep the code readable and easy to follow. I only used the most mature features of Triton, such as load and store, rather than things like TMA and swizzle.",
        "translatedContent": "我希望保持程式碼的可讀性與易於理解。我只使用了 Triton 最成熟的功能，如 load 與 store，而非 TMA 或 swizzle 等較複雜的功能。"
      },
      {
        "row": 6,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 7,
        "rowsha": "B4A7UsDCpxuCRDFo6LmWvxpEwAz3AW8SmytKNung4aI=",
        "originContent": "### LoRA",
        "translatedContent": "### LoRA"
      },
      {
        "row": 8,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 9,
        "rowsha": "ERRKxcm0HlEK0Yc9MYK4B8FQarT+mYwdpOIOsE1TlkU=",
        "originContent": "The LoRA for the fused linear layer is define by first creating a LoRA for the linear layer in each expert, then stack them along the experts dimension. For the weight tensor with shape `(num_experts, out_features, in_features)`, the two LoRA weights have shape `lora_A: (num_experts, lora_rank, in_features), lora_B: (num_experts, out_features, lora_rank)`. Therefore, a previously trained LoRA can be losslessly converted to the fused format.",
        "translatedContent": "融合線性層的 LoRA 是先在每個專家中的線性層建立 LoRA，然後沿著專家維度堆疊。對於形狀為 `(num_experts, out_features, in_features)` 的權重張量，兩個 LoRA 權重分別為 `lora_A: (num_experts, lora_rank, in_features), lora_B: (num_experts, out_features, lora_rank)`。因此，先前訓練好的 LoRA 可以無損地轉換成融合格式。"
      },
      {
        "row": 10,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 11,
        "rowsha": "XaNc5BSMoTxd9+1kC2j8xa8k2RJ2W5+uSLjigg8bAzs=",
        "originContent": "The functions in [`qwen3_moe_fused/convert.py`](https://github.com/woct0rdho/transformers-qwen3-moe-fused/blob/master/qwen3_moe_fused/convert.py) can convert a model or a LoRA between the fused and the unfused formats. After you train a LoRA in the fused format, you can convert it to the unfused format, then convert it to other formats such as GGUF.",
        "translatedContent": "[`qwen3_moe_fused/convert.py`](https://github.com/woct0rdho/transformers-qwen3-moe-fused/blob/master/qwen3_moe_fused/convert.py) 中的函式可以在融合與非融合格式間轉換模型或 LoRA。當你在融合格式下訓練好 LoRA 後，可以轉換回非融合格式，再轉換成其他格式，如 GGUF。"
      },
      {
        "row": 12,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 13,
        "rowsha": "F8LwUOZ4tDJeV2W/K2C9N0HW21H4xuvFdGr6cF8VLK4=",
        "originContent": "### TODO",
        "translatedContent": "### TODO"
      },
      {
        "row": 14,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 15,
        "rowsha": "etYMqHTAJxW80LImsF70XwmH5TYyDZfgewtMKauNerQ=",
        "originContent": "* Fuse 4-bit dequant and MoE linear, see [`qwen3_moe_fused/quantize/layer.py`](https://github.com/woct0rdho/transformers-qwen3-moe-fused/blob/master/qwen3_moe_fused/quantize/layer.py). Currently I've written a kernel in [`qwen3_moe_fused/grouped_gemm/forward_4bit.py`](https://github.com/woct0rdho/transformers-qwen3-moe-fused/blob/master/qwen3_moe_fused/grouped_gemm/forward_4bit.py) but it's slower than the unfused version when the batch size is large.",
        "translatedContent": "* 融合 4-bit 反量化與 MoE 線性層，詳見 [`qwen3_moe_fused/quantize/layer.py`](https://github.com/woct0rdho/transformers-qwen3-moe-fused/blob/master/qwen3_moe_fused/quantize/layer.py)。目前我在 [`qwen3_moe_fused/grouped_gemm/forward_4bit.py`](https://github.com/woct0rdho/transformers-qwen3-moe-fused/blob/master/qwen3_moe_fused/grouped_gemm/forward_4bit.py) 寫了一個核心，但批次大小大時速度比非融合版慢。"
      },
      {
        "row": 16,
        "rowsha": "bjXcem56RfLZD5s9MgOCU6sBlow9N8qFTyJb6lMrfZo=",
        "originContent": "* Multi-GPU support. I don't have multiple GPUs at home so I'm not focusing on this. Maybe worth checking [OpenSloth](https://github.com/anhvth/opensloth).",
        "translatedContent": "* 多 GPU 支援。我家沒有多 GPU，所以目前沒專注在這塊。或許值得參考 [OpenSloth](https://github.com/anhvth/opensloth)。"
      },
      {
        "row": 17,
        "rowsha": "K+rUaMJtX70JswVTwpDYqTPr9ROBjXM8aejWxUyjTWs=",
        "originContent": "* Upstream to Transformers or Unsloth. If you have any idea how to do this, please open an issue. Transformers itself never includes Triton or CUDA kernels in the package, but they have a [HuggingFace Kernels](https://github.com/huggingface/kernels) project for them, and the [vLLM MoE kernels](https://huggingface.co/kernels-community/moe) are already there.",
        "translatedContent": "* 提交至 Transformers 或 Unsloth。如果你有任何想法，請開 issue。Transformers 本身不會在套件中包含 Triton 或 CUDA 核心，但他們有一個 [HuggingFace Kernels](https://github.com/huggingface/kernels) 專案，且 [vLLM MoE 核心](https://huggingface.co/kernels-community/moe) 已經存在那裡。"
      },
      {
        "row": 18,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 19,
        "rowsha": "yQxCDp1zblVb6PkKhbgZ2Y8kW5iQPeq17cih4/6NsYY=",
        "originContent": "### License",
        "translatedContent": "### 授權"
      },
      {
        "row": 20,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 21,
        "rowsha": "Rz3kmllDGAyfmM/sY1wXCW9B/q95GSJiDQh5Z2q0//I=",
        "originContent": "The files in `qwen3_moe_fused/grouped_gemm/` are modified from the Unsloth MoE kernels so they are AGPLv3 licensed, see the [explanation](https://github.com/unslothai/unsloth/discussions/2890#discussioncomment-13675890). For more robust and performant integration, it's possible to use the MIT licensed [triton-kernels](https://github.com/triton-lang/triton/tree/main/python/triton_kernels/triton_kernels) as an alternative.",
        "translatedContent": "`qwen3_moe_fused/grouped_gemm/` 中的檔案是修改自 Unsloth MoE 核心，因此採用 AGPLv3 授權，詳見 [說明](https://github.com/unslothai/unsloth/discussions/2890#discussioncomment-13675890)。若要更穩健且高效的整合，可以使用 MIT 授權的 [triton-kernels](https://github.com/triton-lang/triton/tree/main/python/triton_kernels/triton_kernels) 作為替代。"
      },
      {
        "row": 22,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 23,
        "rowsha": "xvGmKtVjDeYOEy69I3XUJ/A2PXViR38HXuLvrGQ0IU0=",
        "originContent": "The rest of this repo, including files modified from Transformers, PEFT, and bitsandbytes, are Apache-2.0 licensed.",
        "translatedContent": "本倉庫其餘部分，包括修改自 Transformers、PEFT 與 bitsandbytes 的檔案，均採用 Apache-2.0 授權。"
      },
      {
        "row": 24,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  }
]