[
  {
    "row": 1,
    "rowsha": "4fnB711iyDvWoa0sKnw+8nYcLwawM1UTLQBb3f+uY/I=",
    "originContent": "# Qwen3 MoE Fused",
    "translatedContent": "# Qwen3 MoE Fused"
  },
  {
    "row": 2,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 3,
    "rowsha": "E8ZuWnU15dO/sgSrAS2zLXz2v6/wQQRlilP1EcTpZs8=",
    "originContent": "The Qwen3 MoE model (and all other MoE models) in HF Transformers is notoriously slow, because it uses a [for loop](https://github.com/huggingface/transformers/blob/bdf5fb70aa11782cce22027d76879f71f4e41c1e/src/transformers/models/qwen3_moe/modular_qwen3_moe.py#L103) to access the experts. The purpose of this repo is to fine-tune Qwen3-30B-A3B on a single GPU with 24GB VRAM and achieve high throughput. The implementation is compatible with the HF Transformers ecosystem, such as LoRA, bitsandbytes 4-bit quantization, and Unsloth. See [`example_train_30b_a3b_unsloth.py`](https://github.com/woct0rdho/transformers-qwen3-moe-fused/blob/master/example_train_30b_a3b_unsloth.py) for the usage.",
    "translatedContent": "HF TransformersのQwen3 MoEモデル（および他のすべてのMoEモデル）は、[forループ](https://github.com/huggingface/transformers/blob/bdf5fb70aa11782cce22027d76879f71f4e41c1e/src/transformers/models/qwen3_moe/modular_qwen3_moe.py#L103)を使ってエキスパートにアクセスするため、非常に遅いです。このリポジトリの目的は、24GB VRAMの単一GPUでQwen3-30B-A3Bをファインチューニングし、高スループットを実現することです。実装はLoRA、bitsandbytes 4ビット量子化、Unslothなど、HF Transformersエコシステムと互換性があります。使用例は[`example_train_30b_a3b_unsloth.py`](https://github.com/woct0rdho/transformers-qwen3-moe-fused/blob/master/example_train_30b_a3b_unsloth.py)を参照してください。"
  },
  {
    "row": 4,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 5,
    "rowsha": "Pmbwz3LvqDEsoiXIcoWxD2/gbOhXynGxYe92Z2EgDPc=",
    "originContent": "## Fused linear layer",
    "translatedContent": "## Fused linear layer"
  },
  {
    "row": 6,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 7,
    "rowsha": "047G9Ex30bXGjZUejfj91sdJaMGjkewJppnmN8SAWcU=",
    "originContent": "The critical part is to implement the [`moe_fused_linear`](https://github.com/woct0rdho/transformers-qwen3-moe-fused/blob/master/qwen3_moe_fused/functional.py) function:",
    "translatedContent": "重要な部分は[`moe_fused_linear`](https://github.com/woct0rdho/transformers-qwen3-moe-fused/blob/master/qwen3_moe_fused/functional.py)関数の実装です："
  },
  {
    "row": 8,
    "rowsha": "8bkBhHOQsO1+N058HkZOwXtGpCfEh6WtbL0pBkBQg9U=",
    "originContent": "```",
    "translatedContent": "```"
  },
  {
    "row": 9,
    "rowsha": "q87k1IVVbIctgBDM3CY56OuF+LdL8R4pQtnzu0fEfPo=",
    "originContent": "output[b, o] = sum_i weight[selected_experts[b], o, i] * input[b, i]",
    "translatedContent": "output[b, o] = sum_i weight[selected_experts[b], o, i] * input[b, i]"
  },
  {
    "row": 10,
    "rowsha": "8bkBhHOQsO1+N058HkZOwXtGpCfEh6WtbL0pBkBQg9U=",
    "originContent": "```",
    "translatedContent": "```"
  },
  {
    "row": 11,
    "rowsha": "MGxH1pV1ODf3IykcFMW7nwdj0eKYz9zxqp/QSAB2fEY=",
    "originContent": "There are already several good implementations, such as [triton-kernels](https://github.com/triton-lang/triton/blob/dd1c3d429d1c24904722ac699ea5750bc694c4d6/python/triton_kernels/triton_kernels/matmul_ogs.py), [llama.cpp](https://github.com/ggml-org/llama.cpp/blob/a0535ffa0d35fccfec3e1a0a3bfc9dbb6054d7c0/ggml/src/ggml-cuda/ggml-cuda.cu#L2065), [vLLM](https://github.com/vllm-project/vllm/blob/015fab8c2fa4db8776f7e91abd50371911673d88/vllm/model_executor/layers/fused_moe/fused_moe.py). `torch._grouped_mm` is also being implemented. We need to sort `input` by the experts to improve the memory coalescence of `weight`.",
    "translatedContent": "すでにいくつかの優れた実装があります。例えば、[triton-kernels](https://github.com/triton-lang/triton/blob/dd1c3d429d1c24904722ac699ea5750bc694c4d6/python/triton_kernels/triton_kernels/matmul_ogs.py)、[llama.cpp](https://github.com/ggml-org/llama.cpp/blob/a0535ffa0d35fccfec3e1a0a3bfc9dbb6054d7c0/ggml/src/ggml-cuda/ggml-cuda.cu#L2065)、[vLLM](https://github.com/vllm-project/vllm/blob/015fab8c2fa4db8776f7e91abd50371911673d88/vllm/model_executor/layers/fused_moe/fused_moe.py)があります。`torch._grouped_mm`も実装中です。`weight`のメモリコアレセンスを改善するために`input`をエキスパートごとにソートする必要があります。"
  },
  {
    "row": 12,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 13,
    "rowsha": "ZUmetfWCpNh4LWV3piJ21Oyu/2zS1wBvLO1OqURV6e8=",
    "originContent": "The implementation in this repo is largely based on the MoE kernel in [Unsloth](https://github.com/unslothai/unsloth/blob/2bfc39b6387577457834059c59f83fcdb954c9bd/unsloth/kernels/moe), which is based on the Triton [grouped GEMM](https://triton-lang.org/main/getting-started/tutorials/08-grouped-gemm.html). I've added strides, masks, and autotune configs for small or 'thin' matrices, which are needed for LoRA.",
    "translatedContent": "このリポジトリの実装は主に[Unsloth](https://github.com/unslothai/unsloth/blob/2bfc39b6387577457834059c59f83fcdb954c9bd/unsloth/kernels/moe)のMoEカーネルに基づいており、これはTritonの[grouped GEMM](https://triton-lang.org/main/getting-started/tutorials/08-grouped-gemm.html)に基づいています。LoRAに必要な小さいまたは「薄い」行列用のストライド、マスク、自動チューニング設定を追加しました。"
  },
  {
    "row": 14,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 15,
    "rowsha": "YvkpS78wp3XNDaUQX5/fRMuSxzcGUKIe6j8TmHssxQw=",
    "originContent": "I aim to keep the code readable and easy to follow. I only used the most mature features of Triton, such as load and store, rather than things like TMA and swizzle.",
    "translatedContent": "コードは読みやすく、追いやすいままに保つことを目標としています。TMAやスウィズルのようなものではなく、ロードやストアのようなTritonの最も成熟した機能のみを使用しました。"
  },
  {
    "row": 16,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 17,
    "rowsha": "B4A7UsDCpxuCRDFo6LmWvxpEwAz3AW8SmytKNung4aI=",
    "originContent": "### LoRA",
    "translatedContent": "### LoRA"
  },
  {
    "row": 18,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 19,
    "rowsha": "ERRKxcm0HlEK0Yc9MYK4B8FQarT+mYwdpOIOsE1TlkU=",
    "originContent": "The LoRA for the fused linear layer is define by first creating a LoRA for the linear layer in each expert, then stack them along the experts dimension. For the weight tensor with shape `(num_experts, out_features, in_features)`, the two LoRA weights have shape `lora_A: (num_experts, lora_rank, in_features), lora_B: (num_experts, out_features, lora_rank)`. Therefore, a previously trained LoRA can be losslessly converted to the fused format.",
    "translatedContent": "融合線形層のLoRAは、まず各エキスパート内の線形層に対してLoRAを作成し、それをエキスパート次元に沿って積み重ねることで定義されます。形状が`(num_experts, out_features, in_features)`の重みテンソルに対し、2つのLoRA重みは`lora_A: (num_experts, lora_rank, in_features)、lora_B: (num_experts, out_features, lora_rank)`の形状を持ちます。したがって、以前に訓練されたLoRAは損失なく融合形式に変換可能です。"
  },
  {
    "row": 20,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 21,
    "rowsha": "XaNc5BSMoTxd9+1kC2j8xa8k2RJ2W5+uSLjigg8bAzs=",
    "originContent": "The functions in [`qwen3_moe_fused/convert.py`](https://github.com/woct0rdho/transformers-qwen3-moe-fused/blob/master/qwen3_moe_fused/convert.py) can convert a model or a LoRA between the fused and the unfused formats. After you train a LoRA in the fused format, you can convert it to the unfused format, then convert it to other formats such as GGUF.",
    "translatedContent": "[`qwen3_moe_fused/convert.py`](https://github.com/woct0rdho/transformers-qwen3-moe-fused/blob/master/qwen3_moe_fused/convert.py)の関数は、モデルまたはLoRAを融合形式と非融合形式の間で変換できます。融合形式でLoRAを訓練した後、非融合形式に変換し、さらにGGUFなどの他の形式に変換可能です。"
  },
  {
    "row": 22,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 23,
    "rowsha": "F8LwUOZ4tDJeV2W/K2C9N0HW21H4xuvFdGr6cF8VLK4=",
    "originContent": "### TODO",
    "translatedContent": "### TODO"
  },
  {
    "row": 24,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 25,
    "rowsha": "etYMqHTAJxW80LImsF70XwmH5TYyDZfgewtMKauNerQ=",
    "originContent": "* Fuse 4-bit dequant and MoE linear, see [`qwen3_moe_fused/quantize/layer.py`](https://github.com/woct0rdho/transformers-qwen3-moe-fused/blob/master/qwen3_moe_fused/quantize/layer.py). Currently I've written a kernel in [`qwen3_moe_fused/grouped_gemm/forward_4bit.py`](https://github.com/woct0rdho/transformers-qwen3-moe-fused/blob/master/qwen3_moe_fused/grouped_gemm/forward_4bit.py) but it's slower than the unfused version when the batch size is large.",
    "translatedContent": "* 4ビットの非量子化とMoE線形層の融合。詳しくは[`qwen3_moe_fused/quantize/layer.py`](https://github.com/woct0rdho/transformers-qwen3-moe-fused/blob/master/qwen3_moe_fused/quantize/layer.py)を参照。現在、[`qwen3_moe_fused/grouped_gemm/forward_4bit.py`](https://github.com/woct0rdho/transformers-qwen3-moe-fused/blob/master/qwen3_moe_fused/grouped_gemm/forward_4bit.py)にカーネルを書いていますが、バッチサイズが大きい場合は非融合版より遅いです。"
  },
  {
    "row": 26,
    "rowsha": "bjXcem56RfLZD5s9MgOCU6sBlow9N8qFTyJb6lMrfZo=",
    "originContent": "* Multi-GPU support. I don't have multiple GPUs at home so I'm not focusing on this. Maybe worth checking [OpenSloth](https://github.com/anhvth/opensloth).",
    "translatedContent": "* マルチGPU対応。自宅に複数GPUがないため、現在は重点を置いていません。[OpenSloth](https://github.com/anhvth/opensloth)を確認する価値があるかもしれません。"
  },
  {
    "row": 27,
    "rowsha": "K+rUaMJtX70JswVTwpDYqTPr9ROBjXM8aejWxUyjTWs=",
    "originContent": "* Upstream to Transformers or Unsloth. If you have any idea how to do this, please open an issue. Transformers itself never includes Triton or CUDA kernels in the package, but they have a [HuggingFace Kernels](https://github.com/huggingface/kernels) project for them, and the [vLLM MoE kernels](https://huggingface.co/kernels-community/moe) are already there.",
    "translatedContent": "* TransformersまたはUnslothへの上流統合。方法についてアイデアがあれば、ぜひIssueを開いてください。Transformers自体はTritonやCUDAカーネルをパッケージに含めませんが、[HuggingFace Kernels](https://github.com/huggingface/kernels)というプロジェクトがあり、[vLLM MoEカーネル](https://huggingface.co/kernels-community/moe)も既に存在します。"
  },
  {
    "row": 28,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 29,
    "rowsha": "yQxCDp1zblVb6PkKhbgZ2Y8kW5iQPeq17cih4/6NsYY=",
    "originContent": "### License",
    "translatedContent": "### ライセンス"
  },
  {
    "row": 30,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 31,
    "rowsha": "Rz3kmllDGAyfmM/sY1wXCW9B/q95GSJiDQh5Z2q0//I=",
    "originContent": "The files in `qwen3_moe_fused/grouped_gemm/` are modified from the Unsloth MoE kernels so they are AGPLv3 licensed, see the [explanation](https://github.com/unslothai/unsloth/discussions/2890#discussioncomment-13675890). For more robust and performant integration, it's possible to use the MIT licensed [triton-kernels](https://github.com/triton-lang/triton/tree/main/python/triton_kernels/triton_kernels) as an alternative.",
    "translatedContent": "`qwen3_moe_fused/grouped_gemm/`内のファイルはUnsloth MoEカーネルから改変されたもので、AGPLv3ライセンスです。詳細は[説明](https://github.com/unslothai/unsloth/discussions/2890#discussioncomment-13675890)を参照してください。より堅牢で高性能な統合には、MITライセンスの[triton-kernels](https://github.com/triton-lang/triton/tree/main/python/triton_kernels/triton_kernels)を代替として使用可能です。"
  },
  {
    "row": 32,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 33,
    "rowsha": "xvGmKtVjDeYOEy69I3XUJ/A2PXViR38HXuLvrGQ0IU0=",
    "originContent": "The rest of this repo, including files modified from Transformers, PEFT, and bitsandbytes, are Apache-2.0 licensed.",
    "translatedContent": "Transformers、PEFT、bitsandbytesから改変されたファイルを含むこのリポジトリの残りはApache-2.0ライセンスです。"
  },
  {
    "row": 34,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  }
]