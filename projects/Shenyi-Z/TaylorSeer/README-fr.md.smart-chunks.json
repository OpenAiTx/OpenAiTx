[
  {
    "Id": 1,
    "Content": "<div align=center>\n  \n# [ICCV 2025] *TaylorSeer*: From Reusing to Forecasting: Accelerating Diffusion Models with *TaylorSeers*\n\n<p>\n<a href='https://arxiv.org/abs/2503.06923'><img src='https://img.shields.io/badge/Paper-arXiv-red'></a>\n<a href='https://taylorseer.github.io/TaylorSeer/'><img src='https://img.shields.io/badge/Project-Page-blue'></a>\n</p>\n\n</div>\n\n## 🔥 News\n\n* `2025/06/26` 💥💥 TaylorSeer is honored to be accepted by ICCV 2025!\n\n* `2025/05/03` 🚀🚀 TaylorSeer for HiDream is released.\n\n* `2025/03/30` 🚀🚀 TaylorSeer for Wan2.1 is released.\n\n* `2025/03/30` 🚀🚀 The Diffusers inference scripts for TaylorSeers and the xDiT scripts applicable for multi-GPU parallel inference have been officially released.\n\n* `2025/03/10` 🚀🚀 Our latest work \"From Reusing to Forecasting: Accelerating Diffusion Models with TaylorSeers\" is released! Codes are available at [TaylorSeer](https://github.com/Shenyi-Z/TaylorSeer)! TaylorSeer supports lossless compression at a rate of 4.99x on FLUX.1-dev (with a latency speedup of 3.53x) and high-quality acceleration at a compression rate of 5.00x on HunyuanVideo (with a latency speedup of 4.65x)! We hope *TaylorSeer* can move the paradigm of feature caching methods from reusing to forecasting.For more details, please refer to our latest research paper.\n* `2025/02/19` 🚀🚀 ToCa solution for **FLUX** has been officially released after adjustments, now achieving up to **3.14× lossless acceleration** (in FLOPs)!\n* `2025/01/22` 💥💥 ToCa is honored to be accepted by ICLR 2025!\n* `2024/12/29` 🚀🚀 We release our work [DuCa](https://arxiv.org/abs/2412.18911) about accelerating diffusion transformers for FREE, which achieves nearly lossless acceleration of **2.50×** on [OpenSora](https://github.com/hpcaitech/Open-Sora)! 🎉 **DuCa also overcomes the limitation of ToCa by fully supporting FlashAttention, enabling broader compatibility and efficiency improvements.**\n* `2024/12/24` 🤗🤗 We release an open-sourse repo \"[Awesome-Token-Reduction-for-Model-Compression](https://github.com/xuyang-liu16/Awesome-Token-Reduction-for-Model-Compression)\", which collects recent awesome token reduction papers! Feel free to contribute your suggestions!\n* `2024/12/10` 💥💥 Our team's recent work, **SiTo** (https://github.com/EvelynZhang-epiclab/SiTo), has been accepted to **AAAI 2025**. It accelerates diffusion models through adaptive **Token Pruning**.\n* `2024/07/15` 🤗🤗 We release an open-sourse repo \"[Awesome-Generation-Acceleration](https://github.com/xuyang-liu16/Awesome-Generation-Acceleration)\", which collects recent awesome generation accleration papers! Feel free to contribute your suggestions!\n\n<details>\n  <summary><strong>Abstract</strong></summary>\n\n  Diffusion Transformers (DiT) have revolutionized high-fidelity image and video synthesis, yet their computational demands remain prohibitive for real-time applications. To solve this problem, feature caching has been proposed to accelerate diffusion models by caching the features in the previous timesteps and then reusing them in the following timesteps. However, at timesteps with significant intervals, the feature similarity in diffusion models decreases substantially, leading to a pronounced increase in errors introduced by feature caching, significantly harming the generation quality. To solve this problem, we propose TaylorSeer, which firstly shows that features of diffusion models at future timesteps can be predicted based on their values at previous timesteps. Based on the fact that features change slowly and continuously across timesteps, TaylorSeer employs a differential method to approximate the higher-order derivatives of features and predict features in future timesteps with Taylor series expansion. Extensive experiments demonstrate its significant effectiveness in both image and video synthesis, especially in high acceleration ratios. For instance, it achieves an almost lossless acceleration of 4.99 $\\times$ on FLUX and 5.00 $\\times$ on HunyuanVideo without additional training. On DiT, it achieves $3.41$ lower FID compared with previous SOTA at $4.53$ $\\times$ acceleration.\n\n</details>\n\n## 🧩 Community Contributions\n\nThanks to all the open-source contributors for their strong support! We’d love to hear from you!\n\n* ComfyUI-TaylorSeer-philipy1219 (FP8 Inference on FLUX, more video models coming): [ComfyUI-TaylorSeer-philipy1219](https://github.com/philipy1219/ComfyUI-TaylorSeer) by [philipy1219](https://github.com/philipy1219).\n\n## 🛠 Installation\n",
    "ContentSha": "37uFfQBn0Cp076D5K4hJQxTj573ffMx7qA31hLCBg/4=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "<div align=center>\n  \n# [ICCV 2025] *TaylorSeer* : De la réutilisation à la prévision : Accélérer les modèles de diffusion avec *TaylorSeers*\n\n<p>\n<a href='https://arxiv.org/abs/2503.06923'><img src='https://img.shields.io/badge/Paper-arXiv-red'></a>\n<a href='https://taylorseer.github.io/TaylorSeer/'><img src='https://img.shields.io/badge/Project-Page-blue'></a>\n</p>\n\n</div>\n\n## 🔥 Actualités\n\n* `2025/06/26` 💥💥 TaylorSeer a l'honneur d'être accepté par ICCV 2025 !\n\n* `2025/05/03` 🚀🚀 TaylorSeer pour HiDream est publié.\n\n* `2025/03/30` 🚀🚀 TaylorSeer pour Wan2.1 est publié.\n\n* `2025/03/30` 🚀🚀 Les scripts d'inférence Diffusers pour TaylorSeers et les scripts xDiT applicables à l'inférence parallèle multi-GPU ont été officiellement publiés.\n\n* `2025/03/10` 🚀🚀 Notre dernier travail \"De la réutilisation à la prévision : Accélérer les modèles de diffusion avec TaylorSeers\" est publié ! Les codes sont disponibles sur [TaylorSeer](https://github.com/Shenyi-Z/TaylorSeer) ! TaylorSeer supporte une compression sans perte à un taux de 4,99x sur FLUX.1-dev (avec une accélération de latence de 3,53x) et une accélération de haute qualité à un taux de compression de 5,00x sur HunyuanVideo (avec une accélération de latence de 4,65x) ! Nous espérons que *TaylorSeer* pourra faire évoluer le paradigme des méthodes de mise en cache des caractéristiques de la réutilisation à la prévision. Pour plus de détails, veuillez vous référer à notre dernier article de recherche.\n* `2025/02/19` 🚀🚀 La solution ToCa pour **FLUX** a été officiellement publiée après ajustements, atteignant désormais jusqu'à **3,14× d'accélération sans perte** (en FLOPs) !\n* `2025/01/22` 💥💥 ToCa a l'honneur d'être accepté par ICLR 2025 !\n* `2024/12/29` 🚀🚀 Nous publions gratuitement notre travail [DuCa](https://arxiv.org/abs/2412.18911) sur l'accélération des transformateurs de diffusion, qui atteint une accélération quasi sans perte de **2,50×** sur [OpenSora](https://github.com/hpcaitech/Open-Sora) ! 🎉 **DuCa surmonte également la limitation de ToCa en supportant pleinement FlashAttention, permettant une compatibilité et des améliorations d'efficacité plus larges.**\n* `2024/12/24` 🤗🤗 Nous publions un dépôt open-source \"[Awesome-Token-Reduction-for-Model-Compression](https://github.com/xuyang-liu16/Awesome-Token-Reduction-for-Model-Compression)\", qui rassemble les récents excellents articles sur la réduction de tokens ! N'hésitez pas à contribuer avec vos suggestions !\n* `2024/12/10` 💥💥 Le travail récent de notre équipe, **SiTo** (https://github.com/EvelynZhang-epiclab/SiTo), a été accepté à **AAAI 2025**. Il accélère les modèles de diffusion grâce à la **pruning adaptative des tokens**.\n* `2024/07/15` 🤗🤗 Nous publions un dépôt open-source \"[Awesome-Generation-Acceleration](https://github.com/xuyang-liu16/Awesome-Generation-Acceleration)\", qui rassemble les récents excellents articles sur l'accélération de génération ! N'hésitez pas à contribuer avec vos suggestions !\n\n<details>\n  <summary><strong>Résumé</strong></summary>\n\n  Les Diffusion Transformers (DiT) ont révolutionné la synthèse d'images et de vidéos à haute fidélité, mais leurs exigences computationnelles restent prohibitives pour les applications en temps réel. Pour résoudre ce problème, la mise en cache des caractéristiques a été proposée pour accélérer les modèles de diffusion en mettant en cache les caractéristiques des pas de temps précédents puis en les réutilisant aux pas de temps suivants. Cependant, aux pas de temps avec des intervalles importants, la similarité des caractéristiques dans les modèles de diffusion diminue considérablement, entraînant une augmentation marquée des erreurs introduites par la mise en cache des caractéristiques, ce qui nuit fortement à la qualité de génération. Pour résoudre ce problème, nous proposons TaylorSeer, qui montre d'abord que les caractéristiques des modèles de diffusion aux pas de temps futurs peuvent être prédites à partir de leurs valeurs aux pas de temps précédents. Basé sur le fait que les caractéristiques changent lentement et continuellement au fil des pas de temps, TaylorSeer emploie une méthode différentielle pour approximer les dérivées d'ordre supérieur des caractéristiques et prédire les caractéristiques aux pas de temps futurs avec une expansion en série de Taylor. Des expériences approfondies démontrent son efficacité significative tant en synthèse d'images que de vidéos, particulièrement pour des taux d'accélération élevés. Par exemple, il atteint une accélération quasi sans perte de 4,99 $\\times$ sur FLUX et 5,00 $\\times$ sur HunyuanVideo sans entraînement supplémentaire. Sur DiT, il obtient un FID inférieur de $3,41$ comparé au précédent état de l'art à une accélération de $4,53$ $\\times$.\n\n</details>\n\n## 🧩 Contributions de la communauté\n\nMerci à tous les contributeurs open-source pour leur fort soutien ! Nous serions ravis d’avoir de vos nouvelles !\n\n* ComfyUI-TaylorSeer-philipy1219 (inférence FP8 sur FLUX, plus de modèles vidéo à venir) : [ComfyUI-TaylorSeer-philipy1219](https://github.com/philipy1219/ComfyUI-TaylorSeer) par [philipy1219](https://github.com/philipy1219).\n\n## 🛠 Installation\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "fagPb+qexaXZ3eHptDc4ivZK9kTnn6hX3OIFMxv7aKA=",
        "originContent": "<div align=center>",
        "translatedContent": "<div align=center>"
      },
      {
        "row": 2,
        "rowsha": "bBefIeb2K2KQVdirQPRU7QLki2hWORNHO4V9NjjiOyg=",
        "originContent": "  ",
        "translatedContent": "  "
      },
      {
        "row": 3,
        "rowsha": "MOtTqT65KO6UuOPpgVNTUNy7DBii0E9VU9QXzmbDlRo=",
        "originContent": "# [ICCV 2025] *TaylorSeer*: From Reusing to Forecasting: Accelerating Diffusion Models with *TaylorSeers*",
        "translatedContent": "# [ICCV 2025] *TaylorSeer* : De la réutilisation à la prévision : Accélérer les modèles de diffusion avec *TaylorSeers*"
      },
      {
        "row": 4,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 5,
        "rowsha": "M/J5lGcXcoeikmB4ChB6yY6mPdYWX2f8wNdHZ9CoIJA=",
        "originContent": "<p>",
        "translatedContent": "<p>"
      },
      {
        "row": 6,
        "rowsha": "mz6OHfHOKolNYRAUj7PcksyTTRde9MDQh/L+SZDoFfo=",
        "originContent": "<a href='https://arxiv.org/abs/2503.06923'><img src='https://img.shields.io/badge/Paper-arXiv-red'></a>",
        "translatedContent": "<a href='https://arxiv.org/abs/2503.06923'><img src='https://img.shields.io/badge/Paper-arXiv-red'></a>"
      },
      {
        "row": 7,
        "rowsha": "s05rwhJ5NOYGsjO9U5kI5HYTz2AEGGz9Fbptc/Pug9c=",
        "originContent": "<a href='https://taylorseer.github.io/TaylorSeer/'><img src='https://img.shields.io/badge/Project-Page-blue'></a>",
        "translatedContent": "<a href='https://taylorseer.github.io/TaylorSeer/'><img src='https://img.shields.io/badge/Project-Page-blue'></a>"
      },
      {
        "row": 8,
        "rowsha": "dSdvPNAZSmR86FDDSF6tkQUCVfI9qmACHOR5tThOetY=",
        "originContent": "</p>",
        "translatedContent": "</p>"
      },
      {
        "row": 9,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 10,
        "rowsha": "qsMmUbEPVnxGG5tPJV1vsfpoWbU2jYvZpRr5IKshzyM=",
        "originContent": "</div>",
        "translatedContent": "</div>"
      },
      {
        "row": 11,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 12,
        "rowsha": "D7bG/VBJXSi2hpB980ek7pr4pjcctcRkfLZHbhVcyVg=",
        "originContent": "## 🔥 News",
        "translatedContent": "## 🔥 Actualités"
      },
      {
        "row": 13,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 14,
        "rowsha": "4+OZ9EqFu/vmVQzOQg6e3fSByT5AG7kgw0p/8ZZtXvg=",
        "originContent": "* `2025/06/26` 💥💥 TaylorSeer is honored to be accepted by ICCV 2025!",
        "translatedContent": "* `2025/06/26` 💥💥 TaylorSeer a l'honneur d'être accepté par ICCV 2025 !"
      },
      {
        "row": 15,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 16,
        "rowsha": "nWHnk9X5bw0hFy3WJgq/jX5Go8T0fpLu1GZwDSTAXQ4=",
        "originContent": "* `2025/05/03` 🚀🚀 TaylorSeer for HiDream is released.",
        "translatedContent": "* `2025/05/03` 🚀🚀 TaylorSeer pour HiDream est publié."
      },
      {
        "row": 17,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 18,
        "rowsha": "nkoB/ObNg2vVPOSBVJEmU88VVwnAhcgFXCZqkw1bSY0=",
        "originContent": "* `2025/03/30` 🚀🚀 TaylorSeer for Wan2.1 is released.",
        "translatedContent": "* `2025/03/30` 🚀🚀 TaylorSeer pour Wan2.1 est publié."
      },
      {
        "row": 19,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 20,
        "rowsha": "3NpXU50QD+KRAHj2dNcg+rCjixMNdTA/nGF1u6E0oe0=",
        "originContent": "* `2025/03/30` 🚀🚀 The Diffusers inference scripts for TaylorSeers and the xDiT scripts applicable for multi-GPU parallel inference have been officially released.",
        "translatedContent": "* `2025/03/30` 🚀🚀 Les scripts d'inférence Diffusers pour TaylorSeers et les scripts xDiT applicables à l'inférence parallèle multi-GPU ont été officiellement publiés."
      },
      {
        "row": 21,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 22,
        "rowsha": "9NG8DDgu4GTSLg6UyU6J2MUqXFt01vZBE1vkyAWpvuo=",
        "originContent": "* `2025/03/10` 🚀🚀 Our latest work \"From Reusing to Forecasting: Accelerating Diffusion Models with TaylorSeers\" is released! Codes are available at [TaylorSeer](https://github.com/Shenyi-Z/TaylorSeer)! TaylorSeer supports lossless compression at a rate of 4.99x on FLUX.1-dev (with a latency speedup of 3.53x) and high-quality acceleration at a compression rate of 5.00x on HunyuanVideo (with a latency speedup of 4.65x)! We hope *TaylorSeer* can move the paradigm of feature caching methods from reusing to forecasting.For more details, please refer to our latest research paper.",
        "translatedContent": "* `2025/03/10` 🚀🚀 Notre dernier travail \"De la réutilisation à la prévision : Accélérer les modèles de diffusion avec TaylorSeers\" est publié ! Les codes sont disponibles sur [TaylorSeer](https://github.com/Shenyi-Z/TaylorSeer) ! TaylorSeer supporte une compression sans perte à un taux de 4,99x sur FLUX.1-dev (avec une accélération de latence de 3,53x) et une accélération de haute qualité à un taux de compression de 5,00x sur HunyuanVideo (avec une accélération de latence de 4,65x) ! Nous espérons que *TaylorSeer* pourra faire évoluer le paradigme des méthodes de mise en cache des caractéristiques de la réutilisation à la prévision. Pour plus de détails, veuillez vous référer à notre dernier article de recherche."
      },
      {
        "row": 23,
        "rowsha": "HP4JYN9AqV5x9xftKuxud6Lmj5FbX6yZVjvsYTmL3X4=",
        "originContent": "* `2025/02/19` 🚀🚀 ToCa solution for **FLUX** has been officially released after adjustments, now achieving up to **3.14× lossless acceleration** (in FLOPs)!",
        "translatedContent": "* `2025/02/19` 🚀🚀 La solution ToCa pour **FLUX** a été officiellement publiée après ajustements, atteignant désormais jusqu'à **3,14× d'accélération sans perte** (en FLOPs) !"
      },
      {
        "row": 24,
        "rowsha": "OUJy4nRS5cJZhTQ/bIHSb7tR3CPpzmEdfy9FQsPvkKY=",
        "originContent": "* `2025/01/22` 💥💥 ToCa is honored to be accepted by ICLR 2025!",
        "translatedContent": "* `2025/01/22` 💥💥 ToCa a l'honneur d'être accepté par ICLR 2025 !"
      },
      {
        "row": 25,
        "rowsha": "JP7Uqbgmmu3w73rzXhFcbs+DuZhRhuwWmv0rp0Z8zBQ=",
        "originContent": "* `2024/12/29` 🚀🚀 We release our work [DuCa](https://arxiv.org/abs/2412.18911) about accelerating diffusion transformers for FREE, which achieves nearly lossless acceleration of **2.50×** on [OpenSora](https://github.com/hpcaitech/Open-Sora)! 🎉 **DuCa also overcomes the limitation of ToCa by fully supporting FlashAttention, enabling broader compatibility and efficiency improvements.**",
        "translatedContent": "* `2024/12/29` 🚀🚀 Nous publions gratuitement notre travail [DuCa](https://arxiv.org/abs/2412.18911) sur l'accélération des transformateurs de diffusion, qui atteint une accélération quasi sans perte de **2,50×** sur [OpenSora](https://github.com/hpcaitech/Open-Sora) ! 🎉 **DuCa surmonte également la limitation de ToCa en supportant pleinement FlashAttention, permettant une compatibilité et des améliorations d'efficacité plus larges.**"
      },
      {
        "row": 26,
        "rowsha": "76EiBVgv/EgSTVYsjHggBTygkeo1yu7OIgVRvPD3Ixs=",
        "originContent": "* `2024/12/24` 🤗🤗 We release an open-sourse repo \"[Awesome-Token-Reduction-for-Model-Compression](https://github.com/xuyang-liu16/Awesome-Token-Reduction-for-Model-Compression)\", which collects recent awesome token reduction papers! Feel free to contribute your suggestions!",
        "translatedContent": "* `2024/12/24` 🤗🤗 Nous publions un dépôt open-source \"[Awesome-Token-Reduction-for-Model-Compression](https://github.com/xuyang-liu16/Awesome-Token-Reduction-for-Model-Compression)\", qui rassemble les récents excellents articles sur la réduction de tokens ! N'hésitez pas à contribuer avec vos suggestions !"
      },
      {
        "row": 27,
        "rowsha": "/8zh2xS0BXOpvQxdWqtvtS7GCz9dpRDj/KNLKaArh/s=",
        "originContent": "* `2024/12/10` 💥💥 Our team's recent work, **SiTo** (https://github.com/EvelynZhang-epiclab/SiTo), has been accepted to **AAAI 2025**. It accelerates diffusion models through adaptive **Token Pruning**.",
        "translatedContent": "* `2024/12/10` 💥💥 Le travail récent de notre équipe, **SiTo** (https://github.com/EvelynZhang-epiclab/SiTo), a été accepté à **AAAI 2025**. Il accélère les modèles de diffusion grâce à la **pruning adaptative des tokens**."
      },
      {
        "row": 28,
        "rowsha": "O7eC1AedMiPQpbcvrbKc4glnjZa6qtgwQDRRhUGn+1w=",
        "originContent": "* `2024/07/15` 🤗🤗 We release an open-sourse repo \"[Awesome-Generation-Acceleration](https://github.com/xuyang-liu16/Awesome-Generation-Acceleration)\", which collects recent awesome generation accleration papers! Feel free to contribute your suggestions!",
        "translatedContent": "* `2024/07/15` 🤗🤗 Nous publions un dépôt open-source \"[Awesome-Generation-Acceleration](https://github.com/xuyang-liu16/Awesome-Generation-Acceleration)\", qui rassemble les récents excellents articles sur l'accélération de génération ! N'hésitez pas à contribuer avec vos suggestions !"
      },
      {
        "row": 29,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 30,
        "rowsha": "U8eO5ZMev1PH3G030/YyNbNyaS1S4k9n5Of/5MkmvMU=",
        "originContent": "<details>",
        "translatedContent": "<details>"
      },
      {
        "row": 31,
        "rowsha": "qLx6Z+0MU0Auz7F41A3Jc4dR+cSkzESO1JVS8/SKcDc=",
        "originContent": "  <summary><strong>Abstract</strong></summary>",
        "translatedContent": "  <summary><strong>Résumé</strong></summary>"
      },
      {
        "row": 32,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 33,
        "rowsha": "YIqf7kN0c/54TcKx6d15MTu4MoUYVK2v7TZvknoQpw4=",
        "originContent": "  Diffusion Transformers (DiT) have revolutionized high-fidelity image and video synthesis, yet their computational demands remain prohibitive for real-time applications. To solve this problem, feature caching has been proposed to accelerate diffusion models by caching the features in the previous timesteps and then reusing them in the following timesteps. However, at timesteps with significant intervals, the feature similarity in diffusion models decreases substantially, leading to a pronounced increase in errors introduced by feature caching, significantly harming the generation quality. To solve this problem, we propose TaylorSeer, which firstly shows that features of diffusion models at future timesteps can be predicted based on their values at previous timesteps. Based on the fact that features change slowly and continuously across timesteps, TaylorSeer employs a differential method to approximate the higher-order derivatives of features and predict features in future timesteps with Taylor series expansion. Extensive experiments demonstrate its significant effectiveness in both image and video synthesis, especially in high acceleration ratios. For instance, it achieves an almost lossless acceleration of 4.99 $\\times$ on FLUX and 5.00 $\\times$ on HunyuanVideo without additional training. On DiT, it achieves $3.41$ lower FID compared with previous SOTA at $4.53$ $\\times$ acceleration.",
        "translatedContent": "  Les Diffusion Transformers (DiT) ont révolutionné la synthèse d'images et de vidéos à haute fidélité, mais leurs exigences computationnelles restent prohibitives pour les applications en temps réel. Pour résoudre ce problème, la mise en cache des caractéristiques a été proposée pour accélérer les modèles de diffusion en mettant en cache les caractéristiques des pas de temps précédents puis en les réutilisant aux pas de temps suivants. Cependant, aux pas de temps avec des intervalles importants, la similarité des caractéristiques dans les modèles de diffusion diminue considérablement, entraînant une augmentation marquée des erreurs introduites par la mise en cache des caractéristiques, ce qui nuit fortement à la qualité de génération. Pour résoudre ce problème, nous proposons TaylorSeer, qui montre d'abord que les caractéristiques des modèles de diffusion aux pas de temps futurs peuvent être prédites à partir de leurs valeurs aux pas de temps précédents. Basé sur le fait que les caractéristiques changent lentement et continuellement au fil des pas de temps, TaylorSeer emploie une méthode différentielle pour approximer les dérivées d'ordre supérieur des caractéristiques et prédire les caractéristiques aux pas de temps futurs avec une expansion en série de Taylor. Des expériences approfondies démontrent son efficacité significative tant en synthèse d'images que de vidéos, particulièrement pour des taux d'accélération élevés. Par exemple, il atteint une accélération quasi sans perte de 4,99 $\\times$ sur FLUX et 5,00 $\\times$ sur HunyuanVideo sans entraînement supplémentaire. Sur DiT, il obtient un FID inférieur de $3,41$ comparé au précédent état de l'art à une accélération de $4,53$ $\\times$."
      },
      {
        "row": 34,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 35,
        "rowsha": "iKxEwaqPw8IiEEWQ6czEzAvKi1nrFO/wqlPbG+tChcM=",
        "originContent": "</details>",
        "translatedContent": "</details>"
      },
      {
        "row": 36,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 37,
        "rowsha": "vVMKG86X38wpzVR64MO4O04P+r93zrGe0ew8fB75VJE=",
        "originContent": "## 🧩 Community Contributions",
        "translatedContent": "## 🧩 Contributions de la communauté"
      },
      {
        "row": 38,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 39,
        "rowsha": "NhqQVhWscK75IALoDKNa+hzMcIiPuhqHLJjuDbSctMA=",
        "originContent": "Thanks to all the open-source contributors for their strong support! We’d love to hear from you!",
        "translatedContent": "Merci à tous les contributeurs open-source pour leur fort soutien ! Nous serions ravis d’avoir de vos nouvelles !"
      },
      {
        "row": 40,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 41,
        "rowsha": "woBL8XbSNmvsJez4Ev755tcLFMQBHiYdzCnwQ4Dm+lo=",
        "originContent": "* ComfyUI-TaylorSeer-philipy1219 (FP8 Inference on FLUX, more video models coming): [ComfyUI-TaylorSeer-philipy1219](https://github.com/philipy1219/ComfyUI-TaylorSeer) by [philipy1219](https://github.com/philipy1219).",
        "translatedContent": "* ComfyUI-TaylorSeer-philipy1219 (inférence FP8 sur FLUX, plus de modèles vidéo à venir) : [ComfyUI-TaylorSeer-philipy1219](https://github.com/philipy1219/ComfyUI-TaylorSeer) par [philipy1219](https://github.com/philipy1219)."
      },
      {
        "row": 42,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 43,
        "rowsha": "Uib95QyFy+G2xAAPKyoC5mR8MEqKhXKN6jTjMNlQ47w=",
        "originContent": "## 🛠 Installation",
        "translatedContent": "## 🛠 Installation"
      },
      {
        "row": 44,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 2,
    "Content": "``` cmd\ngit clone https://github.com/Shenyi-Z/TaylorSeer.git\n```",
    "ContentSha": "tuWrp9bDtQRlcwPecUQOIggRbo+xLp0FrvKAqYyhPhI=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "``` cmd\ngit clone https://github.com/Shenyi-Z/TaylorSeer.git\n```",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "m+Ye/ZAlgvAp+ebS8N9WTXZep69SFbfwiK0O7Et0+Ro=",
        "originContent": "``` cmd",
        "translatedContent": "``` cmd"
      },
      {
        "row": 2,
        "rowsha": "cS+FAoCjHksjXqzHsY5NCYms9p+hgMbt55zlSipAEdM=",
        "originContent": "git clone https://github.com/Shenyi-Z/TaylorSeer.git",
        "translatedContent": "git clone https://github.com/Shenyi-Z/TaylorSeer.git"
      },
      {
        "row": 3,
        "rowsha": "8bkBhHOQsO1+N058HkZOwXtGpCfEh6WtbL0pBkBQg9U=",
        "originContent": "```",
        "translatedContent": "```"
      }
    ],
    "IsCodeBlock": true
  },
  {
    "Id": 3,
    "Content": "\n\n## TaylorSeer-FLUX\n\nTaylorSeer achieved a lossless computational compression of 4.99 $\\times$ and a Latency Speedup of 3.53 $\\times$ on FLUX.1-dev, as measured by [ImageReward](https://github.com/THUDM/ImageReward) for comprehensive quality. To run TaylorSeer-FLUX, see [TaylorSeer-FLUX](https://raw.githubusercontent.com/Shenyi-Z/TaylorSeer/main/TaylorSeer-FLUX.md).\n\nBesides, We have provided examples of inference scripts for the **diffusers version**, as well as multi-GPU parallel **xDiT inference scripts**. You can also conduct tests based on them, located at [TaylorSeers-Diffusers](./TaylorSeers-Diffusers ) and [TaylorSeers-xDiT](./TaylorSeers-xDiT) respectively.\n\n## TaylorSeer-HunyuanVideo\n\nTaylorSeer achieved a computational compression of 5.00 $\\times$ and a remarkable Latency Speedup of 4.65 $\\times$ on HunyuanVideo, as comprehensively measured by the [VBench](https://github.com/Vchitect/VBench) metric. Compared to previous methods, it demonstrated significant improvements in both acceleration efficiency and quality. To run TaylorSeer-HunyuanVideo, see [TaylorSeer-HunyuanVideo](https://raw.githubusercontent.com/Shenyi-Z/TaylorSeer/main/TaylorSeer-HunyuanVideo.md).\n\nIn addition, our scripts also support multi-GPU parallel acceleration implemented by HunyuanVideo using xDiT. In this case, the acceleration effect brought by the cache and the acceleration effect of multi-GPU parallelism are independent of each other and multiply, achieving extremely high acceleration effects.\n\n## TayorSeer-DiT\n\nTaylorSeer achieved a lossless computational compression of 2.77 $\\times$ on the base model DiT, as comprehensively evaluated by metrics such as FID. Its performance across various acceleration ratios significantly surpassed previous methods. For instance, in an extreme scenario with a 4.53 $\\times$ compression ratio, TaylorSeer's FID only increased by 0.33 from the non-accelerated baseline of 2.32, reaching 2.65, while ToCa and DuCa exhibited FID scores above 6.0 under the same conditions. To run TaylorSeer-DiT,see [TaylorSeer-DiT](https://raw.githubusercontent.com/Shenyi-Z/TaylorSeer/main/TaylorSeer-DiT.md).\n\n## TaylorSeer-Wan2.1\n\nWe implemented the TaylorSeer acceleration method on Wan2.1, with support for multi-GPU parallel inference. The installation and inference commands for TaylorSeer-Wan2.1 are fully compatible with those of Wan2.1. To run TaylorSeer-Wan2.1, see [TaylorSeer-Wan2.1](https://raw.githubusercontent.com/Shenyi-Z/TaylorSeer/main/TaylorSeer-Wan2.1.md).\n\n## TaylorSeer-HiDream\n\nThe recently open-sourced image generation model **HiDream**, despite its impressive output quality, faces increasing demands for acceleration due to its longer inference time. We applied **TaylorSeer** to accelerate HiDream’s inference, achieving a **72% reduction in runtime**. For more details, see [TaylorSeer-HiDream](https://raw.githubusercontent.com/Shenyi-Z/TaylorSeer/main/TaylorSeer-HiDream.md).\n\n## 👍 Acknowledgements\n\n- Thanks to [DiT](https://github.com/facebookresearch/DiT) for their great work and codebase upon which we build TaylorSeer-DiT.\n- Thanks to [FLUX](https://github.com/black-forest-labs/flux) for their great work and codebase upon which we build TaylorSeer-FLUX.\n- Thanks to [HiDream](https://github.com/HiDream-ai/HiDream-I1) for their great work and codebase upon which we build TaylorSeer-HiDream.\n- Thanks to [HunyuanVideo](https://github.com/Tencent/HunyuanVideo) for their great work and codebase upon which we build TaylorSeer-HunyuanVideo.\n- Thanks to [Wan2.1](https://github.com/Wan-Video/Wan2.1) for their great work and codebase upon which we build TaylorSeer-Wan2.1.\n- Thanks to [ImageReward](https://github.com/THUDM/ImageReward) for Text-to-Image quality evaluation.\n- Thanks to [VBench](https://github.com/Vchitect/VBench) for Text-to-Video quality evaluation.\n\n\n## 📌 Citation\n",
    "ContentSha": "j4spOn6j9j6K+1441QrOXQV28NxAb0gvoZxPRt2rZOk=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "\n## TaylorSeer-FLUX\n\nTaylorSeer a réalisé une compression computationnelle sans perte de 4,99 $\\times$ et une accélération de latence de 3,53 $\\times$ sur FLUX.1-dev, mesurées par [ImageReward](https://github.com/THUDM/ImageReward) pour une qualité globale. Pour exécuter TaylorSeer-FLUX, voir [TaylorSeer-FLUX](https://raw.githubusercontent.com/Shenyi-Z/TaylorSeer/main/TaylorSeer-FLUX.md).\n\nDe plus, nous avons fourni des exemples de scripts d'inférence pour la **version diffusers**, ainsi que des scripts d'inférence **xDiT** en parallèle multi-GPU. Vous pouvez également réaliser des tests basés sur ceux-ci, situés respectivement à [TaylorSeers-Diffusers](./TaylorSeers-Diffusers ) et [TaylorSeers-xDiT](./TaylorSeers-xDiT).\n\n## TaylorSeer-HunyuanVideo\n\nTaylorSeer a atteint une compression computationnelle de 5,00 $\\times$ et une remarquable accélération de latence de 4,65 $\\times$ sur HunyuanVideo, mesurées de manière exhaustive par la métrique [VBench](https://github.com/Vchitect/VBench). Par rapport aux méthodes précédentes, il a démontré des améliorations significatives tant en efficacité d'accélération qu'en qualité. Pour exécuter TaylorSeer-HunyuanVideo, voir [TaylorSeer-HunyuanVideo](https://raw.githubusercontent.com/Shenyi-Z/TaylorSeer/main/TaylorSeer-HunyuanVideo.md).\n\nDe plus, nos scripts prennent également en charge l'accélération parallèle multi-GPU mise en œuvre par HunyuanVideo utilisant xDiT. Dans ce cas, l'effet d'accélération apporté par le cache et celui du parallélisme multi-GPU sont indépendants l'un de l'autre et se multiplient, atteignant des effets d'accélération extrêmement élevés.\n\n## TayorSeer-DiT\n\nTaylorSeer a réalisé une compression computationnelle sans perte de 2,77 $\\times$ sur le modèle de base DiT, évaluée de manière exhaustive par des métriques telles que le FID. Ses performances à divers ratios d'accélération ont largement surpassé les méthodes précédentes. Par exemple, dans un scénario extrême avec un ratio de compression de 4,53 $\\times$, le FID de TaylorSeer n'a augmenté que de 0,33 par rapport à la référence non accélérée de 2,32, atteignant 2,65, tandis que ToCa et DuCa affichaient des scores FID supérieurs à 6,0 dans les mêmes conditions. Pour exécuter TaylorSeer-DiT, voir [TaylorSeer-DiT](https://raw.githubusercontent.com/Shenyi-Z/TaylorSeer/main/TaylorSeer-DiT.md).\n\n## TaylorSeer-Wan2.1\n\nNous avons implémenté la méthode d'accélération TaylorSeer sur Wan2.1, avec prise en charge de l'inférence parallèle multi-GPU. Les commandes d'installation et d'inférence pour TaylorSeer-Wan2.1 sont entièrement compatibles avec celles de Wan2.1. Pour exécuter TaylorSeer-Wan2.1, voir [TaylorSeer-Wan2.1](https://raw.githubusercontent.com/Shenyi-Z/TaylorSeer/main/TaylorSeer-Wan2.1.md).\n\n## TaylorSeer-HiDream\n\nLe modèle de génération d'images récemment open source **HiDream**, malgré sa qualité de sortie impressionnante, fait face à des demandes croissantes d'accélération en raison de son temps d'inférence plus long. Nous avons appliqué **TaylorSeer** pour accélérer l'inférence de HiDream, réalisant une **réduction de 72 % du temps d'exécution**. Pour plus de détails, voir [TaylorSeer-HiDream](https://raw.githubusercontent.com/Shenyi-Z/TaylorSeer/main/TaylorSeer-HiDream.md).\n\n## 👍 Remerciements\n\n- Merci à [DiT](https://github.com/facebookresearch/DiT) pour leur excellent travail et leur base de code sur laquelle nous avons construit TaylorSeer-DiT.\n- Merci à [FLUX](https://github.com/black-forest-labs/flux) pour leur excellent travail et leur base de code sur laquelle nous avons construit TaylorSeer-FLUX.\n- Merci à [HiDream](https://github.com/HiDream-ai/HiDream-I1) pour leur excellent travail et leur base de code sur laquelle nous avons construit TaylorSeer-HiDream.\n- Merci à [HunyuanVideo](https://github.com/Tencent/HunyuanVideo) pour leur excellent travail et leur base de code sur laquelle nous avons construit TaylorSeer-HunyuanVideo.\n- Merci à [Wan2.1](https://github.com/Wan-Video/Wan2.1) pour leur excellent travail et leur base de code sur laquelle nous avons construit TaylorSeer-Wan2.1.\n- Merci à [ImageReward](https://github.com/THUDM/ImageReward) pour l'évaluation de la qualité texte-image.\n- Merci à [VBench](https://github.com/Vchitect/VBench) pour l'évaluation de la qualité texte-vidéo.\n\n\n## 📌 Citation\n\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 2,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## TaylorSeer-FLUX"
      },
      {
        "row": 3,
        "rowsha": "L2TzcgjTuvv3I11CQ/GvH9lYRpj+s4SJYD9K/E2y5xU=",
        "originContent": "## TaylorSeer-FLUX",
        "translatedContent": ""
      },
      {
        "row": 4,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "TaylorSeer a réalisé une compression computationnelle sans perte de 4,99 $\\times$ et une accélération de latence de 3,53 $\\times$ sur FLUX.1-dev, mesurées par [ImageReward](https://github.com/THUDM/ImageReward) pour une qualité globale. Pour exécuter TaylorSeer-FLUX, voir [TaylorSeer-FLUX](https://raw.githubusercontent.com/Shenyi-Z/TaylorSeer/main/TaylorSeer-FLUX.md)."
      },
      {
        "row": 5,
        "rowsha": "chkx6Gc6ouZXsRcw0q01pp6iUgffxNIeRz+rDYu9/8g=",
        "originContent": "TaylorSeer achieved a lossless computational compression of 4.99 $\\times$ and a Latency Speedup of 3.53 $\\times$ on FLUX.1-dev, as measured by [ImageReward](https://github.com/THUDM/ImageReward) for comprehensive quality. To run TaylorSeer-FLUX, see [TaylorSeer-FLUX](https://raw.githubusercontent.com/Shenyi-Z/TaylorSeer/main/TaylorSeer-FLUX.md).",
        "translatedContent": ""
      },
      {
        "row": 6,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "De plus, nous avons fourni des exemples de scripts d'inférence pour la **version diffusers**, ainsi que des scripts d'inférence **xDiT** en parallèle multi-GPU. Vous pouvez également réaliser des tests basés sur ceux-ci, situés respectivement à [TaylorSeers-Diffusers](./TaylorSeers-Diffusers ) et [TaylorSeers-xDiT](./TaylorSeers-xDiT)."
      },
      {
        "row": 7,
        "rowsha": "A6kLZqPZQw64dlrFOjpMypZsNMO14xJmlmpv3pw8ezs=",
        "originContent": "Besides, We have provided examples of inference scripts for the **diffusers version**, as well as multi-GPU parallel **xDiT inference scripts**. You can also conduct tests based on them, located at [TaylorSeers-Diffusers](./TaylorSeers-Diffusers ) and [TaylorSeers-xDiT](./TaylorSeers-xDiT) respectively.",
        "translatedContent": ""
      },
      {
        "row": 8,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## TaylorSeer-HunyuanVideo"
      },
      {
        "row": 9,
        "rowsha": "TvDelfZTkkxA47gKZAdSw/WDfVwOxAFEDF0XiTfHVe4=",
        "originContent": "## TaylorSeer-HunyuanVideo",
        "translatedContent": ""
      },
      {
        "row": 10,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "TaylorSeer a atteint une compression computationnelle de 5,00 $\\times$ et une remarquable accélération de latence de 4,65 $\\times$ sur HunyuanVideo, mesurées de manière exhaustive par la métrique [VBench](https://github.com/Vchitect/VBench). Par rapport aux méthodes précédentes, il a démontré des améliorations significatives tant en efficacité d'accélération qu'en qualité. Pour exécuter TaylorSeer-HunyuanVideo, voir [TaylorSeer-HunyuanVideo](https://raw.githubusercontent.com/Shenyi-Z/TaylorSeer/main/TaylorSeer-HunyuanVideo.md)."
      },
      {
        "row": 11,
        "rowsha": "dvznffegIKSG4P+ItV8gb99Z1QttpWlLRDzlqMddPoI=",
        "originContent": "TaylorSeer achieved a computational compression of 5.00 $\\times$ and a remarkable Latency Speedup of 4.65 $\\times$ on HunyuanVideo, as comprehensively measured by the [VBench](https://github.com/Vchitect/VBench) metric. Compared to previous methods, it demonstrated significant improvements in both acceleration efficiency and quality. To run TaylorSeer-HunyuanVideo, see [TaylorSeer-HunyuanVideo](https://raw.githubusercontent.com/Shenyi-Z/TaylorSeer/main/TaylorSeer-HunyuanVideo.md).",
        "translatedContent": ""
      },
      {
        "row": 12,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "De plus, nos scripts prennent également en charge l'accélération parallèle multi-GPU mise en œuvre par HunyuanVideo utilisant xDiT. Dans ce cas, l'effet d'accélération apporté par le cache et celui du parallélisme multi-GPU sont indépendants l'un de l'autre et se multiplient, atteignant des effets d'accélération extrêmement élevés."
      },
      {
        "row": 13,
        "rowsha": "Z3A/KHyeuxpEPv1W3z2nEo0Q6/Tl7PgTbK61vPvql+4=",
        "originContent": "In addition, our scripts also support multi-GPU parallel acceleration implemented by HunyuanVideo using xDiT. In this case, the acceleration effect brought by the cache and the acceleration effect of multi-GPU parallelism are independent of each other and multiply, achieving extremely high acceleration effects.",
        "translatedContent": ""
      },
      {
        "row": 14,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## TayorSeer-DiT"
      },
      {
        "row": 15,
        "rowsha": "7Y5OqYcgEa7ffvscRZL+2h7W//4Wv8MkKo/KciYC+Is=",
        "originContent": "## TayorSeer-DiT",
        "translatedContent": ""
      },
      {
        "row": 16,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "TaylorSeer a réalisé une compression computationnelle sans perte de 2,77 $\\times$ sur le modèle de base DiT, évaluée de manière exhaustive par des métriques telles que le FID. Ses performances à divers ratios d'accélération ont largement surpassé les méthodes précédentes. Par exemple, dans un scénario extrême avec un ratio de compression de 4,53 $\\times$, le FID de TaylorSeer n'a augmenté que de 0,33 par rapport à la référence non accélérée de 2,32, atteignant 2,65, tandis que ToCa et DuCa affichaient des scores FID supérieurs à 6,0 dans les mêmes conditions. Pour exécuter TaylorSeer-DiT, voir [TaylorSeer-DiT](https://raw.githubusercontent.com/Shenyi-Z/TaylorSeer/main/TaylorSeer-DiT.md)."
      },
      {
        "row": 17,
        "rowsha": "jKbN9crC/lW32o3a4odXocjXPpUX4BRy6Px6c5/OADI=",
        "originContent": "TaylorSeer achieved a lossless computational compression of 2.77 $\\times$ on the base model DiT, as comprehensively evaluated by metrics such as FID. Its performance across various acceleration ratios significantly surpassed previous methods. For instance, in an extreme scenario with a 4.53 $\\times$ compression ratio, TaylorSeer's FID only increased by 0.33 from the non-accelerated baseline of 2.32, reaching 2.65, while ToCa and DuCa exhibited FID scores above 6.0 under the same conditions. To run TaylorSeer-DiT,see [TaylorSeer-DiT](https://raw.githubusercontent.com/Shenyi-Z/TaylorSeer/main/TaylorSeer-DiT.md).",
        "translatedContent": ""
      },
      {
        "row": 18,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## TaylorSeer-Wan2.1"
      },
      {
        "row": 19,
        "rowsha": "5DYFJIi4H43keQh/zs8HrC/0OvovNgdNRAgI1iPeXz4=",
        "originContent": "## TaylorSeer-Wan2.1",
        "translatedContent": ""
      },
      {
        "row": 20,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "Nous avons implémenté la méthode d'accélération TaylorSeer sur Wan2.1, avec prise en charge de l'inférence parallèle multi-GPU. Les commandes d'installation et d'inférence pour TaylorSeer-Wan2.1 sont entièrement compatibles avec celles de Wan2.1. Pour exécuter TaylorSeer-Wan2.1, voir [TaylorSeer-Wan2.1](https://raw.githubusercontent.com/Shenyi-Z/TaylorSeer/main/TaylorSeer-Wan2.1.md)."
      },
      {
        "row": 21,
        "rowsha": "yw71nREC8h91wgq8ShsYXooL5pNuIYSukhKjsfMdofI=",
        "originContent": "We implemented the TaylorSeer acceleration method on Wan2.1, with support for multi-GPU parallel inference. The installation and inference commands for TaylorSeer-Wan2.1 are fully compatible with those of Wan2.1. To run TaylorSeer-Wan2.1, see [TaylorSeer-Wan2.1](https://raw.githubusercontent.com/Shenyi-Z/TaylorSeer/main/TaylorSeer-Wan2.1.md).",
        "translatedContent": ""
      },
      {
        "row": 22,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## TaylorSeer-HiDream"
      },
      {
        "row": 23,
        "rowsha": "S9sampwK6RNX9MCgwJoggioDY+17FhuB/kPyHb5UgxQ=",
        "originContent": "## TaylorSeer-HiDream",
        "translatedContent": ""
      },
      {
        "row": 24,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "Le modèle de génération d'images récemment open source **HiDream**, malgré sa qualité de sortie impressionnante, fait face à des demandes croissantes d'accélération en raison de son temps d'inférence plus long. Nous avons appliqué **TaylorSeer** pour accélérer l'inférence de HiDream, réalisant une **réduction de 72 % du temps d'exécution**. Pour plus de détails, voir [TaylorSeer-HiDream](https://raw.githubusercontent.com/Shenyi-Z/TaylorSeer/main/TaylorSeer-HiDream.md)."
      },
      {
        "row": 25,
        "rowsha": "NQ+OZmBUlgODN0R1c2zfq7AZz9G4P4M99o3RBv7xXRk=",
        "originContent": "The recently open-sourced image generation model **HiDream**, despite its impressive output quality, faces increasing demands for acceleration due to its longer inference time. We applied **TaylorSeer** to accelerate HiDream’s inference, achieving a **72% reduction in runtime**. For more details, see [TaylorSeer-HiDream](https://raw.githubusercontent.com/Shenyi-Z/TaylorSeer/main/TaylorSeer-HiDream.md).",
        "translatedContent": ""
      },
      {
        "row": 26,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## 👍 Remerciements"
      },
      {
        "row": 27,
        "rowsha": "u0Hxe1TnAcDLUJOZsTSfCbn/hsIWD27vYpN/fqVX6NY=",
        "originContent": "## 👍 Acknowledgements",
        "translatedContent": ""
      },
      {
        "row": 28,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "- Merci à [DiT](https://github.com/facebookresearch/DiT) pour leur excellent travail et leur base de code sur laquelle nous avons construit TaylorSeer-DiT."
      },
      {
        "row": 29,
        "rowsha": "CMyznQjDV0UsyA0kRFrsBu46YX9RCtwQPWaz6kDw57w=",
        "originContent": "- Thanks to [DiT](https://github.com/facebookresearch/DiT) for their great work and codebase upon which we build TaylorSeer-DiT.",
        "translatedContent": "- Merci à [FLUX](https://github.com/black-forest-labs/flux) pour leur excellent travail et leur base de code sur laquelle nous avons construit TaylorSeer-FLUX."
      },
      {
        "row": 30,
        "rowsha": "g/lnC2dBLYng5nlpIMvvux4+WxpFGTfff9SbzUWHVBQ=",
        "originContent": "- Thanks to [FLUX](https://github.com/black-forest-labs/flux) for their great work and codebase upon which we build TaylorSeer-FLUX.",
        "translatedContent": "- Merci à [HiDream](https://github.com/HiDream-ai/HiDream-I1) pour leur excellent travail et leur base de code sur laquelle nous avons construit TaylorSeer-HiDream."
      },
      {
        "row": 31,
        "rowsha": "yuga1q5FZmOSqw1xc0FAfu7Lu5HXOyeKTvtX/Iw2da0=",
        "originContent": "- Thanks to [HiDream](https://github.com/HiDream-ai/HiDream-I1) for their great work and codebase upon which we build TaylorSeer-HiDream.",
        "translatedContent": "- Merci à [HunyuanVideo](https://github.com/Tencent/HunyuanVideo) pour leur excellent travail et leur base de code sur laquelle nous avons construit TaylorSeer-HunyuanVideo."
      },
      {
        "row": 32,
        "rowsha": "FVkMzLsD/lLWDhQC+24S3sV+KYWrlNI6VWt9cXL8Wi4=",
        "originContent": "- Thanks to [HunyuanVideo](https://github.com/Tencent/HunyuanVideo) for their great work and codebase upon which we build TaylorSeer-HunyuanVideo.",
        "translatedContent": "- Merci à [Wan2.1](https://github.com/Wan-Video/Wan2.1) pour leur excellent travail et leur base de code sur laquelle nous avons construit TaylorSeer-Wan2.1."
      },
      {
        "row": 33,
        "rowsha": "Eyj54ONx3tEFKlo9ycLBA5oO5Nl/y1cGnSKuJJlbEoY=",
        "originContent": "- Thanks to [Wan2.1](https://github.com/Wan-Video/Wan2.1) for their great work and codebase upon which we build TaylorSeer-Wan2.1.",
        "translatedContent": "- Merci à [ImageReward](https://github.com/THUDM/ImageReward) pour l'évaluation de la qualité texte-image."
      },
      {
        "row": 34,
        "rowsha": "GjsWMErOgzFsSLuXdW1YI/Y3mLGiO0y5QoT5y8zAKqQ=",
        "originContent": "- Thanks to [ImageReward](https://github.com/THUDM/ImageReward) for Text-to-Image quality evaluation.",
        "translatedContent": "- Merci à [VBench](https://github.com/Vchitect/VBench) pour l'évaluation de la qualité texte-vidéo."
      },
      {
        "row": 35,
        "rowsha": "KlJSWNedKVvQL9mxodUIC4dZ1eBs/4O9za9u2hnJQp8=",
        "originContent": "- Thanks to [VBench](https://github.com/Vchitect/VBench) for Text-to-Video quality evaluation.",
        "translatedContent": ""
      },
      {
        "row": 36,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 37,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": "## 📌 Citation"
      },
      {
        "row": 38,
        "rowsha": "A4soJLbZDJ5jsV21yB+YcMlje6BkPWiyME67jma7Kxo=",
        "originContent": "## 📌 Citation",
        "translatedContent": ""
      },
      {
        "row": 39,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  },
  {
    "Id": 4,
    "Content": "```bibtex\n@article{TaylorSeer2025,\n  title={From Reusing to Forecasting: Accelerating Diffusion Models with TaylorSeers},\n  author={Liu, Jiacheng and Zou, Chang and Lyu, Yuanhuiyi and Chen, Junjie and Zhang, Linfeng},\n  journal={arXiv preprint arXiv:2503.06923},\n  year={2025}\n}\n```",
    "ContentSha": "fen2LVp21p8N9q/eae7nT9vcoFlYE3VN97IgBC75x9g=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "```bibtex\n@article{TaylorSeer2025,\n  title={From Reusing to Forecasting: Accelerating Diffusion Models with TaylorSeers},\n  author={Liu, Jiacheng and Zou, Chang and Lyu, Yuanhuiyi and Chen, Junjie and Zhang, Linfeng},\n  journal={arXiv preprint arXiv:2503.06923},\n  year={2025}\n}\n```",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "o+TmyQ6wneV6/FQB6aUlRSjIGr2/YLJtnz5uxBgsScQ=",
        "originContent": "```bibtex",
        "translatedContent": "```bibtex"
      },
      {
        "row": 2,
        "rowsha": "/ty4Z1XAwebEXbgzwWSaIIwNKg9g4LtzTihn1TP3w/o=",
        "originContent": "@article{TaylorSeer2025,",
        "translatedContent": "@article{TaylorSeer2025,"
      },
      {
        "row": 3,
        "rowsha": "o+GEpfSsb6+HkPZus7oKYmeYIa0sYGPChFIDFE7bKJY=",
        "originContent": "  title={From Reusing to Forecasting: Accelerating Diffusion Models with TaylorSeers},",
        "translatedContent": "  title={From Reusing to Forecasting: Accelerating Diffusion Models with TaylorSeers},"
      },
      {
        "row": 4,
        "rowsha": "9Nti1MS5YJh1jS6YOETdLSqimPafh+aTqXwIUL0aqvY=",
        "originContent": "  author={Liu, Jiacheng and Zou, Chang and Lyu, Yuanhuiyi and Chen, Junjie and Zhang, Linfeng},",
        "translatedContent": "  author={Liu, Jiacheng and Zou, Chang and Lyu, Yuanhuiyi and Chen, Junjie and Zhang, Linfeng},"
      },
      {
        "row": 5,
        "rowsha": "g/zic5erjOtHBZRc431Y3hZJDd8qGKo8zBOAc1c/nmA=",
        "originContent": "  journal={arXiv preprint arXiv:2503.06923},",
        "translatedContent": "  journal={arXiv preprint arXiv:2503.06923},"
      },
      {
        "row": 6,
        "rowsha": "6oyqJVJ20XoZhlyGjaewQyTQsSBBtVAYmqUtTwjLbAg=",
        "originContent": "  year={2025}",
        "translatedContent": "  year={2025}"
      },
      {
        "row": 7,
        "rowsha": "0Qs2qnSlm89KiBhYN/ZYr682Ru/yuxbDko0OkzXpRdI=",
        "originContent": "}",
        "translatedContent": "}"
      },
      {
        "row": 8,
        "rowsha": "8bkBhHOQsO1+N058HkZOwXtGpCfEh6WtbL0pBkBQg9U=",
        "originContent": "```",
        "translatedContent": "```"
      }
    ],
    "IsCodeBlock": true
  },
  {
    "Id": 5,
    "Content": "\n## :e-mail: Contact\n\nIf you have any questions, please email [`shenyizou@outlook.com`](https://raw.githubusercontent.com/Shenyi-Z/TaylorSeer/main/mailto:shenyizou@outlook.com).\n\n",
    "ContentSha": "71xBxPlelSoIdJqUiI+RvDt6idhGFPOYbo/OlJNyPAU=",
    "SectionType": "",
    "StartLine": 0,
    "EndLine": 0,
    "Translation": "\n## :e-mail: Contact\n\nSi vous avez des questions, veuillez envoyer un courriel à [`shenyizou@outlook.com`](https://raw.githubusercontent.com/Shenyi-Z/TaylorSeer/main/mailto:shenyizou@outlook.com).\n\n",
    "Status": "ok",
    "RowTranslations": [
      {
        "row": 1,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 2,
        "rowsha": "K5bAg1EjOpNN4wve36Z6v3sJb4XtaMFL5UG4PpfakXM=",
        "originContent": "## :e-mail: Contact",
        "translatedContent": "## :e-mail: Contact"
      },
      {
        "row": 3,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 4,
        "rowsha": "+O5l7O4UKOwm22BlC1HWUnYRp176sGuJ1pgHlKEIMY4=",
        "originContent": "If you have any questions, please email [`shenyizou@outlook.com`](https://raw.githubusercontent.com/Shenyi-Z/TaylorSeer/main/mailto:shenyizou@outlook.com).",
        "translatedContent": "Si vous avez des questions, veuillez envoyer un courriel à [`shenyizou@outlook.com`](https://raw.githubusercontent.com/Shenyi-Z/TaylorSeer/main/mailto:shenyizou@outlook.com)."
      },
      {
        "row": 5,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      },
      {
        "row": 6,
        "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
        "originContent": "",
        "translatedContent": ""
      }
    ],
    "IsCodeBlock": false
  }
]