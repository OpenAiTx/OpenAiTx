<div align=center>
  
# [ICCV 2025] *TaylorSeer*: De Reutilizar a Predecir: Acelerando Modelos de Difusi贸n con *TaylorSeers*

<p>
<a href='https://arxiv.org/abs/2503.06923'><img src='https://img.shields.io/badge/Paper-arXiv-red'></a>
<a href='https://taylorseer.github.io/TaylorSeer/'><img src='https://img.shields.io/badge/Project-Page-blue'></a>
</p>

</div>

##  Noticias

* `2025/06/26` ヰ 隆TaylorSeer tiene el honor de ser aceptado en ICCV 2025!

* `2025/05/03`  Se lanza TaylorSeer para HiDream.

* `2025/03/30`  Se lanza TaylorSeer para Wan2.1.

* `2025/03/30`  Los scripts de inferencia Diffusers para TaylorSeers y los scripts xDiT aplicables para inferencia paralela multi-GPU han sido lanzados oficialmente.

* `2025/03/10`  隆Nuestro trabajo m谩s reciente "De Reutilizar a Predecir: Acelerando Modelos de Difusi贸n con TaylorSeers" ha sido publicado! Los c贸digos est谩n disponibles en [TaylorSeer](https://github.com/Shenyi-Z/TaylorSeer)! TaylorSeer soporta compresi贸n sin p茅rdidas a una tasa de 4.99x en FLUX.1-dev (con una aceleraci贸n en latencia de 3.53x) y aceleraci贸n de alta calidad a una tasa de compresi贸n de 5.00x en HunyuanVideo (con una aceleraci贸n en latencia de 4.65x)! Esperamos que *TaylorSeer* pueda mover el paradigma de los m茅todos de cach茅 de caracter铆sticas de reutilizaci贸n a predicci贸n. Para m谩s detalles, por favor consulte nuestro art铆culo de investigaci贸n m谩s reciente.
* `2025/02/19`  La soluci贸n ToCa para **FLUX** ha sido oficialmente lanzada tras ajustes, logrando ahora hasta **3.14 aceleraci贸n sin p茅rdidas** (en FLOPs)!
* `2025/01/22` ヰ 隆ToCa tiene el honor de ser aceptado en ICLR 2025!
* `2024/12/29`  Publicamos nuestro trabajo [DuCa](https://arxiv.org/abs/2412.18911) sobre la aceleraci贸n de transformadores de difusi贸n GRATIS, que logra una aceleraci贸n casi sin p茅rdidas de **2.50** en [OpenSora](https://github.com/hpcaitech/Open-Sora)!  **DuCa tambi茅n supera la limitaci贸n de ToCa al soportar completamente FlashAttention, permitiendo una mayor compatibilidad y mejoras de eficiencia.**
* `2024/12/24`  Publicamos un repositorio de c贸digo abierto "[Awesome-Token-Reduction-for-Model-Compression](https://github.com/xuyang-liu16/Awesome-Token-Reduction-for-Model-Compression)", que recopila recientes trabajos sobresalientes sobre reducci贸n de tokens! 隆No dudes en contribuir con tus sugerencias!
* `2024/12/10` ヰ El trabajo reciente de nuestro equipo, **SiTo** (https://github.com/EvelynZhang-epiclab/SiTo), ha sido aceptado en **AAAI 2025**. Acelera modelos de difusi贸n mediante **Poda Adaptativa de Tokens**.
* `2024/07/15`  Publicamos un repositorio de c贸digo abierto "[Awesome-Generation-Acceleration](https://github.com/xuyang-liu16/Awesome-Generation-Acceleration)", que recopila recientes trabajos sobresalientes sobre aceleraci贸n de generaci贸n! 隆No dudes en contribuir con tus sugerencias!

<details>
  <summary><strong>Resumen</strong></summary>

  Los Transformadores de Difusi贸n (DiT) han revolucionado la s铆ntesis de im谩genes y videos de alta fidelidad, pero sus demandas computacionales siguen siendo prohibitivas para aplicaciones en tiempo real. Para resolver este problema, se ha propuesto el cach茅 de caracter铆sticas para acelerar modelos de difusi贸n almacenando las caracter铆sticas en pasos de tiempo anteriores y reutiliz谩ndolas en los siguientes. Sin embargo, en pasos de tiempo con intervalos significativos, la similitud de caracter铆sticas en modelos de difusi贸n disminuye sustancialmente, lo que conduce a un aumento pronunciado en los errores introducidos por el cach茅 de caracter铆sticas, da帽ando significativamente la calidad de la generaci贸n. Para solucionar este problema, proponemos TaylorSeer, que primero demuestra que las caracter铆sticas de los modelos de difusi贸n en pasos de tiempo futuros pueden predecirse bas谩ndose en sus valores en pasos de tiempo anteriores. Basado en el hecho de que las caracter铆sticas cambian lenta y continuamente a trav茅s de los pasos de tiempo, TaylorSeer emplea un m茅todo diferencial para aproximar las derivadas de orden superior de las caracter铆sticas y predecir las caracter铆sticas en pasos de tiempo futuros con la expansi贸n en series de Taylor. Experimentos extensos demuestran su efectividad significativa tanto en s铆ntesis de im谩genes como de video, especialmente en altas tasas de aceleraci贸n. Por ejemplo, logra una aceleraci贸n casi sin p茅rdidas de 4.99 $\times$ en FLUX y 5.00 $\times$ en HunyuanVideo sin entrenamiento adicional. En DiT, logra un FID $3.41$ menor en comparaci贸n con el estado del arte previo con una aceleraci贸n de $4.53$ $\times$.

</details>

## З Contribuciones de la Comunidad

隆Gracias a todos los colaboradores de c贸digo abierto por su fuerte apoyo! 隆Nos encantar铆a saber de ti!

* ComfyUI-TaylorSeer-philipy1219 (Inferencia FP8 en FLUX, m谩s modelos de video pr贸ximamente): [ComfyUI-TaylorSeer-philipy1219](https://github.com/philipy1219/ComfyUI-TaylorSeer) por [philipy1219](https://github.com/philipy1219).

##  Instalaci贸n

``` cmd
git clone https://github.com/Shenyi-Z/TaylorSeer.git
```

## TaylorSeer-FLUX

TaylorSeer logr贸 una compresi贸n computacional sin p茅rdidas de 4.99 $\times$ y una aceleraci贸n de latencia de 3.53 $\times$ en FLUX.1-dev, medida por [ImageReward](https://github.com/THUDM/ImageReward) para calidad integral. Para ejecutar TaylorSeer-FLUX, consulte [TaylorSeer-FLUX](https://raw.githubusercontent.com/Shenyi-Z/TaylorSeer/main/TaylorSeer-FLUX.md).

Adem谩s, hemos proporcionado ejemplos de scripts de inferencia para la **versi贸n diffusers**, as铆 como scripts de inferencia paralela multi-GPU **xDiT**. Tambi茅n puede realizar pruebas basadas en ellos, ubicados en [TaylorSeers-Diffusers](./TaylorSeers-Diffusers ) y [TaylorSeers-xDiT](./TaylorSeers-xDiT) respectivamente.

## TaylorSeer-HunyuanVideo

TaylorSeer logr贸 una compresi贸n computacional de 5.00 $\times$ y una notable aceleraci贸n de latencia de 4.65 $\times$ en HunyuanVideo, medida integralmente por la m茅trica [VBench](https://github.com/Vchitect/VBench). En comparaci贸n con m茅todos anteriores, demostr贸 mejoras significativas tanto en eficiencia de aceleraci贸n como en calidad. Para ejecutar TaylorSeer-HunyuanVideo, consulte [TaylorSeer-HunyuanVideo](https://raw.githubusercontent.com/Shenyi-Z/TaylorSeer/main/TaylorSeer-HunyuanVideo.md).

Adem谩s, nuestros scripts tambi茅n soportan aceleraci贸n paralela multi-GPU implementada por HunyuanVideo usando xDiT. En este caso, el efecto de aceleraci贸n aportado por la cach茅 y el efecto de aceleraci贸n del paralelismo multi-GPU son independientes entre s铆 y se multiplican, logrando efectos de aceleraci贸n extremadamente altos.

## TayorSeer-DiT

TaylorSeer logr贸 una compresi贸n computacional sin p茅rdidas de 2.77 $\times$ en el modelo base DiT, evaluado integralmente por m茅tricas como FID. Su rendimiento en varios ratios de aceleraci贸n super贸 significativamente a m茅todos anteriores. Por ejemplo, en un escenario extremo con una relaci贸n de compresi贸n de 4.53 $\times$, el FID de TaylorSeer solo aument贸 0.33 respecto a la l铆nea base sin acelerar de 2.32, alcanzando 2.65, mientras que ToCa y DuCa exhibieron puntuaciones FID superiores a 6.0 bajo las mismas condiciones. Para ejecutar TaylorSeer-DiT, consulte [TaylorSeer-DiT](https://raw.githubusercontent.com/Shenyi-Z/TaylorSeer/main/TaylorSeer-DiT.md).

## TaylorSeer-Wan2.1

Implementamos el m茅todo de aceleraci贸n TaylorSeer en Wan2.1, con soporte para inferencia paralela multi-GPU. Los comandos de instalaci贸n e inferencia para TaylorSeer-Wan2.1 son totalmente compatibles con los de Wan2.1. Para ejecutar TaylorSeer-Wan2.1, consulte [TaylorSeer-Wan2.1](https://raw.githubusercontent.com/Shenyi-Z/TaylorSeer/main/TaylorSeer-Wan2.1.md).

## TaylorSeer-HiDream

El modelo de generaci贸n de im谩genes recientemente abierto **HiDream**, a pesar de su impresionante calidad de salida, enfrenta demandas crecientes de aceleraci贸n debido a su mayor tiempo de inferencia. Aplicamos **TaylorSeer** para acelerar la inferencia de HiDream, logrando una **reducci贸n del 72% en el tiempo de ejecuci贸n**. Para m谩s detalles, consulte [TaylorSeer-HiDream](https://raw.githubusercontent.com/Shenyi-Z/TaylorSeer/main/TaylorSeer-HiDream.md).

##  Agradecimientos

- Gracias a [DiT](https://github.com/facebookresearch/DiT) por su gran trabajo y base de c贸digo sobre la cual construimos TaylorSeer-DiT.
- Gracias a [FLUX](https://github.com/black-forest-labs/flux) por su gran trabajo y base de c贸digo sobre la cual construimos TaylorSeer-FLUX.
- Gracias a [HiDream](https://github.com/HiDream-ai/HiDream-I1) por su gran trabajo y base de c贸digo sobre la cual construimos TaylorSeer-HiDream.
- Gracias a [HunyuanVideo](https://github.com/Tencent/HunyuanVideo) por su gran trabajo y base de c贸digo sobre la cual construimos TaylorSeer-HunyuanVideo.
- Gracias a [Wan2.1](https://github.com/Wan-Video/Wan2.1) por su gran trabajo y base de c贸digo sobre la cual construimos TaylorSeer-Wan2.1.
- Gracias a [ImageReward](https://github.com/THUDM/ImageReward) por la evaluaci贸n de calidad Texto-a-Imagen.
- Gracias a [VBench](https://github.com/Vchitect/VBench) por la evaluaci贸n de calidad Texto-a-Video.


##  Citaci贸n


```bibtex
@article{TaylorSeer2025,
  title={From Reusing to Forecasting: Accelerating Diffusion Models with TaylorSeers},
  author={Liu, Jiacheng and Zou, Chang and Lyu, Yuanhuiyi and Chen, Junjie and Zhang, Linfeng},
  journal={arXiv preprint arXiv:2503.06923},
  year={2025}
}
```

## :e-mail: Contacto

Si tiene alguna pregunta, por favor env铆e un correo electr贸nico a [`shenyizou@outlook.com`](https://raw.githubusercontent.com/Shenyi-Z/TaylorSeer/main/mailto:shenyizou@outlook.com).



---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-29

---