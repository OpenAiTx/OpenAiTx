<translate-content># GraspVLA: 십억 규모 합성 행동 데이터로 사전학습된 그랩핑 기초 모델
[![arXiv](https://img.shields.io/badge/arXiv-2505.03233-df2a2a.svg)](https://arxiv.org/pdf/2505.03233)
[![Static Badge](https://img.shields.io/badge/Project-Page-a)](https://pku-epic.github.io/GraspVLA-web/)

<!-- [Shengliang Deng](https://shengliangd.github.io/about/), [Mi Yan](https://miyandoris.github.io/), [Songlin Wei](https://songlin.github.io/), Haixin Ma, Yuxin Yang, [Jiayi Chen](https://jychen18.github.io/), Zhiqi Zhang, Taoyu Yang, Xuheng Zhang, [Heming Cui](https://i.cs.hku.hk/~heming/), [Zhizheng Zhang](https://scholar.google.com/citations?user=X7M0I8kAAAAJ&hl=en), [He Wang](https://hughw19.github.io/) -->

우리는 합성 데이터만을 사용하여 비용 효율적인 VLA 모델 사전학습 방식을 제안하며, 직접적인 시뮬레이션-실제 전이와 로봇 그랩핑의 강력한 제로샷 일반화를 달성합니다. 주요 기여는 다음과 같습니다:

- **SynGrasp-1B**: 240개 객체 카테고리와 10,000개 이상의 객체를 포함하는 십억 프레임 규모의 합성 그랩핑 데이터셋.

- **GraspVLA**: SynGrasp-1B로 사전학습된 VLA 모델로, 미세조정 없이 실제 세계 그랩핑에 대한 제로샷 일반화를 달성.

- **통합 CoT 프레임워크**: GraspVLA는 자기회귀적 인지와 흐름 매칭 기반 행동 생성을 단일 추론 프로세스로 통합하여, 합성 행동 데이터와 인터넷 규모의 의미 데이터에 대한 공동 학습을 가능하게 하여 개방형 어휘 그랩핑을 지원.

![teaser](https://raw.githubusercontent.com/PKU-EPIC/GraspVLA/main/./figs/teaser.jpg)

TODO 목록:
- [ ] 보조 자료 공개
- [ ] 모델 가중치 공개
- [ ] SynGrasp-1B 데이터셋 공개

[![License](https://licensebuttons.net/l/by-nc/4.0/88x31.png)](LICENSE)</translate-content>

---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2025-07-15

---