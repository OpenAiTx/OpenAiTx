[
  {
    "row": 1,
    "rowsha": "P2qYbvTYlOMcUIHHajG/ywqP7sc2pfoswunCw0cOgDk=",
    "originContent": "# GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data",
    "translatedContent": "# GraspVLA: 10億規模の合成アクションデータで事前学習された把持基盤モデル"
  },
  {
    "row": 2,
    "rowsha": "h8eG3N7VZU4WXpl+3X9UWoszREusoGUz+m3dflWVQnc=",
    "originContent": "[![arXiv](https://img.shields.io/badge/arXiv-2505.03233-df2a2a.svg)](https://arxiv.org/pdf/2505.03233)",
    "translatedContent": "[![arXiv](https://img.shields.io/badge/arXiv-2505.03233-df2a2a.svg)](https://arxiv.org/pdf/2505.03233)"
  },
  {
    "row": 3,
    "rowsha": "GcD/1abS+gFx3rpaS5Hufm7AB1D/FPKw5UX40hdDPnE=",
    "originContent": "[![Static Badge](https://img.shields.io/badge/Project-Page-a)](https://pku-epic.github.io/GraspVLA-web/)",
    "translatedContent": "[![Static Badge](https://img.shields.io/badge/Project-Page-a)](https://pku-epic.github.io/GraspVLA-web/)"
  },
  {
    "row": 4,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 5,
    "rowsha": "hfznSf6Ww2BjLioZmtV5J0z8no62zOq+aBPVnNHVwTU=",
    "originContent": "<!-- [Shengliang Deng](https://shengliangd.github.io/about/), [Mi Yan](https://miyandoris.github.io/), [Songlin Wei](https://songlin.github.io/), Haixin Ma, Yuxin Yang, [Jiayi Chen](https://jychen18.github.io/), Zhiqi Zhang, Taoyu Yang, Xuheng Zhang, [Heming Cui](https://i.cs.hku.hk/~heming/), [Zhizheng Zhang](https://scholar.google.com/citations?user=X7M0I8kAAAAJ&hl=en), [He Wang](https://hughw19.github.io/) -->",
    "translatedContent": "<!-- [Shengliang Deng](https://shengliangd.github.io/about/), [Mi Yan](https://miyandoris.github.io/), [Songlin Wei](https://songlin.github.io/), Haixin Ma, Yuxin Yang, [Jiayi Chen](https://jychen18.github.io/), Zhiqi Zhang, Taoyu Yang, Xuheng Zhang, [Heming Cui](https://i.cs.hku.hk/~heming/), [Zhizheng Zhang](https://scholar.google.com/citations?user=X7M0I8kAAAAJ&hl=en), [He Wang](https://hughw19.github.io/) -->"
  },
  {
    "row": 6,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 7,
    "rowsha": "ClMqPTgEteI2M7nVgTu2B1FbmfK1UpbTC9BmdKYR6ck=",
    "originContent": "We present a cost-effective pretraining paradigm for VLA models using only synthetic data, achieving direct sim-to-real transfer and strong zero-shot generalizability for robotic grasping. Key contributions include:",
    "translatedContent": "本研究では、合成データのみを用いたコスト効率の高いVLAモデルの事前学習パラダイムを提案し、直接的なシムツーリアル転送とロボット把持の強力なゼロショット汎化を実現します。主な貢献は以下の通りです："
  },
  {
    "row": 8,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 9,
    "rowsha": "NcdmBdxAC9KQsH/26cH7JJARoV/2Y3LsN8F4EDxzigo=",
    "originContent": "- **SynGrasp-1B**: a billion-frame synthetic grasping dataset, spanning 240 object categories and 10,000+ objects.",
    "translatedContent": "- **SynGrasp-1B**：24万カテゴリ、10,000以上のオブジェクトを網羅する10億フレームの合成把持データセット。"
  },
  {
    "row": 10,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 11,
    "rowsha": "9KrSo4Gh6Jys4IV5chHJ1c6wFWnk16seAS1el8D1aZQ=",
    "originContent": "- **GraspVLA**: a VLA model pretrained on SynGrasp-1B that achieves zero-shot generalization to real-world grasping without fine-tuning.",
    "translatedContent": "- **GraspVLA**：SynGrasp-1Bで事前学習されたVLAモデルで、ファインチューニングなしに実世界の把持へゼロショット汎化を達成。"
  },
  {
    "row": 12,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 13,
    "rowsha": "w9khj90UNtKTZeUYe6C1qHRBPQK1kjUuREyzuZDZq2o=",
    "originContent": "- **Unified CoT Framework**: GraspVLA integrates autoregressive perception and flow-matching-based action generation into a single reasoning process, enabling joint training on synthetic action data and internet-scale semantic data for open-vocabulary grasping.",
    "translatedContent": "- **統一CoTフレームワーク**：GraspVLAは自己回帰的知覚とフローマッチングに基づくアクション生成を単一の推論プロセスに統合し、合成アクションデータとインターネット規模の意味データの共同学習を可能にし、オープンボキャブラリー把持を実現。"
  },
  {
    "row": 14,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 15,
    "rowsha": "PSlyJ1uERmjdSspZQM/psov1Ox9kzll/4gzOCDWSlWw=",
    "originContent": "![teaser](https://raw.githubusercontent.com/PKU-EPIC/GraspVLA/main/./figs/teaser.jpg)",
    "translatedContent": "![teaser](https://raw.githubusercontent.com/PKU-EPIC/GraspVLA/main/./figs/teaser.jpg)"
  },
  {
    "row": 16,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 17,
    "rowsha": "N8D+AZ+ph8V6ZHtw9iQvghn7dtVTmVJtqRrrGnKN3ZQ=",
    "originContent": "TODO List:",
    "translatedContent": "TODO リスト:"
  },
  {
    "row": 18,
    "rowsha": "2wWHDqA1PfFoe/2tY1XOTJSk8dKe2cvJx5o6PFwklbo=",
    "originContent": "- [ ] Release the supplementary material",
    "translatedContent": "- [ ] 補足資料の公開"
  },
  {
    "row": 19,
    "rowsha": "rdp/AQ0Xk1WtzytMctvXuuiIheBD6BmZYTuEuf878Nc=",
    "originContent": "- [ ] Release model weights",
    "translatedContent": "- [ ] モデル重みの公開"
  },
  {
    "row": 20,
    "rowsha": "9W+5jQrgevd3flHYenpyQgSMgMMZSFItwYPK70kM0gI=",
    "originContent": "- [ ] Release SynGrasp-1B dataset",
    "translatedContent": "- [ ] SynGrasp-1Bデータセットの公開"
  },
  {
    "row": 21,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 22,
    "rowsha": "0UDozCHddBF6FlFGK68EMjiHStyWkEXXyWYWPOXXA8A=",
    "originContent": "[![License](https://licensebuttons.net/l/by-nc/4.0/88x31.png)](LICENSE)",
    "translatedContent": "[![License](https://licensebuttons.net/l/by-nc/4.0/88x31.png)](LICENSE)"
  }
]