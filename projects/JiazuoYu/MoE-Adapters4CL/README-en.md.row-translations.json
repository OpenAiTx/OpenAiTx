[
  {
    "row": 1,
    "rowsha": "LU1MnFdz3C5PxIJX/+pOYgXAj/IvqsefF+GKtYOpcVQ=",
    "originContent": "# MoE-Adapters4CL",
    "translatedContent": "# MoE-Adapters4CL"
  },
  {
    "row": 2,
    "rowsha": "344NH8SFBGTtLWlE2U4p0oCC2e1+hr0vZvM3+cEp+lk=",
    "originContent": "Code for paper \"[**Boosting Continual Learning of Vision-Language Models via Mixture-of-Experts Adapters**](https://arxiv.org/abs/2403.11549)\" CVPR2024.",
    "translatedContent": "Code for paper \"[**Boosting Continual Learning of Vision-Language Models via Mixture-of-Experts Adapters**](https://arxiv.org/abs/2403.11549)\" CVPR2024."
  },
  {
    "row": 3,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 4,
    "rowsha": "j2m8/Rf4OhNBtAZdHSFuUFBE6NBtExCFuaofstzruU0=",
    "originContent": "## Table of Contents",
    "translatedContent": "## Table of Contents"
  },
  {
    "row": 5,
    "rowsha": "rODNKqAtBMTk8cGA6mqf42v0FKEpKwEG7dLuPR4ktT0=",
    "originContent": "  - [Abstract](#Abstract)",
    "translatedContent": "  - [Abstract](#Abstract)"
  },
  {
    "row": 6,
    "rowsha": "OY9K2b0/Vc0QQZVUQ4ujcaRRrQyAef1Y0e1K4ixIevg=",
    "originContent": "  - [Approach](#Approach)",
    "translatedContent": "  - [Approach](#Approach)"
  },
  {
    "row": 7,
    "rowsha": "oPAuO/yx5Ua2LRC3x37Z9WRvBBfQBgjh8F4Pk8fKwRM=",
    "originContent": "  - [Install](#Install)",
    "translatedContent": "  - [Install](#Install)"
  },
  {
    "row": 8,
    "rowsha": "ADUK98MIbpd/YpK0MdAZmXMT2Pf6UhErPYOYZ/gvm6w=",
    "originContent": "  - [Data preparation](#Data-preparation)",
    "translatedContent": "  - [Data preparation](#Data-preparation)"
  },
  {
    "row": 9,
    "rowsha": "cF2fkHtTIxk3MdZe+Z+FZOnTcs1rxL5PoK5Jhe3GPeA=",
    "originContent": "  - [Getting Started](#getting-started)",
    "translatedContent": "  - [Getting Started](#getting-started)"
  },
  {
    "row": 10,
    "rowsha": "bCM/A4xAMokMpscnveBvj5sg7DkpIhI3wqqHAkE1Myc=",
    "originContent": "    - [Model ckpt](#Model-ckpt)",
    "translatedContent": "    - [Model ckpt](#Model-ckpt)"
  },
  {
    "row": 11,
    "rowsha": "U3SxSvPrJ37wXvZD3J4VPTXVttn5o9OgR5Y/pcTT1/A=",
    "originContent": "    - [MTCL](#MTCL)",
    "translatedContent": "    - [MTCL](#MTCL)"
  },
  {
    "row": 12,
    "rowsha": "GxKd1M2hSeBq4ujHfdvnLiU4kiV2bG5QbyKPebjgc3o=",
    "originContent": "      - [Test](#Test)",
    "translatedContent": "      - [Test](#Test)"
  },
  {
    "row": 13,
    "rowsha": "By9ikcMP4AkVYOraEQye+li8jqxlvg5qMm7tzHSQ4Ow=",
    "originContent": "      - [Train](#Train)",
    "translatedContent": "      - [Train](#Train)"
  },
  {
    "row": 14,
    "rowsha": "PNY2JklLhvmjaUKMIr9U7pgGlii6V3n61tgCw/2FnC4=",
    "originContent": "    - [CIL](#CIL)",
    "translatedContent": "    - [CIL](#CIL)"
  },
  {
    "row": 15,
    "rowsha": "By9ikcMP4AkVYOraEQye+li8jqxlvg5qMm7tzHSQ4Ow=",
    "originContent": "      - [Train](#Train)",
    "translatedContent": "      - [Train](#Train)"
  },
  {
    "row": 16,
    "rowsha": "iUnj/JkaM12BvKU9bnE9A5rarzK2ixs945nWfXaN3/8=",
    "originContent": "  - [Citation](#Citation)",
    "translatedContent": "  - [Citation](#Citation)"
  },
  {
    "row": 17,
    "rowsha": "GdzZVx+zlLphNCBpZgOwIlPj46apyxLOWDOezYLx3D8=",
    "originContent": "  - [Acknowledgement](#Acknowledgement)",
    "translatedContent": "  - [Acknowledgement](#Acknowledgement)"
  },
  {
    "row": 18,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 19,
    "rowsha": "JmwhuBCuZXUfpw84ioZ+lNGA55/RXGntVNrn3x0pFms=",
    "originContent": "## Abstract",
    "translatedContent": "## Abstract"
  },
  {
    "row": 20,
    "rowsha": "CuRZZhgAxuJtTXFL8+kSYkaVC/S6I+VQV32nLjmbbuU=",
    "originContent": "Continual learning can empower vision-language models to continuously acquire new knowledge, without the need for access to the entire historical dataset. However, mitigating the performance degradation in large-scale models is non-trivial due to (i) parameter shifts throughout lifelong learning and (ii) significant computational burdens associated with full-model tuning. In this work, we present a parameter-efficient continual learning framework to alleviate long-term forgetting in incremental learning with vision-language models. Our approach involves the dynamic expansion of a pre-trained CLIP model, through the integration of Mixture-of-Experts (MoE) adapters in response to new tasks. To preserve the zero-shot recognition capability of vision-language models, we further introduce a Distribution Discriminative Auto-Selector (DDAS) that automatically routes in-distribution and out-of-distribution inputs to the MoE Adapter and the original CLIP, respectively. Through extensive experiments across various settings, our proposed method consistently outperforms previous state-of-the-art approaches while concurrently reducing parameter training burdens by 60%. ",
    "translatedContent": "Continual learning can empower vision-language models to continuously acquire new knowledge, without the need for access to the entire historical dataset. However, mitigating the performance degradation in large-scale models is non-trivial due to (i) parameter shifts throughout lifelong learning and (ii) significant computational burdens associated with full-model tuning. In this work, we present a parameter-efficient continual learning framework to alleviate long-term forgetting in incremental learning with vision-language models. Our approach involves the dynamic expansion of a pre-trained CLIP model, through the integration of Mixture-of-Experts (MoE) adapters in response to new tasks. To preserve the zero-shot recognition capability of vision-language models, we further introduce a Distribution Discriminative Auto-Selector (DDAS) that automatically routes in-distribution and out-of-distribution inputs to the MoE Adapter and the original CLIP, respectively. Through extensive experiments across various settings, our proposed method consistently outperforms previous state-of-the-art approaches while concurrently reducing parameter training burdens by 60%. "
  },
  {
    "row": 21,
    "rowsha": "y7iGcK8tXnldKR8pIUqbT83v9P/krrFgTgt5JxZDjHI=",
    "originContent": "## Approach",
    "translatedContent": "## Approach"
  },
  {
    "row": 22,
    "rowsha": "vaJRVQvwR49BcqiuspHWVD/qCQFQuCw+KIGlNyHq+ZA=",
    "originContent": "___",
    "translatedContent": "___"
  },
  {
    "row": 23,
    "rowsha": "htfHNU9EXC6uugBBeJznAowizFSp/+YzyrJjzPVtx8M=",
    "originContent": "![example image](https://raw.githubusercontent.com/JiazuoYu/MoE-Adapters4CL/MoE-Adapters/fig/framework.png)",
    "translatedContent": "![example image](https://raw.githubusercontent.com/JiazuoYu/MoE-Adapters4CL/MoE-Adapters/fig/framework.png)"
  },
  {
    "row": 24,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 25,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 26,
    "rowsha": "mDBLgkO9nyYfW6eXpRm1Z9h+QbQo1yz1PTdPg+Z5UOE=",
    "originContent": "## Install",
    "translatedContent": "## Install"
  },
  {
    "row": 27,
    "rowsha": "rTMjNc+qNIxb1xXDR5hEYiJw1fc4muBwuBsTXVIaIho=",
    "originContent": "```bash",
    "translatedContent": "```bash"
  },
  {
    "row": 28,
    "rowsha": "GAKfBMW8K0TQ4E+370aZ+faSY1IhRw9leCL+fdU8ZJI=",
    "originContent": "conda create -n MoE_Adapters4CL python=3.9",
    "translatedContent": "conda create -n MoE_Adapters4CL python=3.9"
  },
  {
    "row": 29,
    "rowsha": "uuXU66VJu3zH/zlrlPGnUVOOV7cdi74t1EtA1B3NvZU=",
    "originContent": "conda activate MoE_Adapters4CL",
    "translatedContent": "conda activate MoE_Adapters4CL"
  },
  {
    "row": 30,
    "rowsha": "kJ1D+crN4XjmYEnqRDMdOJy9q+pH9Nx3P88WpHk14Cg=",
    "originContent": "conda install pytorch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 pytorch-cuda=11.8 -c pytorch -c nvidia",
    "translatedContent": "conda install pytorch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 pytorch-cuda=11.8 -c pytorch -c nvidia"
  },
  {
    "row": 31,
    "rowsha": "Y8UxdLKIJGrE1zMruCPsMtMGmyF0KseQdjMuphQDp+o=",
    "originContent": "cd cil",
    "translatedContent": "cd cil"
  },
  {
    "row": 32,
    "rowsha": "9jQ5Tmvmy0Rca8gZGuieLw3iHyIU3Ba5zS4ICtZgsdw=",
    "originContent": "pip install -r requirements.txt",
    "translatedContent": "pip install -r requirements.txt"
  },
  {
    "row": 33,
    "rowsha": "8bkBhHOQsO1+N058HkZOwXtGpCfEh6WtbL0pBkBQg9U=",
    "originContent": "```",
    "translatedContent": "```"
  },
  {
    "row": 34,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 35,
    "rowsha": "r826vq6z75JctjfbnbfEtftksR7Fz4pNMNYhhi5wZ3g=",
    "originContent": "## Data preparation",
    "translatedContent": "## Data preparation"
  },
  {
    "row": 36,
    "rowsha": "z75T5OOBGbsmfBL8UbBi3rkSN6Wa55ajPXbKmy1+ZBY=",
    "originContent": "Target Datasets: Aircraft, Caltech101,CIFAR10, CIFAR100, DTD, EuroSAT, Flowers, Food, MNIST, OxfordPet,StanfordCars, SUN397, TinyImagenet.",
    "translatedContent": "Target Datasets: Aircraft, Caltech101, CIFAR10, CIFAR100, DTD, EuroSAT, Flowers, Food, MNIST, OxfordPet, StanfordCars, SUN397, TinyImagenet."
  },
  {
    "row": 37,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 38,
    "rowsha": "wL+mqZPZ9SN2c5gLtgVbSMNgOEY3HeQu1vgw4d4CNGU=",
    "originContent": "If you have problems with Caltech101, you can refer to [issue#6](https://github.com/JiazuoYu/MoE-Adapters4CL/issues/6).",
    "translatedContent": "If you have problems with Caltech101, you can refer to [issue#6](https://github.com/JiazuoYu/MoE-Adapters4CL/issues/6)."
  },
  {
    "row": 39,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 40,
    "rowsha": "cTY3+D3PhZGkVnYrczj3vkSXNyVFHILmVRYIrnaRTno=",
    "originContent": "More details can refer to [datasets.md](https://raw.githubusercontent.com/JiazuoYu/MoE-Adapters4CL/MoE-Adapters/mtil%2Fdatasets.md) of [ZSCL](https://github.com/Thunderbeee/ZSCL). Big thanks to them for their awesome work!",
    "translatedContent": "More details can refer to [datasets.md](https://raw.githubusercontent.com/JiazuoYu/MoE-Adapters4CL/MoE-Adapters/mtil%2Fdatasets.md) of [ZSCL](https://github.com/Thunderbeee/ZSCL). Big thanks to them for their awesome work!"
  },
  {
    "row": 41,
    "rowsha": "0TwZzQU5KBsIddl3mCHQCtcRtR3hz8W3lc24Stk1euQ=",
    "originContent": "## Model ckpt",
    "translatedContent": "## Model ckpt"
  },
  {
    "row": 42,
    "rowsha": "SaekvDtBt/wpr328fjKYMssZ8oKYSbEjOaBZg348EEU=",
    "originContent": "|                  | Model                                                                | Link |",
    "translatedContent": "|                  | Model                                                                | Link |"
  },
  {
    "row": 43,
    "rowsha": "f+gGTac5FdsKjGtn2jCgBafqmCViO8iA9bK+FQcZ/kM=",
    "originContent": "|------------------|----------------------------------------------------------------------|---------------------------------------------------------------------- |",
    "translatedContent": "|------------------|----------------------------------------------------------------------|---------------------------------------------------------------------- |"
  },
  {
    "row": 44,
    "rowsha": "RjklwrZtF+VExgxqKVFQapmWUOTHDTHYPf56w9t5ExA=",
    "originContent": "| full_shot_order1 | full_shot_order1_1000iters.pth                  | [Baidu Disk](https://pan.baidu.com/s/1brWYIMrv34fhdc4kC9B0_g?pwd=p3zp) / [Google Drive](https://drive.google.com/drive/folders/1f2GB2kmBYoxzWI9E33XqPnkIKrAB2fh9?usp=sharing)      |",
    "translatedContent": "| full_shot_order1 | full_shot_order1_1000iters.pth                  | [Baidu Disk](https://pan.baidu.com/s/1brWYIMrv34fhdc4kC9B0_g?pwd=p3zp) / [Google Drive](https://drive.google.com/drive/folders/1f2GB2kmBYoxzWI9E33XqPnkIKrAB2fh9?usp=sharing)      |"
  },
  {
    "row": 45,
    "rowsha": "Kh/VwsuCFQQDyFCvOLI4V7fCKMM9xECXN8fG5gIQ+CA=",
    "originContent": "| few_shot_order1  | few_shot_order1_1000iters.pth | [Baidu Disk](https://pan.baidu.com/s/1Z7q3tTLdRFN3zmtkj3_i4g?pwd=4edw) / [Google Drive](https://drive.google.com/drive/folders/1f2GB2kmBYoxzWI9E33XqPnkIKrAB2fh9?usp=sharing)       |",
    "translatedContent": "| few_shot_order1  | few_shot_order1_1000iters.pth | [Baidu Disk](https://pan.baidu.com/s/1Z7q3tTLdRFN3zmtkj3_i4g?pwd=4edw) / [Google Drive](https://drive.google.com/drive/folders/1f2GB2kmBYoxzWI9E33XqPnkIKrAB2fh9?usp=sharing)       |"
  },
  {
    "row": 46,
    "rowsha": "E+CrMjBT/g1xHYtptRRqIVRD90oeZAW3VvrkrB7fxok=",
    "originContent": "## MTCL",
    "translatedContent": "## MTCL"
  },
  {
    "row": 47,
    "rowsha": "k9XrtnF+aM4LH57QFOj3agZa6Kyq48DpQpjFnTVaHOA=",
    "originContent": "### Test stage",
    "translatedContent": "### Test stage"
  },
  {
    "row": 48,
    "rowsha": "hyiH5WPnWVf/wgsCEzJQTy3dCo85ZMuTBwhjv68Tza0=",
    "originContent": "Example:",
    "translatedContent": "Example:"
  },
  {
    "row": 49,
    "rowsha": "yAImN7jsPpGYOloIjXOOaqHWCqGMjeYyJrHvvqwPvIw=",
    "originContent": "1. Move the checkpoints to MoE-Adapters4CL/ckpt",
    "translatedContent": "1. Move the checkpoints to MoE-Adapters4CL/ckpt"
  },
  {
    "row": 50,
    "rowsha": "BEcMM8I5jrBxjpmZco7hW7vgE0MnoSf2qfxrTW0IBxQ=",
    "originContent": "2. ```cd MoE-Adapters4CL/mtil```",
    "translatedContent": "2. ```cd MoE-Adapters4CL/mtil```"
  },
  {
    "row": 51,
    "rowsha": "SSpdvzM6J8P/0j/0l2zrQAlroHRt1BUqwHc7QP65GDg=",
    "originContent": "3. Run the script ```bash srcipts/test/Full_Shot_order1.sh ```",
    "translatedContent": "3. Run the script ```bash srcipts/test/Full_Shot_order1.sh ```"
  },
  {
    "row": 52,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 53,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 54,
    "rowsha": "1IM1Qbhr0xSaeNaVclWZWv+/b55ywxTDLFlG3xgaQ5c=",
    "originContent": "### Train stage",
    "translatedContent": "### Train stage"
  },
  {
    "row": 55,
    "rowsha": "hyiH5WPnWVf/wgsCEzJQTy3dCo85ZMuTBwhjv68Tza0=",
    "originContent": "Example:",
    "translatedContent": "Example:"
  },
  {
    "row": 56,
    "rowsha": "yAImN7jsPpGYOloIjXOOaqHWCqGMjeYyJrHvvqwPvIw=",
    "originContent": "1. Move the checkpoints to MoE-Adapters4CL/ckpt",
    "translatedContent": "1. Move the checkpoints to MoE-Adapters4CL/ckpt"
  },
  {
    "row": 57,
    "rowsha": "BEcMM8I5jrBxjpmZco7hW7vgE0MnoSf2qfxrTW0IBxQ=",
    "originContent": "2. ```cd MoE-Adapters4CL/mtil```",
    "translatedContent": "2. ```cd MoE-Adapters4CL/mtil```"
  },
  {
    "row": 58,
    "rowsha": "8DWrPTqlqO5pcOVt5c/Jz/3/K/S5jbTVfh301iftD9U=",
    "originContent": "3. Run the script ```bash srcipts/train/train_full_shot_router11_experts22_1000iters.sh```",
    "translatedContent": "3. Run the script ```bash srcipts/train/train_full_shot_router11_experts22_1000iters.sh```"
  },
  {
    "row": 59,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 60,
    "rowsha": "fyUJBOmdKmQgYGfhBm2zQf9Xu1zvjMbk0Mjr761lCXk=",
    "originContent": "## Class Incremental Learning",
    "translatedContent": "## Class Incremental Learning"
  },
  {
    "row": 61,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 62,
    "rowsha": "1IM1Qbhr0xSaeNaVclWZWv+/b55ywxTDLFlG3xgaQ5c=",
    "originContent": "### Train stage",
    "translatedContent": "### Train stage"
  },
  {
    "row": 63,
    "rowsha": "hyiH5WPnWVf/wgsCEzJQTy3dCo85ZMuTBwhjv68Tza0=",
    "originContent": "Example:",
    "translatedContent": "Example:"
  },
  {
    "row": 64,
    "rowsha": "gwuUvslrqATKPgIzIJSxv/e9cb7XL8yNVKE0lI2ov8s=",
    "originContent": "1. ```cd cil```",
    "translatedContent": "1. ```cd cil```"
  },
  {
    "row": 65,
    "rowsha": "E+TUrykwGvF6czaIgn7+gQYR1Z03eJUpiXhgaKh0QOY=",
    "originContent": "2. ```bash run_cifar100-2-2.sh ```",
    "translatedContent": "2. ```bash run_cifar100-2-2.sh ```"
  },
  {
    "row": 66,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 67,
    "rowsha": "ZwTp5ajUmpHTJefyHhIKzXcG2wnB1jv8iv8cvmdcb/g=",
    "originContent": "## Citation",
    "translatedContent": "## Citation"
  },
  {
    "row": 68,
    "rowsha": "8bkBhHOQsO1+N058HkZOwXtGpCfEh6WtbL0pBkBQg9U=",
    "originContent": "```",
    "translatedContent": "```"
  },
  {
    "row": 69,
    "rowsha": "6x1cChX3P2R4vz7Xay6LDGG4E2IO6b+TTIUKD3V303M=",
    "originContent": "@inproceedings{yu2024boosting,",
    "translatedContent": "@inproceedings{yu2024boosting,"
  },
  {
    "row": 70,
    "rowsha": "QZCPuP4qFfYJyb1PfRcQdQqFif3EdkCEEh5UEPBfSbc=",
    "originContent": "  title={Boosting continual learning of vision-language models via mixture-of-experts adapters},",
    "translatedContent": "  title={Boosting continual learning of vision-language models via mixture-of-experts adapters},"
  },
  {
    "row": 71,
    "rowsha": "t4UK7wJq54Fufo9AZmh/LeZjxN7rINSvkreFq8WYTxc=",
    "originContent": "  author={Yu, Jiazuo and Zhuge, Yunzhi and Zhang, Lu and Hu, Ping and Wang, Dong and Lu, Huchuan and He, You},",
    "translatedContent": "  author={Yu, Jiazuo and Zhuge, Yunzhi and Zhang, Lu and Hu, Ping and Wang, Dong and Lu, Huchuan and He, You},"
  },
  {
    "row": 72,
    "rowsha": "IuLbTK6RIf/1AHEIMMS+xeW8Y5WE78EEpz/IImkCoF4=",
    "originContent": "  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},",
    "translatedContent": "  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},"
  },
  {
    "row": 73,
    "rowsha": "kvS2dvY0Zs2nfml2U4Vf82tUupNp7oSjdApnZk7fLJU=",
    "originContent": "  pages={23219--23230},",
    "translatedContent": "  pages={23219--23230},"
  },
  {
    "row": 74,
    "rowsha": "9vunU7Tk7EiCUudFrq1MUG4YCfjMDMPMikZF6BT/eLU=",
    "originContent": "  year={2024}",
    "translatedContent": "  year={2024}"
  },
  {
    "row": 75,
    "rowsha": "0Qs2qnSlm89KiBhYN/ZYr682Ru/yuxbDko0OkzXpRdI=",
    "originContent": "}",
    "translatedContent": "}"
  },
  {
    "row": 76,
    "rowsha": "8bkBhHOQsO1+N058HkZOwXtGpCfEh6WtbL0pBkBQg9U=",
    "originContent": "```",
    "translatedContent": "```"
  },
  {
    "row": 77,
    "rowsha": "zeUL2mcUYd628fTHqknKcxv2uqjN5wj1hlMFcVnzrpU=",
    "originContent": "## Acknowledgement",
    "translatedContent": "## Acknowledgement"
  },
  {
    "row": 78,
    "rowsha": "sNRpEJpVx7ad1hA3kEXpo0V9QbEO16nYskj4DztIOvc=",
    "originContent": "Our repo is built on [wise-ft](https://github.com/mlfoundations/wise-ft), [Continual-CLIP](https://github.com/vgthengane/Continual-CLIP/tree/master) and [ZSCL](https://github.com/Thunderbeee/ZSCL). We thank the authors for sharing their codes.",
    "translatedContent": "Our repo is built on [wise-ft](https://github.com/mlfoundations/wise-ft), [Continual-CLIP](https://github.com/vgthengane/Continual-CLIP/tree/master) and [ZSCL](https://github.com/Thunderbeee/ZSCL). We thank the authors for sharing their codes."
  },
  {
    "row": 79,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 80,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  },
  {
    "row": 81,
    "rowsha": "47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=",
    "originContent": "",
    "translatedContent": ""
  }
]