<div align="center">
<h2>Super-r√©solution d‚Äôimages r√©elles avec pr√©servation de la structure fine via un entra√Ænement VAE par transfert</h2>

üö© Accept√© par ICCV2025

[Qiaosi Yi](https://dblp.org/pid/249/8335.html)<sup>1,2</sup>
| [Shuai Li](https://scholar.google.com/citations?hl=zh-CN&user=Bd73ldQAAAAJ)<sup>1</sup>
| [Rongyuan Wu](https://scholar.google.com/citations?user=A-U8zE8AAAAJ&hl=zh-CN)<sup>1,2</sup>
| [Lingchen Sun](https://scholar.google.com/citations?hl=zh-CN&tzom=-480&user=ZCDjTn8AAAAJ)<sup>1,2</sup>
| [Yuhui Wu](https://dblp.org/pid/41/915-1.html)<sup>1,2</sup>
| [Lei Zhang](https://www4.comp.polyu.edu.hk/~cslzhang)<sup>1,2</sup>

<sup>1</sup>L‚ÄôUniversit√© Polytechnique de Hong Kong, <sup>2</sup>Institut de Recherche OPPO
</div>

[![](https://img.shields.io/badge/ArXiv%20-Paper-b31b1b?logo=arxiv&logoColor=red)](https://arxiv.org/pdf/2507.20291)&nbsp; [![weights](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-model%20weights-blue)](https://huggingface.co/Joypop/TVTSR/tree/main)


## ‚è∞ Mise √† jour
- **2025.7.29** : Article publi√© sur [ArXiv](https://arxiv.org/pdf/2507.20291).
- **2025.7.28** : Le code d‚Äôentra√Ænement et de test est publi√©.
- **2025.7.24** : Le d√©p√¥t est publi√©.

:star: Si TVT vous aide pour vos images ou projets, merci d‚Äô√©toiler ce d√©p√¥t. Merci ! :hugs:

### √Ä FAIRE 
- [x] Publier le code pour l‚Äôinf√©rence.
- [x] Mettre √† jour le code pour l‚Äôentra√Ænement.
- [ ] VAED4 en fp16.


## ‚öô D√©pendances et installation
```shell
## git clone this repository
git clone https://github.com/Joyies/TVT.git
cd TVT


# create an environment
conda create -n TVT python=3.10
conda activate TVT
pip install --upgrade pip
pip install -r requirements.txt
```

## üèÇ Inf√©rence rapide

### Super-r√©solution d'image r√©elle

#### √âtape 1 : T√©l√©charger les mod√®les pr√©-entra√Æn√©s
- T√©l√©chargez les mod√®les SD-2.1-base pr√©-entra√Æn√©s depuis [![weights](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-SD%202.1%20Base-blue)](https://huggingface.co/Joypop/stable-diffusion-2-1-base)&nbsp;
- T√©l√©chargez les poids des mod√®les ([VAED4](https://huggingface.co/Joypop/TVTSR/tree/main/ckp), [mod√®le TVT](https://huggingface.co/Joypop/TVTSR/tree/main/ckp), [TVTUNet](https://huggingface.co/Joypop/TVTSR/tree/main/ckp), [DAPE](https://huggingface.co/Joypop/TVTSR/tree/main/ckp), et [RAM](https://huggingface.co/Joypop/TVTSR/tree/main/ckp)) depuis [![weights](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-model%20weights-blue)](https://huggingface.co/Joypop/TVTSR/tree/main)&nbsp; et placez les poids dans le dossier `ckp/` :

#### √âtape 2 : Pr√©parez les donn√©es de test et lancez la commande de test 
Vous pouvez modifier input_path et output_path pour lancer la commande de test. input_path est le chemin de l'image de test et output_path est le chemin o√π les images de sortie sont sauvegard√©es.
```
python TVT/inferences/inference.py \
--input_image input_path \
--output_dir output_path \
--pretrained_path ckp/model_TVT.pkl \
--pretrained_model_name_or_path stabilityai/stable-diffusion-2-1-base \
--pretrained_unet_path ckp/TVTUNet \
--vae4d_path ckp/vae.ckpt \
--ram_ft_path ckp/DAPE.pth \
--negprompt 'dotted, noise, blur, lowres, smooth' \
--prompt 'clean, high-resolution, 8k' \
--upscale 4 \
--time_step 1
```
or 
```
bash scripts/test/test_realsr.sh
```
Nous fournissons √©galement le code de tuiles pour √©conomiser la m√©moire GPU lors de l'inf√©rence. Vous pouvez ex√©cuter la commande et modifier la taille des tuiles et le pas en fonction de la VRAM de votre appareil.
```
python TVT/inferences/inference_tile.py \
--input_image input_path \
--output_dir output_path \
--pretrained_path ckp/model_TVT.pkl \
--pretrained_model_name_or_path stabilityai/stable-diffusion-2-1-base \
--pretrained_unet_path ckp/TVTUNet \
--vae4d_path ckp/vae.ckpt \
--ram_ft_path ckp/DAPE.pth \
--negprompt 'dotted, noise, blur, lowres, smooth' \
--prompt 'clean, high-resolution, 8k' \
--upscale 4 \
--time_step 1 \
--tiled_size 96 \
--tiled_overlap 32
```

## üöÑ Phase d'entra√Ænement

### Entra√Æner VAED4 sur les jeux de donn√©es OpenImage et LSDIR.
#### √âtape 1 : Pr√©parer les donn√©es d'entra√Ænement
T√©l√©chargez le [jeu de donn√©es OpenImage](https://storage.googleapis.com/openimages/web/index.html) et le [jeu de donn√©es LSIDR](https://github.com/ofsoundof/LSDIR). Pour chaque image du jeu de donn√©es LSDIR, d√©coupez plusieurs patches d‚Äôimage de 512√ó512 en utilisant une fen√™tre glissante avec un pas de 64 pixels ;

#### √âtape 2 : Entra√Æner VAED4
Le [code LDM](https://github.com/CompVis/latent-diffusion) est utilis√© pour entra√Æner VAED4. 


### Entra√Æner TVTSR sur les jeux de donn√©es Real-ISR
#### √âtape 1 : Pr√©parer les donn√©es d'entra√Ænement

T√©l√©chargez le [jeu de donn√©es LSIDR](https://github.com/ofsoundof/LSDIR) et les 10 000 premi√®res images du [jeu de donn√©es FFHQ](https://github.com/NVlabs/ffhq-dataset). Ensuite, effectuez une augmentation des donn√©es sur le jeu de donn√©es d'entra√Ænement. Plus pr√©cis√©ment, pour chaque image du jeu LSDIR, d√©coupez plusieurs patches d‚Äôimage de 512√ó512 en utilisant une fen√™tre glissante avec un pas de 64 pixels ; pour le jeu FFHQ, redimensionnez directement toutes les images √† 512√ó512.

#### √âtape 2 : Entra√Æner le mod√®le Real-ISR

1. T√©l√©chargez les mod√®les [VAED4](https://huggingface.co/Joypop/TVTSR/tree/main/ckp), [TVTUNet](https://huggingface.co/Joypop/TVTSR/tree/main/ckp) et [RAM](https://huggingface.co/Joypop/TVTSR/tree/main/ckp), et placez-les dans le dossier `ckp/`. 

2. Lancez l‚Äôentra√Ænement.
    ```shell
   accelerate launch --gpu_ids=0,1,2,3, --num_processes=4 TVT/train_TVTSR/train.py \
    --pretrained_model_name_or_path="stabilityai/stable-diffusion-2-1-base" \
    --pretrained_model_name_or_path_vsd="stabilityai/stable-diffusion-2-1-base" \
    --pretrained_unet_path='ckp/TVTUNet' \
    --vae4d_path='ckp/vae.ckpt' \
    --dataset_folder="data_path" \
    --testdataset_folder="test_path" \
    --resolution=512 \
    --learning_rate=5e-5 \
    --train_batch_size=2 \
    --gradient_accumulation_steps=2 \
    --enable_xformers_memory_efficient_attention \
    --eval_freq 500 \
    --checkpointing_steps 500 \
    --mixed_precision='fp16' \
    --report_to "tensorboard" \
    --output_dir="output_path" \
    --lora_rank_unet_vsd=4 \
    --lora_rank_unet=4 \
    --lambda_lpips=2 \
    --lambda_l2=1 \
    --lambda_vsd=1 \
    --lambda_vsd_lora=1 \
    --min_dm_step_ratio=0.25 \
    --max_dm_step_ratio=0.75 \
    --use_vae_encode_lora \
    --align_method="adain" \
    --use_online_deg \
    --deg_file_path="params_TVT.yml" \
    --negative_prompt='painting, oil painting, illustration, drawing, art, sketch, oil painting, cartoon, CG Style, 3D render, unreal engine, blurring, dirty, messy, worst quality, low quality, frames, watermark, signature, jpeg artifacts, deformed, lowres, over-smooth' \
    --test_image_prep='no_resize' \
    --time_step=1 \
    --tracker_project_name "experiment_track_name"
    ```
    or
    ```shell
   bash scripts/train/train.sh
    ```

## üîó Citations
Si notre code aide votre recherche ou votre travail, veuillez envisager de citer notre article.
Voici les r√©f√©rences BibTeX :

```
@article{yi2025fine,
  title={Fine-structure Preserved Real-world Image Super-resolution via Transfer VAE Training},
  author={Yi, Qiaosi and Li, Shuai and Wu, Rongyuan and Sun, Lingchen and Wu, Yuhui and Zhang, Lei},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  year={2025}
}
```

## ¬©Ô∏è Licence
Ce projet est publi√© sous la [licence Apache 2.0](LICENSE).

## üìß Contact
Si vous avez des questions, veuillez contacter : qiaosiyijoyies@gmail.com

## Remerciements
Ce projet est bas√© sur [diffusers](https://github.com/huggingface/diffusers), [LDM](https://github.com/CompVis/latent-diffusion), [OSEDiff](https://github.com/cswry/OSEDiff) et [PiSA-SR](https://github.com/csslc/PiSA-SR). Merci pour ce travail remarquable. 

<details>
<summary>statistiques</summary>

![visiteurs](https://visitor-badge.laobi.icu/badge?page_id=Joyies/TVT)

</details>


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2026-02-22

---