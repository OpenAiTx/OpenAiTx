<div align="center">

# TAPIP3D : Suivi de Tout Point dans une G√©om√©trie 3D Persistante
<a href="https://arxiv.org/abs/2504.14717"><img src='https://img.shields.io/badge/arXiv-Paper-red?logo=arxiv&logoColor=white' alt='arXiv'></a>
<a href='https://tapip3d.github.io'><img src='https://img.shields.io/badge/Project_Page-Website-green?logo=googlechrome&logoColor=white' alt='Page du Projet'></a>

[Bowei Zhang](https://scholar.google.com/citations?user=tYH72AYAAAAJ)<sup>1,2</sup>*, [Lei Ke](https://www.kelei.site/)<sup>1</sup>\*, [Adam W. Harley](https://adamharley.com/)<sup>3</sup>, [Katerina Fragkiadaki](https://www.cs.cmu.edu/~katef/)<sup>1</sup>

<sup>1</sup>Carnegie Mellon University   &nbsp;  <sup>2</sup>Peking University &nbsp;  <sup>3</sup>Stanford University

**NeurIPS 2025**

\* Contribution √âgale

<!-- <a href='https://huggingface.co/spaces/your-username/project'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Live_Demo-blue'></a> -->

</div>

<img src="https://raw.githubusercontent.com/zbw001/TAPIP3D/main/./media/teaser1.gif" width="100%" alt="Pr√©sentation de TAPIP3D">


---

### üöÄ Actualit√©s
- **(2025.12.28)** üî• Nous avons mis √† jour le code de **Formation** et d'**√âvaluation** ! Consultez les nouvelles sections ci-dessous.

## Vue d'ensemble
**TAPIP3D** est une m√©thode de suivi 3D **feed-forward** √† long terme des points dans des s√©quences vid√©o monoculaires RGB et RGB-D. Elle introduit une repr√©sentation en nuage de caract√©ristiques 3D qui √©l√®ve les caract√©ristiques d‚Äôimage dans un espace de coordonn√©es mondiales persistant, annulant le mouvement de la cam√©ra et permettant une estimation pr√©cise des trajectoires entre les images.

Nous fournissons une [illustration vid√©o d√©taill√©e](https://neurips.cc/virtual/2025/loc/san-diego/poster/117634#:~:text=Within%20this%20stabilized%203D%20representation,trained%20checkpoints%20will%20be%20public.) de notre TAPIP3D.

## Installation
### Installation des d√©pendances

1. Pr√©parez l‚Äôenvironnement
```bash
conda create -n tapip3d python=3.10
conda activate tapip3d

pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 "xformers>=0.0.27" --index-url https://download.pytorch.org/whl/cu124
pip install torch-scatter -f https://data.pyg.org/whl/torch-2.4.1+cu124.html
pip install -r requirements.txt
```

2. Compiler pointops2

```bash
cd third_party/pointops2
LIBRARY_PATH=$CONDA_PREFIX/lib:$LIBRARY_PATH python setup.py install
cd ../..
```

3. Compiler megasam
```bash
cd third_party/megasam/base
LIBRARY_PATH=$CONDA_PREFIX/lib:$LIBRARY_PATH python setup.py install
cd ../../..
```

### T√©l√©chargement des points de contr√¥le

T√©l√©chargez notre point de contr√¥le du mod√®le TAPIP3D [ici](https://huggingface.co/zbww/tapip3d/resolve/main/tapip3d_final.pth) vers `checkpoints/tapip3d_final.pth`

Si vous souhaitez ex√©cuter TAPIP3D sur des vid√©os monoculaires, vous devez pr√©parer manuellement les points de contr√¥le suivants pour ex√©cuter MegaSAM :

1. T√©l√©chargez le point de contr√¥le DepthAnything V1 depuis [ici](https://huggingface.co/spaces/LiheYoung/Depth-Anything/resolve/main/checkpoints/depth_anything_vitl14.pth) et placez-le dans `third_party/megasam/Depth-Anything/checkpoints/depth_anything_vitl14.pth`

2. T√©l√©chargez le point de contr√¥le RAFT depuis [ici](https://drive.google.com/drive/folders/1sWDsfuZ3Up38EUQt7-JDTT1HcGHuJgvT) et placez-le dans `third_party/megasam/cvd_opt/raft-things.pth`

De plus, les points de contr√¥le de [MoGe](https://wangrc.site/MoGePage/) et [UniDepth](https://github.com/lpiccinelli-eth/UniDepth.git) seront t√©l√©charg√©s automatiquement lors de l'ex√©cution de la d√©mo. Veuillez vous assurer que votre connexion r√©seau est disponible.

## Utilisation de la d√©mo

Nous fournissons un script de d√©mo simple `inference.py`, ainsi que des donn√©es d'entr√©e d'exemple situ√©es dans le r√©pertoire `demo_inputs/`.

Le script accepte en entr√©e soit un fichier vid√©o `.mp4`, soit un fichier `.npz`. Si vous fournissez un fichier `.npz`, il doit suivre le format suivant :

- `video` : tableau de forme (T, H, W, 3), dtype : uint8
- `depths` (optionnel) : tableau de forme (T, H, W), dtype : float32
- `intrinsics` (optionnel) : tableau de forme (T, 3, 3), dtype : float32
- `extrinsics` (optionnel) : tableau de forme (T, 4, 4), dtype : float32

√Ä des fins de d√©monstration, le script utilise une grille de points 32x32 sur la premi√®re image comme requ√™tes.


### Inf√©rence avec vid√©o monoculaire

En fournissant une vid√©o via `--input_path`, le script ex√©cute d'abord [MegaSAM](https://github.com/mega-sam/mega-sam) avec [MoGe](https://wangrc.site/MoGePage/) pour estimer les cartes de profondeur et les param√®tres de la cam√©ra. Ensuite, le mod√®le traitera ces entr√©es dans le cadre global.

**D√©mo 1**

<img src="https://raw.githubusercontent.com/zbw001/TAPIP3D/main/./media/demo1.gif" width="100%" alt="D√©mo 1">

Pour lancer l'inf√©rence :

```bash
python inference.py --input_path demo_inputs/sheep.mp4 --checkpoint checkpoints/tapip3d_final.pth --resolution_factor 2
```

Un fichier npz sera enregistr√© dans `outputs/inference/`. Pour visualiser les r√©sultats :

```bash
python visualize.py <result_npz_path>
```

**Demo 2**

<img src="https://raw.githubusercontent.com/zbw001/TAPIP3D/main/./media/demo2.gif" width="100%" alt="Demo 2">

```bash
python inference.py --input_path demo_inputs/pstudio.mp4 --checkpoint checkpoints/tapip3d_final.pth --resolution_factor 2
```

**Inf√©rence avec Profondeurs et Param√®tres de Cam√©ra Connus**

Si un fichier `.npz` contenant les quatre cl√©s (`rgb`, `depths`, `intrinsics`, `extrinsics`) est fourni, le mod√®le fonctionnera dans un cadre global align√©, g√©n√©rant des trajectoires de points en coordonn√©es mondiales.
Nous fournissons un exemple de fichier `.npz` [ici](https://huggingface.co/zbww/tapip3d/resolve/main/demo_inputs/dexycb.npz?download=true) et veuillez le placer dans le r√©pertoire `demo_inputs/`.

**D√©mo 3**

<img src="https://raw.githubusercontent.com/zbw001/TAPIP3D/main/./media/demo3.gif" width="100%" alt="D√©mo 3">

```bash
python inference.py --input_path demo_inputs/dexycb.npz --checkpoint checkpoints/tapip3d_final.pth --resolution_factor 2
```

## Entra√Ænement et √âvaluation

### 1. Pr√©paration du Jeu de Donn√©es
Veuillez consulter [DATASET.md](https://raw.githubusercontent.com/zbw001/TAPIP3D/main/DATASET.md) pour les instructions sur la pr√©paration des jeux de donn√©es pour l‚Äôentra√Ænement et l‚Äô√©valuation.

### 2. Entra√Ænement
Pour commencer l‚Äôentra√Ænement, ex√©cutez :
```bash
bash scripts/train.sh
```
- `experiment_name` : Le nom de l'ex√©cution affich√© sur **WandB**.  
- `experiment_id` : Un identifiant unique. Relancer avec le m√™me `experiment_id` **reprendra automatiquement** l'entra√Ænement √† partir du dernier point de contr√¥le.

### 3. √âvaluation  
Pour √©valuer un point de contr√¥le, ex√©cutez :
```bash
bash scripts/eval.sh
```
Vous pouvez sp√©cifier le mod√®le √† √©valuer en modifiant la variable `checkpoint` dans `scripts/eval.sh`.

## Citation
Si vous trouvez ce projet utile, veuillez envisager de le citer :

```
@article{tapip3d,
  title={TAPIP3D: Tracking Any Point in Persistent 3D Geometry},
  author={Zhang, Bowei and Ke, Lei and Harley, Adam W and Fragkiadaki, Katerina},
  journal={arXiv preprint arXiv:2504.14717},
  year={2025}
}
```


---

Tranlated By [Open Ai Tx](https://github.com/OpenAiTx/OpenAiTx) | Last indexed: 2026-02-12

---