# Open WebUI ğŸ‘‹

![GitHub stars](https://img.shields.io/github/stars/open-webui/open-webui?style=social)
![GitHub forks](https://img.shields.io/github/forks/open-webui/open-webui?style=social)
![GitHub watchers](https://img.shields.io/github/watchers/open-webui/open-webui?style=social)
![GitHub repo size](https://img.shields.io/github/repo-size/open-webui/open-webui)
![GitHub language count](https://img.shields.io/github/languages/count/open-webui/open-webui)
![GitHub top language](https://img.shields.io/github/languages/top/open-webui/open-webui)
![GitHub last commit](https://img.shields.io/github/last-commit/open-webui/open-webui?color=red)
[![Discord](https://img.shields.io/badge/Discord-Open_WebUI-blue?logo=discord&logoColor=white)](https://discord.gg/5rJgQTnV4s)
[![](https://img.shields.io/static/v1?label=Sponsor&message=%E2%9D%A4&logo=GitHub&color=%23fe8e86)](https://github.com/sponsors/tjbck)

**Open WebUI est une plateforme IA auto-hÃ©bergÃ©e [extensible](https://docs.openwebui.com/features/plugin/), riche en fonctionnalitÃ©s et conviviale, conÃ§ue pour fonctionner entiÃ¨rement hors-ligne.** Elle prend en charge divers moteurs LLM tels que **Ollama** et les **API compatibles OpenAI**, avec un **moteur dâ€™infÃ©rence intÃ©grÃ©** pour le RAG, ce qui en fait une **solution de dÃ©ploiement IA puissante**.

![DÃ©monstration Open WebUI](./demo.gif)

> [!TIP]  
> **Vous cherchez une [offre Entreprise](https://docs.openwebui.com/enterprise) ?** â€“ **[Contactez notre Ã©quipe commerciale dÃ¨s aujourd'hui !](mailto:sales@openwebui.com)**
>
> Profitez de **capacitÃ©s amÃ©liorÃ©es**, dont **thÃ©matisation et branding sur mesure**, **support SLA**, **versions LTS** et **bien plus !**

Pour plus dâ€™informations, consultez notre [documentation Open WebUI](https://docs.openwebui.com/).

## FonctionnalitÃ©s clÃ©s dâ€™Open WebUI â­

- ğŸš€ **Installation sans effort** : Installez facilement avec Docker ou Kubernetes (kubectl, kustomize ou helm) pour une expÃ©rience sans tracas avec prise en charge des images taguÃ©es `:ollama` et `:cuda`.

- ğŸ¤ **IntÃ©gration API Ollama/OpenAI** : IntÃ©grez aisÃ©ment les API compatibles OpenAI pour des conversations polyvalentes avec les modÃ¨les Ollama. Personnalisez lâ€™URL de lâ€™API OpenAI pour vous connecter Ã  **LMStudio, GroqCloud, Mistral, OpenRouter, et plus encore**.

- ğŸ›¡ï¸ **Permissions granulaires et groupes dâ€™utilisateurs** : En permettant aux administrateurs de crÃ©er des rÃ´les et permissions dÃ©taillÃ©s, nous garantissons un environnement utilisateur sÃ©curisÃ©. Cette granularitÃ© renforce la sÃ©curitÃ© et permet des expÃ©riences personnalisÃ©es, favorisant lâ€™engagement et la responsabilitÃ© des utilisateurs.

- ğŸ“± **Design adaptatif** : Profitez dâ€™une expÃ©rience fluide sur PC de bureau, ordinateur portable et appareils mobiles.

- ğŸ“± **Application Web Progressive (PWA) pour mobile** : BÃ©nÃ©ficiez dâ€™une expÃ©rience proche dâ€™une application native sur votre mobile grÃ¢ce Ã  notre PWA, permettant un accÃ¨s hors-ligne sur localhost et une interface utilisateur fluide.

- âœ’ï¸ğŸ”¢ **Prise en charge complÃ¨te de Markdown et LaTeX** : AmÃ©liorez votre expÃ©rience LLM avec un support complet de Markdown et LaTeX pour des interactions enrichies.

- ğŸ¤ğŸ“¹ **Appels vocaux/vidÃ©o mains-libres** : Profitez dâ€™une communication fluide avec les fonctionnalitÃ©s intÃ©grÃ©es dâ€™appel vocal et vidÃ©o, rendant lâ€™environnement de chat plus dynamique et interactif.

- ğŸ› ï¸ **CrÃ©ateur de modÃ¨les** : CrÃ©ez facilement des modÃ¨les Ollama via lâ€™interface Web. Ajoutez des personnages/agents personnalisÃ©s, personnalisez les Ã©lÃ©ments de chat et importez des modÃ¨les simplement grÃ¢ce Ã  lâ€™intÃ©gration de la [communautÃ© Open WebUI](https://openwebui.com/).

- ğŸ **Outil natif dâ€™appel de fonctions Python** : AmÃ©liorez vos LLM avec un Ã©diteur de code intÃ©grÃ© dans lâ€™espace outils. Apportez vos propres fonctions Python (BYOF) en ajoutant simplement vos fonctions pures Python, permettant une intÃ©gration fluide avec les LLM.

- ğŸ“š **IntÃ©gration RAG locale** : DÃ©couvrez le futur des interactions de chat avec un support innovant de la gÃ©nÃ©ration augmentÃ©e par rÃ©cupÃ©ration (RAG). Chargez des documents directement dans le chat ou ajoutez des fichiers Ã  votre bibliothÃ¨que pour les accÃ©der facilement via la commande `#` avant une requÃªte.

- ğŸ” **Recherche web pour RAG** : Effectuez des recherches web avec des fournisseurs comme `SearXNG`, `Google PSE`, `Brave Search`, `serpstack`, `serper`, `Serply`, `DuckDuckGo`, `TavilySearch`, `SearchApi` et `Bing`, et injectez les rÃ©sultats directement dans votre expÃ©rience de chat.

- ğŸŒ **Navigation web intÃ©grÃ©e** : IntÃ©grez facilement des sites web dans votre expÃ©rience de chat en utilisant la commande `#` suivie dâ€™une URL. Cette fonctionnalitÃ© enrichit la profondeur de vos conversations.

- ğŸ¨ **IntÃ©gration de la gÃ©nÃ©ration dâ€™images** : Incorporez la gÃ©nÃ©ration dâ€™images via lâ€™API AUTOMATIC1111, ComfyUI (local) ou DALL-E dâ€™OpenAI (externe), pour enrichir vos chats de contenus visuels dynamiques.

- âš™ï¸ **Conversations multi-modÃ¨les** : Interagissez simultanÃ©ment avec plusieurs modÃ¨les, tirant parti de leurs points forts respectifs pour des rÃ©ponses optimales. Profitez dâ€™un ensemble diversifiÃ© de modÃ¨les en parallÃ¨le.

- ğŸ” **ContrÃ´le dâ€™accÃ¨s basÃ© sur les rÃ´les (RBAC)** : Garantissez un accÃ¨s sÃ©curisÃ© avec des permissions restreintes ; seuls les utilisateurs autorisÃ©s peuvent accÃ©der Ã  votre Ollama et seuls les administrateurs peuvent crÃ©er/tÃ©lÃ©charger des modÃ¨les.

- ğŸŒğŸŒ **Support multilingue** : Utilisez Open WebUI dans votre langue grÃ¢ce Ã  la prise en charge de lâ€™internationalisation (i18n). Rejoignez-nous pour Ã©largir la liste des langues ! Nous recherchons activement des contributeurs !

- ğŸ§© **Pipelines, support des plugins Open WebUI** : IntÃ©grez facilement des logiques personnalisÃ©es et des bibliothÃ¨ques Python via le [Pipelines Plugin Framework](https://github.com/open-webui/pipelines). Lancez votre instance Pipelines, dÃ©finissez lâ€™URL OpenAI sur celle de Pipelines, et explorez de nombreuses possibilitÃ©s. [Exemples](https://github.com/open-webui/pipelines/tree/main/examples) : **Appel de fonctions**, **limitation de dÃ©bit** utilisateur, **suivi dâ€™utilisation** avec Langfuse, **traduction en direct avec LibreTranslate** pour le multilingue, **filtrage de messages toxiques** et bien plus.

- ğŸŒŸ **Mises Ã  jour continues** : Nous nous engageons Ã  amÃ©liorer Open WebUI avec des mises Ã  jour rÃ©guliÃ¨res, correctifs et nouvelles fonctionnalitÃ©s.

Vous souhaitez en savoir plus sur les fonctionnalitÃ©s dâ€™Open WebUI ? Consultez notre [documentation Open WebUI](https://docs.openwebui.com/features) pour une vue dâ€™ensemble complÃ¨te !

## Sponsors ğŸ™Œ

#### Ã‰meraude

<table>
  <tr>
    <td>
      <a href="https://n8n.io/" target="_blank">
        <img src="https://docs.openwebui.com/sponsors/logos/n8n.png" alt="n8n" style="width: 8rem; height: 8rem; border-radius: .75rem;" />
      </a>
    </td>
    <td>
      N8N â€¢ Votre interface a-t-elle dÃ©jÃ  un backend ?<br>Essayez <a href="https://n8n.io/">n8n</a>
    </td>
  </tr>
  <tr>
    <td>
      <a href="https://warp.dev/open-webui" target="_blank">
        <img src="https://docs.openwebui.com/sponsors/logos/warp.png" alt="n8n" style="width: 8rem; height: 8rem; border-radius: .75rem;" />
      </a>
    </td>
    <td>
      <a href="https://warp.dev/open-webui">Warp</a> â€¢ Le terminal intelligent pour les dÃ©veloppeurs
    </td>
  </tr>
</table>

---

Nous sommes extrÃªmement reconnaissants envers nos sponsors pour leur soutien gÃ©nÃ©reux. Leurs contributions nous aident Ã  maintenir et Ã  amÃ©liorer notre projet, assurant ainsi la qualitÃ© de notre travail pour la communautÃ©. Merci !

## Comment installer ğŸš€

### Installation via pip Python ğŸ

Open WebUI peut Ãªtre installÃ© avec pip, le gestionnaire de paquets Python. Avant de commencer, assurez-vous dâ€™utiliser **Python 3.11** pour Ã©viter tout problÃ¨me de compatibilitÃ©.

1. **Installer Open WebUI** :
   Ouvrez votre terminal et exÃ©cutez la commande suivante pour installer Open WebUI :

   ```bash
   pip install open-webui
   ```

2. **Lancer Open WebUI** :
   AprÃ¨s lâ€™installation, dÃ©marrez Open WebUI avec :

   ```bash
   open-webui serve
   ```

Cela lancera le serveur Open WebUI, accessible Ã  lâ€™adresse [http://localhost:8080](http://localhost:8080)

### DÃ©marrage rapide avec Docker ğŸ³

> [!NOTE]  
> Veuillez noter que certains environnements Docker nÃ©cessitent des configurations supplÃ©mentaires. Si vous rencontrez des problÃ¨mes de connexion, notre guide dÃ©taillÃ© dans la [documentation Open WebUI](https://docs.openwebui.com/) est lÃ  pour vous aider.

> [!WARNING]
> Lors de lâ€™utilisation de Docker pour installer Open WebUI, assurez-vous dâ€™inclure `-v open-webui:/app/backend/data` dans votre commande Docker. Cette Ã©tape est cruciale pour que votre base de donnÃ©es soit bien montÃ©e et Ã©viter toute perte de donnÃ©es.

> [!TIP]  
> Pour utiliser Open WebUI avec Ollama inclus ou lâ€™accÃ©lÃ©ration CUDA, nous recommandons dâ€™utiliser nos images officielles taguÃ©es `:cuda` ou `:ollama`. Pour activer CUDA, vous devez installer le [Nvidia CUDA container toolkit](https://docs.nvidia.com/dgx/nvidia-container-runtime-upgrade/) sur votre systÃ¨me Linux/WSL.

### Installation avec la configuration par dÃ©faut

- **Si Ollama est sur votre ordinateur**, utilisez cette commande :

  ```bash
  docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
  ```

- **Si Ollama est sur un autre serveur**, utilisez cette commande :

  Pour vous connecter Ã  Ollama sur un autre serveur, changez `OLLAMA_BASE_URL` par lâ€™URL du serveur :

  ```bash
  docker run -d -p 3000:8080 -e OLLAMA_BASE_URL=https://exemple.com -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
  ```

- **Pour exÃ©cuter Open WebUI avec le support GPU Nvidia**, utilisez cette commande :

  ```bash
  docker run -d -p 3000:8080 --gpus all --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:cuda
  ```

### Installation pour utilisation exclusive de lâ€™API OpenAI

- **Si vous nâ€™utilisez que lâ€™API OpenAI**, utilisez cette commande :

  ```bash
  docker run -d -p 3000:8080 -e OPENAI_API_KEY=your_secret_key -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
  ```

### Installer Open WebUI avec Ollama intÃ©grÃ©

Cette mÃ©thode dâ€™installation utilise une seule image de conteneur regroupant Open WebUI et Ollama, permettant une mise en place simplifiÃ©e en une commande. Choisissez la commande appropriÃ©e selon votre matÃ©riel :

- **Avec support GPU** :
  Utilisez les ressources GPU avec la commande suivante :

  ```bash
  docker run -d -p 3000:8080 --gpus=all -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama
  ```

- **Pour CPU uniquement** :
  Si vous nâ€™utilisez pas de GPU, utilisez plutÃ´t cette commande :

  ```bash
  docker run -d -p 3000:8080 -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama
  ```

Ces deux commandes permettent une installation intÃ©grÃ©e et simplifiÃ©e dâ€™Open WebUI et Ollama, pour un dÃ©marrage rapide.

AprÃ¨s installation, accÃ©dez Ã  Open WebUI Ã  lâ€™adresse [http://localhost:3000](http://localhost:3000). Bonne utilisation ! ğŸ˜„

### Autres mÃ©thodes dâ€™installation

Nous proposons diffÃ©rentes alternatives, y compris des mÃ©thodes dâ€™installation natives (hors Docker), Docker Compose, Kustomize et Helm. Consultez notre [documentation Open WebUI](https://docs.openwebui.com/getting-started/) ou rejoignez notre [communautÃ© Discord](https://discord.gg/5rJgQTnV4s) pour un accompagnement complet.

### DÃ©pannage

Vous rencontrez des problÃ¨mes de connexion ? Notre [documentation Open WebUI](https://docs.openwebui.com/troubleshooting/) vous guide. Pour toute aide supplÃ©mentaire et rejoindre notre communautÃ© dynamique, rendez-vous sur le [Discord Open WebUI](https://discord.gg/5rJgQTnV4s).

#### Open WebUI : Erreur de connexion au serveur

Si vous avez des problÃ¨mes de connexion, câ€™est souvent parce que le conteneur docker WebUI ne peut pas atteindre le serveur Ollama Ã  127.0.0.1:11434 (host.docker.internal:11434) Ã  lâ€™intÃ©rieur du conteneur. Utilisez lâ€™option `--network=host` dans votre commande Docker pour rÃ©soudre ce problÃ¨me. Notez que le port change de 3000 Ã  8080, donnant le lien : `http://localhost:8080`.

**Exemple de commande Docker** :

```bash
docker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```

### Garder votre installation Docker Ã  jour

Pour mettre Ã  jour votre installation locale Docker vers la derniÃ¨re version, utilisez [Watchtower](https://containrrr.dev/watchtower/) :

```bash
docker run --rm --volume /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower --run-once open-webui
```

Ã€ la fin de la commande, remplacez `open-webui` par le nom de votre conteneur si besoin.

Consultez notre guide de mise Ã  jour dans la [documentation Open WebUI](https://docs.openwebui.com/getting-started/updating).

### Utilisation de la branche Dev ğŸŒ™

> [!WARNING]
> La branche `:dev` contient les derniÃ¨res fonctionnalitÃ©s instables. Utilisez-la Ã  vos risques et pÃ©rils : elle peut contenir des bugs ou des fonctionnalitÃ©s incomplÃ¨tes.

Pour tester les toutes derniÃ¨res fonctionnalitÃ©s, en acceptant une certaine instabilitÃ©, utilisez le tag `:dev` ainsi :

```bash
docker run -d -p 3000:8080 -v open-webui:/app/backend/data --name open-webui --add-host=host.docker.internal:host-gateway --restart always ghcr.io/open-webui/open-webui:dev
```

### Mode hors-ligne

Si vous exÃ©cutez Open WebUI dans un environnement hors-ligne, dÃ©finissez la variable dâ€™environnement `HF_HUB_OFFLINE` Ã  `1` pour empÃªcher toute tentative de tÃ©lÃ©chargement de modÃ¨les depuis Internet.

```bash
export HF_HUB_OFFLINE=1
```

## Et ensuite ? ğŸŒŸ

DÃ©couvrez les prochaines fonctionnalitÃ©s sur notre feuille de route dans la [documentation Open WebUI](https://docs.openwebui.com/roadmap/).

## Licence ğŸ“œ

Ce projet est sous licence [Open WebUI License](LICENSE), une licence BSD-3-Clause rÃ©visÃ©e. Vous bÃ©nÃ©ficiez des mÃªmes droits que la licence BSD-3 classique : utilisation, modification et distribution du logiciel, y compris dans des produits propriÃ©taires et commerciaux, avec des restrictions minimales. Lâ€™unique exigence supplÃ©mentaire est de prÃ©server la marque "Open WebUI", comme prÃ©cisÃ© dans le fichier LICENSE. Pour les conditions complÃ¨tes, consultez le [document LICENSE](LICENSE). ğŸ“„

## Support ğŸ’¬

Pour toute question, suggestion ou demande dâ€™aide, ouvrez une issue ou rejoignez notre
[communautÃ© Discord Open WebUI](https://discord.gg/5rJgQTnV4s) pour Ã©changer avec nous ! ğŸ¤

## Historique des Ã©toiles

<a href="https://star-history.com/#open-webui/open-webui&Date">
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=open-webui/open-webui&type=Date&theme=dark" />
    <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=open-webui/open-webui&type=Date" />
    <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=open-webui/open-webui&type=Date" />
  </picture>
</a>

---

CrÃ©Ã© par [Timothy Jaeryang Baek](https://github.com/tjbck) â€“ AmÃ©liorons ensemble Open WebUI ! ğŸ’ª

---

[Powered By OpenAiTx](https://github.com/OpenAiTx/OpenAiTx)

---