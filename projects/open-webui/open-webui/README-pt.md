# Open WebUI 👋

![GitHub stars](https://img.shields.io/github/stars/open-webui/open-webui?style=social)
![GitHub forks](https://img.shields.io/github/forks/open-webui/open-webui?style=social)
![GitHub watchers](https://img.shields.io/github/watchers/open-webui/open-webui?style=social)
![GitHub repo size](https://img.shields.io/github/repo-size/open-webui/open-webui)
![GitHub language count](https://img.shields.io/github/languages/count/open-webui/open-webui)
![GitHub top language](https://img.shields.io/github/languages/top/open-webui/open-webui)
![GitHub last commit](https://img.shields.io/github/last-commit/open-webui/open-webui?color=red)
[![Discord](https://img.shields.io/badge/Discord-Open_WebUI-blue?logo=discord&logoColor=white)](https://discord.gg/5rJgQTnV4s)
[![](https://img.shields.io/static/v1?label=Sponsor&message=%E2%9D%A4&logo=GitHub&color=%23fe8e86)](https://github.com/sponsors/tjbck)

**Open WebUI é uma [plataforma de IA auto-hospedada](https://docs.openwebui.com/features/plugin/) extensível, rica em recursos e fácil de usar, projetada para operar totalmente offline.** Suporta vários interpretadores de LLM como **Ollama** e **APIs compatíveis com OpenAI**, com **motor de inferência integrado** para RAG, tornando-se uma **solução poderosa de implantação de IA**.

![Open WebUI Demo](./demo.gif)

> [!TIP]  
> **Procurando um [Plano Empresarial](https://docs.openwebui.com/enterprise)?** – **[Fale com nosso time de vendas hoje!](mailto:sales@openwebui.com)**
>
> Obtenha **capacidades aprimoradas**, incluindo **personalização de tema e marca**, **suporte a SLA**, **versões LTS** e **muito mais!**

Para mais informações, consulte nossa [Documentação Open WebUI](https://docs.openwebui.com/).

## Principais Recursos do Open WebUI ⭐

- 🚀 **Configuração Fácil**: Instale facilmente usando Docker ou Kubernetes (kubectl, kustomize ou helm) para uma experiência sem complicações com suporte para imagens marcadas como `:ollama` e `:cuda`.

- 🤝 **Integração API Ollama/OpenAI**: Integre APIs compatíveis com OpenAI para conversas versáteis junto com modelos Ollama. Personalize a URL da API OpenAI para conectar com **LMStudio, GroqCloud, Mistral, OpenRouter e mais**.

- 🛡️ **Permissões Granulares e Grupos de Usuários**: Permitindo que administradores criem funções e permissões detalhadas, garantimos um ambiente seguro. Essa granularidade aprimora a segurança e permite experiências personalizadas para os usuários, fomentando senso de responsabilidade.

- 📱 **Design Responsivo**: Tenha uma experiência fluida em PC, notebook e dispositivos móveis.

- 📱 **Progressive Web App (PWA) para Mobile**: Tenha uma experiência nativa no seu dispositivo móvel com nossa PWA, com acesso offline no localhost e interface fluida.

- ✒️🔢 **Suporte Completo a Markdown e LaTeX**: Eleve sua experiência com LLM com recursos completos de Markdown e LaTeX para interações enriquecidas.

- 🎤📹 **Chamada de Voz/Vídeo Sem Uso das Mãos**: Comunicação fluida com recursos integrados de chamada de voz e vídeo, tornando o chat mais dinâmico e interativo.

- 🛠️ **Construtor de Modelos**: Crie modelos Ollama facilmente via Web UI. Adicione personagens/agentes personalizados, ajuste elementos do chat e importe modelos via integração com a [Comunidade Open WebUI](https://openwebui.com/).

- 🐍 **Ferramenta Nativa de Chamadas de Função Python**: Potencialize seus LLMs com editor de código integrado no workspace de ferramentas. Traga sua própria função (BYOF) adicionando funções Python puras, integrando-as aos LLMs.

- 📚 **Integração RAG Local**: Explore o futuro das interações de chat com suporte inovador ao Retrieval Augmented Generation (RAG). Carregue documentos diretamente no chat ou adicione arquivos à biblioteca, acessando-os facilmente com o comando `#` antes de uma consulta.

- 🔍 **Pesquisa Web para RAG**: Realize pesquisas na web usando provedores como `SearXNG`, `Google PSE`, `Brave Search`, `serpstack`, `serper`, `Serply`, `DuckDuckGo`, `TavilySearch`, `SearchApi` e `Bing`, injetando resultados direto no chat.

- 🌐 **Capacidade de Navegação Web**: Integre websites ao chat usando o comando `#` seguido de uma URL. Incorpore conteúdos web diretamente nas conversas, enriquecendo suas interações.

- 🎨 **Integração de Geração de Imagens**: Incorpore geração de imagens usando opções como API AUTOMATIC1111 ou ComfyUI (local), e DALL-E da OpenAI (externo), enriquecendo o chat com conteúdo visual dinâmico.

- ⚙️ **Conversas com Vários Modelos**: Interaja com diversos modelos simultaneamente, aproveitando seus pontos fortes para respostas ótimas. Potencialize sua experiência com um conjunto diversificado de modelos em paralelo.

- 🔐 **Controle de Acesso Baseado em Função (RBAC)**: Garanta acesso seguro com permissões restritas; apenas pessoas autorizadas acessam seu Ollama, e apenas administradores podem criar/puxar modelos.

- 🌐🌍 **Suporte Multilíngue**: Use o Open WebUI no idioma de sua preferência com suporte internacional (i18n). Ajude-nos a expandir os idiomas suportados! Procuramos colaboradores!

- 🧩 **Pipelines, Suporte a Plugins Open WebUI**: Integre lógica personalizada e bibliotecas Python usando o [Pipelines Plugin Framework](https://github.com/open-webui/pipelines). Inicie sua instância Pipelines, defina a URL OpenAI para a URL do Pipelines e explore possibilidades. [Exemplos](https://github.com/open-webui/pipelines/tree/main/examples) incluem **Chamadas de Função**, **Limite de Taxa** para controlar acesso, **Monitoramento de Uso** com Langfuse, **Tradução ao Vivo com LibreTranslate** para suporte multilíngue, **Filtro de Mensagens Tóxicas** e muito mais.

- 🌟 **Atualizações Contínuas**: Estamos comprometidos em melhorar o Open WebUI com atualizações regulares, correções e novos recursos.

Quer saber mais sobre os recursos do Open WebUI? Confira nossa [documentação](https://docs.openwebui.com/features) para uma visão completa!

## Patrocinadores 🙌

#### Esmeralda

<table>
  <tr>
    <td>
      <a href="https://n8n.io/" target="_blank">
        <img src="https://docs.openwebui.com/sponsors/logos/n8n.png" alt="n8n" style="width: 8rem; height: 8rem; border-radius: .75rem;" />
      </a>
    </td>
    <td>
      N8N • Sua interface já tem backend?<br>Experimente o <a href="https://n8n.io/">n8n</a>
    </td>
  </tr>
  <tr>
    <td>
      <a href="https://warp.dev/open-webui" target="_blank">
        <img src="https://docs.openwebui.com/sponsors/logos/warp.png" alt="n8n" style="width: 8rem; height: 8rem; border-radius: .75rem;" />
      </a>
    </td>
    <td>
      <a href="https://warp.dev/open-webui">Warp</a> • O terminal inteligente para desenvolvedores
    </td>
  </tr>
</table>

---

Somos extremamente gratos pelo apoio generoso dos nossos patrocinadores. Suas contribuições nos ajudam a manter e melhorar o projeto, garantindo que possamos entregar trabalho de qualidade à comunidade. Obrigado!

## Como Instalar 🚀

### Instalação via pip do Python 🐍

O Open WebUI pode ser instalado usando o pip, o instalador de pacotes Python. Antes de prosseguir, certifique-se de estar usando **Python 3.11** para evitar problemas de compatibilidade.

1. **Instale o Open WebUI**:
   Abra o terminal e execute o comando a seguir para instalar o Open WebUI:

   ```bash
   pip install open-webui
   ```

2. **Executando o Open WebUI**:
   Após a instalação, inicie o Open WebUI executando:

   ```bash
   open-webui serve
   ```

Isso iniciará o servidor Open WebUI, que pode ser acessado em [http://localhost:8080](http://localhost:8080)

### Início Rápido com Docker 🐳

> [!NOTE]  
> Observe que em certos ambientes Docker, podem ser necessárias configurações adicionais. Se encontrar problemas de conexão, nosso guia detalhado na [Documentação Open WebUI](https://docs.openwebui.com/) está pronto para ajudar.

> [!WARNING]
> Ao usar Docker para instalar o Open WebUI, inclua `-v open-webui:/app/backend/data` no comando Docker. Este passo é crucial para garantir que seu banco de dados seja montado corretamente e evitar perda de dados.

> [!TIP]  
> Se quiser usar o Open WebUI com Ollama incluído ou aceleração CUDA, recomendamos as imagens oficiais com tag `:cuda` ou `:ollama`. Para ativar CUDA, instale o [Nvidia CUDA container toolkit](https://docs.nvidia.com/dgx/nvidia-container-runtime-upgrade/) no seu sistema Linux/WSL.

### Instalação com Configuração Padrão

- **Se o Ollama está no seu computador**, use este comando:

  ```bash
  docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
  ```

- **Se o Ollama está em outro servidor**, use este comando:

  Para conectar ao Ollama em outro servidor, altere o `OLLAMA_BASE_URL` para a URL do servidor:

  ```bash
  docker run -d -p 3000:8080 -e OLLAMA_BASE_URL=https://exemplo.com -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
  ```

- **Para executar o Open WebUI com suporte a GPU Nvidia**, use este comando:

  ```bash
  docker run -d -p 3000:8080 --gpus all --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:cuda
  ```

### Instalação apenas para uso da API OpenAI

- **Se for usar somente a API OpenAI**, use este comando:

  ```bash
  docker run -d -p 3000:8080 -e OPENAI_API_KEY=sua_chave_secreta -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
  ```

### Instalando Open WebUI com Ollama Integrado

Este método usa uma única imagem de container que inclui Open WebUI com Ollama, facilitando a configuração em um único comando. Escolha o comando de acordo com seu hardware:

- **Com Suporte a GPU**:
  Use recursos da GPU executando:

  ```bash
  docker run -d -p 3000:8080 --gpus=all -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama
  ```

- **Apenas CPU**:
  Se não usar GPU, use este comando:

  ```bash
  docker run -d -p 3000:8080 -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama
  ```

Ambos os comandos facilitam a instalação integrada do Open WebUI e Ollama, para que tudo funcione rapidamente.

Após a instalação, acesse o Open WebUI em [http://localhost:3000](http://localhost:3000). Aproveite! 😄

### Outros Métodos de Instalação

Oferecemos alternativas de instalação, incluindo métodos nativos (sem Docker), Docker Compose, Kustomize e Helm. Visite nossa [Documentação Open WebUI](https://docs.openwebui.com/getting-started/) ou entre em nossa [comunidade Discord](https://discord.gg/5rJgQTnV4s) para orientação completa.

### Solução de Problemas

Está com problemas de conexão? Nossa [Documentação Open WebUI](https://docs.openwebui.com/troubleshooting/) pode ajudar. Para mais suporte e para participar da comunidade, acesse o [Open WebUI Discord](https://discord.gg/5rJgQTnV4s).

#### Open WebUI: Erro de Conexão com o Servidor

Se está enfrentando problemas de conexão, geralmente é porque o container WebUI não consegue acessar o servidor Ollama em 127.0.0.1:11434 (host.docker.internal:11434) dentro do container. Use a flag `--network=host` no comando Docker para resolver. Note que a porta muda de 3000 para 8080, resultando em: `http://localhost:8080`.

**Exemplo de Comando Docker**:

```bash
docker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```

### Mantendo sua Instalação Docker Atualizada

Para atualizar sua instalação local Docker para a versão mais recente, use o [Watchtower](https://containrrr.dev/watchtower/):

```bash
docker run --rm --volume /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower --run-once open-webui
```

No final do comando, substitua `open-webui` pelo nome do seu container, se for diferente.

Veja nosso Guia de Atualização disponível na [Documentação Open WebUI](https://docs.openwebui.com/getting-started/updating).

### Usando a Branch Dev 🌙

> [!WARNING]
> A branch `:dev` contém os recursos e mudanças mais instáveis e recentes. Use por sua conta e risco, pois pode conter bugs ou recursos incompletos.

Se quiser experimentar recursos de ponta e aceitar instabilidade ocasional, use a tag `:dev` assim:

```bash
docker run -d -p 3000:8080 -v open-webui:/app/backend/data --name open-webui --add-host=host.docker.internal:host-gateway --restart always ghcr.io/open-webui/open-webui:dev
```

### Modo Offline

Se estiver usando o Open WebUI em ambiente offline, defina a variável de ambiente `HF_HUB_OFFLINE` para `1` para evitar tentativas de baixar modelos da internet.

```bash
export HF_HUB_OFFLINE=1
```

## O que Vem a Seguir? 🌟

Descubra recursos futuros no nosso roadmap na [Documentação Open WebUI](https://docs.openwebui.com/roadmap/).

## Licença 📜

Este projeto está licenciado sob a [Licença Open WebUI](LICENSE), uma licença BSD-3-Clause revisada. Você recebe os mesmos direitos da BSD-3 clássica: pode usar, modificar e distribuir o software, inclusive em produtos proprietários e comerciais, com restrições mínimas. O único requisito adicional é preservar a marca "Open WebUI", conforme detalhado no arquivo LICENSE. Para termos completos, consulte o [LICENSE](LICENSE). 📄

## Suporte 💬

Se tiver dúvidas, sugestões ou precisar de ajuda, abra uma issue ou entre em nossa
[comunidade Discord do Open WebUI](https://discord.gg/5rJgQTnV4s) para conversar conosco! 🤝

## Histórico de Estrelas

<a href="https://star-history.com/#open-webui/open-webui&Date">
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=open-webui/open-webui&type=Date&theme=dark" />
    <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=open-webui/open-webui&type=Date" />
    <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=open-webui/open-webui&type=Date" />
  </picture>
</a>

---

Criado por [Timothy Jaeryang Baek](https://github.com/tjbck) – Vamos tornar o Open WebUI ainda mais incrível juntos! 💪

---

[Powered By OpenAiTx](https://github.com/OpenAiTx/OpenAiTx)

---