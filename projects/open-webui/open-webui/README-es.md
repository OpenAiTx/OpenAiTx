# Open WebUI ğŸ‘‹

![Estrellas de GitHub](https://img.shields.io/github/stars/open-webui/open-webui?style=social)
![Forks de GitHub](https://img.shields.io/github/forks/open-webui/open-webui?style=social)
![Watchers de GitHub](https://img.shields.io/github/watchers/open-webui/open-webui?style=social)
![TamaÃ±o del repositorio en GitHub](https://img.shields.io/github/repo-size/open-webui/open-webui)
![Cantidad de lenguajes en GitHub](https://img.shields.io/github/languages/count/open-webui/open-webui)
![Lenguaje principal en GitHub](https://img.shields.io/github/languages/top/open-webui/open-webui)
![Ãšltimo commit en GitHub](https://img.shields.io/github/last-commit/open-webui/open-webui?color=red)
[![Discord](https://img.shields.io/badge/Discord-Open_WebUI-blue?logo=discord&logoColor=white)](https://discord.gg/5rJgQTnV4s)
[![](https://img.shields.io/static/v1?label=Sponsor&message=%E2%9D%A4&logo=GitHub&color=%23fe8e86)](https://github.com/sponsors/tjbck)

**Open WebUI es una plataforma de IA autoalojada, [extensible](https://docs.openwebui.com/features/plugin/), rica en funcionalidades y fÃ¡cil de usar, diseÃ±ada para operar completamente sin conexiÃ³n.** Soporta varios motores LLM como **Ollama** y **APIs compatibles con OpenAI**, con un **motor de inferencia integrado** para RAG, lo que lo convierte en una **potente soluciÃ³n de despliegue de IA**.

![DemostraciÃ³n de Open WebUI](./demo.gif)

> [!TIP]  
> **Â¿Buscas un [Plan Empresarial](https://docs.openwebui.com/enterprise)?** â€“ **[Â¡Habla hoy mismo con nuestro equipo de ventas!](mailto:sales@openwebui.com)**
>
> ObtÃ©n **capacidades mejoradas**, incluyendo **personalizaciÃ³n de temas y branding**, **soporte SLA (Acuerdo de Nivel de Servicio)**, **versiones con soporte a largo plazo (LTS)**, Â¡y mucho mÃ¡s!

Para mÃ¡s informaciÃ³n, asegÃºrate de consultar nuestra [DocumentaciÃ³n de Open WebUI](https://docs.openwebui.com/).

## Funcionalidades Clave de Open WebUI â­

- ğŸš€ **InstalaciÃ³n sin esfuerzo**: Instala fÃ¡cilmente usando Docker o Kubernetes (kubectl, kustomize o helm) para una experiencia libre de complicaciones, con soporte para imÃ¡genes etiquetadas `:ollama` y `:cuda`.

- ğŸ¤ **IntegraciÃ³n API Ollama/OpenAI**: Integra de manera sencilla APIs compatibles con OpenAI para conversaciones versÃ¡tiles junto con modelos Ollama. Personaliza la URL de la API de OpenAI para enlazar con **LMStudio, GroqCloud, Mistral, OpenRouter y mÃ¡s**.

- ğŸ›¡ï¸ **Permisos Granulares y Grupos de Usuarios**: Permite a los administradores crear roles y permisos detallados, asegurando un entorno seguro. Esta granularidad no solo mejora la seguridad, sino que tambiÃ©n permite experiencias personalizadas y fomenta el sentido de pertenencia y responsabilidad entre los usuarios.

- ğŸ“± **DiseÃ±o Responsivo**: Disfruta de una experiencia fluida en PC de escritorio, portÃ¡til y dispositivos mÃ³viles.

- ğŸ“± **AplicaciÃ³n Web Progresiva (PWA) para MÃ³vil**: Disfruta de una experiencia similar a una app nativa en tu dispositivo mÃ³vil con nuestra PWA, que permite acceso offline en localhost y una interfaz de usuario fluida.

- âœ’ï¸ğŸ”¢ **Soporte completo de Markdown y LaTeX**: Eleva tu experiencia LLM con capacidades completas de Markdown y LaTeX para una interacciÃ³n enriquecida.

- ğŸ¤ğŸ“¹ **Llamadas de Voz/Video Manos Libres**: Experimenta comunicaciÃ³n fluida con funciones integradas de llamadas de voz y video manos libres, permitiendo un entorno de chat mÃ¡s dinÃ¡mico e interactivo.

- ğŸ› ï¸ **Constructor de Modelos**: Crea fÃ¡cilmente modelos Ollama a travÃ©s de la interfaz web. Crea y aÃ±ade personajes/agentes personalizados, personaliza elementos del chat e importa modelos fÃ¡cilmente mediante la integraciÃ³n con la [Comunidad Open WebUI](https://openwebui.com/).

- ğŸ **Herramienta Nativa de Llamada de Funciones Python**: Mejora tus LLMs con soporte de editor de cÃ³digo en el Ã¡rea de herramientas. Trae tus propias funciones (BYOF) simplemente agregando funciones Python puras, permitiendo integraciÃ³n fluida con LLMs.

- ğŸ“š **IntegraciÃ³n RAG Local**: SumÃ©rgete en el futuro de las interacciones de chat con el revolucionario soporte de Retrieval Augmented Generation (RAG). Esta funciÃ³n integra sin problemas la interacciÃ³n con documentos en tu chat. Puedes cargar documentos directamente al chat o aÃ±adir archivos a tu biblioteca, accediendo fÃ¡cilmente a ellos usando el comando `#` antes de una consulta.

- ğŸ” **BÃºsqueda Web para RAG**: Realiza bÃºsquedas web usando proveedores como `SearXNG`, `Google PSE`, `Brave Search`, `serpstack`, `serper`, `Serply`, `DuckDuckGo`, `TavilySearch`, `SearchApi` y `Bing` e inyecta los resultados directamente en tu experiencia de chat.

- ğŸŒ **Capacidad de NavegaciÃ³n Web**: Integra sitios web en tu chat usando el comando `#` seguido de una URL. Esta funciÃ³n permite incorporar contenido web directamente en tus conversaciones, enriqueciendo la interacciÃ³n.

- ğŸ¨ **IntegraciÃ³n de GeneraciÃ³n de ImÃ¡genes**: Incorpora capacidades de generaciÃ³n de imÃ¡genes utilizando opciones como la API AUTOMATIC1111 o ComfyUI (local), y DALL-E de OpenAI (externo), enriqueciendo tu chat con contenido visual dinÃ¡mico.

- âš™ï¸ **Conversaciones con MÃºltiples Modelos**: InteractÃºa fÃ¡cilmente con varios modelos simultÃ¡neamente, aprovechando sus puntos fuertes para respuestas Ã³ptimas. Mejora tu experiencia utilizando una variedad de modelos en paralelo.

- ğŸ” **Control de Acceso Basado en Roles (RBAC)**: Garantiza acceso seguro con permisos restringidos; solo personas autorizadas pueden acceder a tu Ollama y los derechos exclusivos de creaciÃ³n/descarga de modelos estÃ¡n reservados para administradores.

- ğŸŒğŸŒ **Soporte MultilingÃ¼e**: Experimenta Open WebUI en tu idioma preferido con soporte de internacionalizaciÃ³n (i18n). Â¡Ãšnete a expandir los idiomas soportados! Â¡Buscamos activamente colaboradores!

- ğŸ§© **Pipelines y Soporte para Plugins de Open WebUI**: Integra lÃ³gica personalizada y librerÃ­as Python en Open WebUI usando el [Framework de Plugins Pipelines](https://github.com/open-webui/pipelines). Lanza tu instancia de Pipelines, configura la URL de OpenAI a la de Pipelines y explora posibilidades infinitas. [Ejemplos](https://github.com/open-webui/pipelines/tree/main/examples) incluyen **Llamada de Funciones**, **LÃ­mites de Usuario** para controlar acceso, **Monitoreo de Uso** con herramientas como Langfuse, **TraducciÃ³n en Vivo con LibreTranslate** para soporte multilingÃ¼e, **Filtrado de Mensajes TÃ³xicos** y mucho mÃ¡s.

- ğŸŒŸ **Actualizaciones Continuas**: Estamos comprometidos a mejorar Open WebUI con actualizaciones, correcciones y nuevas funciones de manera regular.

Â¿Quieres saber mÃ¡s sobre las funciones de Open WebUI? Consulta nuestra [documentaciÃ³n](https://docs.openwebui.com/features) para una visiÃ³n completa.

## Patrocinadores ğŸ™Œ

#### Esmeralda

<table>
  <tr>
    <td>
      <a href="https://n8n.io/" target="_blank">
        <img src="https://docs.openwebui.com/sponsors/logos/n8n.png" alt="n8n" style="width: 8rem; height: 8rem; border-radius: .75rem;" />
      </a>
    </td>
    <td>
      N8N â€¢ Â¿Tu interfaz ya tiene backend?<br>Prueba <a href="https://n8n.io/">n8n</a>
    </td>
  </tr>
  <tr>
    <td>
      <a href="https://warp.dev/open-webui" target="_blank">
        <img src="https://docs.openwebui.com/sponsors/logos/warp.png" alt="n8n" style="width: 8rem; height: 8rem; border-radius: .75rem;" />
      </a>
    </td>
    <td>
      <a href="https://warp.dev/open-webui">Warp</a> â€¢ La terminal inteligente para desarrolladores
    </td>
  </tr>
</table>

---

Estamos increÃ­blemente agradecidos por el generoso apoyo de nuestros patrocinadores. Sus contribuciones nos ayudan a mantener y mejorar nuestro proyecto, asegurando que podamos seguir ofreciendo trabajo de calidad a nuestra comunidad. Â¡Gracias!

## CÃ³mo Instalar ğŸš€

### InstalaciÃ³n vÃ­a Python pip ğŸ

Open WebUI puede instalarse usando pip, el instalador de paquetes de Python. Antes de proceder, asegÃºrate de estar usando **Python 3.11** para evitar problemas de compatibilidad.

1. **Instala Open WebUI**:
   Abre tu terminal y ejecuta el siguiente comando para instalar Open WebUI:

   ```bash
   pip install open-webui
   ```

2. **Ejecutar Open WebUI**:
   DespuÃ©s de la instalaciÃ³n, puedes iniciar Open WebUI ejecutando:

   ```bash
   open-webui serve
   ```

Esto iniciarÃ¡ el servidor de Open WebUI, al que puedes acceder en [http://localhost:8080](http://localhost:8080)

### Inicio RÃ¡pido con Docker ğŸ³

> [!NOTE]  
> Ten en cuenta que en ciertos entornos Docker pueden requerirse configuraciones adicionales. Si experimentas problemas de conexiÃ³n, nuestra guÃ­a detallada en la [DocumentaciÃ³n de Open WebUI](https://docs.openwebui.com/) estÃ¡ lista para ayudarte.

> [!WARNING]
> Al usar Docker para instalar Open WebUI, asegÃºrate de incluir `-v open-webui:/app/backend/data` en tu comando de Docker. Este paso es crucial para que tu base de datos se monte correctamente y evitar la pÃ©rdida de datos.

> [!TIP]  
> Si deseas utilizar Open WebUI con Ollama incluido o aceleraciÃ³n CUDA, recomendamos usar nuestras imÃ¡genes oficiales etiquetadas con `:cuda` o `:ollama`. Para habilitar CUDA, debes instalar el [Nvidia CUDA container toolkit](https://docs.nvidia.com/dgx/nvidia-container-runtime-upgrade/) en tu sistema Linux/WSL.

### InstalaciÃ³n con ConfiguraciÃ³n Predeterminada

- **Si Ollama estÃ¡ en tu computadora**, usa este comando:

  ```bash
  docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
  ```

- **Si Ollama estÃ¡ en otro servidor**, usa este comando:

  Para conectar a Ollama en otro servidor, cambia el `OLLAMA_BASE_URL` por la URL del servidor:

  ```bash
  docker run -d -p 3000:8080 -e OLLAMA_BASE_URL=https://example.com -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
  ```

- **Para ejecutar Open WebUI con soporte Nvidia GPU**, usa este comando:

  ```bash
  docker run -d -p 3000:8080 --gpus all --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:cuda
  ```

### InstalaciÃ³n para Uso Solo con OpenAI API

- **Si solo vas a usar la API de OpenAI**, usa este comando:

  ```bash
  docker run -d -p 3000:8080 -e OPENAI_API_KEY=your_secret_key -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
  ```

### InstalaciÃ³n de Open WebUI con Soporte Ollama Integrado

Este mÃ©todo usa una sola imagen de contenedor que integra Open WebUI con Ollama, permitiendo una configuraciÃ³n sencilla en un solo comando. Elige el comando adecuado segÃºn tu hardware:

- **Con Soporte GPU**:
  Utiliza recursos de GPU ejecutando el siguiente comando:

  ```bash
  docker run -d -p 3000:8080 --gpus=all -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama
  ```

- **Solo CPU**:
  Si no usas GPU, usa este comando:

  ```bash
  docker run -d -p 3000:8080 -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama
  ```

Ambos comandos facilitan una instalaciÃ³n integrada y sin complicaciones tanto de Open WebUI como de Ollama, asegurando que puedas poner todo en marcha rÃ¡pidamente.

Tras la instalaciÃ³n, puedes acceder a Open WebUI en [http://localhost:3000](http://localhost:3000). Â¡DisfrÃºtalo! ğŸ˜„

### Otros MÃ©todos de InstalaciÃ³n

Ofrecemos varias alternativas de instalaciÃ³n, incluyendo mÃ©todos nativos sin Docker, Docker Compose, Kustomize y Helm. Visita nuestra [DocumentaciÃ³n de Open WebUI](https://docs.openwebui.com/getting-started/) o Ãºnete a nuestra [comunidad en Discord](https://discord.gg/5rJgQTnV4s) para una guÃ­a completa.

### SoluciÃ³n de Problemas

Â¿Tienes problemas de conexiÃ³n? Nuestra [DocumentaciÃ³n de Open WebUI](https://docs.openwebui.com/troubleshooting/) te puede ayudar. Para mÃ¡s asistencia y para unirte a nuestra comunidad, visita el [Discord de Open WebUI](https://discord.gg/5rJgQTnV4s).

#### Open WebUI: Error de ConexiÃ³n al Servidor

Si experimentas problemas de conexiÃ³n, a menudo es porque el contenedor Docker de WebUI no puede alcanzar el servidor Ollama en 127.0.0.1:11434 (host.docker.internal:11434) dentro del contenedor. Usa el flag `--network=host` en tu comando Docker para resolverlo. Nota que el puerto cambia de 3000 a 8080, resultando en el enlace: `http://localhost:8080`.

**Ejemplo de Comando Docker**:

```bash
docker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```

### Manteniendo Tu InstalaciÃ³n Docker Actualizada

Si deseas actualizar tu instalaciÃ³n local de Docker a la Ãºltima versiÃ³n, puedes hacerlo con [Watchtower](https://containrrr.dev/watchtower/):

```bash
docker run --rm --volume /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower --run-once open-webui
```

En la Ãºltima parte del comando, reemplaza `open-webui` por el nombre de tu contenedor si es diferente.

Consulta nuestra guÃ­a de actualizaciÃ³n disponible en la [DocumentaciÃ³n de Open WebUI](https://docs.openwebui.com/getting-started/updating).

### Usando la Rama Dev ğŸŒ™

> [!WARNING]
> La rama `:dev` contiene las funciones y cambios mÃ¡s recientes e inestables. Ãšsala bajo tu propio riesgo, ya que puede tener errores o funciones incompletas.

Si quieres probar las funciones mÃ¡s novedosas y no te importa cierta inestabilidad, puedes usar la etiqueta `:dev` asÃ­:

```bash
docker run -d -p 3000:8080 -v open-webui:/app/backend/data --name open-webui --add-host=host.docker.internal:host-gateway --restart always ghcr.io/open-webui/open-webui:dev
```

### Modo Offline

Si ejecutas Open WebUI en un entorno sin conexiÃ³n, puedes establecer la variable de entorno `HF_HUB_OFFLINE` en `1` para evitar intentos de descarga de modelos de internet.

```bash
export HF_HUB_OFFLINE=1
```

## Â¿QuÃ© sigue? ğŸŒŸ

Descubre las prÃ³ximas funciones en nuestra hoja de ruta en la [DocumentaciÃ³n de Open WebUI](https://docs.openwebui.com/roadmap/).

## Licencia ğŸ“œ

Este proyecto estÃ¡ licenciado bajo la [Licencia Open WebUI](LICENSE), una licencia BSD-3-Clause revisada. Recibes los mismos derechos que la clÃ¡sica licencia BSD-3: puedes usar, modificar y distribuir el software, incluso en productos propietarios y comerciales, con mÃ­nimas restricciones. El Ãºnico requisito adicional es preservar el branding "Open WebUI", tal como se detalla en el archivo LICENSE. Para los tÃ©rminos completos, consulta el documento [LICENSE](LICENSE). ğŸ“„

## Soporte ğŸ’¬

Si tienes preguntas, sugerencias o necesitas ayuda, abre un issue o Ãºnete a nuestra
[comunidad de Open WebUI en Discord](https://discord.gg/5rJgQTnV4s) para conectar con nosotros. ğŸ¤

## Historial de Estrellas

<a href="https://star-history.com/#open-webui/open-webui&Date">
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=open-webui/open-webui&type=Date&theme=dark" />
    <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=open-webui/open-webui&type=Date" />
    <img alt="GrÃ¡fico de Historial de Estrellas" src="https://api.star-history.com/svg?repos=open-webui/open-webui&type=Date" />
  </picture>
</a>

---

Creado por [Timothy Jaeryang Baek](https://github.com/tjbck) - Â¡Hagamos Open WebUI aÃºn mÃ¡s increÃ­ble juntos! ğŸ’ª

---

[Powered By OpenAiTx](https://github.com/OpenAiTx/OpenAiTx)

---