# Open WebUI ğŸ‘‹

![GitHub stars](https://img.shields.io/github/stars/open-webui/open-webui?style=social)
![GitHub forks](https://img.shields.io/github/forks/open-webui/open-webui?style=social)
![GitHub watchers](https://img.shields.io/github/watchers/open-webui/open-webui?style=social)
![GitHub repo size](https://img.shields.io/github/repo-size/open-webui/open-webui)
![GitHub language count](https://img.shields.io/github/languages/count/open-webui/open-webui)
![GitHub top language](https://img.shields.io/github/languages/top/open-webui/open-webui)
![GitHub last commit](https://img.shields.io/github/last-commit/open-webui/open-webui?color=red)
[![Discord](https://img.shields.io/badge/Discord-Open_WebUI-blue?logo=discord&logoColor=white)](https://discord.gg/5rJgQTnV4s)
[![](https://img.shields.io/static/v1?label=Sponsor&message=%E2%9D%A4&logo=GitHub&color=%23fe8e86)](https://github.com/sponsors/tjbck)

**Open WebUI ist eine [erweiterbare](https://docs.openwebui.com/features/plugin/), funktionsreiche und benutzerfreundliche selbst gehostete KI-Plattform, die komplett offline betrieben werden kann.** Sie unterstÃ¼tzt verschiedene LLM-Runners wie **Ollama** und **OpenAI-kompatible APIs** sowie eine **integrierte Inferenz-Engine** fÃ¼r RAG und ist somit eine **leistungsstarke KI-BereitstellungslÃ¶sung**.

![Open WebUI Demo](./demo.gif)

> [!TIP]  
> **Sie suchen einen [Enterprise-Plan](https://docs.openwebui.com/enterprise)?** â€“ **[Sprechen Sie noch heute mit unserem Vertrieb!](mailto:sales@openwebui.com)**
>
> Erhalten Sie **erweiterte Funktionen**, einschlieÃŸlich **individuellem Theming und Branding**, **Support fÃ¼r Service Level Agreements (SLA)**, **Long-Term Support (LTS) Versionen** und **mehr!**

FÃ¼r weitere Informationen besuchen Sie unsere [Open WebUI Dokumentation](https://docs.openwebui.com/).

## Hauptfunktionen von Open WebUI â­

- ğŸš€ **Einfache Einrichtung**: Problemlos mit Docker oder Kubernetes (kubectl, kustomize oder helm) installieren â€“ fÃ¼r eine stressfreie Erfahrung mit UnterstÃ¼tzung fÃ¼r sowohl `:ollama` als auch `:cuda` getaggte Images.

- ğŸ¤ **Ollama/OpenAI API-Integration**: Integrieren Sie OpenAI-kompatible APIs ganz einfach fÃ¼r vielseitige Konversationen neben Ollama-Modellen. Passen Sie die OpenAI API-URL an, um sich mit **LMStudio, GroqCloud, Mistral, OpenRouter und mehr** zu verbinden.

- ğŸ›¡ï¸ **Granulare Berechtigungen und Benutzergruppen**: Administratoren kÃ¶nnen detaillierte Benutzerrollen und -berechtigungen erstellen, um eine sichere Benutzerumgebung zu gewÃ¤hrleisten. Diese GranularitÃ¤t erhÃ¶ht nicht nur die Sicherheit, sondern ermÃ¶glicht auch individuelle Benutzererlebnisse und fÃ¶rdert Verantwortung und Identifikation der Nutzer.

- ğŸ“± **Responsives Design**: Ein durchgÃ¤ngiges Erlebnis auf Desktop-PC, Laptop und MobilgerÃ¤ten genieÃŸen.

- ğŸ“± **Progressive Web App (PWA) fÃ¼r MobilgerÃ¤te**: Erleben Sie eine native App-Ã¤hnliche Erfahrung auf Ihrem MobilgerÃ¤t mit unserer PWA, die Offline-Zugriff auf localhost und eine nahtlose BenutzeroberflÃ¤che bietet.

- âœ’ï¸ğŸ”¢ **VollstÃ¤ndige Markdown- und LaTeX-UnterstÃ¼tzung**: Verbessern Sie Ihr LLM-Erlebnis mit umfassender Markdown- und LaTeX-FunktionalitÃ¤t fÃ¼r eine bereicherte Interaktion.

- ğŸ¤ğŸ“¹ **FreihÃ¤ndiges Sprach-/Videoanruf-Feature**: Erleben Sie nahtlose Kommunikation mit integrierten freihÃ¤ndigen Sprach- und Videoanruffunktionen fÃ¼r eine dynamische und interaktive Chat-Umgebung.

- ğŸ› ï¸ **Model Builder**: Erstellen Sie Ollama-Modelle ganz einfach Ã¼ber die Web-OberflÃ¤che. Erstellen und fÃ¼gen Sie benutzerdefinierte Charaktere/Agenten hinzu, passen Sie Chat-Elemente an und importieren Sie Modelle mÃ¼helos durch die [Open WebUI Community](https://openwebui.com/)-Integration.

- ğŸ **Native Python Function Calling Tool**: Verbessern Sie Ihre LLMs mit integriertem Code-Editor-Support im Tools-Arbeitsbereich. Bringen Sie Ihre eigenen Python-Funktionen ein (BYOF), indem Sie einfach reine Python-Funktionen hinzufÃ¼gen und so eine nahtlose Integration mit LLMs ermÃ¶glichen.

- ğŸ“š **Lokale RAG-Integration**: Entdecken Sie die Zukunft der Chat-Interaktionen mit bahnbrechender Retrieval Augmented Generation (RAG)-UnterstÃ¼tzung. Diese Funktion integriert Dokumenteninteraktionen nahtlos in Ihre Chat-Erfahrung. Sie kÃ¶nnen Dokumente direkt in den Chat laden oder Dateien zu Ihrer Dokumentenbibliothek hinzufÃ¼gen und sie mit dem `#`-Befehl vor einer Anfrage aufrufen.

- ğŸ” **Websuche fÃ¼r RAG**: FÃ¼hren Sie Websuchen mit Anbietern wie `SearXNG`, `Google PSE`, `Brave Search`, `serpstack`, `serper`, `Serply`, `DuckDuckGo`, `TavilySearch`, `SearchApi` und `Bing` durch und binden Sie die Ergebnisse direkt in Ihre Chat-Erfahrung ein.

- ğŸŒ **Web-Browsing-FunktionalitÃ¤t**: Integrieren Sie Webseiten mithilfe des `#`-Befehls gefolgt von einer URL direkt in Ihren Chat. So kÃ¶nnen Sie Webinhalte direkt in Ihre Konversationen einbinden und die Interaktionen bereichern.

- ğŸ¨ **Bildgenerierung-Integration**: Integrieren Sie Bildgenerierungsfunktionen nahtlos mit Optionen wie AUTOMATIC1111 API oder ComfyUI (lokal) sowie OpenAIs DALL-E (extern) und bereichern Sie Ihr Chat-Erlebnis mit dynamischen visuellen Inhalten.

- âš™ï¸ **Multi-Model-Konversationen**: Kommunizieren Sie mÃ¼helos gleichzeitig mit verschiedenen Modellen und nutzen Sie deren individuelle StÃ¤rken fÃ¼r optimale Antworten. Steigern Sie Ihr Erlebnis durch die parallele Nutzung verschiedener Modelle.

- ğŸ” **Rollenbasierte Zugriffskontrolle (RBAC)**: Sorgen Sie fÃ¼r sicheren Zugriff mit eingeschrÃ¤nkten Berechtigungen; nur autorisierte Personen kÃ¶nnen auf Ihr Ollama zugreifen, und das Erstellen/Herunterladen von Modellen ist ausschlieÃŸlich Administratoren vorbehalten.

- ğŸŒğŸŒ **Mehrsprachige UnterstÃ¼tzung**: Nutzen Sie Open WebUI in Ihrer bevorzugten Sprache dank Internationalisierung (i18n). Helfen Sie uns, die unterstÃ¼tzten Sprachen zu erweitern â€“ wir suchen aktiv nach Mitwirkenden!

- ğŸ§© **Pipelines, Open WebUI Plugin-UnterstÃ¼tzung**: Integrieren Sie benutzerdefinierte Logik und Python-Bibliotheken nahtlos in Open WebUI mit dem [Pipelines Plugin Framework](https://github.com/open-webui/pipelines). Starten Sie Ihre Pipelines-Instanz, setzen Sie die OpenAI-URL auf die Pipelines-URL und entdecken Sie unendliche MÃ¶glichkeiten. [Beispiele](https://github.com/open-webui/pipelines/tree/main/examples) umfassen **Function Calling**, Benutzer-**Rate Limiting** zur Zugangskontrolle, **Usage Monitoring** mit Tools wie Langfuse, **Live-Ãœbersetzung mit LibreTranslate** fÃ¼r Mehrsprachigkeit, **Filterung toxischer Nachrichten** und vieles mehr.

- ğŸŒŸ **Kontinuierliche Updates**: Wir verpflichten uns, Open WebUI durch regelmÃ¤ÃŸige Updates, Fehlerbehebungen und neue Funktionen stÃ¤ndig zu verbessern.

MÃ¶chten Sie mehr Ã¼ber die Funktionen von Open WebUI erfahren? Werfen Sie einen Blick auf unsere [Open WebUI Dokumentation](https://docs.openwebui.com/features) fÃ¼r einen umfassenden Ãœberblick!

## Sponsoren ğŸ™Œ

#### Smaragd

<table>
  <tr>
    <td>
      <a href="https://n8n.io/" target="_blank">
        <img src="https://docs.openwebui.com/sponsors/logos/n8n.png" alt="n8n" style="width: 8rem; height: 8rem; border-radius: .75rem;" />
      </a>
    </td>
    <td>
      N8N â€¢ Hat Ihre Schnittstelle schon ein Backend?<br>Probieren Sie <a href="https://n8n.io/">n8n</a>
    </td>
  </tr>
  <tr>
    <td>
      <a href="https://warp.dev/open-webui" target="_blank">
        <img src="https://docs.openwebui.com/sponsors/logos/warp.png" alt="n8n" style="width: 8rem; height: 8rem; border-radius: .75rem;" />
      </a>
    </td>
    <td>
      <a href="https://warp.dev/open-webui">Warp</a> â€¢ Das intelligente Terminal fÃ¼r Entwickler
    </td>
  </tr>
</table>

---

Wir sind unseren groÃŸzÃ¼gigen Sponsoren unglaublich dankbar. Ihre UnterstÃ¼tzung hilft uns, unser Projekt zu pflegen und zu verbessern, sodass wir weiterhin qualitativ hochwertige Arbeit fÃ¼r unsere Community liefern kÃ¶nnen. Vielen Dank!

## Installation ğŸš€

### Installation Ã¼ber Python pip ğŸ

Open WebUI kann mit pip, dem Python-Paketmanager, installiert werden. Stellen Sie vorab sicher, dass Sie **Python 3.11** verwenden, um KompatibilitÃ¤tsprobleme zu vermeiden.

1. **Open WebUI installieren**:
   Ã–ffnen Sie Ihr Terminal und fÃ¼hren Sie folgenden Befehl aus, um Open WebUI zu installieren:

   ```bash
   pip install open-webui
   ```

2. **Open WebUI starten**:
   Nach der Installation kÃ¶nnen Sie Open WebUI mit folgendem Befehl starten:

   ```bash
   open-webui serve
   ```

Dadurch wird der Open WebUI Server gestartet, den Sie unter [http://localhost:8080](http://localhost:8080) erreichen kÃ¶nnen.

### Schnellstart mit Docker ğŸ³

> [!NOTE]  
> Bitte beachten Sie, dass je nach Docker-Umgebung zusÃ¤tzliche Konfigurationen erforderlich sein kÃ¶nnen. Bei Verbindungsproblemen hilft unsere ausfÃ¼hrliche Anleitung in der [Open WebUI Dokumentation](https://docs.openwebui.com/) weiter.

> [!WARNING]
> Bei der Docker-Installation von Open WebUI achten Sie darauf, den Parameter `-v open-webui:/app/backend/data` in Ihren Docker-Befehl aufzunehmen. Dieser Schritt ist entscheidend, damit Ihre Datenbank korrekt eingebunden wird und kein Datenverlust entsteht.

> [!TIP]  
> Wenn Sie Open WebUI mit Ollama-UnterstÃ¼tzung oder CUDA-Beschleunigung nutzen mÃ¶chten, empfehlen wir unsere offiziellen Images mit dem Tag `:cuda` oder `:ollama`. Um CUDA zu aktivieren, installieren Sie das [Nvidia CUDA Container Toolkit](https://docs.nvidia.com/dgx/nvidia-container-runtime-upgrade/) auf Ihrem Linux/WSL-System.

### Installation mit Standardkonfiguration

- **Wenn Ollama auf Ihrem Computer lÃ¤uft**, verwenden Sie diesen Befehl:

  ```bash
  docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
  ```

- **Wenn Ollama auf einem anderen Server lÃ¤uft**, verwenden Sie diesen Befehl:

  Um eine Verbindung zu Ollama auf einem anderen Server herzustellen, Ã¤ndern Sie die `OLLAMA_BASE_URL` auf die Server-URL:

  ```bash
  docker run -d -p 3000:8080 -e OLLAMA_BASE_URL=https://example.com -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
  ```

- **Um Open WebUI mit Nvidia-GPU-UnterstÃ¼tzung zu betreiben**, verwenden Sie diesen Befehl:

  ```bash
  docker run -d -p 3000:8080 --gpus all --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:cuda
  ```

### Installation fÃ¼r die reine Nutzung der OpenAI API

- **Wenn Sie ausschlieÃŸlich die OpenAI API verwenden**, nutzen Sie diesen Befehl:

  ```bash
  docker run -d -p 3000:8080 -e OPENAI_API_KEY=your_secret_key -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
  ```

### Open WebUI mit integriertem Ollama-Support installieren

Diese Installationsmethode nutzt ein einzelnes Container-Image, das Open WebUI mit Ollama bÃ¼ndelt und so eine einfache Einrichtung mit nur einem Befehl ermÃ¶glicht. WÃ¤hlen Sie den passenden Befehl entsprechend Ihrer Hardware:

- **Mit GPU-UnterstÃ¼tzung**:
  Nutzen Sie Ihre GPU mit diesem Befehl:

  ```bash
  docker run -d -p 3000:8080 --gpus=all -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama
  ```

- **Nur CPU**:
  Wenn Sie keine GPU verwenden, nutzen Sie diesen Befehl:

  ```bash
  docker run -d -p 3000:8080 -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama
  ```

Beide Befehle ermÃ¶glichen eine integrierte und unkomplizierte Installation von Open WebUI und Ollama, sodass Sie sofort loslegen kÃ¶nnen.

Nach der Installation erreichen Sie Open WebUI unter [http://localhost:3000](http://localhost:3000). Viel SpaÃŸ! ğŸ˜„

### Weitere Installationsmethoden

Wir bieten verschiedene Alternativen an, darunter eine native Installation ohne Docker, Docker Compose, Kustomize und Helm. Besuchen Sie unsere [Open WebUI Dokumentation](https://docs.openwebui.com/getting-started/) oder treten Sie unserer [Discord-Community](https://discord.gg/5rJgQTnV4s) bei, um eine ausfÃ¼hrliche Anleitung zu erhalten.

### Fehlerbehebung

Haben Sie Verbindungsprobleme? Unsere [Open WebUI Dokumentation](https://docs.openwebui.com/troubleshooting/) hilft Ihnen weiter. FÃ¼r zusÃ¤tzliche UnterstÃ¼tzung und um Teil unserer lebendigen Community zu werden, besuchen Sie den [Open WebUI Discord](https://discord.gg/5rJgQTnV4s).

#### Open WebUI: Serververbindungsfehler

Wenn Sie Verbindungsprobleme haben, liegt es oft daran, dass der WebUI-Docker-Container den Ollama-Server unter 127.0.0.1:11434 (host.docker.internal:11434) im Container nicht erreichen kann. Verwenden Sie das `--network=host` Flag im Docker-Befehl, um das Problem zu beheben. Beachten Sie, dass sich der Port von 3000 auf 8080 Ã¤ndert, was zum Link `http://localhost:8080` fÃ¼hrt.

**Beispiel Docker-Befehl**:

```bash
docker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```

### Docker-Installation aktuell halten

Um Ihre lokale Docker-Installation auf die neueste Version zu aktualisieren, kÃ¶nnen Sie [Watchtower](https://containrrr.dev/watchtower/) verwenden:

```bash
docker run --rm --volume /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower --run-once open-webui
```

Ersetzen Sie im letzten Teil des Befehls `open-webui` durch den Namen Ihres Containers, falls dieser abweicht.

Siehe unsere Aktualisierungsanleitung in der [Open WebUI Dokumentation](https://docs.openwebui.com/getting-started/updating).

### Nutzung des Dev-Branches ğŸŒ™

> [!WARNING]
> Der `:dev`-Branch enthÃ¤lt die neuesten, aber noch instabilen Funktionen und Ã„nderungen. Nutzung auf eigenes Risiko, da Bugs oder unvollstÃ¤ndige Features auftreten kÃ¶nnen.

Wenn Sie die neuesten experimentellen Funktionen testen mÃ¶chten und gelegentliche InstabilitÃ¤ten akzeptieren, nutzen Sie das `:dev`-Tag wie folgt:

```bash
docker run -d -p 3000:8080 -v open-webui:/app/backend/data --name open-webui --add-host=host.docker.internal:host-gateway --restart always ghcr.io/open-webui/open-webui:dev
```

### Offline-Modus

Wenn Sie Open WebUI in einer Offline-Umgebung betreiben, setzen Sie die Umgebungsvariable `HF_HUB_OFFLINE` auf `1`, um das Herunterladen von Modellen aus dem Internet zu verhindern.

```bash
export HF_HUB_OFFLINE=1
```

## Was kommt als NÃ¤chstes? ğŸŒŸ

Entdecken Sie kommende Funktionen auf unserer Roadmap in der [Open WebUI Dokumentation](https://docs.openwebui.com/roadmap/).

## Lizenz ğŸ“œ

Dieses Projekt ist unter der [Open WebUI License](LICENSE), einer Ã¼berarbeiteten BSD-3-Clause-Lizenz, lizenziert. Sie erhalten die gleichen Rechte wie bei der klassischen BSD-3-Lizenz: Sie kÃ¶nnen die Software nutzen, modifizieren und weiterverbreiten, auch in proprietÃ¤ren und kommerziellen Produkten, mit minimalen EinschrÃ¤nkungen. Die einzige zusÃ¤tzliche Anforderung ist die Beibehaltung des "Open WebUI"-Brandings, wie in der LICENSE-Datei beschrieben. Die vollstÃ¤ndigen Bedingungen finden Sie im [LICENSE](LICENSE)-Dokument. ğŸ“„

## Support ğŸ’¬

Bei Fragen, Anregungen oder wenn Sie Hilfe benÃ¶tigen, erÃ¶ffnen Sie bitte ein Issue oder treten Sie unserer
[Open WebUI Discord Community](https://discord.gg/5rJgQTnV4s) bei, um mit uns in Kontakt zu treten! ğŸ¤

## Star History

<a href="https://star-history.com/#open-webui/open-webui&Date">
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=open-webui/open-webui&type=Date&theme=dark" />
    <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=open-webui/open-webui&type=Date" />
    <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=open-webui/open-webui&type=Date" />
  </picture>
</a>

---

Erstellt von [Timothy Jaeryang Baek](https://github.com/tjbck) â€“ Lassen Sie uns Open WebUI gemeinsam noch besser machen! ğŸ’ª

---

[Powered By OpenAiTx](https://github.com/OpenAiTx/OpenAiTx)

---